[{"title":"Introducing Argument Bundles","type":0,"sectionRef":"#","url":"/blog/arg-bungle-intro","content":"There are many cases where stored procedures require complex arguments using data shapes well known to higher level languages or that come from the schema. There is already some affordance for this sort of thing in the form of this kind of pattern: (I'll continue to use this simple example as I discuss the generalization below) create table Person ( id text primary key, name text not null, address text not null, birthday real ); Then maybe something like this create proc insert_person(like Person) begin insert into Person from arguments; end; The above expands into: create proc insert_person( id_ text not null, name_ text not null, address_ text not null, birthday_ real) begin insert into Person(id, name, address, birthday) values(id_, name_, address_, birthday_); end; And I think we can all agree the sugared version is a lot easier to reason about and much less prone to errors as well. Those features have been in the language for a long time and that's all fine and well but it isn't general enough to handle the usual mix of situations. For instance what if you need a procedure that works with two people? A hypothetical insert_two_peopleprocedure cannot be written with the old form. This is where argument bundles come in. The idea here is to name the bundle which provides useful reference. To wit: create proc insert_two_people(p1 like Person, p2 like Person) begin call insert_person(from p1); call insert_person(from p2); end; or alternatively create proc insert_two_people(p1 like Person, p2 like Person) begin insert into Person from p1; insert into Person from p2; end; So what's going on here? Well, there are lots of reasons to keep the API to procedures simple and adding general purpose structured types would be at odds with that. It would require lots of knowledge about C structure layout and whatnot. And trying to call from java would require very complex JNI for any such procedure. So we avoid all that. We keep simple arguments. The above expands into: create proc insert_person( p1_id text not null, p1_name text not null, p1_address text not null, p1_birthday real, p2_id text not null, p2_name text not null, p2_address text not null, p2_birthday real) begin insert into Person(id, name, address, birthday) values(p1_id, p1_name, p1_address, p1_birthday); insert into Person(id, name, address, birthday) values(p2_id, p2_name, p2_address, p2_birthday); end; Or course the types don't have to be the same, you can create and name shapes of your choice. The language allow you to use an argument bundle in all the places that a cursor was previously a valid source. That includes insert,fetch, update cursor, and procedure calls. You can refer to the arguments by their expanded name p1_addressor alternatively p1.address means the same thing. Here's another example showing a silly but illustrative thing you could do: create proc insert_lotsa_people(P like Person) begin declare C cursor like P; fetch C from P; declare i integer not null; set i := 0; while (i &lt; 20) begin update cursor C using printf(&quot;id_%d&quot;, i) id; insert into Person from C; end; end; The above shows that you can use a bundle as the source of a shape elsewhere, and you can use a bundle as a source of data to load a cursor. After which you can do all the usual value cursor things like out statements and so forth. In order to call procedures with argument bundles more readily from other languages, the JSON output now includes additional information about where procedure arguments originated; The field with this information is creatively called &quot;argOrigin:&quot; and it has 3 forms. &quot;arg_name&quot; -&gt; the argument is not an expansion of anything&quot;T arg_name&quot; -&gt; the argument came from like T there will be one arg for each member of Tthe formal argument name for this arg will be argnameif T is procedure arguments like p1 arguments then you'll get &quot;p1[arguments] arg_name&quot; &quot;name T arg_name&quot; -&gt; the argument came from name like T (a named bundle) there will be one arg for each member of Tthe formal argument name for this arg will be T_arg_nameT could be procedure arguments as above If the source of an argument was a cursor or argument bundle name you get instead that thing's shape source name this is always better because cursor names and bundle names are not globally unique. If the cursor had an anonymous source (e.g. like select 1 x) then you get the useless shape name &quot;select&quot; this is an indicator that you should make some ad hoc struct for this procedure because there is no useful name for the arg bundle's type None of this matters unless you're trying to make wrappers for a CQL procedure for some other language and you'd like to have your wrapper deal with structs rather than all loose arguments. the JSON basically tells you the structs. Interestingly, argument bundles resulted in a significant reduction of code in the compiler. The argument bundle name has to be usable in the contexts where a cursor was previously usable. It is another source of shaped data. Getting that to work proved to be super simple as the two forms look almost identical to the compiler -- no coincidence there. So very little code was required to make from [cursor_name] work with from [any_shape_name] in the half dozen or so places that this construct is allowed (e.g. procedure call arguments, insert statements, etc.). However, there was as much code associated with from arguments as there was from cursor_name. And the code was nearly identical.. When argument bundles were introduced the natural thing to do was to create an artifical bundle called &quot;arguments&quot; which represents the bundle that is ALL the arguments. With that done, all the code for from arguments could be deleted because arguments itself was a valid shape name. Hence insert into T from arguments &quot;just works&quot;. And so half the rewrites were deleted. The only cost was that the form from arguments like shape became the cursor formfrom arguments(like shape) which only adds mandatory parens to a form that was largely unused anyway (there were two cases in our entire codebase). The cursor form is more general as you can do from C(like A, like B) to get the fields that match A then those that match B. Arguments get this for free as well (well, at the cost of parens). So overall, this feature was added, and the compiler got smaller and cleaner. Only the test suite had to grow. Stay safe out there.","keywords":""},{"title":"Introducing Blob Storage","type":0,"sectionRef":"#","url":"/blog/blob-storage","content":"","keywords":""},{"title":"Introduction and Context​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#introduction-and-context","content":"The general idea here is that you might want to store composite data in a single column in the database. This is a common way to get more generic schema, the idea being that you can have one or more blob columns that store in tables a lot of data that doesn't have to be indexed. You could store it in other ways, like a JSON blob or some such, but we'll be using blobs as the basis for storage here -- hence the name blob &quot;storage&quot;. "},{"title":"How do I define one of these blobs?​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#how-do-i-define-one-of-these-blobs","content":"In SQL/CQL, the main way you define structures, especially those that you want to maintain, is with tables. Hence we introduce this @attribute(cql:blob_storage) create table news_info( who text, what text, when_ long -- timestamp of some kind );  The blob_storage attribute indicates that the thing we're about to define here is not really going to be a materialized table. As a result, you will not be able to (e.g.) DROP the table or SELECT from it, and there will be no schema upgrade for it should you request one. However, the usual schema rules still apply which help you to create compatible versions of this structure. For instance, new columns can be added only at the end, and only if they are nullable. Here we add source to the schema in a hypothetical &quot;version 6&quot;. Note that schema versions move forward globally in the schema, not locally in one table; this implies there are versions 1-5 elsewhere, not shown. @attribute(cql:blob_storage) create table news_info( who text, what text, when_ long -- timestamp of some kind source text @create(6) );  Additionally, since the storage is not backed by SQL with SQL's constraint system, default values and constraints are not allowed in a table marked with cql:blob_storage; it's just data. Similarly, triggers, views, and indices may not use the &quot;table&quot;. "},{"title":"Where do you keep your blob storage?​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#where-do-you-keep-your-blob-storage","content":"Naturally, blob storage goes in a blob field, but recall CQL has discriminated types so we could make something like this: create table info( id long primary key, news_info blob&lt;news_info&gt; );  From a SQL perspective news_info is just a blob. That means if you want to do a WHERE clause or something like that on the info, you're out of luck. Maybe you could write a user-defined function to crack the blob and so forth but really this isn't the point. If you're using this feature then, by construction, you don't need to index on this data. It's simply not suitable for use at all if you need field-at-a-time access within SQL. "},{"title":"How do I make one of these blobs?​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#how-do-i-make-one-of-these-blobs","content":"The natural place that CQL stores structures is in value cursors so the most natural thing to do is to provide a variation of the SETstatement that lets you load a blob from a cursor like so: create proc make_blob(like news_info, out result blob&lt;news_info&gt;) begin declare c cursor like news_info; fetch c from arguments; set result from cursor c; END;  This declares a cursor, loads it from argument values, and converts it to a blob. Of course all of the usual cursor building forms can be used to power your blob creation, you just do one serialization at the end. The above is assembling a blob from arguments but you could equally make the blob from data. create proc get_news_info(id_ long not null, out result blob&lt;news_info&gt;) begin -- use our columns sugar syntax for getting just news_info columns from -- a table with potentially lots of stuff (or an error if it's missing columns) declare c cursor for select columns(like news_info) from some_source_of_info where info.id = id_; fetch c; set result from cursor c; END;  There are many cursor fetch forms, including dummy data forms and other interesting bits of sugar. You can fetch a cursor from arguments, from other cursors, and even combinations. We want all of that to work for blobs as well without adding tons of new syntax and code generation. The obvious way to accomplish that is for cursors to be the source of blobs. "},{"title":"How do I unpack one of these blobs?​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#how-do-i-unpack-one-of-these-blobs","content":"Again, the normal way that you work with records in CQL is by creating suitable cursors. These can be economically accessed on a field-by-field basis. What we need is a way to easily recreate a cursor from the blob so we can read the data values. This gives rise to this form: let b := (select news_info from info where id = id_ if nothing null); declare c cursor like b; fetch c from b; -- note this can fail -- now use c.who, c.what, etc.  Data loaded in a cursor is very economical to access on a field-by-field basis, and, since the deserialization of the blob happens all at once, that is also economical. Importantly, we cannot assume that the blob is well formed, it could be coming from anywhere. For secure-code reasons we must assume it is hostile. Hence the decoding validates the shape, internal lengths, and so forth. If we had instead started with something this: let b := (select news_info from info where id = id_ if nothing null);  Then maybe we might like to write: if b.who == 'U2') then ... end if;  However, this sort of thing would be very uneconomical. For one thing, the blob does not have fixed-offset fields: It is carrying all the serialized data for the string fields and so forth. Each &quot;dot&quot; operation would be costly and, furthermore, each &quot;dot&quot; operation could fail if the blob is badly formed. Having to deal with a b.who that might fail seems very bad indeed. Once you have the cursor you can make new blobs with different combinations, slice the cursor fields using the LIKE operator, return the cursor with OUT, or OUT UNION, or pass the blob fields as arguments to functions using the FROM forms. Cursors already are super flexible in terms of what you can do with their contents. "},{"title":"What is the representation of one of these blobs?​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#what-is-the-representation-of-one-of-these-blobs","content":"It's important that we allow the blobs to evolve over time, so each blob has to be self-describing. We also want to be able to throw an error if you use the wrong kind of blob when loading a cursor, so the blob has to contain the following: the number of columns in the blob data type when it was storedthe type of each field is encoded as a single plain-text character the types are bool, int, long, (double) real, (string) text, blob;we use 'f' (flag) for bools, hence &quot;fildsb&quot;these are encoded with one letter each, upper case meaning 'not null' so the storage might be &quot;LFss&quot;the buffer begins with a null terminated string that serve for both the count and the types Each nullable field may be present or null; 1 bit is used to store this fact. The bits are in an array of bytes that comes immediately after the type info (which implicitly tells us its size)Boolean values are likewise encoded as bits within the same array, so the total number of bits stored is nullables plus booleans (nullable booleans use 2 bits)If you are reading a newer version of a record from an older piece of data that is missing a column then the column is assumed to be NULLAny columns you add after the initial version (using @create) must be nullable; this is normal for adding columns to existing schemaIntegers and longs are stored in varint format after zigzag encodingText is stored inline in null terminated strings (embedded nulls are not allowed in CQL text)Nested blobs are stored inline, with a length prefix encoded like any other int "},{"title":"What about more than one row in a blob?​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#what-about-more-than-one-row-in-a-blob","content":"Well, this is a bit more advanced but in principle this could be done as well. To make it useful, we would want to make a new cursor type that can iterate over rows in a blob. The syntax leaves room for this, something like so: declare c cursor for blob b; loop fetch c begin -- the usual stuff end;  This cursor would be another variation; it would keep its current index into the blob to read data out of it. Such a blob would also have to include a count of rows as part of its storage. However, that's future looking. There is no such support at present. "},{"title":"Conclusion​","type":1,"pageTitle":"Introducing Blob Storage","url":"/blog/blob-storage#conclusion","content":"With a fairly modest amount of work, we now support structured storage natively and have pretty rich language constructs. We carefully chose language constructs that lead to economical serialization and deserialization patterns and a record format that is versioned well, without resorting to something super loose like JSON. As with many other features, it's possible to replace the (de)serialization with code of your choice by supplying your own runtime methods. So for instance, thrift encoding is possible; though it is more flexible than is strictly necessary for the few SQL data types, it might be convenient. Storage types that are going to be persisted in the database or go over a wire-protocol should be managed like schema with the usual validation rules. On the other hand, formats that will be used only transiently in memory can be changed at whim from version to version. As mentioned above, the design specifically considers cases where a new client discovers and old-format blob (with fewer columns) and, the reverse, cases where an old client recieves a datagram from a new client with too many columns. Appendix​ A more complete example is included for reference. @attribute(cql:blob_storage) create table news_info( who text, what text, when_ long -- timestamp of some kind ); -- a place where the blob appears in storage create table some_table( x integer, y integer, news_blob blob&lt;news_info&gt; ); -- a procedure that creates the blob from loose args create proc make_blob(like news_info, out result blob&lt;news_info&gt;) begin declare c cursor like news_info; fetch c from arguments; set result from cursor c; end; -- a procedure that cracks the blob create proc crack_blob(data blob&lt;news_info&gt;) begin declare c cursor like news_info; fetch c from data; out c; end; -- a procedure that cracks the blob into loose args if needed -- the OUT statement was created specifically to allow you to avoid this sort mass OUT awfulness create proc crack_blob_to_vars( data blob&lt;news_info&gt;, out who text, out what text, out when_ long) begin declare c cursor like news_info; fetch c from data; set who := c.who; set what := c.what; set when_ := c.when_; end; -- this just defines a shape for the part we are keeping from the original structure declare proc my_basic_columns() ( x int, y int ); -- this just defines a shape for the result we want -- we're never actually defining this procedure declare proc my_result_shape() ( like my_basic_columns, like news_info ); create proc select_and_crack(whatever_condition bool) begin declare c cursor for select * from some_table where whatever_condition; loop fetch c begin -- crack the blob in c declare n cursor like news_info; fetch n from blob c.news_blob; -- assemble the result we want from the parts we have declare result cursor like my_result_shape; fetch result from values (from c like my_basic_columns, from n); -- emit one row out union result; end; end;  "},{"title":"More Flexible Cursor Patterns Using \"Boxing\"","type":0,"sectionRef":"#","url":"/blog/boxed-cursors-intro","content":"I was reviewing the update posting that just went out and I realized I'd forgotten to mention another big ticket item. So consider this an appendix to the update. In some cases we started seeing a need to &quot;ship cursors around&quot; a little bit more flexibly. Note shipping values around is already doable so this new work is largely about being able to create a &quot;statement cursor&quot; in one procedure and consume it safely elsewhere. The general pattern looks like this: Declare a statement cursor as usual, maybe something like this: declare C cursor for select * from shape_source; -- or declare C cursor for call proc_that_returns_a_shape(); Make an object that can hold a cursor: declare obj object&lt;T cursor&gt;; Where T is the name of a shape. It can be a table name, or a view name, or it can be the name of the canonical procedure that returns the result. You really want this to be some kind of global name though. Something you can get with a #include in various places. In this case choices for T might be shape_source the table or proc_that_returns_a_shape the procedure. Remember you can always make a fake procedure that returns a result to sort of typedef a shape name. e.g. declare proc my_shape() (id integer not null, name text); The procedure here my_shape doesn’t have to actually ever be created, in fact it’s probably better if it doesn’t. You won’t call it, you’re just using its hypothetical result as a shape. This could be useful if you have several procedures like proc_that_returns_a_shape that all return my_shape. At this point you could use the cursor maybe something like: loop fetch C begin -- do stuff with C end; Those are the usual patterns and they let you consume statement cursors sort of “up” from where it was created, but what if you want some worker procedures that consume a cursor there is no good way to pass your cursor down again. Well, there wasn't. Now there is. Let's go back to that box object creation and use it -- recap: declare the box that holds the cursor (T changed to my_shape for this example) declare obj object&lt;my_shape cursor&gt;; -- box the cursor into the object (the cursor shape must match the box shape) set obj from cursor C; The variable obj can now be passed around as usual. Then, later, you can &quot;unbox&quot; it to get a cursor back. Like so -- unboxing a cursor from an object declare D cursor for obj; These primitives will allow cursors to be passed around with managed lifetime. Example: -- consumes a cursor create proc cursor_user(box object&lt;my_shape cursor&gt;) begin declare C cursor for box; -- the cursors shape will be my_shape matching box loop fetch C begin -- do something with C end; end; -- captures a cursor and passes it on create proc cursor_boxer() begin declare C cursor for select * from something_like_my_shape; declare box object&lt;my_shape cursor&gt; set box from cursor C; -- produces error if shape doesn't match call cursor_user(box); end; Importantly, once you box a cursor the underlying SQLite statement’s lifetime is managed by the box object with normal retain/release semantics so timely release becomes imperative. With this pattern it's possible to, for instance, consume some of the rows in one procedure and the rest in another procedure. Now, the main reason for doing this is if you have some standard helper methods that can get a cursor from a variety of places and process it. But remember, that boxing isn’t the usual pattern at all and returning cursors in a box, while possible, should be avoided in favor of the simpler pattern of doing your select or call at the end to compute the result as we do now, if only because then then lifetime is very simple in all those cases. Durably storing a boxed cursor could lead to all manner of problems -- it's just like holding on to a sqlite3_stmt * for a long time. Actually &quot;just like&quot; is an understatement, it's exactly the same as holding on to a statement for a long time with all the same problems because that is exactly what's going on here. So, good generalization, but possibly less Pit of Success, especially with complex box patterns. So watch the sharp edges.","keywords":""},{"title":"Using the LIKE form in the SELECT statement","type":0,"sectionRef":"#","url":"/blog/columns-sugar","content":"One of the signature features of the CQL language is the ability to use the &quot;LIKE&quot; form to slice out columns that conform to a shape. This notion appears in many places in the language. For instance if I have a table Foo. I can make a cursor for that shape like so: declare C cursor like Foo; Which says I want the columns of C to be like the columns of Foo. If I have a cursor D that has the Foo columns but maybe more and maybe in a different order I can load Cas follows: fetch C from D(like Foo) Which again saves me from having to list all the (potentially dozens) of Foo columns. This construct is in many places: declare proc P(like Foo) begin insert into Foo from arguments; end; even declare proc P(f like Foo, b like Bar) begin insert into Foo from f; insert into Bar from b; end; And other examples... This is discussed more fully inChapter 5 of the Guide. However, one of the few places that shapes are interesting but not supported was in the select list. And so, just a couple of days ago, we added the COLUMNS construct to the language which allows for a sugared syntax for extracting columns in bulk. It's kind of a generalization of the select T.*pattern but with CQL-style slicing and type-checking. These forms are supported: columns from a join table or tables -- same as A.* select columns(A) from ...; -- same as A.*, B.* select columns(A, B) from ...; columns from a particular join table that match a shape -- the columns of A that match the shape Foo select columns(A like Foo) from ...; -- get the Foo shape from A and the Far shape from B select columns(A like Foo, B like Bar) from ...; columns from any join table that match a shape --- get the Foo shape from anywhere in the join select columns(like Foo) from ...; -- get the Foo and Bar shapes, from anywhere in the join select columns(like Foo, like Bar) from ...; specific columns -- x and y columns plus the foo shape select columns(T1.x, T2.y, like Foo) from ...; distinct columns from the above (not distinct values!) -- removes duplicate column names -- e.g. there will be one copy of 'pk' select columns(distinct A, B) from A join B using(pk); -- if both Foo and Bar have an (e.g.) 'id' field you only get one copy select columns(distinct like Foo, like Bar) from ...; -- if a specific column is mentioned it is always included -- but later clauses that are not a specific column will avoid it -- if F or B has an x it won't appear again, just T.x select columns(distinct T.x, F like Foo, B like Bar) from F, B ..; Of course this is all just sugar, so it all ends up being a column list with table qualifications -- but the syntax is very powerful. For instance, for narrowing a wide table, or for fusing joins that share common keys -- just the Foo columns select columns(like Foo) from Superset_Of_Foo_From_Many_Joins_Even; -- only one copy of 'pk' select columns(distinct A,B,C) from A join B using (pk) join C using (pk); And of course you can define shapes however you like and then use them to slice off column chucks of your choice. There are many ways to build up shapes from other shapes. Probably the easiest is to declare procedures that return the shape you want and never actual create them. E.g. declare proc shape1() (x integer, y real, z text); declare proc shape2() (like shape1, u bool, v bool); With this combination you can easily define common column shapes and slice them out of complex queries without having to type the columns names over and over... Note that the COLUMNS(...) form is not a general replacement for the select list. For instance, general expressions are not allowed inside of COLUMNS(...) but, where extraction of lots of columns is needed, or even re-ordering of colummns, it's a very good option indeed and it composes well with the other select features. This was the last significant area where shapes are useful but totally absent.","keywords":""},{"title":"Introducing Declare Enum","type":0,"sectionRef":"#","url":"/blog/declare-enum-intro","content":"There is an unfortunate pattern of hard coding constants in SQL which I think comes from the fact that there's not an especially good way to encode constants in SQL. Things are a little better In CG/SQL's CQL language because it's normal to run things through the pre-processor first so you can do things like: #define BUSINESS_TYPE_RESTAURANT 1 #define BUSINESS_TYPE_LAUNDROMAT 2 Having done so, you could write: insert into Business using &quot;Rico's Laundry&quot; name, BUSINESS_TYPE_LAUNDROMAT type; -- by the time SQL sees this it becomes insert into Business(name, type) values('Rico''s Laundry', 2); And at least you don't have to see these loose '2' values all over. An especially unfortunate form is the below, in which the auther is clearly crying for a symbol to use: insert into Business using &quot;Rico's Laundry&quot; name, 2 type; /* laundromat */ But if we use #define the language knows nothing of the names and it can't help you manage them or export them consistently or anything like that. I guess #define is pretty useful in several langauges (C and C++) so you could maybe #include the macros somehow but that doesn't seem especially great. And if you need them in Java you're getting no help at all. So to this world we add enumerated constants. This is a bit short of enumerated types as we'll see later. You can now write something like this: declare enum business_type integer ( restuarant, laundromat, corner_store = 11+3 /* math added for demo purposes only */ ); After this: select business_type.corner_store; is the same as select 14; And that is exactly what SQLite will see, the literal 14. What's going on here? There's just a few rules: the enumeration can be any numeric type (bool, integer, long integer, real)the values of the enumeration start at 1 (i.e. if there is no = expression the first item will be 1, not 0)if you don't specify a value, the next value is the previous value + 1if you do specify a value it can be any constant expression and it will be cast to the type of the enumeration (even if thatis lossy)the enumeration can refer to previous values in itself with no qualification (big = 100.0, medium = big/2, small = medium/2)the enumeration can refer to previously defined enumerations as usual (code = business_type.restaurant)Once the enumeration is defined you refer to its members in a fully qualified fashion enum_name.member_name elsewhere Why is this better than macros? Well for one thing the enum values can be checked at their declaration site, so if you have errors you will hear about them in a more reasonable place. But additionally since the structure is known to the compiler it can give you useful information in the outputs. In the .h files you get: enum business_type { business_type__restaurant = 1, business_type__laundromat = 2, business_type__corner_store = 14 }; In case of floating point values such as: declare enum floating real ( one = 1.0, two = 2.0, e = 2.71828, pi = 3.14159 ); You get: // enum floating (floating point values) #define floating__one 1.000000e+00 #define floating__two 2.000000e+00 #define floating__e 2.718280e+00 #define floating__pi 3.141590e+00 Which is unfortunately the best you can do since C has no floating point enums. But in both cases the enums section of the JSON has the name of the enums and their members and values ready to go. With these values you can readily generate (with moustache or something) the language interfaces of your choice. This is a real help if you're trying to make helpers to call your CQL from say Java or something. To do all this we needed to add some constant folding and general evaluation to the compiler. It's not much, just the normal numeric types and null. The supported operations include: +, -, *, /, %, |, &amp;, &lt;&lt;, &gt;&gt;, ~, and, or, not, ==, &lt;=, &gt;=, !=, &lt;, &gt;, the cast operator and the case forms. These are enough to make a lot of very interesting expressions, all of which are envaluated at compile time. While the constant folding was added to allow for rich enum expressions, there is also the const() primitive in the language now which can appear anywhere a literal could appear. This allows you do things that were previously not allowed such as: create table something( x integer default const((1&lt;&lt;16)|0xf) /* again the math is just for illustration */ ); The const form is also very useful in macros: #define SOMETHING const(12+3) This form ensures that the constant will be evaluated at compile time. Const can also also nest so you can build these kinds of macros from other macros or you can build enums this way. Anywhere you might need literals, you can use const. Importantly, no enumerated data types were added to the language to do any of this. The values can help you to achieve some correctness by avoiding transcription mistakes but there is no additional type-safety provided here. Indeed given the rich mix between these types in SQLite, and with SQLite having no knowledge of enumerations at all it would be tricky to do a complete job. Still, this might happen in the future. But for now, declaring constants that are really an intimate part of your schema is now possible and the addition of the constants to the .h files and the .json output should hopefully make these generally useful. At least we might see less of the hard-coded constant business with good values baked right into the schema declarations. Happy Holidays.","keywords":""},{"title":"Introducing General Purpose Error Tracing","type":0,"sectionRef":"#","url":"/blog/error-tracing-intro","content":"Today we made a couple of minor changes in the code generation to take care of some lingering issues. The first is that when you did a throw inside a catch to basically rethrow the error, you would lose the error code if something had succeeded within the catch handler. The old codegen looked something like this: catch_start_1: { printf(&quot;error\\n&quot;); cql_best_error(&amp;_rc_) goto cql_cleanup; } The problem being that while the printf above is fine and well, if you did any SQL operation then _rc_ would be clobbered and you'd end up throwing an unrelated error code. cql_best_error would at least make sure it was a failure code (SQLITE_ERROR) but the original error code was lost. The new code looks like this: catch_start_1: { _rc_thrown_ = _rc_; printf(&quot;error\\n&quot;); _rc_ = cql_best_error(_rc_thrown_); goto cql_cleanup; } So now if there are db operations, the original return code is still preserved. Note: you still lose sqlite3_errmsg() because SQLite doesn't know that cleanup logic is running. This brings us to the second new thing: general purpose error traces. Error checking of result codes happens very consistently in CQL output. The usual pattern looks something like this: _rc_ = cql_exec(_db_, &quot;SAVEPOINT base_proc_savepoint&quot;); if (_rc_ != SQLITE_OK) goto cql_cleanup; or if it's inside a try block a little different... very little actually // try { _rc_ = cql_exec(_db_, &quot;RELEASE SAVEPOINT base_proc_savepoint&quot;); if (_rc_ != SQLITE_OK) goto catch_start_8; // ... the rest of the try block } Basically if the local _rc_ doersn't match the necessary condition we goto the appropriate error label... either the relevant catch block or else the procedure's cleanup code. We generalize this a bit now so that it looks like this: if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } -- or, in a catch... if (_rc_ != SQLITE_OK) { cql_error_trace(); goto catch_start_8; } Now the default implementation of cql_error_trace() is in cqlrt.h which you can and should customize. I'll be writing more about that later but suffice to say you're supposed to replace cqlrt.h and cqlrt.c with suitable runtime helpers for your environment while keeping cqlrt_common.h and cqlrt_common.c fixed. So for instance, your cqlrt.h could look like this: #ifndef CQL_TRACING_ENABLED #define cql_error_trace() #else // whatever tracing you want, for example this might help in test code. #define cql_error_trace() \\ fprintf(stderr, &quot;Error at %s:%d in %s: %d %s\\n&quot;, __FILE__, __LINE__, _PROC_, _rc_, sqlite3_errmsg(_db_)) #endif So then when you need to debug problems involving lots of error recovery you can watch the entire chain of events easily. Note that there are some useful variables there: In any procedure _db_ is the current database and _rc_ is the most recent return code from SQLite. __FILE__ and __LINE__of course come from the preprocessor. and _PROC_ (one underscore) is now generated by the compiler. Every procedure's body now begins with: #undef _PROC_ #define _PROC_ &quot;the_current_procedure&quot; So by defining your own cql_error_trace macro you can cause whatever logging you need to happen. Note this can be very expensive indeed because this happens a lot and even the string literals needed are a significant cost. So generally this should be off for production builds and enabled as needed for debug builds. The default implementation is just an empty block #define cql_error_trace() But the hook is enough to light up whatever logging you might need, and you can use sqlite3_errmsg() before that message is gone. Good hunting.","keywords":""},{"title":"Error Tracing Helper Macro","type":0,"sectionRef":"#","url":"/blog/error-tracing-macro","content":"Following up on the last blog entry, I thought it would be useful to present a simple error tracing macro that you can use to see what kind of error flow is going on when you're having trouble understanding why a procedure is returning an error code. The idea is we want to create a macro that we can use like this: BEGIN_VERBOSE_STDERR_TRACING; -- Some procedure(s) that you want to trace END_VERBOSE_STDERR_TRACING; We can do that with something like the below macros. These particular ones cause the output to go to stderr via fprintf but if that isn't what you need you can simply edit the macro. The macros looks like this: -- manually force tracing on by redefining the cql_error_trace macro #define BEGIN_VERBOSE_STDERR_TRACING \\ @echo c, &quot;#undef cql_error_trace\\n&quot;; \\ @echo c, &quot;#define cql_error_trace() fprintf(stderr, \\&quot;CQL Trace at %s:%d in %s: %d %s\\\\n\\&quot;, __FILE__, __LINE__, _PROC_, _rc_, sqlite3_errmsg(_db_))\\n&quot; #define END_VERBOSE_STDERR_TRACING \\ @echo c, &quot;#undef cql_error_trace\\n&quot;; \\ @echo c, &quot;#define cql_error_trace()\\n&quot; So basically it's telling CQL to emit a #define into its output stream. In this case: #define cql_error_trace() fprintf(stderr, &quot;CQL Trace at %s:%d in %s: %d %s\\n&quot;, __FILE__, __LINE__, _PROC_, _rc_, sqlite3_errmsg(_db_)) You could change that to any function you like, you can have it dump the errors where you like, or you can make it some dummy function you add so that you can set a breakpoint on it. Whatever you do, do not leave your code with this sort of tracing enabled -- it's far too expensive in terms of code size. But it's perfect if you have this one procedure that is failing and it's hard for you to see where. Obviously if you're making a custom trace thingy you don't need the macro at all, you can just emit your own #define with @echo as needed. Note: @echo is quite a sledgehammer so don't use it lightly and not in production code but it is quite helpful for this sort of thing. CQL tests often use it to help make things visible to the tests. If you use @echo in weird ways you might not get working code when the codegen changes in the future. The relevant state that is available to you inside a macro like this is: __FILE__ the current filename (comes from the C pre-processor, this is the .c file name not the .sql)__LINE__ the current line number (comes from the C pre-processor, this is the .c line number)_rc_ the current SQLite result code (always the current return code in every CQL procedure that uses SQLite)_db_ the current SQLite database pointer (always the current database in every CQL procedure that uses SQLite)_PROC_ the current procedure name (CQL has a #define for this for you)","keywords":""},{"title":"Introducing Expression Fragments","type":0,"sectionRef":"#","url":"/blog/expression-frags","content":"Following on the heels of shared fragments, we're introducing the same kind of thing for shared fragments that are expressions rather than tables. The syntax is as follows: -- this isn't very exciting because regular max would do the job @attribute(cql:shared_fragment) create proc max_func(x integer, y integer) begin select case when x &gt;= y then x else y end; end; The above can be used in the context of a SQL statement like so: select max_func(T1.column1, T1.column2) the_max from foo T1; The consequence of the above is that the body of max_func is inlined into the generated SQL. However, like the other shared fragments, this is done in such a way that the text can be shared between instances so you only pay for the cost of the text* in your program one time, no matter how many time you use it. * You still pay for the cost of a pointer to the text. In particular, for the above, the compiler will generate the following SQL: select ( select case when x &gt;= y then x else y end from (select T1.column1 x, column2 y)) But each line will be its own string literal, so, more accurately, it will concatenate the following three strings: &quot;select (&quot;, // string1 &quot; select case when x &gt;= y then x else y end&quot;, // string2 &quot; from (select T1.column1 x, column2 y))&quot; // string3 Importantly, string2 is fixed for any given fragment. The only thing that changes is string3, i.e., the arguments. The C compiler, and then the linker, will unify the string2 literal across all translation units so you only pay for the cost of that text one time. It also means that the text of the arguments appears exactly one time, no matter how complex they are. For these benefits, we pay the cost of the select wrapper on the arguments. This is cost is frequently negative. Consider this following: select max_func((select max(T.m) from T), (select max(U.m) from U)) A direct expansion of the above would result in something like this: case when (select max(T.m) from T) &gt;= (select max(U.m) from U) then (select max(T.m) from T) else (select max(U.m) from U) end; The above could be accomplished with a simple pre-processor macro, but the fragments code generates the following: select ( select case when x &gt;= y then x else y end from select (select max(T.m) from T) x, (select max(U.m) from U) y)) Expression fragments can nest, so you could write: @attribute(cql:shared_fragment) create proc max3_func(x integer, y integer, z integer) begin select max_func(x, max_func(y, z)); end; Again, this particular example is a waste because regular max would already do the job. To give another example, common mappings from one kind of code to another using case/when can be written and shared this way: -- this sort of thing happens all the time @attribute(cql:shared_fragment) create proc remap(x integer not null) begin select case x when 1 then 1001 when 2 then 1057 when 3 then 2010 when 4 then 2011 else 9999 end; end; In the following: select remap(T1.c), remap(T2.d), remap(T3.e) from C, D, E; The text for remap will appear three times in the generated SQL query but only one time in your binary. Restrictions: the function must consist of exactly one simple select statement no FROM, WHERE, HAVING, etc. -- the result is an expression the select list must have exactly one value Note: the expression can be a nested SELECT which could have all the usual SELECT elements the usual shared fragment rules apply, e.g. no out-parameters, exactly one statement, etc. FAQ: Q: Why does the expression fragment have a select in it? A: Expression fragments are only interesting in SQL contexts where normal procedure and function calls are not available. The select keyword makes it clear to the author and the compiler that the expression will be evaluated by SQLite and the rules for what is allowed to go in the expression are the SQLite rules. Q: Why no FROM clause? A: We're trying to produce an expression, not a table-value with one column. If you want a table-value with one column, the original shared fragments solution already do exactly that. This gives you a solution for sharing code in, say, the WHERE clause or the select list. Q: Isn't this just the same as doing, say, #define max_func(x,y) case when (x) &gt;= (y) then x else y end;? A: Macros can give you a ton of flexibility, but they have many problems: if the macro has an error, you see the error in the call site with really bad diagnostic infothe compiler doesn't know that the sharing is going on so it won't be able to share text between call sitesthe arguments can be evaluated many times each which could be expensive, bloaty, or wrongthere is no type-checking of arguments to the macro so you may or may not get compilation errors after expansionyou have to deal with all the usual pre-processor hazards In general, macros can be used for meta-programming (as in C and C++), but that doesn't mean it's a good idea.","keywords":""},{"title":"Introducing Backed Tables","type":0,"sectionRef":"#","url":"/blog/backed-tables","content":"","keywords":""},{"title":"Introduction and Context​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#introduction-and-context","content":"Most production databases include some tables that are fairly generic, they use maybe a simple key-value combination to store some simple settings or something like that. In the course of feature development this kind of thing comes up pretty often and in large client applications (like Messenger, but certainly not limited to Messenger) there are many small features that need a little bit of state. It's easy enough to model whatever state you need with a table or two but this soon results in an explosion of tiny tables. In some cases there are only a few rows of configuration data and indeed the situation can be so bad that the text of the schema for the little state table is larger than the sum of all the data you will ever store there. This is a bit tragic because SQLite has initialization cost associated with each table. So these baby tables are really not paying for themselves at all. What we'd like to do is use some kind of generic table as the backing store for many of these small tables while preserving type safety. The cost of access might be a bit higher but since data volumes are expected to be low anyway this would be a good trade-off. And we can have as many as we like. In some cases the state doesn't even need to be persisted, so we're talking about tables in an in-memory database. Here low cost of initialization is especially important. And lastly, if your product has dozens or even hundreds of small features like this, the likelihood that all of them are even used in a session is quite low and so again, having a low fixed cost for the schema is a good thing. No need to create 100 in-memory tables on the off chance that they are needed. See also the related feature: blob storage. "},{"title":"How do I define one of these backed tables?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#how-do-i-define-one-of-these-backed-tables","content":"First you need a place to store the data, we define a backing table in the usual way. A simple backing table is just a key/value store and looks like this: @ATTRIBUTE(cql:backing_table) CREATE TABLE backing( k BLOB PRIMARY KEY, v BLOB NOT NULL );  The backing_table attribute indicates that the table we're about to define is to be used for backing storage. At present it is signficantly restricted. It has to have exactly two columns, both of which are blobs, one is the key and one is the value. It should be either baseline schema or annotated with@create as it is expected to be precious data. If it's an in-memory table the versioning is somewhat moot but really the backing store is not supposed to change over time, that's the point. In future versions we expect to allow some number of additional physical columns which can be used by the backed tables (discussed below) but for now it's this simple pattern. Backed table looks like this: @ATTRIBUTE(cql:backed_by=backing) CREATE TABLE backed( id INTEGER PRIMARY KEY, name TEXT NOT NULL, bias REAL ); @ATTRIBUTE(cql:backed_by=backing) CREATE TABLE backed2( id INTEGER PRIMARY KEY, name TEXT NOT NULL );  The backed_by attribute indicates that the table we're about to define is not really going to be its own table. As a result, you will not be able to (e.g.) DROP the table or CREATE INDEX or CREATE TRIGGER on it, and there will be no schema upgrade for it should you request one. It may not contain constraints as there would be no way to enforce them. But as compensation for these restrictions it can be changed freely and has no physical schema cost associated with it. "},{"title":"How do I read this data?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#how-do-i-read-this-data","content":"To understand how this works imagine that we had a view for each backed table which simply read the blobs out of the backing store and then extracted the backed columns using some blob extraction functions. This would work, but then we'd be trading view schema for table schema so the schema savings we're trying to achieve would go up in smoke. We might be lost here but CQL already has something very &quot;view like&quot; and that's the shared fragment structure. So what we do instead of views is to automatically create a shared fragment just like the view we could have made. They look like this: @ATTRIBUTE(cql:shared_fragment) CREATE PROC _backed () BEGIN SELECT rowid, cql_blob_get(T.k, backed.id) AS id, cql_blob_get(T.v, backed.name) AS name, cql_blob_get(T.v, backed.bias) AS bias FROM backing AS T WHERE cql_blob_get_type(T.k) = 2105552408096159860L; END;  So some things to notice right away: First, this fragment has the right shape, but the shared fragment doesn't directly call blob extractors. Rather it uses these cql_blob_get. The point of this is to make the actual blob functions configurable. The test suites include some very simple extraction functions for blobs with just integers in them, but you can create whatever blob format you want. You could use the blob storagefeature for encoding or you can encode it as you see fit. You can even have different encodings in different backed tables. Second, there is a type code embedded in the procedure. The type code is a hash of the type name and the names and types of all the not-null fields in the backed table. The hash is arbitrary but repeatable, any system can compute the same hash and find the records they want without having to share headers. The actual hash is open source but it's just a SHA256 reduced to 64 bits with some name canonicalization. Shortly the JSON will also include the relevant hashes so you can easily consume them without even having to know the hash function. Here's the slightly smaller shared fragment for backed2 @ATTRIBUTE(cql:shared_fragment) CREATE PROC _backed2 () BEGIN SELECT rowid, cql_blob_get(T.k, backed2.id) AS id, cql_blob_get(T.v, backed2.name) AS name FROM backing AS T WHERE cql_blob_get_type(T.k) = -1844763880292276559L; END;  As you can see it's very similar -- the type hash is different and of course it has different columns. "},{"title":"Why does the type hash include only the non-null fields?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#why-does-the-type-hash-include-only-the-non-null-fields","content":"The idea is that the backed table might change over time and you can add new optional fields without invalidating your existing data. If you change the name of the type or if you add new not null fields the type identity changes and any data you have in the backing table will basically be ignored because the type hash will not match. "},{"title":"What do cql_blob_get and cql_blob_get_type turn into?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#what-do-cql_blob_get-and-cql_blob_get_type-turn-into","content":"You can configure them as you see fit. By default cql_blob_get turns into either bgetkey or bgetval depending on if you are reading from the key blob or the value blob. The directives for configuring this function are: @blob_get_key bgetkey offset; @blob_get_val bgetval;  You can configure the system to ask for the column by offset (this is normal for the primary key because it has a fixed number of columns for any given key type and they are all mandatory), or by hash code (this is normal for the value type because it might be missing some columns and so offset is probably not appropriate). However both are configurable so you want to do key by hashcode simply omit the &quot;offset&quot; part of the directive. Likewise if your values are offset addressable you can add &quot;offset&quot; to the value directive. Here the offset means the zero based ordinal of the column in the key or the value. The type access functions are similarly configurable (they never need a code or an offset). @blob_get_key_type bgetkey_type; @blob_get_val_type bgetval_type;  "},{"title":"What does this end up looking like?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#what-does-this-end-up-looking-like","content":"Armed with these basic transforms we can already do a simple transform to make select statement work. Suppose CQL sees: declare C cursor for select * from backed;  We can make this work with a simple transform:  DECLARE C CURSOR FOR WITH backed (*) AS (CALL _backed()) SELECT * FROM backed;  Now remember _backed was the automatically created shared fragment. Basically, if we see a select statement that mentions any backed table we simply add a call to the corresponding shared fragment in the WITH clause. This effectively creates that &quot;view&quot; we need. And because we're using the shared fragment form, all users of this fragment will share the text. So there's no schema and the text of the backed appears only once in the binary. More precisely we get this: WITH backed (rowid, id, name, bias) AS ( SELECT rowid, bgetkey(T.k, 0), -- 0 is offset of backed.id in key blob bgetval(T.v, -6639502068221071091L), -- note hash of backed.name bgetval(T.v, -3826945563932272602L) -- note hash of backed.bias FROM backing AS T WHERE bgetkey_type(T.k) = 2105552408096159860L) SELECT rowid, id, name, bias FROM backed;  Now with this in mind we can see that it would be very beneficial to also add this: CREATE INDEX backing_index ON backing(bgetkey_type(k));  or more cleanly: CREATE INDEX backing_index ON backing(cql_blob_get_type(k));  Either of these result in a computed index on the row type stored in the blob. Other physical indices might be helpful too and these can potentially be shared by many backed tables, or used in partial indicies. Of course your type function might be named something other than the default bgetkey_type. Now consider a slightly more complex example: A slightly more complex example: select T1.* from backed T1 join backed2 T2 where T1.id = T2.id;  becomes: WITH backed (rowid, id, name, bias) AS (CALL _backed()), backed2 (rowid, id, name) AS (CALL _backed2()) SELECT T1.* FROM backed AS T1 INNER JOIN backed2 AS T2 WHERE T1.id = T2.id;  Now even though two different backed tables will be using the backing store the select &quot;just works&quot;. All the compiler had to do was add both backed table fragments. And of course if backed was joined against itself, that would also just work. "},{"title":"How do I insert data like this?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#how-do-i-insert-data-like-this","content":"Consider: insert into backed values (1, &quot;n001&quot;, 1.2), (2, &quot;n002&quot;, 3.7);  This has to insert into the backing storage and convert the various values into key and value blobs. A simple transform does this job as well:  WITH _vals (id, name, bias) AS ( VALUES(1, &quot;n001&quot;, 1.2), (2, &quot;n002&quot;, 3.7) ) INSERT INTO backing(k, v) SELECT cql_blob_create(backed, V.id, backed.id), cql_blob_create(backed, V.name, backed.name, V.bias, backed.bias) FROM _vals AS V;  What's going on here? Well, the issue is that the data to be inserted can be arbitrarily complicated. It might refer to all kinds of things. In this case it's just literal values but in general it could be anything. So the transform takes the original values and puts them in a _vals(...) CTE. Then we insert into the backing store by converting _vals into blobs -- one for the key and one for the value. There is only the one place we need to do this for any given insert statement no matter now many items or how complex the insertion is. cql_blob_create similarly expands to a user configured function with optional hash codes and mandatory field types. There is default configuration that corresponds to this: @blob_create_key bcreatekey offset; @blob_create_val bcreateval;  The final SQL looks like this: WITH _vals (id, name, bias) AS ( VALUES(1, &quot;n001&quot;, 1.2), (2, &quot;n002&quot;, 3.7) ) INSERT INTO backing(k, v) SELECT bcreatekey(2105552408096159860, V.id, 1), -- type 1 is integer, offset implied bcreateval(2105552408096159860, -6639502068221071091, V.name, 4, -- hash as before, type 4 is text, -3826945563932272602, V.bias, 3) -- hash as before, type 3 is real, FROM _vals AS V  Note that both blobs have the same overall type code (2105552408096159860) as before. The key blob did not use per-field type codes, so the argument positions give the implied offset. In contrast the value blob is using hash codes (offset was not specified). This configuration is typical. A more complex insert works just as well: insert into backed select id+10, name||'x', bias+3 from backed where id &lt; 3;  The above insert statement is a bit of a mess. It's taking some of the backed data and using it to create new backed data. But the simple transforms we have work just as before. We add the needed backed CTE and create _vals like before. WITH backed (*) AS (CALL _backed()), _vals (id, name, bias) AS ( SELECT id + 10, name || 'x', bias + 3 FROM backed WHERE id &lt; 3 ) INSERT INTO backing(k, v) SELECT cql_blob_create(backed, V.id, backed.id), cql_blob_create(backed, V.name, backed.name, V.bias, backed.bias) FROM _vals AS V;  Looking closely at the above we see a few things: cql_blob_create will expand as before (not shown)we added backed(*) as usual_vals once again just has the exact unchanged insert clausethe insert into backing(k, v) part is identical, the same recipe always works "},{"title":"How does the delete operation work?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#how-does-the-delete-operation-work","content":"Now let's look at a simple delete example: delete from backed where id = 7;  Now remember we're again looking for a pattern that will generalize when the where condition gets crazy. But fortunately this is not so hard. The following form is fully general: WITH backed (*) AS (CALL _backed()) DELETE FROM backing WHERE rowid IN ( SELECT rowid FROM backed WHERE id = 7 );  All we had to do here was: add the usual _backed CTEmove the original WHERE clause into a subordinate SELECT that gives us the rowids to delete. With the backed table in scope, any WHERE clause works. If other backed tables are mentioned, the compiler simply adds those as usual. Here's a more complicated delete, it's a bit crazy but illustrative: delete from backed where id in (select id from backed2 where name like '%x%');  So this is using rows in backed2 to decide which rows to deleted in backed. The same simple transform works directly. WITH backed2 (*) AS (CALL _backed2()), backed (*) AS (CALL _backed()) DELETE FROM backing WHERE rowid IN ( SELECT rowid FROM backed WHERE id IN ( SELECT id FROM backed2 WHERE name LIKE '%x%' ) );  What happened here: the WHERE clause went directly into the body of the rowid selectbacked was used as before but now we also need backed2 The delete pattern does not need any additional cql helpers beyond what we've already seen. "},{"title":"What about updating tables?​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#what-about-updating-tables","content":"The update statement is the most complicated of the bunch and it requires all the tricks from all the previous statements plus one more. First, we'll need two more blob helpers that are configurable. By default they look like this: @blob_update_key bupdatekey offset; @blob_update_val bupdateval;  These are used to replace particular columns in a stored blob. Now let's start with a very simple update to see now it all works: update backed set name = 'foo' where id = 5;  Fundamentally we need to do these things: the target of the update has to end up being the backing tablewe need the backed table CTE so we can do the filteringwe want to use the rowid trick to figure out which rows to update which handles our where clausewe need to modify the existing key and/or value blobs rather than create them from scratch Let's see how this looks: WITH backed (*) AS (CALL _backed()) UPDATE backing SET v = cql_blob_update(v, 'foo', backed.name) WHERE rowid IN (SELECT rowid FROM backed WHERE id = 5);  Tearing this down a bit: we needed the normal CTE so that we can use backed rowsthe WHERE clause moved into a WHERE rowid sub-select just like in the DELETE casewe changed the SET targets to be k and v very much like the INSERT case, except we used an update helper that takes the current blob and creates a new blob to store the helper is varargs so as we'll see it can mutate many columns in one call This gives us a working update statement... with one hitch. It's possible to use the existing column values in the update expressions and there's no way to use our backed CTE to get them since the final update has to be all relative to the backing table. Let's look at another example to illustrate the problem: update backed set name = name || 'y' where bias &lt; 5;  So this is adding the letter 'y' to some rows. Kind of goofy but similar mutations do happen and have to work. To make this work the reference toname inside of the set expression has to change. We end up with something like this: WITH backed (*) AS (CALL _backed()) UPDATE backing SET v = cql_blob_update(v, cql_blob_get(v, backed.name) || 'y', backed.name) WHERE rowid IN (SELECT rowid FROM backed WHERE bias &lt; 5);  Importantly the reference to name in the set expression was changed to cql_blob_get(v, backed.name) -- extracting the name from the value blob. After which it is appended with 'y' as usual. The rest of the pattern is just as it was, in fact literally everything else is unchanged. But it's easy to see that the WHERE clause could be arbitrarily complex and it just works. Since the UPDATE statement has no FROM clause only the fields in the target table might need to be rewritten, so in this case name, id, and bias were possible but only name was mentioned. After the cql_blob_get and cql_blob_update are expanded the result looks like this: WITH backed (rowid, id, name, bias) AS ( SELECT rowid, bgetkey(T.k, 0), bgetval(T.v, -6639502068221071091L), bgetval(T.v, -3826945563932272602L) FROM backing AS T WHERE bgetkey_type(T.k) = 2105552408096159860L ) UPDATE backing SET v = bupdateval( v, -6639502068221071091L, bgetval(v, -6639502068221071091L) || 'y', 4 ) WHERE rowid IN (SELECT rowid FROM backed WHERE bias &lt; 5);  The blob update function for the value blob requires the original blob, the hash or offset to update, the new value, and the type of the new value. The blob update function for the key blob is the same (blob, hash/offset, value) but the type is not required since the key blob necessarily has all the fields present because they are necessarily not null. Therefore the type codes are already all present and so the type of every column is known. The value blob might be missing nullable values hence their type might not be stored/known. To illustrate these cases we can make another small example; we'll set up yet another small table that uses the same backing store: @attribute(cql:backed_by=backing) create table meta( name text, state long, prev_state long, primary key(name, state) );  This update mixes all kinds of values around... update meta set state = state + 1, prev_state = state where name = 'foo';  And the final output will be: WITH meta (rowid, name, state, prev_state) AS ( SELECT rowid, bgetkey(T.k, 0), bgetkey(T.k, 1), bgetval(T.v, -4464241499905806900) FROM backing AS T WHERE bgetkey_type(T.k) = 3397981749045545394 ) SET k = bupdatekey(k, bgetkey(k, 1) + 1, 1), v = bupdateval(v, -4464241499905806900, bgetkey(k, 1), 2) WHERE rowid IN (SELECT rowid FROM meta WHERE name = 'foo');  As expected the bupdatekey call gets the column offset (1) but not the type code (2). bupdateval gets a hash code and a type. All of these transforms are live in the code as of a few days ago. The upshot is that, if you write some simple encoding and decoding functions, you can have very flexible blob storage. "},{"title":"Appendix​","type":1,"pageTitle":"Introducing Backed Tables","url":"/blog/backed-tables#appendix","content":"If you want to refer to your blob functions in your own code, such as for indices you'll also need to do something like this: declare select function bgetkey_type(b blob) long; declare select function bgetval_type(b blob) long; declare select function bgetkey(b blob, iarg integer) long; declare select function bgetval(b blob, iarg integer) long; declare select function bcreateval no check blob; declare select function bcreatekey no check blob; declare select function bupdateval no check blob; declare select function bupdatekey no check blob;  bgetval and bgetkey are not readily declarable generally because their result is polymorphic so it's preferable to use cql_blob_get as above which then does the rewrite for you. But it is helpful to have a UDF declaration for each of the above, especially if you want the --rt query_plan output to work seamlessly. Typically bgetval would only be needed in the context of a create index statement. "},{"title":"Change in No-Result Semantics","type":0,"sectionRef":"#","url":"/blog/free-empty-results","content":"Important change in CQL semantics. Previously if you did an early return, or fall through the end, from a procedure that is supposed to return a result set but did not in fact provide one, you would get a fake SQLITE_ERROR. Now you get an empty result set for &quot;free&quot;. This interpretation seems much more natural and avoids a lot of really annoying stub selects to comply with the contract. This also works for the out statement in the same fashion. If you want to return an error, use throw. This is a lot more natural... examples: -- this gives you an empty result set if x &lt;= 0 create proc maybe_return(x integer) begin if x &gt; 0 then select * from foo where foo.y &gt; x; end if; end; -- so does this create proc maybe_return(x integer) begin if x &lt;= 0 then return; end if; select * from foo where foo.y &gt; x; end; -- so does this create proc maybe_out(x integer) begin if x &lt;= 0 then declare C cursor for select etc. out C; end if; end; ","keywords":""},{"title":"Using the FROM construct in more places","type":0,"sectionRef":"#","url":"/blog/from-general","content":"This new feature is a pretty simple generalization of the FROM construct as applied to expression lists. Note this isn't the same as using FROM the usual way in a select statement. An example will clear this right up. Suppose you wanted to create a procedure that inserts a row into a table. You could write this: create table Shape_xy (x int, y int); create proc insert_xy(like Shape_xy) begin insert into Shape_xy from arguments; end; Here we're using from to introduce some shape of values. It can appear in a lot of places. Suppose now I want to insert two of those shapes. I could write this slightly more complicated procedure: create proc insert_two_xy(xy1 like Shape_xy, xy2 like Shape_xy) begin call insert_xy(from xy1); call insert_xy(from xy2); end; This also composes with cursors, so maybe you need to get two xy values from diverse locations. You can mix and match. create proc write_xy() begin declare C cursor for select T.x, T.y from somewhere T; fetch C; declare D cursor for select T.x, T.y from somewhere_else T; fetch D; if C and D then -- strange combos for illustration only call insert_two_xy(from C, from D); call insert_two_xy(from D, 5, 3); call insert_two_xy(4, 2, from C); call insert_two_xy(4, from C, 8); end if; end; So, as you can see, we can start from data in one or more cursors and we can turn that data, plus other expressions, into arguments, composing them as we like. This gives you the ability to call procedures and functions using shapes from a mixed set of sources. None of this is new. However, the other places where expression lists happen -- fetch, update cursor, and insert -- only allowed you specify a single object as the input source such as insert into Shape_xy from C. With a little work, this is trivially generalized so that all value lists can use the from construct. Here's a complete example showing all the new forms. create table Shape_xy (x int, y int); create table Shape_uv (u text, v text); create table Shape_uvxy (like Shape_xy, like Shape_uv); create proc ShapeTrix() begin declare C cursor for select Shape_xy.*, '1' u, '2' v from Shape_xy; fetch C; -- This new form is equivalent to the old form: -- insert into Shape_xy from C(like Shape_xy) -- but the values(...) form generalizes, see below. insert into Shape_xy values(from C like Shape_xy); declare D cursor for select * from Shape_uv; fetch D; declare R cursor like Shape_uvxy; -- This form works just like the function call case -- that was previously supported (it uses the same code even). -- This form lets you load R from any combination of sources -- as long as you make a suitable row. fetch R from values (from C like Shape_xy, from D); -- Same thing is supported in update cursor -- the x, y come from C and the u,v come from D.x, D.y. -- Note that C.u and C.v would not even be type compatible. update cursor R from values (from C like Shape_xy, from D); -- And in a select-values clause declare S cursor for with cte(l,m,n,o) as (values (from C like Shape_xy, from D)) select * from cte; fetch S; insert into Shape_uvxy from S; end; As you can see, you can choose a subset of the from shape using like. These combinations let you flexibily assemble rows of data for cursors, calls, and insertions, using any combination of data sources you might want, without resorting to listing every column by hand.","keywords":""},{"title":"Control Flow Analysis in CQL","type":0,"sectionRef":"#","url":"/blog/flow-analysis","content":"","keywords":""},{"title":"Improving Nullability​","type":1,"pageTitle":"Control Flow Analysis in CQL","url":"/blog/flow-analysis#improving-nullability","content":"As of mid-2021, and with increasing sophistication throughout the remainder of the year, CQL has been able to infer that a variable of a nullable type must not be NULL within a portion of a user's program: DECLARE PROC another_proc(t0 TEXT NOT NULL, t1 TEXT NOT NULL); CREATE PROC some_proc(t0 TEXT, t1 TEXT) BEGIN IF t0 IS NULL RETURN; -- `t0` must be nonnull here if we made it this far IF t1 IS NOT NULL THEN -- `t0` and `t1` are nonnull here CALL another_proc(t0, t1); ELSE -- `t0` is nonnull here CALL another_proc(t0, &quot;default&quot;); END IF; END;  The ability of the CQL compiler to infer non-nullability greatly reduces the need to use the functions ifnull_crash and ifnull_throw to coerce values to a nonnull type—functions that, if they are ever used incorrectly, usually result in programs misbehaving. For a detailed description and many additional examples of what is possible—CQL can handle much more than what is shown above—see the user guide's section on nullability improvements. "},{"title":"Enforcing Initialization Before Use​","type":1,"pageTitle":"Control Flow Analysis in CQL","url":"/blog/flow-analysis#enforcing-initialization-before-use","content":"In CQL, it is possible to declare a variable of a nonnull type without giving it a value. If the variable is of a non-reference type, it is assigned a default value of 0. If the variable is of a reference type (BLOB, OBJECT, orTEXT), however, it is simply set to NULL despite the nonnull type as no default value exists. To help prevent accessing a reference variable of a nonnull type and getting back NULL, CQL recently began enforcing that such variables are initialized before use. The following code, therefore, now results in an error: DECLARE t TEXT NOT NULL; CALL requires_text_notnull(t); -- error!  Using the same engine for control flow analysis that is behind nullability improvements, CQL can improve a variable to be initialized: DECLARE t TEXT NOT NULL; IF some_condition THEN SET t := &quot;some example text&quot;; -- `t` is initialized here ELSE THROW; END IF; -- `t` must be initialized here if we made it this far CALL requires_text_notnull(t); -- okay!  Thanks to CQL's ability to understand the control flow of users' programs, the above example works just fine. CQL now also enforces that all procedures with OUT parameters of a nonnull reference type properly initialize said parameters before they return: CREATE PROC some_proc(b BOOL NOT NULL, OUT t TEXT NOT NULL) BEGIN IF b THEN SET t := another_proc(t); -- `t` is initialized here ELSE SET t := yet_another_proc(t); -- `t` is initialized here END IF; -- `t` must be initialized here because all possible -- branches initialized it, so `some_proc` is okay! END;  As with nullability improvements, understanding the nuances of what will be considered initialized is easier if one has a sense for how control flow analysis works in the compiler. "},{"title":"Understanding Control Flow Analysis in CQL​","type":1,"pageTitle":"Control Flow Analysis in CQL","url":"/blog/flow-analysis#understanding-control-flow-analysis-in-cql","content":"To develop an intuition for how control flow analysis works in CQL, let's begin by taking a look at the following example: DECLARE PROC p1(OUT t TEXT NOT NULL); DECLARE PROC p2(i INTEGER NOT NULL, OUT t TEXT NOT NULL); CREATE PROC p0(b BOOL, i INTEGER, OUT t TEXT NOT NULL) BEGIN IF i IS NULL THEN IF b THEN CALL p1(t); ELSE SET t := &quot;&quot;; END IF; RETURN; END IF; IF i == 0 THEN SET t := &quot;&quot;; ELSE IF i &gt; 0 THEN SET t := p2(i); ELSE THROW; END IF; END;  There are a couple of things we must verify in order to ensure the code is type-safe: With regard to the parameters of p0: Since t is an OUT parameter of typeTEXT NOT NULL, p0 must always assign it a value before it returns. If it does not, a caller of p0 may end up with a variable of a NOT NULL type that actually contains NULL. With regard to the calling of p2 in p0: Since p2 requires a first argument of type INTEGER NOT NULL, some sort of check must be performed to ensure that i is not NULL before p2(i) is executed. If we carefully study p0, we can determine that both of the above conditions are satisfied. Making this determination, however, is not exactly trivial, and real-world code is often significantly more complicated than this—and it evolves over time. For these reasons, having a compiler that can make such determinations automatically is critical; most modern production compilers perform these sorts of checks. The easiest way to understand how CQL does its job is to take the above example line-by-line. This is not exactly how CQL works under the hood, but it should provide an intuitive sense of how control flow analysis works in the compiler: ==&gt; CREATE PROC p0(b BOOL, i INTEGER, OUT t TEXT NOT NULL) BEGIN ... END;  Right away, CQL can see that t is declared both OUT and TEXT NOT NULL and thus requires initialization before p0 returns. CQL can, therefore, add a fact about what it is analyzing to its previously null set of facts: t requires initialization. We can then continue: ==&gt; IF i IS NULL THEN ... END IF;  Here, the compiler notices that we're at an IF statement. In CQL, IFstatements contain one or more branches, and the compiler considers everyIF to be the start of a branch group. The same line also indicates the condition for the first branch: i IS NULL. CQL can update its set of facts: t requires initialization.In branch group: In branch when i IS NULL: It then proceeds to the next line:  IF i IS NULL THEN ==&gt; IF b THEN CALL p1(t); ELSE SET t := &quot;&quot;; END IF; RETURN; END IF;  Another branch group and branch: t requires initialization.In branch group: In branch when i IS NULL: In branch group: In branch when b: Continuing:  IF i IS NULL THEN IF b THEN ==&gt; CALL p1(t); ELSE SET t := &quot;&quot;; END IF; RETURN; END IF;  Since p1 takes an OUT argument of type TEXT NOT NULL, this call initializes t, and so CQL can update its set of facts once again: t requires initialization.In branch group: In branch when i IS NULL: In branch group: In branch when b: t is initialized. Jumping ahead a couple of lines:  IF i IS NULL THEN IF b THEN CALL p1(t); ELSE ==&gt; SET t := &quot;&quot;; END IF; RETURN; END IF;  At this point, we're in another branch. We also have yet another fact to add because t is initialized here as well due to the SET: t requires initialization.In branch group: In branch when i IS NULL: In branch group: In branch when b: t is initialized. In ELSE branch: t is initialized. Moving ahead one more line, things get a bit more interesting:  IF i IS NULL THEN IF b THEN CALL p1(t); ELSE SET t := &quot;&quot;; ==&gt; END IF; RETURN; END IF;  Here, we're at the end of an IF, and thus the end of a branch group. Whenever CQL reaches the end of a branch group, it merges the effects of all of its branches. One very important thing to note here is that the current branch group has anELSE branch, and so the set of branches covers all possible cases. That means if something is initialized in every branch within the branch group, we can consider it to be initialized after the branch group has ended: Initialization will always occur. This allows CQL to simplify its set of facts as follows as it leaves the branch group: t requires initialization.In branch group: In branch when i IS NULL: t is initialized. Stepping forward one line again, we reach a RETURN:  IF i IS NULL THEN ... ==&gt; RETURN; END IF;  We're now at a point where we can exit the procedure. CQL will, therefore, verify that if something requires initialization, it has been initialized. Since we have both the facts &quot;t requires initialization&quot; and &quot;t is initialized&quot;, all is well! The fact that the current branch returns early is added to the set of facts: t requires initialization.In branch group: In branch when i IS NULL: t is initialized.Returns. Moving ahead one more line, we reach the end of another branch and branch group, and again something interesting happens:  ... IF i IS NULL THEN ... ==&gt; END IF;  Upon ending the branch group, we know that the branch group has exactly one branch, that the branch is entered only when i IS NULL, and that the branch returns. What that tells CQL is that, if execution is going to continue after the branch group, its sole branch must not have been taken, and so CQL knows the opposite of its condition for entry will be true from this point onward: t requires initialization.i is not null. The next IF is rather similar to what we've seen already in its structure, so we can jump ahead several lines to the next point of interest:  IF i == 0 THEN SET t := &quot;&quot;; ELSE IF i &gt; 0 THEN ==&gt; SET t := p2(i); ELSE THROW; END IF;  Before we analyze the above-indicated line, we have the following set of facts: t requires initialization.i is not null.In branch group: In branch when i == 0: t is initialized. In branch when i &gt; 0: In the call p2(i), we know that i was declared to have type INTEGER and that p2 requires an INTEGER NOT NULL, but we also have the fact &quot;i is not null&quot;. For this reason, we can consider p2(i) to be a valid call. We can also add the fact that t is initialized to our current set of facts: ... In branch when i &gt; 0: t is initialized. NOTE: When it comes to code generation, it is not so simple as to sayp2(i) is valid and proceed as usual. That's because p2 expects an argument of type INTEGER NOT NULL, but we merely have a value of type INTEGER that we happen to know cannot be null: INTEGER NOT NULL and INTEGER do not share the same underlying representation, and so we cannot pass the declared-nullable variable i directly to p2. To solve this problem, CQL rewrites the expression such that p2(i) becomes p2(cql_inferred_notnull(i)), wherecql_inferred_notnull is an internal-only function that handles the nullable-to-nonnull representational conversion for us. This explains its presence in the following examples. Jumping ahead again, we encounter a THROW:  IF i == 0 THEN SET t := &quot;&quot;; ELSE IF i &gt; 0 THEN SET t := p2(cql_inferred_notnull(i)); ELSE ==&gt; THROW; END IF;  The fact that the branch will throw is added to the current set of facts: t requires initialization.i is not null.In branch group: In branch when i == 0: t is initialized. In branch when i &gt; 0: t is initialized. In ELSE branch: Throws. We then proceed to the end of the IF:  IF i == 0 THEN SET t := &quot;&quot;; ELSE IF i &gt; 0 THEN SET t := p2(cql_inferred_notnull(i)); ELSE THROW; ==&gt; END IF;  Once again, CQL merges the effects of the branches in the branch group to finish the analysis of the IF. Since it can see that t was initialized in all branches except the one that throws, and since the branches cover all possible cases, the set of facts is simplified as follows given the knowledge that, ifTHROW was not encountered, t must have been initialized: t requires initialization.i is not null.t is initialized. Moving ahead one final time, we encounter the end of the procedure:  CREATE PROC p0(b BOOL, i INTEGER, OUT t TEXT NOT NULL) BEGIN ... ==&gt; END;  The only thing left to do at this point is to validate that anything requiring initialization has been initialized. Since we have both &quot;t requires initialization&quot; and &quot;t is initialized&quot;, everything is in order. "},{"title":"Looking Ahead​","type":1,"pageTitle":"Control Flow Analysis in CQL","url":"/blog/flow-analysis#looking-ahead","content":"As a recently generalized piece of functionality within the CQL compiler, control flow analysis will soon be used to enforce additional properties of users' programs. In particular, CQL will be able to ensure that cursors are always fetched before they're used and that cursors are always checked to have a row before their fields are accessed. Hopefully you now understand the fundamentals of control flow analysis in CQL and the benefits it brings to your programs. Best wishes for 2022! "},{"title":"Introducing Named Types","type":0,"sectionRef":"#","url":"/blog/named-types-into","content":"A common source of errors in stored procedures is incorrect typing in arguments. For instance, a particular key for an entity might need to be LONG or even always LONG NOT NULL or LONG NOT NULL @SENSITIVE and the only way to do this in the past was maybe with some #define thing. Otherwise you have to diligently get the type right in all the places, and should it ever change, again you have to visit all the places. To help with this situation, and to make code a little more self-describing we add named types to the language. This is a lot like typedef in the C language. They do not create different incompatible types but do let you name things well. You can now write these sorts of forms: declare foo_id type long not null; create table foo( id foo_id primary key autoincrement, name text ); create proc inserter(name_ text, out id foo_id) begin insert into foo(id, name) values(NULL, name_); set id := last_insert_rowid(); end; Refer to the railroad diagram for the grammar details. Additionally any enumerated type can be used as a type name. e.g. declare enum thing integer ( thing1, thing2 ); declare x thing; Enumerations always get &quot;not null&quot; in addition to their base type. This isn't a very complex feature but we hope that it will help create clearer code that is less likely to have type-related bugs.","keywords":""},{"title":"A quick tutorial on LIKE forms","type":0,"sectionRef":"#","url":"/blog/like-forms-tutorial","content":"Everyone knows the usual expression syntax x LIKE y to do a string match. But the CQL compiler also usesLIKE in a different way that's powerful and important. CQL has the notion of data shapes and you useLIKE to refer to them. The simplest source of a data shape, and maybe the most common, is a table. Maybe something like this: create table T( id integer not null, name text not null, age integer not null ); Now suppose you want to write a procedure that can insert a row into that table, You could write create proc insert_into_T( id_ integer not null, name_ text not null, age_ integer not null ) begin insert into T(id, name, age) values(id_, name_, age_); end; This is all fine and well but what if T had 50 columns? That gets old fast. And how can you be sure that you inserted the columns into T in the right order? This second example also compiles even though it's clearly wrong: insert into T(id, name, age) values(age_, name_, id_); And of course you can imagine things get only more complicated with more columns in T. We started adding the LIKE form to ease these issues and to ensure some consistency in the APIs while preventing simple transpostion errors. So you can instead write: create proc insert_into_T(like T) begin insert into T from arguments; end; so here the like T in the argument list simply means &quot;make arguments that are the same as the columns of table T&quot; -- well, almost. It also adds an _ to the end of each name so you end up exactly the same declaration as the long form above. But you won't miss any arguments, and they'll be in the right order. And notice that we used from arguments to indicate that we wanted the values to come from the arguments in order. Again this saves you from a lot of typing and a lot of error checking. You can't get the arguments in the wrong order. These are the most basic patterns. But there are quite a few more. Let's suppose you want to write a procedure that returns in row with the highest age in the above. Maybe you write something like this: create proc highest_age() begin declare C cursor for select * from T; declare M cursor like C; loop fetch C begin if (not M or M.age &lt; C.age) then fetch M from C; end if; end; out M; end; Here we made a cursor M that is the same as the cursor C and then we are going to generate a single row result from the cursor. Note that if you use a cursor name like M in an expression it refers to the hidden boolean that says if the cursor has a row in it or not. So M begins empty and we will load it if it's empty or if the age is higher than what we've already got. Let's show a few more of the forms. Suppose we don't want to return name, just the id and the age. We can change things up a tiny bit. create proc highest_age() begin declare C cursor for select * from T; declare M cursor like select 1 id, 99 age; loop fetch C begin if (not M or M.age &lt; C.age) then fetch M from cursor C(like M); end if; end; out M; end; So two things to notice. We used an ad hoc shape, making a fake select statement that returns the shape we want. This select doesn't run but it does define types and columns easily. Two not null integers in this case. Now M is not the same as C so we can't use the simplest form fetch M from C we have to use the more general form. Fully expanded, what we wrote becomes: FETCH M(id, age) FROM VALUES(C.id, C.age); But as you can see, we didn't have to type all those column names. And that's kind of the point of the LIKE construct. So we've covered a bunch of the shape sources already: a table namea cursor namea select statement that gives the shape in an ad hoc fashion There are three more a view name the return shape of a procedure that returns a result setthe arguments of a procedure View names are pretty simple, and they work the same as table names so we don't need to discuss those. Let's look at some of the other uses with procedures. Suppose we have a procedure that can return a result set shape but we want to be able to mock its results so we can fake whatever result we need for testing. We'll complicate this a bit adding a new table (keeping short table names for the sample to save typing) create table U( id integer not null, email text not null ); And here's a procedure: create proc my_proc() begin select T.*, U.email from T inner join U on T.id = U.id; end; Now we want to be able to make any fake result we want, so maybe want a temp table. No problem: create proc _init_fake_results() begin create temp table if not exists fake_results( like my_proc ); end; create proc add_fake_result(like fake_results) begin insert into fake_results from arguments; end; create proc get_fake_results() begin select * from fake_results; end; The above is very generic and will maintain well. You can see we made a temp table that will have exactly the same shape as whatever my_proc returns. In this case it becomes: CREATE PROC _init_fake_results () BEGIN CREATE TEMP TABLE IF NOT EXISTS fake_results( id INTEGER NOT NULL, name TEXT NOT NULL, age INTEGER NOT NULL, email TEXT NOT NULL ); END; And the rest are patterns we've seem before. The last source of shapes are procedure arguments. There's lots of good cases for those, I wrote an entry on those previously but I'll give a simple example here too. Suppose we have this weird procedure: create proc delete_stuff(age_ integer, name_ text) begin if age_ is not null then delete from T where T.age = age_; end if; if name_ is not null then delete from T where T.name = name_; end if; end; What if we wanted to log any errors that happen here? Maybe make a verison that logs. We can do it like this: create proc delete_and_log(like delete_stuff arguments) begin begin try call delete_stuff(from arguments); end try; begin catch call printf(&quot;delete failed\\n&quot;); -- or whatever throw; end catch; end; The nice thing about this logging wrapper procedure is that if delete_stuff changes, the wrapper will change with it. That covers all of the shape sources and as we saw we can use them to create things like cursors, tables, and argument lists. We can use them to specify a subset of columns that might be of interest when fetching or updating cursors. And we can use them in one last way -- to restrict arguments to a particular shape. Let's see how that works by making the previous logger a little different. Here we added an argument which tells if we should look. And that might look like it would spoil the from arguments part of the forwarding, but there is the final way to use LIKE. create proc delete_and_log2(log bool not null, like delete_stuff arguments) begin if log and age_ is not null then call printf(&quot;deleting %d\\n&quot;, age_); -- or whatever end if; if log and name_ is not null then call printf(&quot;deleting %d\\n&quot;, name_); -- or whatever end if; call delete_stuff(from arguments like delete_stuff arguments); end; So this form lets you use some of your arguments, the ones that match a certain shape. And as we saw in the previous article you can also use from C to pass arguments where C is a cursor and in that case you can also specify that arguments be matched by name from C like shape. In both those cases the formal parameter names of the called procedure are matched against the names of the shape and passed in the order of the formals. So this is like &quot;call by name&quot;, the fields of the cursor or the order of arguments in the argument list might be different than the formals but you'll get the correct items in the correct order regardless, because it matches by name. These forms can save you a lot of typing... and are excellent at avoiding errors and improving maintainability. Where they appear in SQL statements, everything is expanded before it goes to SQLite so SQLite will see normal syntax forms. Which is good because obviously SQLite doesn't know anything about this enhancedLIKE business. In the examples above there were only one or two columns with very short names, but in real world code there can easily be dozens of columns with very long names. In those cases, these forms really shine.","keywords":""},{"title":"Introducing @RC builtin variable","type":0,"sectionRef":"#","url":"/blog/result-variable","content":"We've long needed a way to see the most recent SQLite result code SQLite in the context of say a catch block (most other times you can assume SQLITE_OK was the last result code otherwise control flow would transfer elsewhere. Sometimes SQLITE_ROW or SQLITE_DONE might be the current result code. Soon we'll provide a sample header that declares the most common error codes in an enum but for now you could do something like this: -- pasted from the sqlite.c #define SQLITE_BUSY 5 /* The database file is locked */ -- this is a contrived example create proc get_first_foo(out can_retry bool not null) begin -- can_retry is set to 0 automatically, language semantics guarantee this begin try select foo from bar limit 1; end try; begin catch set can_retry := (@rc == SQLITE_BUSY); throw; -- rethrow the original error end catch; end; ","keywords":""},{"title":"Introducing Parent/Child Result Sets","type":0,"sectionRef":"#","url":"/blog/parent-child","content":"","keywords":""},{"title":"Introduction and Context​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#introduction-and-context","content":"There are many cases where you might want to nest one result set inside of another one. In order to do this ecomomically there was a great desire to be able to run a parent query and a child query and then link the child rows to the parent rows. One way to do this is of course to run one query for each &quot;child&quot; but then you end up with O(n) child queries and if there are sub-children it would beO(n*m) and so forth. What you really want to do here is something more like a join, only without the cross-product part of the join. Many systems have such features, sometimes they are called &quot;chaptered rowsets&quot; but in any case there is a general need for such a thing. We did a bunch of work in the name of Parent/Child results sets but like many goals of this kind it caused us to ripen the CQL language in a variety of ways and its interesting to talk about those changes. Importantly, we wanted to be able to do work of this kind in the language while adding the fewest new notions and basically enabling the language to express a concept like a child rowset in the first place. Here are some things that happened along the way that are interesting. "},{"title":"Cursor Types and Result Types​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#cursor-types-and-result-types","content":"One of the first problems we run into thinking about how a CQL program might express pieces of a rowset and turn them into child results is that you need to be able to hash a row, append row data, and extract a result set from a key. Let's think about that for just a second: in order to do anything at all with a child rowset, no matter how we got such a thing, we have to be able to describe it in a type-safe way. These objects already exist at runtime but they do not appear anywhere in the language explicitly and that was going to have to change. To address this we added a new object type, kind of like we did with boxed statements. A result set has a type that looks like this object &lt;proc_name set&gt;. Here proc_name must the the name of a procedure that returns a result set and the object will represent a result set with the corresponding columns in it. That step may seem like it's super important but actually it's kind of optional, it provides type-safety but the initial versions of the feature just used the type object which works fine provided you make no mistakes... it turns out there are even more fundamental needs that aren't optional. "},{"title":"Creating New Cursor Types From Existing Cursor Types​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#creating-new-cursor-types-from-existing-cursor-types","content":"The first thing you need to be able to to is take the type of the parent query and add to it one more columns to whole the child result set or sets (note that you can have more than one child result set per parent). So for instance you might have a list of people, and one child result might be the names of the schools they attended and another is the names of the jobs they worked. So while adding columns to existing rows might sound like a bizarre thing to do but actually it's actually fundamental to the job here. We must be able to create a new output row is that is the sames as the parent but includes columns for the the child results too. There was no good syntax for this. The cursor declaration forms were: /* option 1 */ declare C cursor like shape_name; /* option 2 */ declare C cursor like select 1 x, &quot;2&quot; y, false z;  The first option implies that you already have a shape from (e.g.) a procedure or table and you want to make an identical cursor. That doesn't work here because we're trying to modify an existing shape, not use it as is. The second form was supposed to be able to create any kind of cursor shape by simply declaring a selectstatement that is an example of what you want to capture. In principle this can define almost anything. However, there's a catch -- you can't get object types to come out of a select so it's hopeless for result set types. And, maybe just as important, you can't just add a few columns to an existing type with any kind of ease, you have to list all columns. Fortunately there was a pretty simple solution to this problem. There were already lots of cases where a typed name list happens in the language -- for example in the return type of a function you can specify something like (id integer, name text). That construction also defines a shape just like a select statement and there was already code to handle all the correctness analysis. Additionally, the LIKE construct can be used in such a list to refer to existing types. So for instance a function that returns all the columns of tables A and B could be defined like so declare function foo() (LIKE A, LIKE B);  So we could solve all the cursor type problems by allowing a typed name list to be used to define a cursor shape. Probably the approach that should have been taken in the first place. The select option seems weird by comparison. With the already existing support for shapes in a type list we could make the result shape for this parent/child case with ease, like so: declare result cursor like (like parent, child_result object&lt;child_proc set&gt;);  So, all the parent columns plus a child result set. Or more than one child result set if needed. Lastly there were going to be cases where we needed to make a new cursor using only some of the field of an existing cursor. The case in particular I'm thinking of is that we might have a big row from the parent and it might have only one or two columns that we need that form the key columns for the child. We didn't have a good way to do that either, but solving this turns out to be simple enough. We already had this form: declare D cursor like C;  we just added: declare D cursor like C(a, b, c);  Which chooses just the 3 named fields from C and makes a cursor with only those. Recently we added the form: declare D cursor like C(-x);  To mean take all the columns of C except x With the a shape for the key fields defined, we can use existing syntax to load the fields economically: fetch D from C(like D);  Which says we want to load D from the fields of C, but using only the columns of D. That operation is of course going to be an exact type match by construction. So now we could describe the key columns from child rows, and the key columns from parent rows. And we could add columns to the parent type to create space to hold child result sets. All of our type problems are solved. Almost. "},{"title":"Cursor Arguments​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#cursor-arguments","content":"It was clear that we would need to be able to do things like &quot;hash a cursor&quot; (any cursor) or &quot;store this row into the appropriate partition&quot; and this requirement meant that we had to be able to write functions that could take any cursor and dynamically do things to it based on its type information. There is no good way to write these generic helper things in CQL, but: we don't need very many of them,it's pretty easy to do that job in C The main thing we need is to create a way to declare such functions and call them a with cursor and the necessary shape info. So we added this notion of being able to call an external function with any cursor. Like so: declare function cursor_hash(C cursor) long not null;  you can call it like so: let hash := cursor_hash(C);  where C is any cursor. When such a call is made the C function cursor_hash gets passed what we call a &quot;dynamic cursor&quot;. This includes: a pointer to the data for the cursorthe count of fieldsthe names of the fieldsthe type/offset of every field in the cursor So you can (e.g.) generically do the hash by applying a hash to each field and then combining all of those. This kind of function works on any cursor and all the extra data about the shape that's needed to make the call is static, so really the cost of the call stays modest. Details of the dynamic cursor type are incqlrt_common.h and there are many example functions now in the cqlrt_common.c file. Again, creating this facility was a pretty minor matter, the compiler already has all this data and uses it to create result sets in the first place. We just allowed other functions to use that same data and made a public type for it. "},{"title":"The Specific Parent/Child Functions​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#the-specific-parentchild-functions","content":"To do the parent/child operations we needed three helper functions: DECLARE FUNC cql_partition_create () CREATE OBJECT&lt;partitioning&gt; NOT NULL; DECLARE FUNC cql_partition_cursor ( part OBJECT&lt;partitioning&gt; NOT NULL, key CURSOR, value CURSOR) BOOL NOT NULL; DECLARE FUNC cql_extract_partition ( part OBJECT&lt;partitioning&gt; NOT NULL, key CURSOR) CREATE OBJECT NOT NULL;  The first function makes a new partitioning. The second function hashes the key columns of a cursor (specified by the key argument) and appends the values provided into a bucket for that key. By making a pass over the child rows you can easily create a partitioning with each unique key combo having a buffer of all the matching rows. The third function is used once the partitioning is done. Given a key again, which you now presumably get from the parent rows, you get the buffer you had accumulated and then make a result set out of it and return that. Note that this function returns the vanilla object type because it could be returning any shape. "},{"title":"Result Set Sugar​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#result-set-sugar","content":"With the type system mentioned above you could now join together any kind of complex parent and child combo you needed, but it might be a lot of code, and it's error prone. This is a good job for a little sugar. So we added some simple syntax to specify the usual partitioning. It looks like this: -- parent and child defined elsewhere declare proc parent(x integer not null) (id integer not null, a integer, b integer); declare proc child(y integer not null) (id integer not null, u text, v text); -- join together parent and child using 'id' create proc parent_child(x_ integer not null, y_ integer not null) begin out union call parent(x_) join call child(y_) using (id); end;  The generated code is simple enough, even though there's a good bit of it. But it's a useful exercise to look at it once. Comments added for clarity. CREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL) BEGIN DECLARE __result__0 BOOL NOT NULL; -- we need a cursor to hold just the key of the child row DECLARE __key__0 CURSOR LIKE child(id); -- we need our partitioning object (there could be more than one per function -- so it gets a number, likewise everything else gets a number LET __partition__0 := cql_partition_create(); -- we invoke the child and then iterate its rows DECLARE __child_cursor__0 CURSOR FOR CALL child(y_); LOOP FETCH __child_cursor__0 BEGIN -- we extract just the key fields (id in this case) FETCH __key__0(id) FROM VALUES(__child_cursor__0.id); -- we add this child to the partition using its key SET __result__0 := cql_partition_cursor(__partition__0, __key__0, __child_cursor__0); END; -- we need a shape for our result, it is the columns of the parent plus the child rowset DECLARE __out_cursor__0 CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER, child1 OBJECT&lt;child SET&gt; NOT NULL); -- now we call the parent and iterate it DECLARE __parent__0 CURSOR FOR CALL parent(x_); LOOP FETCH __parent__0 BEGIN -- we load the key values out of the parent this time, same key fields FETCH __key__0(id) FROM VALUES(__parent__0.id); -- now we create a result row using the parent columns and the child result set FETCH __out_cursor__0(id, a, b, child1) FROM VALUES(__parent__0.id, __parent__0.a, __parent__0.b, cql_extract_partition(__partition__0, __key__0)); -- and then we emit that row OUT UNION __out_cursor__0; END; END;  This code iterates the child once and the parent once and only has two database calls, one for the child and one for the parent. And this is enough to create parent/child result sets for the most common examples. "},{"title":"Result Set Values​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#result-set-values","content":"While the above is probably the most common case, another case can happen where you might want to make a procedure call for each parent row to compute the child. And, more generally, there was no good way to work with result sets from procedure calls other than iterating them with a cursor. The iteration pattern is very good if the data is coming from a select statement -- we don't want to materialize all of the results if we can stream instead. However, when working with result sets the whole point is to create materialized results for use elsewhere. We now had the power to express a result set type with object&lt;proc_name set&gt; but no way to actually get such a set from an existing procedure. Procedures generated them, but they could only be consumed in the C layer. Fortunately this is also an easy problem to solve. We already supported the ability to use procedures as functions in expressions if they had the right signature. We now add the ability to call a procedure that returns a result set and capture that result. Previously this was not supported and would have produced an error. With the new features you can write: declare child_result object&lt;child set&gt;; set child_result := child(args);  or better still: let child_result := child(args);  With this simple change we had the power to write something like this: declare proc parent(x integer not null) (id integer not null, a integer, b integer); declare proc child(id integer not null) (id integer not null, u text, v text); create proc parent_child(x_ integer not null, y_ integer not null) begin -- the result is like the parent with an extra column for the child declare result cursor like (like parent, child object&lt;child set&gt;); -- call the parent and loop over the results declare P cursor for call parent(x_); loop fetch P begin -- compute the child for each P and then emit it fetch result from values(from P, child(P.id)); out union result; end; end;  After the sugar is applied this compiles down to this program: DECLARE PROC parent (x INTEGER NOT NULL) (id INTEGER NOT NULL, a INTEGER, b INTEGER); DECLARE PROC child (id INTEGER NOT NULL) (id INTEGER NOT NULL, u TEXT, v TEXT); CREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL) BEGIN DECLARE result CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER, child OBJECT&lt;child SET&gt;); DECLARE P CURSOR FOR CALL parent(x_); LOOP FETCH P BEGIN FETCH result(id, a, b, child) FROM VALUES(P.id, P.a, P.b, child(P.id)); OUT UNION result; END; END;  The LIKE and FROM forms are very powerful but they aren't new. They do make it a lot easier to express this notion of just adding one more column to the result. Note that the code for emitting the parent_child result before the transformation doesn't need to specify what the columns of the parent are or the columns of the child, only that the parent has at least the id column. Even that could have been removed. This call could have been used instead: fetch result from values(from P, child(from P like child arguments));  That syntax would result in using the columns of P that match the arguments of child -- justP.id in this case. But if there were 7 such columns the sugar might be easier to understand. "},{"title":"Additional Language Support​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#additional-language-support","content":"Last, but not least, to make this more accessible we wanted more support in the generated code. The C interface would have produced generic object results for the child result columns. This isn't wrong exactly but it would mean that a cast would be required in every use case on the native side, and it's easy to get the cast wrong. So the result type of column getters was adjusted to be a child_result_set_ref instead of just cql_object_ref. Similar transforms were needed if column setters were being emitted (yes that's an option!) and of course the Java and Objective C output needed the same transform. "},{"title":"Conclusion​","type":1,"pageTitle":"Introducing Parent/Child Result Sets","url":"/blog/parent-child#conclusion","content":"The prosecution of native support for parent/child result sets in CQL resulted in a bunch of very useful generalizations for declaring and managing cursors. The old special case code for blobs was actually replaced by these forms. The language overall expressiveness increased far more than just the ability to do this one kind of join. It's now possible to write general purpose debug helpers for cursors. It's possible to store and return pre-cooked result sets, creating useful caches and other such combinations. The type extensions to allow extending and narrowing existing types allow even more return flexibility while keeping everything strongly typed. Parent/Child result sets exploit all of these things. "},{"title":"Some updates on the CQL schema upgrade system","type":0,"sectionRef":"#","url":"/blog/schema-notes-2022","content":"","keywords":""},{"title":"Foreword​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#foreword","content":"I was tempted to subtitle this article &quot;How a great idea went horribly, horribly, wrong&quot; but in the final analysis the outcome isn't actually at all horrible. But there are some good lessons here, and it's useful to capture the history while it is still fresh. "},{"title":"Introduction and Context​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#introduction-and-context","content":"The CQL compiler can produce for you, starting from a set of table declarations and schema annotations, a schema upgrader that can upgrade your schema from any previous version to the current version, provided some simple rules are followed. Helpfully, the compiler enforces those rules with plain error messages so that you can reasonably expect your upgrader to work provided all is well with your database connection. Broadly, the entities of the schema are on one of two plans, &quot;create&quot;, and &quot;recreate&quot;. These notions are all discussed in more detail in Chapter 10 of the guide. "},{"title":"The Create Plan​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#the-create-plan","content":"This plan applies strictly to tables, and is used for tables that have precious data that cannot reasonably be restored from say the cloud or some on-device backup. Typically the primary data is on this plan. On this plan you are limited to these operations: new tables can be created (including the so called baseline tables, those having no annotation at all)columns can be added to the end of a table such that an ALTER TABLE ADD COLUMN statement could add themcolumns can be deleted, making them illegal to use in queries but otherwise having no physical consequence in CQL &quot;select from foo&quot; will not include deleted columns hence &quot;&quot; is fully expanded tables can be deleted, leaving a tombstone in the schema the tombstone provides the clue to the upgrader that the table should be dropped if it is found The primary directives for this plan use @create annotations, hence the name. "},{"title":"The Recreate Plan​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#the-recreate-plan","content":"Triggers, Indicies, and Views are all on this plan and tables can be too if they are annotated with @recreateinstead of @create. The idea with this plan is that if the entity changes at all you simply drop the old version and create the new version. This means any change is possible but it also means the upgrade is always destructive: if the upgrader is going to do anything at all it drops all views and all triggers at the start and recreates them at the end this not destructive and takes a lot of weird failure modes off the tablenote steps in the upgrade logic therefore cannot rely on the existence of views or triggers if any index or table changes at all it is dropped and recreated this is done by computing a 64 bit CRC of the entities schema and comparing it to the stored CRCif the CRC is changed the recreate happens Probably the first thing you noticed once you create the notion of recreate for tables is that you really want to do the recreation in groups. There are constellations of schema that have related information and if one of them changes they all need to be updated. This lets you have complex foreign key relationships within this &quot;recreate group&quot;. You'll also notice that a recreate group can have foreign keys within itself and it can make foreign keys to things that are on the create plan but you run into trouble if you try to make foreign keys to some other recreate group. That group might vanish on you, or rather, it might try to vanish and discover that it cannot because of constraint violations. Originally recreate groups could not refer to other groups but recently this was generalized to track a directed acyclic graph of groups. This means that a core group recreating forces the recreation of any groups that refer to it. On this plan its common to end up with a snowflake type schema where the outer parts of the snowflake update often and the inner parts hardly at all. "},{"title":"Overall CRC​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#overall-crc","content":"In addition to the CRCs for the recreate groups, and indices there was a one CRC for overall schema. The upgrader checks this before anything else. If the overall schema CRC matches the current schema then nothing needs to be done (the upgrader has already done its job). If it doesn't match then some steps have to be applied. "},{"title":"Immutable Schema Versions​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#immutable-schema-versions","content":"Other than the cross-group dependencies things began in the form above. The recreate plan was CRC driven and the create plan was version driven. The original design simply generated the appropriate corrections at each schema version and tracked the current version. If the overall CRC had changed, whichever steps you needed were executed. This turned out to be a disaster and it was changed within days. The idea seems fine enough, but the first thing you run into is that two people might make a change to the schema creating say version 5. The problem is if one person adds table X and the other adds table Y they will each run their own and mark themselves as schema 5. When they merge their changes with some other developer, the version 5 upgrade will have already run and they will get nothing despite the fact that v5 includes more for both of them. This is crazytown for developers. So, rather than simply tracking the current schema version, each schema version got its own mini-CRC. The upgrader would run the steps of each version if the CRC was absent or didn't match. With steps like CREATE TABLE IF NOT EXISTS and so forth a merge would result in you getting the other half of the changes for your version and work could accumulate at say schema v5 with no problems. Actual customers would never see this because they only saw completed schema versions. This worked a lot better and lasted about a year. The problem is that the system is based on these &quot;mostly immutable&quot; schema versions. You never restate the past you always give instructions on how to move forward. With the versions being nearly immutable, and the upgrade steps being idempotent, things seemed good. But it turns out neither of those two assumptions was really exactly true. "},{"title":"Mutating Schema Versions​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#mutating-schema-versions","content":"The reality of the schema we created for our platform was that there was one large uber schema that had all the possible schema you might need for a variety of features and any give product could opt in to the features it wanted, thereby getting the necessary schema. The schema system had a good way to partition the schema using regions. The upgrader could work on a set of regions and provide the union of schema in those regions, omitting the rest. Super. Here's where things get exciting. A schema consumer could reasonably decide at some time in the future that it wants new features and so it opts into additonal regions. That's fair enough, but the net of this is that of course new tables appear. Some of these are likely to be in the baseline schema (v0) and some might have appear later (e.g. v5, v10, v21). This is all fine, the CRCs for those versions change and the schema upgrader runs again. Those versions execute and add the correct schema. Perfect. Actually no. "},{"title":"Zombie Tables​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#zombie-tables","content":"About two years into the history of CQL we started noticing that some attempts to delete tables were failing. The DROP commands claimed that there was a constraint problem -- but these should have been leaf tables. What constraint could possibly be the issue? This was the first time a major design flaw with this system was revealed. Previous bugs had been few and had all been silly logic errors or off by one checks in version numbers, that kind of thing, easily fixed. This was a puzzler. But the answer was fortunately available in the set of annotations. Basically, imagine a table &quot;zombie&quot; had been created say in the baseline schema, and then later deleted; suppose it was deleted in version 20. All is well, the upgrade steps for version 20 include a table drop. However, now a team subscribes to more schema, causing the v0 schema to include a few more tables. Here's the problem, when the steps for v0 run again they notice that &quot;zombie&quot; is missing and helpfully create it, thinking this is the right thing to do. But this is a disaster... The &quot;zombie&quot; table is supposed to be deleted in v20 but that CRC is unchanged! So now a table exists that has no business existing. If &quot;zombie&quot; has an FK reference to some other table which we wnat to delete, then all attempts to drop that table will fail because &quot;zombie&quot; is there gumming up the works. Even if it's empty... which it will be in this case. This problem was fixed by having all tables that need deleting be unconditionally deleted at the end of the upgrade and not in the steps for the version in which the delete happened. This meant that the next upgrade purged all the zombies and enabled the correct table drops to start running with no errors. The consequence of this was a 90% reduction in schema upgrade failures! "},{"title":"Unsubscription​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#unsubscription","content":"Another reason for the &quot;immutable&quot; version history to (cough) mutate was a desire to opt out of tables. As described in this sectionwe created an affordance to allow people to unsubscribe from some of the tables they had previously selected. This provided finer-grain control of the schema subscription and also made it possible to undo previous mistakes of over-subscription. However, it was clear from the beginning that you might want to undo an unsubscription at some time in the future. In keeping with schema directives that create a clear history the @unsub and @resub statements were added to the language with lots of rules for correctness. The upgrader did the following things: upon finding an unsubscription at version X that version includes DDL to drop the unsubscribed tablechanges to that table in an future versions were omittedupon finding a resubscription at version Y that version included DDL to create the table as it exists at version Ylater changes to that table are once again emitted as usual This was very nearly right except it had the same problem as the delete case above. A table created in say the baseline might come back as a zombie even though it was unsubscribed. However, now wise to this issue a small fix takes care of the problem. always drop tables in the unsubscribed state at the end just like delete tablesno code is needed to do an unsubscribe at version x (the drop at the end will do the job)a resubscribe at version X first drops the table and then recreates as it exists at version X This gives us a consistent upgrade path again and importantly avoids the problem of a resubscription finding a zombie that prevents it from doing its job. "},{"title":"Performance Optimization 1​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#performance-optimization-1","content":"On July 1, 2022 we made a modest change that reduced the number of SQLite statements required to do a full upgrade. The opportuntity came from the many column existence checks we made before running ALTER TABLE ADD COLUMN. Rather than run a statement that looked like this (SELECT EXISTS(SELECT * FROM sqlite_master WHERE tbl_name = table_name AND sql GLOB column_declaration_pattern)) for each column we first selected all of the table schema out of the sqlite_master table and put it into a hash table keyed by the table name. Reading even a few hundred table names was much faster than running a single statement for each column that needed to be checked -- especially when recreating the schema from scratch. In the most relevant test case this was a 7% improvement. Importantly, it motivated us to add hash tables into cqlrt_common.c and generalize the mechanism for object management so that the cqlrt allows creation of new objects without having to add special support for each one. This new hash table meant that we could do a hash lookup and substring match instead of a sqlite query for each column. "},{"title":"Performance Optimization 2​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#performance-optimization-2","content":"On Oct 11, 2022 we stopped using CRCs for the version checks on the create plan entirely. This was in fact an optimization but it was motivated by a real, but rare, problem. What was happening was something like maybe 1 in 10^5 databases was missing columns. The sequence of events that caused this was very hard to diagnose but the situation was very clear. The database was at say schema version 100. The columns had been added at say version 50. The CRCs indicated that the v50 upgrade had already run so it didn't run again. The columns would now never be added. We had yet to come up with a set of steps that would adequately describe how this happened. I have to guess some combination of a resubscription ran because of one of those &quot;the schema is not really immutable&quot; changes and then &quot;medium&quot; version of the table say v25 was resubscribed but the columns added in say v50 never got readded because v50 thought it had already run. This was getting to be a nightmare but there was a simple solution. We already had created this dictionary that had all the tables and their schema from sqlite master, we were already using it to determine if we needed to add a particular column. The only reason we had version CRCs at all was to allow us to skip steps, but since we could already skip column adds super fast all we needed was to be able to skip table adds -- there is nothing else. Well the same hash table can obviously easily tell us if a table exists. Non-existent tables have no schema and hence are absent from the hash table which is loaded directly from sqlite_master. So the new algorithm, goes something like this: use the version numbers only for orderingbefore adding a table, check if it exists in the table, this is faster htran running CREATE TABLE IF NOT EXISTScheck the columns as beforeattempt each of these every time the overall schema changes, and trust that the fast checks are fast enough On this plan we change the way @unsub and @resub are handled to something much simpler: @unsub acts like an out of band @delete on the table or view to which it is applied the drop happens at the end like before @resub resets that state so the table is considered not deleted if the last operation was @resub To this we add one new rule: the schema upgrader removes any reference to deleted tables entirely they are removed from baselinethey are not included in any upgrade rulesthey are only dropped at the end if they still exist This vastly simplifies unsub/resub and delete. An unsubscribed table will always get cleaned up at the end, just like deleted tables. No strange interim states happen in resub. If a table is resubcribed it just reappears in the schema and the various operations run as usual. The only compromise to this is that we still have a single CRC for the overall baseline schema. However even that could be removed at the expense of more hash table lookups. There is a binary size win for fewer checks and since baseline by definition depends on nothing it seems like safe position to take. This approach was about 13-15% faster in fact, the time saved examining and writing back schema CRCs more than paid for the extra hash table checks (which were ~100x faster than the db operations). And the hash table already existed! The reduction of the CRC checks and removal of vestigial upgrade logic for deleted tables also resulted in a 2.2% reduction of upgrader size for our most important case. "},{"title":"The most recent change and further simplications in unsub/resub logic​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#the-most-recent-change-and-further-simplications-in-unsubresub-logic","content":"With all of this in place it's clear that the various rules for unsubscription and resubscription and the sort of historical playback that was used to try to create these immutable stages is moot. The only thing that matters is if we end in the unsubscribed state. Removing the unsubscribe upgrade steps from the upgrader entirely just simplifies everything. So no @resub is needed at all nor are @unsub version numbers. Presently set to land is set of changes that remove resubcription entirely, to resubscribe you simply remove the @unsubdirective for your table/view. This lets us eliminate a bunch of validations and test cases to get a simpler, clearer, and more easily verifiable upgrader. There's just much less to go wrong. Even crazy cases like &quot;an unsubscription happens in v5, the resubscription happens in v7, a user runs the upgrade and they might have a database that is v4, v5, v6, or v7 (having already been upgraded)&quot;. All of these had different potential flows before. Now they are all the same. All the cases will roll the table forward to v7 from whatever version they might be on with the usual rules and states particular to unsubscription or resubscription. The table is present or it isn't. It is missing columns or it isn't. Same as always. "},{"title":"A Versionless Future​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#a-versionless-future","content":"More thinking is needed here but it's clear that now that we've arrived at this simpler place ALL the version numbers are moot. The only thing we really have to do with version numbers is run ad hoc migrations at the appropriate time, and only once. The rules for migrators would have to change such that they are responsible for finding the state of the schema, and maybe some context could be provided for this. But ad hoc data migrators are very uncommon and regular annotations are much more so. "},{"title":"Conclusion​","type":1,"pageTitle":"Some updates on the CQL schema upgrade system","url":"/blog/schema-notes-2022#conclusion","content":"The fundamental assumption about how schema changes would happen was wrong. Even so, it was close enough that upgrades were over 99.99% successful when the other parts of the system are working ok. This is probably about the best we can hope for given the state of affairs with flash drives on Android devices. The current system is actually pretty close code-wise to what we had planned -- just look at the Oct 11, 2022 diff to see what I mean. It's not that big of a difference in the end. The new system has been deployed for nearly a month now and it is immune basically all of the failure modes of the old. It will take some time before we know what its true reliability is given the low failure rate of both. But we do know the new system is significantly faster. Optimizations 1 and 2 together are over 20% for full installations. I should note that someone who was obviously smarter than me told me that we would land on a solution like this and I didn't believe them. They were of course correct. You know who you are. Sorry I doubted you. "},{"title":"Introducing Select .. If Nothing","type":0,"sectionRef":"#","url":"/blog/select-if-nothing","content":"The nested select statement is frequently misused, in particular if you get no rows back from the expression that's an error. So for instance: set x_ := (select x from foo.x where id = y); This will throw (with a peculiar error, SQLITE_DONE) if there is no such row. Sometimes people try to fix this problem with a nullcheck: set x_ := IFNULL((select x from foo.x where id = y), -1); That doesn't help at all. It's not a null value situation, there's no row at all. set x_ := (select IFNULL(x,-1) from foo.x where id = y), -1); Is likewise unhelpful. To help with this situation we add two forms: -- useful if foo.x is already known to be not null set x_ := (select x from foo.x where id = y IF NOTHING -1); -- useful if foo.x might be null set x_ := (select x from foo.x where id = y IF NOTHING OR NULL -1); Both of these deal with the case where there is no row. The second lets you have a simple default for both no row or null value. That form is equivalent to: set x_ := (select IFNULL(x,-1) from foo.x where id = y IF NOTHING -1); i.e. both problem cases are handled. Of course the -1 here could be any valid expression, even a second (select...)","keywords":""},{"title":"Introducing Shared Fragments","type":0,"sectionRef":"#","url":"/blog/shared-fragments-intro","content":"","keywords":""},{"title":"Generics​","type":1,"pageTitle":"Introducing Shared Fragments","url":"/blog/shared-fragments-intro#generics","content":"A series of useful fragments for generating data would go a long way but there are other applications of fragments and you might want to operate on various data sources without hard coding them all. This is where the generic form of fragments comes in. Consider a case where you want to be able to filter stuffby say name and age. You could create this fragment: @attribute(cql:shared_fragment) CREATE PROC filter_stuff( pattern_ text not null, min_age_ integer not null, max_age_ integer not null) BEGIN WITH source(*) LIKE stuff SELECT * from source S WHERE S.name LIKE pattern_ AND S.age BETWEEN min_age_ and max_age_; END;  Now imagine that we had added the shared fragment annotation to get_stuff (just like the above). We could then write the following: CREATE PROC the_right_stuff( to_include_ text, to_exclude_ text, pattern_ text not null, min_age_ integer not null, max_age_ integer not null) BEGIN WITH get_stuff(*) AS (call get_stuff(to_include_, to_exclude_)), filter_stuff(*) AS (call filter_stuff(pattern_, min_age_, max_age_) using get_stuff as source) SELECT * from filter_stuff S ORDER BY name LIMIT 5; END;  Or with some sugar to forward arguments and assume the CTE name matches, more economically: CREATE PROC the_right_stuff( to_include_ text, to_exclude_ text, pattern_ text not null, min_age_ integer not null, max_age_ integer not null) BEGIN WITH (call get_stuff(*)), (call filter_stuff(*) using get_stuff as source) SELECT * from filter_stuff S ORDER BY name LIMIT 5; END;  The arg syntax (*) simply indicates that the arg names in the caller should match to the same names in the callee. In general call foo(*) expands to call foo(from arguments like foo arguments). * is rather more economical than that. In this example filter_stuff doesn't know where its data will be coming from, you bind its table parameter sourceto a compatible data source of your choice. For example, this would also be legal: CREATE PROC almost_the_right_stuff( pattern_ text not null, min_age_ integer not null, max_age_ integer not null) BEGIN WITH (call filter_stuff(*) using stuff as source) SELECT * from filter_stuff S ORDER BY name LIMIT 5; END;  "},{"title":"Conditionals​","type":1,"pageTitle":"Introducing Shared Fragments","url":"/blog/shared-fragments-intro#conditionals","content":"It's often desirable to have some options in the generated SQL without having to fork your entire query. Shared fragments address this as well with the conditional form. In this form the top level of the fragment is anIF statement and there are a number of alternatives. Here are some simple modifications to the above that illustrate some of the possibilities. @attribute(cql:shared_fragment) CREATE PROC filter_stuff( pattern_ text, min_age_ integer not null, max_age_ integer not null) BEGIN IF pattern_ IS NOT NULL THEN WITH source(*) LIKE stuff SELECT * from source S WHERE S.name LIKE pattern_ AND S.age BETWEEN min_age_ and max_age_; ELSE WITH source(*) LIKE stuff SELECT * from source S WHERE S.age BETWEEN min_age_ and max_age_; END IF; END;  In the above if the input pattern is NULL then it is not considered, it won't be part of the generated SQL at all. Note thatsource (same name) appears in both branches and therefore must be the same type as it will be fulfilled by one actual table parameter. Now the above could have been achieved with something like this: pattern_ IS NULL OR S.name LIKE pattern_  But that would have no useful selectivity. But in general you might be able to avoid joins and so forth with your constraints. Consider something like this hypothetical: @attribute(cql:shared_fragment) CREATE PROC filter_stuff( pattern_ text, min_age_ integer not null, max_age_ integer not null) BEGIN IF pattern_ IS NOT NULL THEN WITH source(*) LIKE stuff SELECT DISTINCT S.* from source S INNER JOIN keywords K WHERE K.keyword LIKE pattern_ AND S.age BETWEEN min_age_ and max_age_; ELSE WITH source(*) LIKE stuff SELECT * from source S WHERE S.age BETWEEN min_age_ and max_age_; END IF; END;  Here we save the DISTINCT and the JOIN if there is no pattern which might be important. Of course there are probably better ways to match keywords but this is just an illustration of what's possible. There are numerous ways this flexibility can be used, again a simple example, a real schema transform would be more complex. @attribute(cql:shared_fragment) CREATE PROC get_stuff( to_include_ text, to_exclude_ text, schema_v2 bool not null) BEGIN IF schema_v2 THEN WITH to_include(id) AS (CALL ids_from_string(to_include_)), to_exclude(id) AS (CALL ids_from_string(to_exclude_)) SELECT * from stuff_2 S WHERE S.id in (select * from to_include) AND S.id not in (select * from to_exclude); ELSE WITH to_include(id) AS (CALL ids_from_string(to_include_)), to_exclude(id) AS (CALL ids_from_string(to_exclude_)) SELECT * from stuff S WHERE S.id in (select * from to_include) AND S.id not in (select * from to_exclude); END IF; END;  "},{"title":"Validation​","type":1,"pageTitle":"Introducing Shared Fragments","url":"/blog/shared-fragments-intro#validation","content":"All of this requires a bunch of checking, at least this: the LIKE forms can only appear in a shared fragmentthe CALL forms must refer to shared fragmentsthe CALL args must be compatiblethe number and type of the provided tables in USING must be correctthe shared fragment must be a single select statement or an IF statement with an ELSE the statement lists of the IF/ELSE combo must all be single select statementsall the choices in the IF block must return the same shape (this is normal for procedures) the shared fragment can't have any out argumentsthe provided fragment arguments cannot themselves use the nested SELECT construct I think this is a total game changer for SQL authoring and should go a long way to making it easier to get your work done on SQLite. A good base set of shared fragments as part any suite of procedures seems like a good idea. There are more details in the section on shared fragments in Chapter 14 of The Guide. These features are in the current build as of today (12/14/2021). Happy Holidays and stay safe. "},{"title":"Introducing Type \"Kinds\"","type":0,"sectionRef":"#","url":"/blog/type-kinds-intro","content":"Further adding to the type calculus of the CQL language we introduced the ability to encode the &quot;kind&quot; of primitive types. This can be used in a number of ways -- like &quot;units&quot; for natural things and like a &quot;type&quot; for synthetic keys and other such. It's easier to illustrate by example. declare job_id type long&lt;job_id&gt;; declare person_id type long&lt;person_id&gt;; declare j job_id; decalre p person_id; set p := j; -- this is an error With the above in place, other expressions like p == j would also produce errors as these long values are no longer type compatible. This is a great way to add enforcement to your schema and procedures. Likewise you can use these annotations to add &quot;units&quot; to your data types. e.g. declare meters type real&lt;meters&gt;; declare grams type real&lt;grams&gt;; declare m meters; declare g grams; Variables of type grams (e.g. g) are not compatible with variables of type meters (e.g. m) even though both are real. Likewise, attemping to insert grams into a column that is typed to meters will give errors. Of course SQLite doesn't know about any of this so all the &lt;&gt; stuff is removed in the generated SQL. This is just about type enforcement at compile time. Enumerations like: declare enum surface integer (paper, canvas); declare enum writer integer (pen, paper, brush); enable this: declare s surface; -- s is now of type integer&lt;surface&gt; declare w writer; -- w is now of type integer&lt;writer&gt; set s := surface.paper; -- ok set s := writer.pen; -- error set w := writer.pencil; -- ok case when s == w then 1 else 0 end; -- error (w/s not comparable) set w := s; -- error again additionally in DML/DDL: create table draw_action( w writer, s surface ); insert into draw_action values(w, s); -- ok insert into draw_action values(s, w); -- error! So the type kinds can be quite helpful when dealing with loose variables. The notion of specific types was added to the language nearly two years ago to support the object type because there was a great desire to prevent object&lt;dictionary&gt; being assigned from object&lt;list&gt; but this &quot;type kind&quot;, whether it's with units (e.g. &quot;meters&quot;, &quot;grams&quot;) or a type name (e.g. &quot;job_id&quot;) adds a lot of high value type checking. The kind can be added, stripped, or changed with a cast operation and the type system allows a constant or variable with no kind (e.g. &quot;1&quot;) to mix and match with any kind so long as the base type is compatible as usual. So you get the most value by using the specific type consistently but you won't go insane adding test cases that use constants for instance. As of this writing the expression kinds are checked for compatibility everywhere plus or minus bugs. There are extensive tests.","keywords":""},{"title":"One Month Update","type":0,"sectionRef":"#","url":"/blog/update","content":"","keywords":""},{"title":"Here's a quick summary of what's been going on:​","type":1,"pageTitle":"One Month Update","url":"/blog/update#heres-a-quick-summary-of-whats-been-going-on","content":"@mingodad gave us an implementation of check and collate column attributes (the check attribute on tables should be easy to add from here)the select function form should never return objects, only SQLite types, enforced@attribute(cql:suppress_result_set) was added to save code gen for procedures that don't need the C result set wrapperscql_cursor_diff_col and cql_cursor_diff_val methods were added to report what's different about two cursors (highly useful in test code)cql_cursor_format was added so you can quickly convert any cursor into columns and values as string for debug output (no matter the shape)sqlite3_changes was added to the builtin list so you don't have to use declare select function to use it anymorecql_get_blob_size was added so you can see how big your blobs are (useful for diagnostics)trim, rtrim and ltrim were added to the builtin list so you can use them without declare select functionthe builtin function ifnull_crash was added so that nullables that have already checked can be safely typecast to not nullthe bug we saw in demo video number 2 where some foreign keys were not properly linked up in autotest code was fixed (yay videos)time functions are now known to be not null for a bunch of simple cases such as 'now' argumentsyou can use the cast(.. as ..) operator on numeric types outside of the SQL context@mingodad replaced all the positional references by named references in cql.y (yes! thank you!)several minor bug fixesthe railroad diagrams were updated NOTE: I often refer to &quot;sugar&quot; in the below. This is short for syntatic sugar which, in case you're not familiar with the term, refers to a syntatically more pleasing way of writing a concept that is otherwise totally doable with normal syntax. Many languages have sugar for forms that are common -- for brevity, clarity, and/or correctness. "},{"title":"And now a few notes on The Big Stuff​","type":1,"pageTitle":"One Month Update","url":"/blog/update#and-now-a-few-notes-on-the-big-stuff","content":"We often add new features to the language to facilitate the writing of tests. The tests have a lot of boilerplate often setting up and calling the same procedures again and again with slightly different arguments. Long argument lists and long insert column lists are especially problematic as these can be very error prone. Here good language constructs are very helpful. We've found good test constructs are often invaluable in production code as well, though in our experience the tests often have a lot more repitition that needs refactoring than production code. To that end we added some very useful things in the last month: "},{"title":"Declare cursors in the shape of a procedure's arguments and use them​","type":1,"pageTitle":"One Month Update","url":"/blog/update#declare-cursors-in-the-shape-of-a-procedures-arguments-and-use-them","content":"The most common way to create a cursor is from a select statement but you can also make a cursor that can hold values for you by declaring it to be LIKE something else with a shape. A classic example is: declare C cursor like some_table;  Now C has the same columns and types as some_table Many procedures have a result type that is also a shape, for instance any procedure that ends with a select statement has a result shape defined by the columns of the select statement. You could always do this sort of thing: declare C cursor like some_proc;  Meaning make C a cursor whose shape is whatever some_procreturns, which is of course exactly the kind of cursor you need to capture the result of some_proc. Now we add: declare C cursor like some_proc arguments;  The idea being that the arguments of some_proc are also a shape (unless it has none). With this done you want to use that cursor to call the procedure -- that being sort of the whole point. So we add this: call some_proc(from C);  How do we use this effectively? Hold on just a second -- for that answer we need one more big tool to really help the syntax. "},{"title":"Loading cursors and inserting columns​","type":1,"pageTitle":"One Month Update","url":"/blog/update#loading-cursors-and-inserting-columns","content":"Loading up a cursor is done with syntax that is very much like an insert statement. An example might be something like this: fetch C(x,y,z) from values(1,2,3);  This is simple enough but it becomes more problematic if there are many values and especially if the values have complex names. To make this a little less error prone CQL now has this sugar form for fetch, insert, and soon update cursor (like maybe before you see this blog). The more readable form is: fetch C using 1 x, 2 y, 3 z;  This form has the values next to their names just like in a select statement, like all sugars, it is automatically rewritten to the normal form. Likewise insert into some_table using 1 id, 'fred' first_name, 'flintstone' last_name, 'bedrock' home_town, 'dino' favorite_pet, 'wilma' life_partner;  becomes insert into some_table(id, first_name, last_name, home_town, favorite_pet, life_partner) values(1, 'fred', 'flintstone', 'bedrock', 'dino', 'wilma');  except the sugar form is much less error prone. This form doesn't generalize to many values but the single row case is super common. Since this form is automatically rewritten SQLite will never see the sugar syntax, it will get the normal syntax. NOTE: the insert rewrite is coming later today, and will likely be live by the time you read this. "},{"title":"Putting these together​","type":1,"pageTitle":"One Month Update","url":"/blog/update#putting-these-together","content":"Let's suppose you have to write a test. You have a procedure test_subject that takes some arguments plus you have another helper procedure test_setup that puts seed data in the right places for your subject. But there are many variations and a lot of what you do between variations is the same. How can you write this economically making it clear what is different between variations without a lot of fuss. Well you can do something like this: -- use defaults for all the named values -- use 'seed' for everything else that isn't named create proc default_setup_args(seed integer not null) begin declare args cursor like test_setup arguments; fetch args using 1334 primary_id, 98012 secondary_id, 'foo' useful_name, 'bar' other_useful_name, 1 fast_mode @dummy_seed(seed); out args; end;  With the above you can easily see which values go to which arguments Your test setup can now look something like this: declare setup_args cursor like test_setup arguments; fetch setup_args from call default_setup_args(1999); update cursor setup_args using 0 fast_mode; -- override fast mode for this test call test_setup(from setup_args);  To call the test subject you probably need some of those setup arguments and maybe some more things. create proc default_subject_args(like default_setup_args, other_thing bool not null) begin declare args cursor like test_subject arguments; fetch args using primary_id primary_id, -- this came from the default_setup_args result secondary_id secondary_id, -- so did this useful_name name, -- the field names don't have to match fast_mode fast_mode, other_thing other_thing; out args; end;  Then the test code declare test_args cursor like test_subject arguments; fetch test_args from call default_subject_args(0); call test_subject(from test_args);  Importantly, the cursor set operations are all by name so the order doesn't matter. Which means even if there are many arguments you don't have to worry that you got them in the wrong order or that they are the wrong type. Effectively you have a simple call by name strategy and you can easily read off the arguments. You could do something similarly brief with helper functions to provide the default arguments but then you can't readily re-use those arguments in later calls or for verification so this way seems a lot more useful in a test context. When it comes time to validate, probably your test subject is returning a cursor from a select that you want to check. A slightly different call will do the job there. "},{"title":"Cursor Differencing​","type":1,"pageTitle":"One Month Update","url":"/blog/update#cursor-differencing","content":"With the setup above you can verify results very easily. Let's change it a little bit: -- same as before, with a cursor declare results cursor for call test_subject(from test_args); -- get the first row fetch results; declare expected cursor like results; fetch expected using setup_args.primary_id primary_id, setup_args.useful_name name, test_args.other_thing other_thing @dummy_seed(1999); -- dummy values for all other columns -- make a macro like EXPECT_CURSOR_EQ(x,y) for this -- if the cursors are different the result is a string with the first -- different column name and the left and right values ready to print call ExpectNull(cql_cursor_diff_val(expected, result));  ExpectEqual could be create proc ExpectNull(t text) begin if t is not null then call printf('%s\\n', t); -- or whatever throw; end if; end;  All that testing support comes from: cursors in the shape of argumentscleaner fetch/insert syntaxcursors passed as argumentscursor differences It kills a lot of boilerplate resulting in tests that are much clearer. And that's what's been going on for the last month in CG/SQL land. If you got this far thanks for reading. If you didn't get this far, you aren't reading this anyway so thanking you is moot =P Stay safe. Rico for CG/SQL P.S. most of these fragments don't actually compile because of missing schema and maybe the odd typo. If there is interest I'll make a demo that works soup to nuts. "},{"title":"Introducing Virtual Tables","type":0,"sectionRef":"#","url":"/blog/virtual-table-into","content":"","keywords":""},{"title":"Case 1 Example​","type":1,"pageTitle":"Introducing Virtual Tables","url":"/blog/virtual-table-into#case-1-example","content":"create virtual table virt_table using my_module as ( id integer not null, name text );  becomes (to SQLite) CREATE TABLE virt_table USING my_module;  Note: empty arguments USING my_module() are not allowed in the SQLite docs but do seem to work in SQLite. We take the position that no args should be done with no parens, at least for now. "},{"title":"Case 2 Example​","type":1,"pageTitle":"Introducing Virtual Tables","url":"/blog/virtual-table-into#case-2-example","content":"create virtual table virt_table using my_module(foo, 'goo', (1.5, (bar, baz))) as ( id integer not null, name text );  CREATE VIRTUAL TABLE virt_table USING my_module(foo, &quot;goo&quot;, (1.5, (bar, baz)));  This form allows for very flexible arguments but not totally arbitrary arguments, so it can still be parsed and validated. "},{"title":"Case 3 Example​","type":1,"pageTitle":"Introducing Virtual Tables","url":"/blog/virtual-table-into#case-3-example","content":"This case recognizes the popular choice that the arguments are often the actual schema declaration for the table in question. So create virtual table virt_table using my_module(arguments following) as ( id integer not null, name text );  becomes CREATE VIRTUAL TABLE virt_table USING my_module( id INTEGER NOT NULL, name TEXT );  The normalized text (keywords capitalized, whitespace normalized) of the table declaration in the as clause is used as the arguments. "},{"title":"Other details​","type":1,"pageTitle":"Introducing Virtual Tables","url":"/blog/virtual-table-into#other-details","content":"Virtual tables go into their own section in the JSON and they include the module and moduleArgs entries, they are additionally marked isVirtual in case you want to use the same processing code for virtual tables as normal tables. The JSON format is otherwise the same, although some things can't happen in virtual tables (e.g. there is no TEMP option so &quot;isTemp&quot; must be false in the JSON. For purposes of schema processing, virtual tables are on the @recreate plan, just like indices, triggers, etc. This is the only option since the alter table form is not allowed on a virtual table. Semantic validation enforces &quot;no alter statements on virtual tables&quot; as well as other things like, no indices, and no triggers, since SQLite does not support any of those things. Finally, because virtual tables are on the @recreate plan, you may not have foreign keys that reference virtual tables. Such keys seem like a bad idea in any case. "},{"title":"Welcome","type":0,"sectionRef":"#","url":"/blog/welcome","content":"Hello everyone! Thank you for visiting the CG/SQL's blog page. If you would like to read the very first blog announcing the project, please go over to the Facebook's Engineering post published in early Octover 2020. Looking forward to working with all of you! Sincerely, CG/SQL Team","keywords":""},{"title":"Chapter 1: Introduction","type":0,"sectionRef":"#","url":"/cql-guide/ch01","content":"","keywords":""},{"title":"Getting Started​","type":1,"pageTitle":"Chapter 1: Introduction","url":"/cql-guide/ch01#getting-started","content":"Before starting this tutorial, make sure you have built the cql executable first in Building CG/SQL. The &quot;Hello World&quot; program rendered in CQL looks like this: hello.sql -- needed to allow vararg calls to C functions declare procedure printf no check; create proc hello() begin call printf(&quot;Hello, world\\n&quot;); end;  This very nearly works exactly as written but we'll need a little bit of glue to wire it all up. First, assuming you have built cql, you should have the power to do this: $ cql --in hello.sql --cg hello.h hello.c  This will produce the C output files hello.c and hello.h which can be readily compiled. However, hello.c will not have a main -- rather it will have a function like this: hello.c ... void hello(void); ...  The declaration of this function can be found in hello.h. Note: hello.h tries to include cqlrt.h. To avoid configuring include paths for the compiler, you might keep cqlrt.h in the same directory as the examples and avoid that complication. Otherwise you must make arrangements for the compiler to be able to find cqlrt.h either by adding it to an INCLUDE path or by adding some -I options to help the compiler find the source. That hello function is not quite adequate to get a running program, which brings us to the next step in getting things running. Typically you have some kind of client program that will execute the procedures you create in CQL. Let's create a simple one in a file we'll creatively name main.c. A very simple CQL main might look like this: main.c #include &lt;stdlib.h&gt; #include &quot;hello.h&quot; int main(int argc, char **argv) { hello(); return 0; }  Now we should be able to do the following: $ cc -o hello main.c hello.c $ ./hello Hello, world  Congratulations, you've printed &quot;Hello, world&quot; with CG/SQL! "},{"title":"Why did this work?​","type":1,"pageTitle":"Chapter 1: Introduction","url":"/cql-guide/ch01#why-did-this-work","content":"A number of things are going on even in this simple program that are worth discussing: the procedure hello had no arguments, and did not use the database therefore its type signature when compiled will be simply void hello(void); so we know how to call ityou can see the declaration for yourself by examining the hello.c or hello.h since nobody used a database we didn't need to initialize onesince there are no actual uses of SQLite we didn't need to provide that libraryfor the same reason we didn't need to include a reference to the CQL runtimethe function printf was declared &quot;no check&quot;, so calling it creates a regular C call using whatever arguments are provided, in this case a stringthe printf function is declared in stdio.h which is pulled in by cqlrt.h, which appears in hello.c, so it will be available to call in the generated C codeCQL allows string literals with double quotes, and those literals may have most C escape sequences in them, so the &quot;\\n&quot; bit works Normal SQL string literals (also supported) use single quotes and do not allow, or need escape characters other than '' to mean one single quote All of these facts put together mean that the normal, simple linkage rules result in an executable that prints the string &quot;Hello, world&quot; and then a newline. "},{"title":"Variables and Arithmetic​","type":1,"pageTitle":"Chapter 1: Introduction","url":"/cql-guide/ch01#variables-and-arithmetic","content":"Borrowing once again from examples in &quot;The C Programming Language&quot;, it's possible to do significant control flow in CQL without reference to databases. The following program illustrates a variety of concepts: -- needed to allow vararg calls to C functions declare procedure printf no check; -- print a conversion table for temperatures from 0 to 300 create proc conversions() begin declare fahr, celsius integer not null; declare lower, upper, step integer not null; set lower := 0; /* lower limit of range */ set upper := 300; /* upper limit of range */ set step := 20; /* step size */ set fahr := lower; while fahr &lt;= upper begin set celsius := 5 * (fahr - 32) / 9; call printf(&quot;%d\\t%d\\n&quot;, fahr, celsius); set fahr := fahr + step; end; end;  You may notice that both the SQL style -- line prefix comments and the C style /* */ forms [note you haven't used the second for of comment style yet]are acceptable comment forms. Indeed, it's actually quite normal to pass CQL source through the C pre-processor before giving it to the CQL compiler, thereby gaining #define and #include as well as other pre-processing options like token pasting in addition to the aforementioned comment forms. More on this later. Like C, in CQL all variables must be declared before they are used. They remain in scope until the end of the procedure in which they are declared, or they are global scoped if they are declared outside of any procedure. The declarations announce the names and types of the local variables. Importantly, variables stay in scope for the whole procedure even if they are declared within a nested begin and end block. The most basic types are the scalar or &quot;unitary&quot; types (as they are referred to in the compiler) type\taliases\tnotesinteger\tint\ta 32 bit integer long\tlong integer\ta 64 bit integer bool\tboolean\tan 8 bit integer, normalized to 0/1 real\tn/a\ta C double text\tn/a\tan immutable string reference blob\tn/a\tan immutable blob reference object\tn/a\tan object reference Note: SQLite makes no distinction between integer storage and long integer storage, but the declarations tell CQL whether it should use the SQLite methods for binding and reading 64-bit or 32-bit quantities when using the variable or column so declared. There will be more notes on these types later, but importantly, all keywords and names in CQL are case insensitive just like in the underlying SQL language. Additionally all of the above may be combined with not null to indicate that a null value may not be stored in that variable (as in the example). When generating the C code, the case used in the declaration becomes the canonical case of the variable and all other cases are converted to that in the emitted code. As a result the C remains case sensitively correct. The size of the reference types is machine dependent, whatever the local pointer size is. The non-reference types use machine independent declarations like int32_t to get exactly the desired sizes in a portable fashion. All variables of a reference type are set to NULL when they are declared, including those that are declared NOT NULL. For this reason, all nonnull reference variables must be initialized (i.e., assigned a value) before anything is allowed to read from them. This is not the case for nonnull variables of a non-reference type, however: They are automatically assigned an initial value of 0, and thus may be read from at any point. The programs execution begins with three assignments: set lower := 0; set upper := 300; set step := 20;  This initializes the variables just like in the isomorphic C code. Statements are seperated by semicolons, just like in C. The table is then printed using a while loop while fahr &lt;= upper begin ... end;  This has the usual meaning, with the statements in the begin/end block being executed repeatedly until the condition becomes false. The body of a begin/end block such as the one in the while statement can contain one or more statements. The typical computation of Celsius temperature ensues with this code: set celsius := 5 * (fahr - 32) / 9; call printf(&quot;%d\\t%d\\n&quot;, fahr, celsius); set fahr := fahr + step;  This computes the celsuis and then prints it out, moving on to the next entry in the table. Importantly, the CQL compiler uses the normal SQLite order of operations, which is NOT the C order of operations. As a result, the compiler may need to add parentheses in the C output to get the correct order; or it may remove some parentheses because they are not needed in the C order even though they were in the SQL order. The printf call operates as before, with the fahr and celsius variables being passed on to the C runtime library for formatting, unchanged. NOTE: when calling unknown foreign functions like printf string literals are simply passed right through unchanged as C string literals. No CQL string object is created. "},{"title":"Basic Conversion Rules​","type":1,"pageTitle":"Chapter 1: Introduction","url":"/cql-guide/ch01#basic-conversion-rules","content":"As a rule, CQL does not perform its own conversions, leaving that instead to the C compiler. An exception to this is that boolean expressions are normalized to a 0 or 1 result before they are stored. However, even with no explicit conversions, there are compatibility checks to ensure that letting the C compiler do the conversions will result in something sensible. The following list summarizes the essential facts/rules as they might be applied when performing a + operation. the numeric types are bool, int, long, realnon-numeric types cannot be combined with numerics, e.g. 1 + 'x' always yields an errorany numeric type combined with itself yields the same typebool combined with int yields intbool or int combined with long yields longbool, int, or long combined with real yields real "},{"title":"Preprocessing Features​","type":1,"pageTitle":"Chapter 1: Introduction","url":"/cql-guide/ch01#preprocessing-features","content":"CQL does not include its own pre-processor but it is designed to consume the output of the C pre-processor. To do this, you can either write the output of the pre-processor to a temporary file and read it into CQL as usual or you can set up a pipeline something like this: $ cc -x c -E your_program.sql | cql --cg your_program.h your_program.c  The above causes the C compiler to invoke only the pre-processor -E and to treat the input as though it were C code -x c even though it is in a .sql file. Later examples will assume that you have configured CQL to be used with the C pre-processor as above. "},{"title":"Chapter 2: Using Data","type":0,"sectionRef":"#","url":"/cql-guide/ch02","content":"","keywords":""},{"title":"A Sample Program​","type":1,"pageTitle":"Chapter 2: Using Data","url":"/cql-guide/ch02#a-sample-program","content":"Suppose we have the following program: hello.sql -- needed to allow vararg calls to C functions declare procedure printf no check; create table my_data(t text not null); create proc hello() begin insert into my_data(t) values(&quot;Hello, world\\n&quot;); declare t text not null; set t := (select * from my_data); call printf('%s', t); end;  That looks like an interesting little baby program and it appears as though it would once again print that most famous of salutations, &quot;Hello, world&quot;. Well, it doesn't. At least, not yet. Let's walk through the various things that are going to go wrong as this will teach us everything we need to know about activating CQL from some environment of your choice. "},{"title":"Providing a Suitable Database​","type":1,"pageTitle":"Chapter 2: Using Data","url":"/cql-guide/ch02#providing-a-suitable-database","content":"CQL is just a compiler, it doesn't know how the code it creates will be provisioned any more than say clang does. It creates functions with predictable signatures so that they can be called from C just as easily as the SQLite API itself, and using the same currency. Our new version of hello now requires a database handle because it performs database operations. Also there are now opportunities for the database operations to fail, and so hello now provides a return code. A new minimal main program might look something like this: main.c #include &lt;stdlib.h&gt; #include &lt;sqlite3.h&gt; #include &quot;hello.h&quot; int main(int argc, char **argv) { sqlite3 *db; int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db); if (rc != SQLITE_OK) { exit(1); /* not exactly world class error handling but that isn't the point */ } rc = hello(db); if (rc != SQLITE_OK) { exit(2); } sqlite3_close(db); }  If we re-run CQL and look in the hello.h output file we'll see that the declaration of the hello function is now: hello.h ... extern CQL_WARN_UNUSED cql_code hello(sqlite3 *_Nonnull _db_); ...  This indicates that the database is used and a SQLite return code is provided. We're nearly there. If you attempt to build the program as before there will be several link-time errors due to missing functions. Typically these are resolved by providing the SQLite library to the command line and also adding the CQL runtime. The new command line looks something like this: $ cc -o hello main.c hello.c cqlrt.c -lsqlite3 $ ./hello Hello, world  The cql runtime can be anywhere you want it to be, and of course the usual C separate compilation methods can be applied. More on that later. But actually, that program doesn't quite work yet. If you run it, you'll get an error result code, not the message &quot;Hello, world&quot;. Let's talk about the final missing bit. "},{"title":"Declaring Schema​","type":1,"pageTitle":"Chapter 2: Using Data","url":"/cql-guide/ch02#declaring-schema","content":"In CQL a loose piece of Data Definition Language (henceforth DDL) does not actually create or drop anything. In most CQL programs the normal situation is that &quot;something&quot; has already created the database and put some data in it. You need to tell the CQL compiler about the schema so that it knows what the tables are and what to expect to find in those tables. This is because typically you're reconnecting to some sort of existing database. So, in CQL, loose DDL simply declares schema, it does not create it. To create schema you have to put the DDL into a procedure you can run. If you do that, then the DDL still serves a declaration, but also the schema will be created when the procedure is executed. We need to change our program a tiny bit. hello.sql -- needed to allow vararg calls to C functions declare procedure printf no check; create proc hello() begin create table my_data(t text not null); insert into my_data(t) values(&quot;Hello, world\\n&quot;); declare t text not null; set t := (select * from my_data); call printf('%s', t); drop table my_data; end;  If we rebuild the program, it will now behave as expected. "},{"title":"Explaining The New Hello World​","type":1,"pageTitle":"Chapter 2: Using Data","url":"/cql-guide/ch02#explaining-the-new-hello-world","content":"Let's go over every important line of the new program, starting from main. int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db);  This statement gives us an empty, private, in-memory only database to work with. This is the simplest case and it's still very useful. The sqlite_open and sqlite_open_v2 functions can be used to create a variety of databases per the SQLite documentation. We'll need such a database to use our procedure, and we use it in the call here: rc = hello(db);  This provides a valid db handle to our procedure. Note that the procedure doesn't know what database it is supposed to operate on, it expects to be handed a suitable database on a silver platter. In fact any given proc could be used with various databases at various times. Just like SQLite, CQL does not enforce any particular database setup; it does what you tell it to. When hello runs we begin with create table my_data(t text not null);  This will create the my_data table with a single column t, of type text not null. That will work because we know we're going to call this with a fresh/empty database. More typically you might do create table if not exists ... or otherwise have a general attach/create phase or something to that effect. We'll dispense with that here. Next we'll run the insert statement: insert into my_data(t) values(&quot;Hello, world\\n&quot;);  This will add a single row to the table. Note that we have again used double quotes, meaning that this is a C string literal. This is highly convenient given the escape sequences. Normally SQLite text has the newlines directly embedded in it; that practice isn't very compiler friendly, hence the alternative. Next we declare a local variable to hold our data: declare t text not null;  Then, we can read back our data: set t := (select * from my_data);  This form of database reading has very limited usability but it does work for this case and it is illustrative. The presence of (select ...) indicates to the CQL compiler that the parenthesized expression should be given to SQLite for evaluation according to the SQLite rules. The expression is statically checked at compile time to ensure that it has exactly one result column. In this case the * is just column t, and actually it would have been clearer to use t directly here but then there wouldn't be a reason to talk about * and multiple columns. At run time, the select query must return exactly one row or an error code will be returned. It's not uncommon to see (select ... limit 1) to force the issue. But that still leaves the possibility of zero rows, which would be an error. We'll talk about more flexible ways to read from the database later. You can declare a variable and assign it in one step with the LET keyword, e.g. let t := (select * from my_data); The code would normally be written in this way but for discussion purposes, these examples continue to avoid LET. At this point it seems wise to bring up the unusual expression evaluation properties of CQL. CQL is by necessity a two-headed beast. On the one side there is a rich expression evaluation language for working with local variables. [What about the other side?] Those expressions are compiled into C logic that emulates the behavior of SQLite on the data. It provides complex expression constructs such as IN and CASE but it is ultimately evaluated by C execution. Alternately, anything that is inside of a piece of SQL is necessarily evaluated by SQLite itself. To make this clearer let's change the example a little bit before we move on. set t := (select &quot;__&quot;||t||' '||1.234 from my_data);  This is a somewhat silly example but it illustrates some important things: even though SQLite doesn't support double quotes, that's no problem because CQL will convert the expression into single quotes with the correct escape values as a matter of course during compilationthe || concatenation operator is evaluated by SQLiteyou can mix and match both kinds of string literals, they will all be the single quote variety by the time SQLite sees themthe || operator has lots of complex formatting conversions (such as converting real values to strings)in fact the conversions are so subtle as to be impossible to emulate in loose C code with any economy, so, like a few other operators, || is only supported in the SQLite context Returning now to our code as written, we see something very familiar: call printf('%s', t);  Note that we've used the single quote syntax here for no good reason other than illustration. There are no escape sequences here so either form would do the job. Importantly, the string literal will not create a string object as before but the text variable t is of course a string reference. Before it can be used in a call to an un-declared function it must be converted into a temporary C string. This might require allocation in general, that allocation is automatically managed. Also, note that CQL assumes that calls to &quot;no check&quot; functions should be emitted as written. In this way you can useprintf even though CQL knows nothing about it. Lastly we have: drop table my_data;  This is not strictly necessary because the database is in memory anyway and the program is about to exit but there it is for illustration. Now the Data Manipulation Language (i.e. insert and select here; and henceforth DML) and the DDL might fail for various reasons. If that happens the proc will goto a cleanup handler and return the failed return code instead of running the rest of the code. Any temporary memory allocations will be freed and any pending SQLite statements will be finalized. More on that later when we discuss error handling. With that we have a much more complicated program that prints &quot;Hello, world&quot; "},{"title":"Introducing Cursors​","type":1,"pageTitle":"Chapter 2: Using Data","url":"/cql-guide/ch02#introducing-cursors","content":"In order to read data with reasonable flexibility, we need a more powerful construction. Let's change our example again and start using some database features. declare procedure printf no check; create proc hello() begin create table my_data( pos integer not null primary key, txt text not null ); insert into my_data values(2, 'World'); insert into my_data values(0, 'Hello'); insert into my_data values(1, 'There'); declare C cursor for select * from my_data order by pos; loop fetch C begin call printf(&quot;%d: %s\\n&quot;, C.pos, C.txt); end; close C; drop table my_data; end;  Reviewing the essential parts of the above. create table my_data( pos integer not null primary key, txt text not null );  The table now includes a position column to give us some ordering. That is the primary key. insert into my_data values(2, 'World');  The insert statements provide both columns, not in the printed order. The insert form where the columns are not specified indicates that all the columns will be present, in order; this is more economical to type. CQL will generate errors at compile time if there are any missing columns or if any of the values are not type compatible with the indicated column. The most important change is here: declare C cursor for select * from my_data order by pos;  We've created a non-scalar variable C, a cursor over the indicated result set. The results will be ordered by pos. loop fetch C begin ... end;  This loop will run until there are no results left (it might not run at all if there are zero rows, that is not an error). The FETCH construct allows you to specify target variables, but if you do not do so, then a synthetic structure is automatically created to capture the projection of the select. In this case the columns are pos and txt. The automatically created storage exactly matches the type of the columns in the select list which could itself be tricky to calculate if the select is complex. In this case the select is quite simple and the columns of the result directly match the schema for my_data. An integer and a string reference. Both not null. call printf(&quot;%d: %s\\n&quot;, C.pos, C.txt);  The storage for the cursor is given the same names as the columns of the projection of the select, in this case the columns were not renamed so pos and txt are the fields in the cursor. Double quotes were used in the format string to get the newline in there easily. close C;  The cursor is automatically released at the end of the procedure but in this case we'd like to release it before thedrop table happens so there is an explicit close. This is frequently elided in favor of the automatic cleanup. There is an open cursor statement as well but it doesn't do anything. It's there because many systems have that construct and it does balance the close. If you compile and run this program, you'll get this output: $ cc -x c -E hello.sql | cql --cg hello.h hello.c $ cc -o hello main.c hello.c cqlrt.c -lsqlite3 $ ./hello 0: Hello 1: There 2: World  So the data was inserted and then sorted. "},{"title":"Going Crazy​","type":1,"pageTitle":"Chapter 2: Using Data","url":"/cql-guide/ch02#going-crazy","content":"We've only scratched the surface of what SQLite can do and most DML constructs are supported by CQL. This includes common table expressions, and even recursive versions of the same. But remember, when it comes to DML, the CQL compiler only has to validate the types and figure out what the result shape will be -- SQLite always does all the heavy lifting of evaluation. All of this means with remarkably little additional code, the example below from the SQLite documentation can be turned into a CQL stored proc using the constructs we have defined above. -- needed to allow vararg calls to C functions declare procedure printf no check; create proc mandelbrot() begin -- this is basically one giant select statement declare C cursor for with recursive -- x from -2.0 to +1.2 xaxis(x) as (select -2.0 union all select x + 0.05 from xaxis where x &lt; 1.2), -- y from -1.0 to +1.0 yaxis(y) as (select -1.0 union all select y + 0.1 from yaxis where y &lt; 1.0), m(iter, cx, cy, x, y) as ( -- initial seed iteration count 0, at each of the points in the above grid select 0 iter, x cx, y cy, 0.0 x, 0.0 y from xaxis, yaxis union all -- the next point is always iter +1, same (x,y) and the next iteration of z^2 + c select iter+1 iter, cx, cy, x*x-y*y + cx x, 2.0*x*y + cy y from m -- stop condition, the point has escaped OR iteration count &gt; 28 where (m.x*m.x + m.y*m.y) &lt; 4.0 and m.iter &lt; 28 ), m2(iter, cx, cy) as ( -- find the last iteration for any given point to get that count select max(iter), cx, cy from m group by cx, cy ), a(t) as ( -- convert the iteration count to a printable character, grouping by line select group_concat(substr(&quot; .+*#&quot;, 1 + min(iter/7,4), 1), '') from m2 group by cy ) -- group all the lines together select rtrim(t) line from a; -- slurp out the data loop fetch C begin call printf(&quot;%s\\n&quot;, C.line); end; end;  This code uses all kinds of SQLite features to produce this text: $ ....# ..#*.. ..+####+. .......+####.... + ..##+*##########+.++++ .+.##################+. .............+###################+.+ ..++..#.....*#####################+. ...+#######++#######################. ....+*################################. #############################################... ....+*################################. ...+#######++#######################. ..++..#.....*#####################+. .............+###################+.+ .+.##################+. ..##+*##########+.++++ .......+####.... + ..+####+. ..#*.. ....# +.  Which probably doesn't come up very often but it does illustrate several things: WITH RECURSIVE actually provides a full lambda calculus so arbitrary computation is possibleYou can use WITH RECURSIVE to create table expressions that are sequences of numbers easily, with no reference to any real data Note: A working version of this code can be found in the sources/demo directory of CG/SQL project.Additional demo code is available in Appendix 10. "},{"title":"Chapter 4: Procedures, Functions, and Control Flow","type":0,"sectionRef":"#","url":"/cql-guide/ch04","content":"","keywords":""},{"title":"Out Parameters​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#out-parameters","content":"Consider this procedure: create procedure echo_integer(in arg1 integer not null, out arg2 integer not null) begin set arg2 := arg1; end;  arg1 has been declared in. This is the default: in arg1 integer not nulland arg1 integer not null mean the exact same thing. arg2, however, has been declared out. When a parameter is declared usingout, arguments for it are passed by reference. This is similar to by-reference arguments in other languages; indeed, they compile into a simple pointer reference in the generated C code. Given that arg2 is passed by reference, the statement set arg2 := arg1;actually updates a variable in the caller. For example: declare x int not null; call echo_integer(42, x); -- `x` is now 42  It is important to note that values cannot be passed into a procedure via anout parameter. In fact, out parameters are immediately assigned a new value as soon as the procedure is called: All nullable out parameters are set to null. Nonnull out parameters of a non-reference type (e.g., integer, long,bool, et cetera) are set to their default values (0, 0.0, false, et cetera). Nonnull out parameters of a reference type (e.g., blob, object, andtext) are set to null as there are no default values for reference types. They must, therefore, be assigned a value within the procedure so that they will not be null when the procedure returns. CQL enforces this. In addition to in and out parameters, there are also inout parameters.inout parameters are, as one might expect, a combination of in and outparameters: The caller passes in a value as with in parameters, but the value is passed by reference as with out parameters. inout parameters allow for code such as the following: create procedure times_two(inout arg integer not null) begin -- note that a variable in the caller is both -- read from and written to set arg := arg + arg; end; let x := 2; call times_two(x); -- `x` is now 4  "},{"title":"Procedure Calls​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#procedure-calls","content":"The usual call syntax is used to invoke a procedure. It returns no value but it can have any number of out arguments.  declare scratch integer not null; call echo_integer(12, scratch); scratch == 12; -- true  Let's go over the most essential bits of control flow. "},{"title":"The IF statement​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#the-if-statement","content":"The CQL IF statement has no syntatic ambiguities at the expense of being somewhat more verbose than many other languages. In CQL the ELSE IF portion is baked into the IF statement, so what you see below is logically a single statement. create proc checker(foo integer, out result integer not null) begin if foo = 1 then set result := 1; else if foo = 2 then set result := 3; else set result := 5; end if; end;  "},{"title":"The WHILE statement​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#the-while-statement","content":"What follows is a simple procedure that counts down its input argument. declare procedure printf no check; create proc looper(x integer not null) begin while x &gt; 0 begin call printf('%d\\n', x); set x := x - 1; end; end;  The WHILE loop has additional keywords that can be used within it to better control the loop. A more general loop might look like this: declare procedure printf no check; create proc looper(x integer not null) begin while 1 begin set x := x - 1; if x &lt; 0 then leave; else if x % 100 = 0 then continue; else if x % 10 = 0 then call printf('%d\\n', x); end if; end; end;  Let's go over this peculiar loop:  while 1 begin ... end;  This is an immediate sign that there will be an unusual exit condition. The loop will never end without one because 1 will never be false.  if x &lt; 0 then leave;  Now here we've encoded our exit condition a bit strangely: we might have done the equivalent job with a normal condition in the predicate part of the while statement but for illustration anyway, when x becomes negative leave will cause us to exit the loop. This is likebreak in C.  else if x % 100 = 0 then continue;  This bit says that on every 100th iteration we go back to the start of the loop. So the next bit will not run, which is the printing.  else if x % 10 = 0 then call printf('%d\\n', x); end if;  Finishing up the control flow, on every 10th iteration we print the value of the loop variable. "},{"title":"The SWITCH Statement​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#the-switch-statement","content":"The CQL SWITCH is designed to map to the C switch statement for better codegen and also to give us the opportunity to do better error checking.SWITCH is a statement like IF not an expression like CASE..WHEN..END so it combines with other statements. The general form looks like this: SWITCH switch-expression [optional ALL VALUES] WHEN expr1, expr2, ... THEN [statement_list] WHEN expr3, ... THEN [statement_list] WHEN expr4 THEN NOTHING ELSE [statement_list] END;  the switch-expression must be a not-null integral type (integer not null or long integer not null)the WHEN expressions [expr1, expr2, etc.] are made from constant integer expressions (e.g. 5, 1+7, 1&lt;&lt;2, or my_enum.thing)the WHEN expressions must be compatible with the switch expression (long constants cannot be used if the switch expression is an integer)the values in the WHEN clauses must be unique (after evaluation)within one of the interior statement lists the LEAVE keyword exits the SWITCH prematurely, just like break in C a LEAVE is not required before the next WHENthere are no fall-through semantics as you can find in C, if fall-through ever comes to SWITCH it will be explicit if the keyword NOTHING is used after THEN it means there is no code for that case, which is useful with ALL VALUES (see below)the ELSE clause is optional and works just like default in C, covering any cases not otherwise explicitly listedif you add ALL VALUES then: the expression must be an from an enum typethe WHEN values must cover every value of the enum enum members that start with a leading _ are by convention considered pseudo values and do not need to be covered there can be no extra WHEN values not in the enumthere can be no ELSE clause (it would defeat the point of listing ALL VALUES which is to get an error if new values come along) Some more complete examples: let x := get_something(); switch x when 1,1+1 then -- constant expressions ok set y := 'small'; -- other stuff when 3,4,5 then set y := 'medium'; -- other stuff when 6,7,8 then set y := 'large'; -- other stuff else set y := 'zomg enormous'; end; declare enum item integer ( pen = 0, pencil, brush, paper, canvas, _count ); let x := get_item(); -- returns one of the above switch x all values when item.pen, item.pencil then call write_something(); when item.brush then nothing -- itemize brush but it needs no code when item.paper, item.canvas then call setup_writing(); end;  Using THEN NOTHING allows the compiler to avoid emitting a useless break in the C code. Hence that choice is better/clearer than when brush then leave; Note that the presence of _count in the enum will not cause an error in the above because it starts with _. The C output for this statement will be a direct mapping to a C switch statement. "},{"title":"The TRY, CATCH, and THROW Statements​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#the-try-catch-and-throw-statements","content":"This example illustrates catching an error from some DML, and recovering rather than letting the error cascade up. This is the common &quot;upsert&quot; pattern (insert or update) declare procedure printf no check; create procedure upsert_foo(id_ integer, t_ text) begin begin try insert into foo(id, t) values(id_, t_) end try; begin catch begin try update foo set t = t_ where id = id_; end try; begin catch call printf(&quot;Error code %d!\\n&quot;, @rc); throw; end catch; end catch; end;  Once again, let's go over this section by section:  begin try insert into foo(id, t) values(id_, t_) end try;  Normally if the insert statement fails, the procedure will exit with a failure result code. Here, instead, we prepare to catch that error.  begin catch begin try update foo set t = t_ where id = id_; end try;  Now, having failed to insert, presumably because a row with the provided id already exists, we try to update that row instead. However that might also fail, so we wrap it in another try. If the update fails, then there is a final catch block:  begin catch call printf(&quot;Error code %d!\\n&quot;, @rc); throw; end catch;  Here we see a usage of the @rc variable to observe the failed error code. In this case we simply print a diagnostic message and then use the throw keyword to rethrow the previous failure (exactly what is stored in @rc). In general, throw will create a failure in the current block using the most recent failed result code from SQLite (@rc) if it is an error, or else the generalSQLITE_ERROR result code if there is no such error. In this case the failure code for the update statement will become the result code of the current procedure. This leaves only the closing markers:  end catch; end;  If control flow reaches the normal end of the procedure it will return SQLITE_OK. "},{"title":"Procedures as Functions: Motivation and Example​","type":1,"pageTitle":"Chapter 4: Procedures, Functions, and Control Flow","url":"/cql-guide/ch04#procedures-as-functions-motivation-and-example","content":"The calling convention for CQL stored procedures often (usually) requires that the procedure returns a result code from SQLite. This makes it impossible to write a procedure that returns a result like a function, as the result position is already used for the error code. You can get around this problem by using out arguments as your return codes. So for instance, this version of the Fibonacci function is possible. -- this works, but it is awkward create procedure fib (in arg integer not null, out result integer not null) begin if (arg &lt;= 2) then set result := 1; else declare t integer not null; call fib(arg - 1, result); call fib(arg - 2, t); set result := t + result; end if; end;  The above works, but the notation is very awkward. CQL has a &quot;procedures as functions&quot; feature that tries to make this more pleasant by making it possible to use function call notation on a procedure whose last argument is an out variable. You simply call the procedure like it was a function and omit the last argument in the call. A temporary variable is automatically created to hold the result and that temporary becomes the logical return of the function. For semantic analysis, the result type of the function becomes the type of the out argument. -- rewritten with function call syntax create procedure fib (in arg integer not null, out result integer not null) begin if (arg &lt;= 2) then set result := 1; else set result := fib(arg - 1) + fib(arg - 2); end if; end;  This form is allowed when: all but the last argument of the procedure was specifiedthe formal parameter for that last argument was marked with out (neither in nor inout are acceptable)the procedure does not return a result set using a select statement or out statement (more on these later) If the procedure in question uses SQLite, or calls something that uses SQLite, then it might fail. If that happens the result code will propagate just like it would have with the usual call form. Any failures can be caught with try/catch as usual. This feature is really only syntatic sugar for the &quot;awkward&quot; form above, but it does allow for slightly better generated C code. "},{"title":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","type":0,"sectionRef":"#","url":"/cql-guide/ch03","content":"","keywords":""},{"title":"Expression Examples​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#expression-examples","content":"The usual arithmetic operators apply in CQL: Example expressions (these are all true) (1 + 2) * 3 == 9 1 + 2 * 3 == 7 6 / 3 == 2 7 - 5 == 2 6 % 5 == 1 5 / 2.5 == 2 7 &amp; 3 == 2 | 1 1 &lt;&lt; 2 == 4  However, before going any further it's important to note that CQL is inherently a two-headed beast. Expressions are either evaluated by transpiling to C (like the predicate of an IF statement, or a variable assignment) or by sending them to SQLIte for evaluation (like expressions inside a SELECT statement or the WHERE part of a DELETE). CQL evaluation rules are designed to be as similar as possible but some variance is inevitable because evaluation is done in two fundamentally different ways. "},{"title":"Operator Precedence​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#operator-precedence","content":"The operator precedence rules in CQL are as follows; the top-most rule binds the most loosely and the bottom-most rule binds the most tightly: ASSIGNMENT: := LOGICAL_OR: OR LOGICAL_AND: AND LOGICAL_NOT: NOT EQUALITY: = == != &lt;&gt; IS [NOT], [NOT] IN, [NOT] LIKE, [NOT] MATCH, [NOT] GLOB, [NOT] BETWEEN INEQUALITY: &lt; &lt;= &gt; &gt;= BINARY: &lt;&lt; &gt;&gt; &amp; | ADDITION: + - MULTIPLICATION: * / % CONCAT: || COLLATE: COLLATE UNARY: ~ -  The above rules are not the same as C's operator precedence rules! Instead, CQL follows SQLite's rules. Parentheses are emitted in the C output as needed to force that order. NOTE: CQL emits minimal parentheses in all outputs. Different parentheses are often needed for SQL output as opposed to C output. "},{"title":"Order of Evaluation​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#order-of-evaluation","content":"In contrast to C, CQL guarantees a left-to-right order of evaluation for arguments. This applies both to arguments provided to the operators mentioned in the previous section as well as arguments provided to procedures. "},{"title":"Variables, Columns, Basic Types and Nullability​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#variables-columns-basic-types-and-nullability","content":"CQL needs type information for both variables in the code and columns in the database. Like SQL, CQL allows variables to hold a NULL value and just as in SQL the absence of NOT NULL implies that NULL is a legal value. Consider these examples: -- real code should use better names than this :) create table all_the_nullables( i1 integer, b1 bool, l1 long, r1 real, t1 text, bl1 blob ); declare i2 integer; declare b2 bool; declare l2 long; declare r2 real; declare t2 text; declare bl2 blob;  ALL of i1, i2, b1, b2, l1, l2, r1, r2, t1, t2, and bl1, bl2 are nullable. In some sense variables and columns declared nullable (by virtue of the missing NOT NULL) are the root sources of nullability in the SQL language. That and the NULL literal. Though there are other sources as we will see. NOT NULL could be added to any of these, e.g. -- real code should use better names than this :) declare i_nn integer not null;  In the context of computing the types of expressions, CQL is statically typed and so it must make a decision about the type of any expression based on the type information at hand at compile time. As a result it handles the static type of an expression conservatively. If the result might be null then the expression is of a nullable type and the compiled code will include an affordance for the possibility of a null value at runtime. The generated code for nullable types is considerably less efficient and so it should be avoided if that is reasonably possible. LET Statement​ You can declare and initialize a variable in one step using the LET form, e.g. LET x := 1;  The named variable is declared to be the exact type of the expression on the right. More on expressions in the coming sections. The right side is often a constant in these cases but does not need to be. LET i := 1; -- integer not null LET l := 1L; -- long not null LET t := &quot;x&quot;; -- text not null LET b := x IS y; -- bool not null LET b := x = y; -- bool (maybe not null depending on x/y)  The pseudo function &quot;nullable&quot; removes not null from the type of its argument but otherwise does no computation. This can be useful to initialize nullable types. LET n_i := nullable(1); -- nullable integer variable initialized to 1 LET n_l := nullable(1L); -- nullable long variable initialized to 1  The pseudo function &quot;sensitive&quot; adds @sensitive to the type of its argument but otherwise does no computation. This also can be useful to initialize nullable types. LET s_i := sensitive(1); -- sensitive nullable integer variable initialized to 1 LET s_l := sensitive(1L); -- sensitive nullable long variable initialized to 1  The @RC special variable​ CQL also has the special built-in variable @RC which refers to the most recent error code returned by a SQLite operation, e.g. 0 == SQLITE_OK, 1 == SQLITE_ERROR. @RC is of type integer not null. Specifically: each catch block captures the error code when it is entered into its own local variablethis variable is created lazily, so it only exists if it is used the variable is called _rc_thrown_n where n is the catch block number in the procedure any reference to @RC refers to the above error variable of the innermost catch block the @RC reference is inif the @RC reference happens outside of any catch block its value is SQLITE_OK (i.e. zero). "},{"title":"Types of Literals​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#types-of-literals","content":"There are a number of literal objects that may be expressed in CQL. These are as follows: String Literals​ A double quoted string is a C style string literal the usual simple C escape sequences are supportedthe \\xNN form for embedded hex characters is supported, howeverthe \\0NNN octal form is not supported, andembedded nulls in string literals (\\0 or \\0x00) are not supported (you must use blobs in such cases) A single quoted string is a SQL style string literal No escape sequences are supported other than '' to indicate a single quote character (this is just like normal SQLite) A sequence of single or double quoted strings separated by whitespace such as &quot;xx&quot; 'yy' &quot;zz&quot; which are concatenated to make one literalThe sequence @FILE(&quot;some_string&quot;) is a special string literal the value of this literal is the path of the current compiland starting at the letters in some_string, orthe entire path of the current compiland if some_string does not occur in the paththe purpose of the @FILE construct is to provide a partial path to a file for diagnostics that is consistent even if the file is built in various different root paths on different build machines Blob Literals​ SQLite Blob literals are supported in SQL contexts (i.e. where they will be processed by SQLite), CQL produces an error if you attempt to use a blob literal in a loose expression Numeric Literals​ All numeric literals are considered to be positive; negative numbers are actually a positive literal combined with unary minus (the negation operator)Base 10 and hexadecimal literals are supportedLiterals with a decimal point are of type REAL and stored as the C type doubleLiterals that can fit in a signed integer without loss, and do not end in the letter L are integer literalsLarger literals, or those ending with the letter L are long integer literals.Literals that begin with 0x are interpreted as hex Examples:  1.3 -- real 2L -- long 123456789123 -- long 123 -- integer 0x10 -- hex integer 0x10L -- hex long integer  The NULL literal​ The use of NULL always gives a nullable result however this literal is special in that it has no storage class. NULL is neither numeric nor string itself but rather mutates into whatever it is first combined with. For instance NULL + 1 results in a nullable integer. Because NULL has no primitive type in some cases where type knowledge is required you might have to use the CAST() function to cast the NULL to a specific type such as CAST(NULL as TEXT). This construct guarantees type consistence in cases like SELECT from different sources combined with UNION ALL Note: constructs like CAST(NULL as TEXT) are always rewritten to just NULL before going to SQLite as the cast is uninteresting except for the type information which SQLite doesn't need/use anyway. Other Considerations​ There are no boolean literals other than the integers 0 and 1. The C pre-processor is often combined with CQL in which case the _FILE_ and _LINE_ directives may be used to create literals; they will be preprocessed into normal literals. The use of _FILE_ can give surprising results in the presence of build systems, hence the existence of @FILE(...). "},{"title":"Const and Enumerations​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#const-and-enumerations","content":"It's possible to use named constants in CQL with nothing more than the C pre-processor features that have already appeared, however use of #define in such a way is not entirely satisfactory. For one thing, CQL will not know these constants exist in any way as they will be replaced before it ever sees them. This means CQL can't provide their values for you in the JSON output for instance. To help with this problem, CQL includes constants, note, this is not the same as enumerated types as we'll see later. You can now write something like this: declare enum business_type integer ( restaurant, laundromat, corner_store = 11+3 /* math added for demo purposes only */ );  After this enum is declared, this: select business_type.corner_store;  is the same as this: select 14;  And that is exactly what SQLite will see, the literal 14. You can also use the enum to define column types: CREATE TABLE businesses ( name TEXT, type business_type );  CQL will then enforce that you use the correct enum to access those columns. For example, this is valid: SELECT * FROM businesses WHERE type = business_type.laundromat;  While this does not type check: SELECT * FROM businesses WHERE type = business_corp_state.delaware;  Enumerations follow these rules: the enumeration can be any numeric type (bool, integer, long integer, real)the values of the enumeration start at 1 (i.e. if there is no = expression the first item will be 1, not 0)if you don't specify a value, the next value is the previous value plus oneif you do specify a value it can be any constant expression and it will be cast to the type of the enumeration (even if that is lossy)the enumeration can refer to previous values in itself with no qualification (big = 100.0, medium = big/2, small = medium/2)the enumeration can refer to previously defined enumerations as usual (code = business_type.restaurant)once the enumeration is defined you refer to its members in a fully qualified fashion enum_name.member_name elsewhere With these forms you get some additional useful output: the JSON includes the enumerations and their values in their own sectionyou can use the @emit_enums directive to put declarations like this into the .h file that corresponds to the current compiland enum business_type { business_type__restaurant = 1, business_type__laundromat = 2, business_type__corner_store = 14 };  Note that C does not allow for floating point enumerations, so in case of floating point values such as: declare enum floating real ( one = 1.0, two = 2.0, e = 2.71828, pi = 3.14159 );  you get: // enum floating (floating point values) #define floating__one 1.000000e+00 #define floating__two 2.000000e+00 #define floating__e 2.718280e+00 #define floating__pi 3.141590e+00  In order to get useful expressions in enumeration values, constant folding and general evaluation was added to the compiler; these expressions work on any numeric type and the literal null. The supported operations include: +, -, *, /, %, |, &amp;, &lt;&lt;, &gt;&gt;, ~, and, or, not, ==, &lt;=, &gt;=, !=, &lt;, &gt;, the cast operator and the case forms (including the iif function). These are enough to make a lot of very interesting expressions, all of which are evaluated at compile time. Constant folding was added to allow for rich enum expressions, but there is also the const() primitive in the language which can appear anywhere a literal could appear. This allows you do things like: create table something( x integer default const((1&lt;&lt;16)|0xf) /* again the math is just for illustration */ );  The const form is also very useful in macros: #define SOMETHING const(12+3)  This form ensures that the constant will be evaluated at compile time. The const pseudo-function can also nest so you can build these kinds of macros from other macros or you can build enum values this way. Anywhere you might need literals, you can use const. "},{"title":"Named Types​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#named-types","content":"A common source of errors in stored procedures is incorrect typing in arguments. For instance, a particular key for an entity might need to be LONG or even always LONG NOT NULL or LONG NOT NULL @SENSITIVE and the only way to do this in the past was maybe with some #define thing. Otherwise you have to diligently get the type right in all the places, and should it ever change, again you have to visit all the places. To help with this situation, and to make the code a little more self-describing we added named types to the language. This is a lot like typedef in the C language. They do not create different incompatible types but they do let you name things well. You can now write these sorts of forms: declare foo_id type long not null; create table foo( id foo_id primary key autoincrement, name text ); create proc inserter(name_ text, out id foo_id) begin insert into foo(id, name) values(NULL, name_); set id := last_insert_rowid(); end; declare function func_return_foo_id() foo_id; declare var foo_id;  Additionally any enumerated type can be used as a type name. e.g. declare enum thing integer ( thing1, thing2 ); declare thing_type type thing;  Enumerations always get &quot;not null&quot; in addition to their base type. Enumerations also have a unique &quot;kind&quot; associated, specifically the above enum has type integer&lt;thing&gt; not null. The rules for type kinds are described below. "},{"title":"Type Kinds​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#type-kinds","content":"Any CQL type can be tagged with a &quot;kind&quot; for instance real can become real&lt;meters&gt;, integer can become integer&lt;job_id&gt;. The idea here is that the additional tag, the &quot;kind&quot; can help prevent type mistakes in arguments, in columns and in procedure calls. For instance: create table things( size real&lt;meters&gt;, duration real&lt;seconds&gt; ); create proc do_something(size_ real&lt;meters&gt;, duration_ real&lt;seconds&gt;) begin insert into things(size, duration) values(size_, duration_); end;  In this situation you couldn't accidentally switch the columns in do_something even though both are real, and indeed SQLite will only see the type real for both. If you have your own variables typed real&lt;size&gt; and real&lt;duration&gt; you can't accidentally do:  call do_something(duration, size);  even though both are real. The type kind won't match. Importantly, an expression with no type kind is compatible with any type kind (or none). Hence all of the below are legal. declare generic real; set generic := size; -- no kind may accept &lt;meters&gt; set generic := duration; -- no kind may accept &lt;seconds&gt; set duration := generic; -- no kind may be stored in &lt;seconds&gt;  Only mixing types where both have a kind, and the kind is different generates errors. This choice allows you to write procedures that (for instance) log any integer or any real, or that return an integer out of a collection. These rules are applied to comparisons, assignments, column updates, anywhere and everywhere types are checked for compatibility. To get the most value out of these constructs, the authors recommend that type kinds be used universally except when the extra compatibility described above is needed (like low level helper functions.) Importantly, type kind can be applied to object types as well, allowing object&lt;dict&gt; to be distinct from object&lt;list&gt;. At run time the kind information is lost. But it does find it's way into the JSON output so external tools also get to see the kinds. "},{"title":"Nullability​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#nullability","content":"Nullability Rules​ Nullability is tracked via CQL's type system. To understand whether or not an expression will be assigned a nullable type, you can follow these rules; they will hopefully be intuitive if you are familiar with SQL: The literal NULL is, of course, always assigned a nullable type. All other literals are nonnull. In general, the type of an expression involving an operator (e.g., +, ==,!=, ~, LIKE, et cetera) is nullable if any of its arguments are nullable. For example, 1 + NULL is assigned the type INTEGER, implying nullability. 1 + 2, however, is assigned the type INTEGER NOT NULL. IN and NOT IN expressions are assigned a nullable type if and only if their left argument is nullable: The nullability of the right side is irrelevant. For example, &quot;foo&quot; IN (a, b) will always have the type BOOL NOT NULL, whereas some_nullable IN (a, b) will have the type BOOL. NOTE: In CQL, the IN operator behaves like a series of equality tests (i.e., == tests, not IS tests), and NOT IN behaves symmetrically. SQLite has slightly different nullability rules for IN and NOT IN. This is the one place where CQL has different evaluation rules from SQLite, by design. The result of IS and IS NOT is always of type BOOL NOT NULL, regardless of the nullability of either argument. For CASE expressions, the result is always of a nullable type if no ELSEclause is given. If an ELSE is given, the result is nullable if any of theTHEN or ELSE expressions are nullable. NOTE: The SQL CASE construct is quite powerful: Unlike the C switchstatement, it is actually an expression. In this sense, it is rather more like a highly generalized ternary a ? b : c operator than a C switch statement. There can be arbitrarily many conditions specified, each with their own result, and the conditions need not be constants; typically, they are not. IFNULL and COALESCE are assigned a NOT NULL type if one or more of their arguments are of a NOT NULL type. In most join operations, the nullability of each column participating in the join is preserved. However, in a LEFT OUTER join, the columns on the right side of the join are always considered nullable; in a RIGHT OUTER join, the columns on the left side of the join are considered nullable. As in most other languages, CQL does not perform evaluation of value-level expressions during type checking. There is one exception to this rule: An expression within a const is evaluated at compilation time, and if its result is then known to be nonnull, it will be given a NOT NULL type. For example, const(NULL or 1) is given the type BOOL NOT NULL, whereas merelyNULL or 1 has the type BOOL. Nullability Improvements​ CQL is able to &quot;improve&quot; the type of some expressions from a nullable type to aNOT NULL type via occurrence typing, also known as flow typing. There are three kinds of improvements that are possible: Positive improvements, i.e., improvements resulting from the knowledge that some condition containing one or more AND-linked IS NOT NULL checks must have been true: IF statements: IF a IS NOT NULL AND c.x IS NOT NULL THEN -- `a` and `c.x` are not null here ELSE IF b IS NOT NULL THEN -- `b` is not null here END IF; CASE expressions: CASE WHEN a IS NOT NULL AND c.x IS NOT NULL THEN -- `a` and `c.x` are not null here WHEN b IS NOT NULL THEN -- `b` is not null here ELSE ... END; IIF expressions: IIF(a IS NOT NULL AND c.x IS NOT NULL, ..., -- `a` and `c.x` are not null here ... ) SELECT expressions: SELECT -- `t.x` and `t.y` are not null here FROM t WHERE x IS NOT NULL AND y IS NOT NULL Negative improvements, i.e., improvements resulting from the knowledge that some condition containing one or more OR-linked IS NULL checks must have been false: IF statements: IF a IS NULL THEN ... ELSE IF c.x IS NULL THEN -- `a` is not null here ELSE -- `a` and `c.x` are not null here END IF; IF statements, guard pattern: IF a IS NULL RETURN; -- `a` is not null here IF c.x IS NULL THEN ... THROW; END IF; -- `a` and `c.x` are not null here CASE expressions: CASE WHEN a IS NULL THEN ... WHEN c.x IS NULL THEN -- `a` is not null here ELSE -- `a` and `c.x` are not null here END; IIF expressions: IIF(a IS NULL OR c.x IS NULL, ..., ... -- `a` and `c.x` are not null here ) Assignment improvements, i.e., improvements resulting from the knowledge that the right side of a statement (or a portion therein) cannot be NULL: SET statements: SET a := 42; -- `a` is not null here NOTE: Assignment improvements from FETCH statements are not currently supported. This may change in a future version of CQL. There are several ways in which improvements can cease to be in effect: The scope of the improved variable or cursor field has ended: IF a IS NOT NULL AND c.x IS NOT NULL THEN -- `a` and `c.x` are not null here END IF; -- `a` and `c.x` are nullable here An improved variable was SET to a nullable value: IF a IS NOT NULL THEN -- `a` is not null here SET a := some_nullable; -- `a` is nullable here END IF; An improved variable was used as an OUT (or INOUT) argument: IF a IS NOT NULL THEN -- `a` is not null here CALL some_procedure_that_requires_an_out_argument(a); -- `a` is nullable here END IF; An improved variable was used as a target for a FETCH statement: IF a IS NOT NULL THEN -- `a` is not null here FETCH c INTO a; -- `a` is nullable here END IF; An improved cursor field was re-fetched: IF c.x IS NOT NULL THEN -- `c.x` is not null here FETCH c; -- `c.x` is nullable here END IF; A procedure call was made (which removes improvements from all globalsbecause the procedure may have mutated any of them; locals are unaffected): IF a IS NOT NULL AND some_global IS NOT NULL THEN -- `a` and `some_global` are not null here CALL some_procedure(); -- `a` is still not null here -- `some_global` is nullable here END IF;  CQL is generally smart enough to understand the control flow of your program and infer nullability appropriately; here are a handful of examples: IF some_condition THEN SET a := 42; ELSE THROW; END IF; -- `a` is not null here because it must have been set to 42 -- if we've made it this far  IF some_condition THEN SET a := 42; ELSE SET a := 100; END IF; -- `a` is not null here because it was set to a value of a -- `NOT NULL` type in all branches and the branches cover -- all of the possible cases  IF a IS NOT NULL THEN IF some_condition THEN SET a := NULL; ELSE -- `a` is not null here despite the above `SET` because -- CQL understands that, if we're here, the previous -- branch must not have been taken END IF; END IF;  IF a IS NOT NULL THEN WHILE some_condition BEGIN -- `x` is nullable here despite `a IS NOT NULL` because -- `a` was set to `NULL` later in the loop and thus `x` -- will be `NULL` when the loop repeats LET x := a; SET a := NULL; ... END; END IF;  Here are some additional details to note regarding conditions: For positive improvements, the check must be exactly of the form IS NOT NULL; other checks that imply a variable or cursor field must not be null when true have no effect: IF a &gt; 42 THEN -- `a` is nullable here END IF; NOTE: This may change in a future version of CQL. For multiple positive improvements to be applied from a single condition, they must be linked by AND expressions along the outer spine of the condition; uses of IS NOT NULL checks that occur as subexpressions within anything other than AND have no effect: IF (a IS NOT NULL AND b IS NOT NULL) OR c IS NOT NULL THEN -- `a`, `b`, and `c` are all nullable here END IF; For negative improvements, the check must be exactly of the form IS NULL; other checks that imply a variable or cursor field must not be null when false have no effect: DECLARE equal_to_null INT; IF a IS equal_to_null THEN ... ELSE -- `a` is nullable here END IF; For multiple negative improvements to be applied from a single condition, they must be linked by OR expressions along the outer spine of the condition; uses of IS NULL checks that occur as subexpressions within anything other than OR have no effect: IF (a IS NULL OR b IS NULL) AND c IS NULL THEN ... ELSE -- `a`, `b`, and `c` are all nullable here END IF;  Forcing Nonnull Types​ If possible, it is best to use the techniques described in &quot;Nullability Improvements&quot; to verify that the value of a nullable type is nonnull before using it as such. Sometimes, however, you may know that a value with a nullable type cannot be null and simply wish to use it as though it were nonnull. The ifnull_crashand ifnull_throw &quot;attesting&quot; functions convert the type of an expression to be nonnull and ensure that the value is nonnull with a runtime check. They cannot be used in SQLite contexts because the functions are not known to SQLite, but they can be used in loose expressions. For example: CREATE PROC square_if_odd(a INT NOT NULL, OUT result INT) BEGIN IF a % 2 = 0 THEN SET result := NULL; ELSE SET result := a * a; END IF; END; -- `x` has type `INT`, but we know it can't be `NULL` let x := call square_if_odd(3); -- `y` has type `INT NOT NULL` let y := ifnull_crash(x);  Above, the ifnull_crash attesting function is used to coerce the expressionx to be of type INT NOT NULL. If our assumptions were somehow wrong, however—and x were, in fact, NULL—our program would crash. As an alternative to crashing, you can use ifnull_throw. The following two pieces of code are equivalent: CREATE PROC y_is_not_null(x INT) BEGIN let y := ifnull_throw(x); END;  CREATE PROC y_is_not_null(x INT) BEGIN DECLARE y INT NOT NULL; IF x IS NOT NULL THEN SET y := x; ELSE THROW; END IF; END;  "},{"title":"Expression Types​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#expression-types","content":"CQL supports a variety of expressions, nearly everything from the SQLite world. The following are the various supported operators; they are presented in order from the weakest binding strength to the strongest. Note that the binding order is NOT the same as C, and in some cases it is radically different (e.g. boolean math) UNION and UNION ALL​ These appear only in the context of SELECT statements. The arms of a compound select may include FROM, WHERE, GROUP BY, HAVING, and WINDOW. If ORDER BY or LIMIT ... OFFSET are present, these apply to the entire UNION. example: select A.x x from A inner join B using(z) union all select C.x x from C where x = 1;  The WHERE applies only to the second select in the union. And each SELECT is evaluated before the the UNION ALL select A.x x from A inner join B using(z) where x = 3 union all select C.x x from C where x = 1 order by x;  The ORDER BY applies to the result of the union, so any results from the 2nd branch will sort before any results from the first branch (because x is constrained in both). Assignment​ Assignment only occurs in the UPDATE statement or in the SET statement. In both cases the left side is a simple target and the right side is a general expression. The expression is evaluated before the assignment. example: SET x := 1 + 3 AND 4; -- + before AND then :=  Logical OR​ The logical OR operator does shortcut evaluation, much like the C || operator (not to be confused with SQL's concatenation operator with the same lexeme). The truth table for logical OR is as follows: A\tB\tA OR B0\t0\t0 0\t1\t1 0\tNULL\tNULL 1\t0\t1 1\t1\t1 1\tNULL\t1 NULL\t0\tNULL NULL\t1\t1 NULL\tNULL\tNULL Logical AND​ The logical AND operator does shortcut evaluation, much like the C &amp;&amp; operator, so if the left side is zero the result is 0 and the right side is not evaluated. The truth table for logical AND is as follows: A\tB\tA AND B0\t0\t0 0\t1\t0 0\tNULL\t0 1\t0\t0 1\t1\t1 1\tNULL\tNULL NULL\t0\t0 NULL\t1\tNULL NULL\tNULL\tNULL BETWEEN and NOT BETWEEN​ These are ternary operators. The general forms are:  expr1 BETWEEN expr2 AND expr3 expr1 NOT BETWEEN expr2 AND expr3  Note that there is an inherent ambiguity in the language because expr2 or expr3 could be logical expressions that include AND. CQL resolves this ambiguity by insisting that expr2 and expr3 be &quot;math expressions&quot; in the grammar. These expressions may not have ungrouped AND or OR operators. Examples:: -- oh hell no (syntax error) a between 1 and 2 and 3; -- all ok a between (1 and 2) and 3; a between 1 and (2 and 3); a between 1 and b between c and d; -- binds left to right a between 1 + 2 and 12 / 2;  Logical NOT​ The one operand of logical NOT must be a numeric. NOT 'x' is illegal. Non-ordering tests !=, &lt;&gt;, =, ==, LIKE, GLOB, MATCH, REGEXP, IN, IS, IS NOT​ These operations do some non-ordered comparison of their two operands. IS and IS NOT never return NULL, So for instance X IS NOT NULL gives the natural answer. x IS y is true if and only if: 1. both x and y are NULL or 2. if they are equal.The other operators return NULL if either operand is NULL and otherwise perform their usual test to produce a boolean!= and &lt;&gt; are equivalent as are = and ==strings and blobs compare equal based on their value, not their identity (i.e. not the string/blob pointer)objects compare equal based on their address, not their content (i.e. reference equality)MATCH, GLOB, and REGEXP are only valid in SQL contexts, LIKE can be used in any context (a helper method to do LIKE in C is provided by SQLite, but not the others)MATCH, GLOB, REGEXP, LIKE, and IN may be prefixed with NOT which reverses their value  NULL IS NULL -- this is true (NULL == NULL) IS NULL -- this is also true because NULL == NULL is not 1, it's NULL. (NULL != NULL) IS NULL -- this is also true because NULL != NULL is not 0, it's also NULL. 'xy' NOT LIKE 'z%'` -- this is true  Ordering comparisons &lt;, &gt;, &lt;=, &gt;=​ These operators do the usual order comparison of their two operands. If either operand is NULL the result is NULLObjects and Blobs may not be compared with these operandsStrings are compared based on their value (as with other comparisons) not their addressNumerics are compared as usual with the usual promotion rules NOTE: CQL uses strcmp for string comparison. In SQL expressions the comparison happens in whatever way SQLite has been configured. Typically general purpose string comparison should be done with helper functions that deal with collation and other considerations. This is a very complex topic and CQL is largely silent on it. Bitwise operators &lt;&lt;, &gt;&gt;, &amp;, |​ These are the bit-manipulation operations. Their binding strength is VERY different than C so beware. And notably the &amp; operator has the same binding strength as the | operator so they bind left to right, which is utterly unlike most systems. Many parentheses are likely to be needed to get the usual &quot;or of ands&quot; patterns codified correctly. Likewise, the shift operators &lt;&lt; and &gt;&gt; are the same strength as &amp; and | which is very atypical. Consider: x &amp; 1 &lt;&lt; 7; -- probably doesn't mean what you think (this is not ambiguous, it's well defined, but unlike C) (x &amp; 1) &lt;&lt; 7; -- means the same as the above x &amp; (1 &lt;&lt; 7) -- probably what you intended  Note that these operators only work on integer and long integer data. If any operand is NULL the result is `NULL. Addition and Subtraction +, -​ These operators do the typical math. Note that there are no unsigned numerics so it's always signed math that is happening here. operands are promoted to the &quot;biggest&quot; type involved as previously described (bool -&gt; int -&gt; long -&gt; real)only numeric operands are legal (no adding strings)if any operand is NULL the result is NULL Multiplication, Division, Modulus *, /, %​ These operators do the typical math. Note that there are no unsigned numerics so it's always signed math that is happening here. operands are promoted to the &quot;biggest&quot; type as previously described (bool -&gt; int -&gt; long -&gt; real)only numeric operands are legal (no multiplying strings)if any operand is NULL the result is NULL EXCEPTION: the % operator doesn't make sense on real values, so real values produce an error. Unary operators -, ~​ Unary negation (-) and bitwise invert (~) are the strongest binding operators. The ~ operator only works on integer types (not text, not real)the usual promotion rules otherwise applyif the operand is NULL the result is NULL "},{"title":"CASE Expressions​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#case-expressions","content":"The case expression has two major forms and provides a great deal of flexibility in an expression. You can kind of think of it as the C ?: operator on steroids. set x := 'y'; select case x when 'y' then 1 when 'z' then 2 else 3 end;  In this form the case expression (x here) is evaluated exactly once and then compared against each when clause. Every when clause must be type compatible with the case expression. The then expression that corresponds to the matching when is evaluated and becomes the result. If no when matches then the else expression is used. If there is no else and no matching when then the result is null. If that's not general enough, there is an alternate form: set y := 'yy'; set z := 'z'; select case when y = 'y' then 1 when z = 'z' then 2 else 3 end;  The second form, where there is no value before the first when keyword, each when expression is a separate independent boolean expression. The first one that evaluates to true causes the corresponding then to be evaluated and that becomes the result. As before, if there is no matching when clause then the result is the else expression if present, or null if there is no else. The result types must be compatible and the best type to hold the answer is selected with the usual promotion rules. SELECT expressions​ Single values can be extracted from SQLite using an inline select expression. For instance: set x_ := (select x from somewhere where id = 1);  The select statement in question must extract exactly one column and the type of the expression becomes the type of the column. This form can appear anywhere an expression can appear, though it is most commonly used in assignments. Something like this would also be valid: if (select x from somewhere where id = 1) == 3 then ... end if;  The select statement can of course be arbitrarily complex. Note, if the select statement returns no rows this will result in the normal error flow. In that case, the error code will be SQLITE_DONE, which is treated like an error because in this context SQLITE_ROW is expected as a result of the select. This is not a typical error code and can be quite surprising to callers. If you're seeing this failure mode it usually means the code had no affordance for the case where there were no rows and probably that situation should have been handled. This is an easy mistake to make, so to avoid it, CQL also supports these more tolerant forms: set x_ := (select x from somewhere where id = 1 if nothing -1);  And even more generally if the schema allows for null values and those are not desired: set x_ := (select x from somewhere where id = 1 if nothing or null -1);  Both of these are much safer to use as only genuine errors (e.g. the table was dropped and no longer exists) will result in the error control flow. Again note that: set x_ := (select ifnull(x,-1) from somewhere where id = 1);  Would not avoid the SQLITE_DONE error code, because no rows returned is not at all the same as a null value returned. The if nothing or null form above is equivalent to the following, but it is more economical, and probably clearer: set x_ := (select ifnull(x,-1) from somewhere where id = 1 if nothing -1);  To compute the type of the overall expression, the rules are almost the same as normal binary operators. In particular: if the default expression is present it must be type compatible with the select result the result type is the smallest type that holds both the select value and the default expression (see normal promotion rules above) object types are not allowed (SQLite cannot return an object)in (select ...) the result type is not null if and only if the select result type is not null (see select statement, many cases)in (select ... if nothing) the result type is not null if and only if both the select result and the default expression types are not null (normal binary rules)in (select ... if nothing or null) the result type is not null if and only if the default expression type is not null Finally, the form (select ... if nothing throw) is allowed; this form is exactly the same as normal(select ...) but makes the explicit that the error control flow will happen if there is no row. Consequently this form is allowed even if @enforce_strict select if nothing is in force. "},{"title":"Marking Data as Sensitive​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#marking-data-as-sensitive","content":"CQL supports the notion of 'sensitive' data in a first class way. You can think of it as very much like nullability; It largely begins by tagging data columns with @sensitive Rather than go through the whole calculus, it's easier to understand by a series of examples. So let's start with a table with some sensitive data. create table with_sensitive( id integer, name text @sensitive, sens integer @sensitive );  The most obvious thing you might do at this point is create a stored proc that would read data out of that table. Maybe something like this: create proc get_sensitive() begin select id as not_sensitive_1, sens + 1 sensitive_1, name as sensitive_2, 'x' as not_sensitive_2, -sens as sensitive_3, sens between 1 and 3 as sensitive_4 from with_sensitive; end;  So looking at that procedure we can see that it's reading sensitive data, so the result will have some sensitive columns in it. the &quot;id&quot; is not sensitive (at least not in this example)sens + 1 is sensitive, math on a sensitive field leaves it sensitivename is sensitive, it began that way and is unchanged'x' is just a string literal, it's not sensitive-sens is sensitive, that's more mathand the between expression is also sensitive Generally sensitivity is &quot;radioactive&quot; - anything it touches becomes sensitive. This is very important because even a simple looking expression like sens IS NOT NULL must lead to a sensitive result or the whole process would be largely useless. It has to be basically impossible to wash away sensitivity. These rules apply to normal expressions as well as expressions in the context of SQL. Accordingly: Sensitive variables can be declared: declare sens integer @sensitive;  Simple operations on the variables are sensitive -- this is sensitive (and the same would be true for any other math) sens + 1;  The IN expression gives you sensitive results if anything about it is sensitive -- all of these are sensitive sens in (1, 2); 1 in (1, sens); (select id in (select sens from with_sensitive));  Similarly sensitive constructs in CASE expressions result in a sensitive output -- not sensitive case 0 when 1 then 2 else 3 end; -- all of these are sensitive case sens when 1 then 2 else 3 end; case 0 when sens then 2 else 3 end; case 0 when 1 then sens else 3 end; case 0 when 1 then 2 else sens end;  Cast operations preserve sensitivity -- sensitive result select cast(sens as INT);  Aggregate functions likewise preserve sensitivity -- all of these are sensitive select AVG(T1.sens) from with_sensitive T1; select MIN(T1.sens) from with_sensitive T1; select MAX(T1.sens) from with_sensitive T1; select SUM(T1.sens) from with_sensitive T1; select COUNT(T1.sens) from with_sensitive T1;  There are many operators that get similar treatment such as COALESCE, IFNULL, IS and IS NOT. Things get more interesting when we come to the EXISTS operator: -- sensitive if and only if any selected column is sensitive exists(select * from with_sensitive) -- sensitive because &quot;info&quot; is sensitive exists(select info from with_sensitive) -- not sensitive because &quot;id&quot; is not sensitive exists(select id from with_sensitive)  If this is making you nervous, it probably should, we need a little more protection because of the way EXISTS is typically used. The predicates matter, consider the following: -- id is now sensitive because the predicate of the where clause was sensitive select id from with_sensitive where sens = 1; -- this expression is now sensitive because id is sensitive in this context exists(select id from with_sensitive where sens = 1)  In general: if the predicate of a WHERE or HAVING clause is sensitive then all columns in the result become sensitive. Similarly when performing joins, if the column specified in the USING clause is sensitive or the predicate of the ON clause is sensitive then the result of the join is considered to be all sensitive columns (even if the columns were not sensitive in the schema). Likewise a sensitive expression in LIMIT or OFFSET will result in 100% sensitive columns as these can be used in a WHERE-ish way. There is no reasonable defense against using LIMIT and testing for the presence or absence of a row as a way to wash away sensitivity so that is a weakness, but the rules that are present are likely to be very helpful. -- join with ON select T1.id from with_sensitive T1 inner join with_sensitive T2 on T1.sens = T2.sens -- join with USING select T1.id from with_sensitive T1 inner join with_sensitive T2 using(sens);  All of these expression and join propagations are designed to make it impossible to simply wash-away sensitivity with a little bit of math. Now we come to enforcement, which boils down to what assignments or &quot;assignment-like&quot; operations we allow. If we have these: declare sens integer @sensitive; declare not_sens integer;  We can use those as stand-ins for lots of expressions, but the essential calculus goes like this: -- assigning a sensitive to a sensitive is ok set sens := sens + 1; -- assigning not sensitive data to a sensitive is ok -- this is needed so you can (e.g.) initialize to zero set sens := not_sens; -- not ok set not_sens := sens;  Now these &quot;assignments&quot; can happen in a variety of ways: you can set an out parameter of your procedurewhen calling a function or procedure, we require: any IN parameters of the target be &quot;assignable&quot; from the value of the argument expressionany OUT parameters of the target be &quot;assignable&quot; from the procedures type to the argument variableany IN/OUT parameters require both the above Now it's possible to write a procedure that accepts sensitive things and returns non-sensitive things. This is fundamentally necessary because the proc must be able return (e.g.) a success code, or encrypted data, that is not sensitive. However, if you write the procedure in CQL it, too, will have to follow the assignment rules and so cheating will be quite hard. The idea here is to make it easy to handle sensitive data well and make typical mistakes trigger errors. With these rules it's possible to compute the the type of procedure result sets and also to enforce IN/OUT parameters. Since the signature of procedures is conveniently generated with --generate_exports good practices are fairly easy to follow and sensitivity checks flow well into your programs. This is a brief summary of CQL semantics for reference types -- those types that are ref counted by the runtime. The three reference types are: TEXTOBJECTBLOB Each of these has their own macro for retain and release though all three actually turn into the exact same code in all the current CQL runtime implementations. In all cases the object is expected to be promptly freed when the reference count falls to zero. "},{"title":"Reference Semantics​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#reference-semantics","content":"Stored Procedure Arguments​ in and inout arguments are not retained on entry to a stored procout arguments are assumed to contain garbage and are nulled without retaining on entryif your out argument doesn't have garbage in it, then it is up to you do release it before you make a callWhen calling a proc with an out argument CQL will release the argument variable before the call site, obeying its own contract Local Variables​ assigning to a local variable retains the object, and then does a release on the previous objectthis order is important; all assignments are done in this way in case of aliasing (release first might accidentally free too soon)CQL calls release on all local variable when the method exits Assigning to an out parameter or a global variable​ out, inout parameters, and global variables work just like local variables except that CQL does not call release at the end of the procedure "},{"title":"Function Return Values​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#function-return-values","content":"Stored procedures do not return values, they only have out arguments and those are well defined as above. Functions however are also supported and they can have either get or create semantics Get Semantics​ If you declare a function like so: declare function Getter() object;  Then CQL assumes that the returned object should follow the normal rules above, retain/release will balance by the end of the procedure for locals and globals or out arguments could retain the object Create Semantics​ If you declare a function like so: declare function Getter() create text;  then CQL assumes that the function created a new result which it is now responsible for releasing. In short, the returned object is assumed to arrive with a retain count of 1 already on it. When CQL stores this return value it will: release the object that was present at the storage location (if any)copy the returned pointer without further retaining it this one time As a result if you store the returned value in a local variable it will be released when the procedure exits (as usual) or if you instead store the result in a global or an out parameter the result will survive to be used later. "},{"title":"Comparison​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#comparison","content":"CQL tries to adhere to normal SQL comparison rules but with a C twist. "},{"title":"OBJECT​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#object","content":"The object type has no value based comparison, so there is no &lt;, &gt; and so forth. The following table is useful. Let's suppose there are exactly two distinct objects 'X' and 'Y' true expressions: X = X X &lt;&gt; Y Y = Y Y &lt;&gt; X X IN (X, Y) X NOT IN (Y) false expressions: X = Y X &lt;&gt; X Y = X Y &lt;&gt; Y X NOT IN (X, Y) null expressions: null = null X &lt;&gt; null x = null null &lt;&gt; null Y &lt;&gt; null y = null null = null resulting in null is particular surprising but consistent with the usual SQL rules. And again, as in SQL, the IS operator returns true for X IS Y even if both are null. "},{"title":"TEXT​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#text","content":"Text has value comparison semantics but normal string comparison is done only with strcmp which is of limited value. Typically you'll want to either delegate the comparison to Sqlite (with (select x &lt; y)) or else use a helper function with a suitable comparison mechanism. For text comparisons including equality: true: if and only if both operands are not null and the comparison matches (using strcmp) false: if and only if both operands are not null and the comparison does not match (using strcmp) null: if and only if at least one operand is null EXAMPLE: 'x' &lt; 'y' is true because strcmp(&quot;x&quot;, &quot;y&quot;) &lt; 0 The IS and IS NOT operators behave similarly to equality and inequality, but never return null. If X is some value that doesn't happen to be null then we have the following: true: null is null X is X X is not null null is not Xfalse: null is not null X is not X X is null null is X The IN and NOT IN operators also work for text using the same value comparisons as above. Additionally there are special text comparison operators such as LIKE, MATCH and GLOB. These comparisons are defined by SQLite. "},{"title":"BLOB​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#blob","content":"Blobs are compared by value (equivalent to memcmp) but have no well-defined ordering. The memcmp order is deemed not helpful as blobs usually have internal structure hence the valid comparisons are only equality and inequality. You can use user defined functions to do better comparisons of your particular blobs if needed. The net comparison behavior is otherwise just like strings. "},{"title":"Sample Code​","type":1,"pageTitle":"Chapter 3: Expressions, Literals, Nullability, Sensitivity","url":"/cql-guide/ch03#sample-code","content":"Out Argument Semantics​ DECLARE FUNCTION foo() OBJECT; CREATE PROC foo_user (OUT baz OBJECT) BEGIN SET baz := foo(); END;  void foo_user(cql_object_ref _Nullable *_Nonnull baz) { *(void **)baz = NULL; // set out arg to non-garbage cql_set_object_ref(baz, foo()); }  Function with Create Semantics​ DECLARE FUNCTION foo() CREATE OBJECT; CREATE PROCEDURE foo_user (INOUT baz OBJECT) BEGIN DECLARE x OBJECT; SET x := foo(); SET baz := foo(); END;  void foo_user(cql_object_ref _Nullable *_Nonnull baz) { cql_object_ref x = NULL; cql_object_release(x); x = foo(); cql_object_release(*baz); *baz = foo(); cql_cleanup: cql_object_release(x); }  Function with Get Semantics​ DECLARE FUNCTION foo() OBJECT; CREATE PROCEDURE foo_user (INOUT baz OBJECT) BEGIN DECLARE x OBJECT; SET x := foo(); SET baz := foo(); END;  void foo_user(cql_object_ref _Nullable *_Nonnull baz) { cql_object_ref x = NULL; cql_set_object_ref(&amp;x, foo()); cql_set_object_ref(baz, foo()); cql_cleanup: cql_object_release(x); }  "},{"title":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","type":0,"sectionRef":"#","url":"/cql-guide/ch05","content":"","keywords":""},{"title":"Statement Cursors​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#statement-cursors","content":"A statement cursor is based on a SQL SELECT statement. A full example might look like this: -- elsewhere create table xy_table(x integer, y integer); declare C cursor for select x, y from xy_table;  When compiled, this will result in creating a SQLite statement object (type sqlite_stmt *) and storing it in a variable called C_stmt. This statement can then be used later in various ways. Here's perhaps the simplest way to use the cursor above: declare x, y integer; fetch C into x, y;  This will have the effect of reading one row from the results of the query into the local variables x and y. These variables might then be used to create some output such as: /* note use of double quotes so that \\n is legal */ call printf(&quot;x:%d y:%d\\n&quot;, ifnull(x, 0), ifnull(y,0));  More generally, there the cursor may or may not be holding fetched values. The cursor variable C can be used by itself as a boolean indicating the presence of a row. So a more complete example might be if C then call printf(&quot;x:%d y:%d\\n&quot;, ifnull(x, 0), ifnull(y,0)); else call printf(&quot;nada\\n&quot;); end if  And even more generally loop fetch C into x, y begin call printf(&quot;x:%d y:%d\\n&quot;, ifnull(x, 0), ifnull(y,0)); end;  The last example above reads all the rows and prints them. Now if the table xy_table had instead had dozens of columns, those declarations would be very verbose and error prone, and frankly annoying, especially if the table definition was changing over time. To make this a little easier, there are so-called 'automatic' cursors. These happen implicitly and include all the necessary storage to exactly match the rows in their statement. Using the automatic syntax for the above looks like so: declare C cursor for select * from xy_table; fetch C; if C then call printf(&quot;x:%d y:%d\\n&quot;, ifnull(C.x, 0), ifnull(C.y,0)); end if;  or the equivalent loop form: declare C cursor for select * from xy_table; loop fetch C begin call printf(&quot;x:%d y:%d\\n&quot;, ifnull(C.x, 0), ifnull(C.y,0)); end;  All the necessary local state is automatically created, hence &quot;automatic&quot; cursor. This pattern is generally preferred, but the loose variables pattern is in some sense more general. In all the cases if the number or type of variables do not match the select statement, semantic errors are produced. "},{"title":"Value Cursors​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#value-cursors","content":"The purpose of value cursors is to make it possible for a stored procedure to work with structures as a unit rather than only field by field. SQL doesn't have the notion of structure types, but structures actually appear pretty directly in many places. Generally we call these things &quot;Shapes&quot; and there are a variety of source for shapes including: the columns of a tablethe projection of a SELECT statementthe columns of a cursorthe result type of a procedure that returns a selectthe arguments of a procedureother things derived from the above Let's first start with how you declare a value cursor. It is providing one of the shape sources above. So: declare C cursor like xy_table; declare C cursor like select 1 a, 'x' b; declare C cursor like (a integer not null, b text not null); declare C cursor like my_view; declare C cursor like my_other_cursor; declare C cursor like my_previously_declared_stored_proc; declare C cursor like my_previously_declared_stored_proc arguments;  Any of those forms define a valid set of columns -- a shape. Note that theselect example in no way causes the query provided to run. Instead, the select statement is analyzed and the column names and types are computed. The cursor gets the same field names and types. Nothing happens at run time. The last two examples assume that there is a stored procedure defined somewhere earlier in the same translation unit and that the procedure returns a result set or has arguments, respectively. In all cases the cursor declaration makes a cursor that could hold the indicated result. That result can then be loaded with FETCH or emitted with OUT or OUT UNION which will be discussed below. Once we have declared a value cursor we can load it with values using FETCH in its value form. Here are some examples: Fetch from compatible values: fetch C from values(1,2);  Fetch from a call to a procedure that returns a single row: fetch C from call my_previously_declared_stored_proc();  Fetch from another cursor: fetch C from D;  In this last case if D is a statement cursor it must also be &quot;automatic&quot; (i.e. it has the storage). This form lets you copy a row and save it for later. For instance, in a loop you could copy the current max-value row into a value cursor and use it after the loop, like so: declare C cursor for select * from somewhere; declare D cursor like C; loop fetch C begin if (not D or D.something &lt; C.something) then fetch D from C; end if; end;  After the loop, D either empty because there were no rows (thus if D would fail) or else it has the row with the maximum value of something, whatever that is. Value cursors are always have their own storage, so you could say all value cursors are &quot;automatic&quot;. And as we saw above, value cursors may or may not be holding a row. declare C cursor like xy_table; if not C then call printf(&quot;this will always print because C starts empty\\n&quot;); end if;  When you call a procedure you may or may not get a row as we'll see below. The third type of cursor is a &quot;result set&quot; cursor but that won't make any sense until we've discussed result sets a little which requires OUT and/or OUT UNIONand so we'll go on to those statements next. As it happens, we are recapitulating the history of cursor features in the CQL language by exploring the system in this way. Benefits of using named typed to declare a cursor​ This form allows any kind of declaration, for instance: declare C cursor like ( id integer not null, val real, flag boolean );  This wouldn't really give us much more than the other forms, however typed name lists can include LIKE in them again, as part of the list. Which means you can do this kind of thing: declare C cursor like (like D, extra1 real, extra2 bool)  You could then load that cursor like so: fetch C from values (from D, 2.5, false);  and now you have D plus 2 more fields which maybe you want to output. Importantly this way of doing it means that C always includes D, even if D changes over time. As long as the extra1 and extra2 fields don't conflict names it will always work. "},{"title":"OUT Statement​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#out-statement","content":"Value cursors were initially designed to create a convenient way for a procedure to return a single row from a complex query without having a crazy number of OUT parameters. It's easiest to illustrate this with an example. Suppose you want to return several variables, the &quot;classic&quot; way to do so would be a procedure like this: create proc get_a_row( id_ integer not null, out got_row bool not null, out w integer not null, out x integer, out y text not null, out z real) begin declare C cursor for select w, x, y, z from somewhere where id = id_; fetch C into w, x, y, z; set got_row := C; end;  This is already verbose, but you can imagine the situation gets very annoying if get_a_row has to produce a couple dozen column values. And of course you have to get the types exactly right. And they might evolve over time. Joy. On the receiving side you get to do something just as annoying: declare w integer not null declare x integer; declare y text; declare z real; declare got_row bool not null; call get_a_row(id, got_row, w, x, y, z);  Using the out statement we get the equivalent functionality with a much simplified pattern. It looks like this: create proc get_a_row(id_ integer not null) begin declare C cursor for select w, x, y, z from somewhere where id = id_; fetch C; out C; end;  To use the new procedure you simply do this: declare C cursor like get_a_row; fetch C from call get_a_row(id);  In fact, originally you did the two steps above in one statement and that was the only way to load a value cursor. Later, the calculus was generalized. The original form still works: declare C cursor fetch from call get_a_row(id);  The OUT statement lets you return a single row economically and lets you then test if there actually was a row and if so, read the columns. It infers all the various column names and types so it is resilient to schema change and generally a lot less error prone than having a large number of out arguments to your procedure. Once you have the result in a value cursor you can do the usual cursor operations to move it around or otherwise work with it. The use of the LIKE keyword to refer to groups of columns spread to other places in CQL as a very useful construct, but it began here with the need to describe a cursor shape economically, by reference. "},{"title":"OUT UNION Statement​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#out-union-statement","content":"The semantics of the OUT statement are that it always produces one row of output (a procedure can produce no row if an out never actually rans but the procedure does use OUT). If an OUT statement runs more than once, the most recent row becomes the result. So the OUT statement really does mirror having one out variable for each output column. This was its intent and procedures that return at most, or exactly, one row are very common so it works well enough. However, in general, one row results do not suffice; you might want to produce a result set from various sources, possibly with some computation as part of the row creation process. To make general results, you need to be able to emit multiple rows from a computed source. This is exactly what OUT UNION provides. Here's a (somewhat contrived) example of the kind of thing you can do with this form: create proc foo(n integer not null) begin declare C cursor like select 1 value; let i := 0; while i &lt; n begin -- emit one row for every integer fetch C from values(i); out union C; set i := i + 1; end; end;  In foo above, we make an entire result set out of thin air. It isn't a very interesting result, but of course any computation would have been possible. This pattern is very flexible as we see below in bar where we merge two different data streams. create table t1(id integer, stuff text, [other things too]); create table t2(id integer, stuff text, [other things too]); create proc bar() begin declare C cursor for select * from t1 order by id; declare D cursor for select * from t2 order by id; fetch C; fetch D; -- we're going to merge these two queries while C or D begin -- if both have a row pick the smaller id if C and D then if C.id &lt; D.id then out union C; fetch C; else out union D; fetch D; end if; else if C then -- only C has a row, emit that out union C; fetch C; else -- only D has a row, emit that out union D; fetch D; end if; end; end;  Just like foo, in bar, each time OUT UNION runs a new row is accumulated. Now, if you build a procedure that ends with a SELECT statement CQL automatically creates a fetcher function that does something like an OUT UNION loop -- it loops over the SQLite statement for the SELECT and fetches each row, materializing a result. With OUT UNION you take manual control of this process, allowing you to build arbitrary result sets. Note that either of C or D above could have been modified, replaced, skipped, normalized, etc. with any kind of computation. Even entirely synthetic rows can be computed and inserted into the output as we saw in foo. "},{"title":"Result Set Cursors​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#result-set-cursors","content":"Now that we have OUT UNION it makes sense to talk about the final type of cursor. OUT UNION makes it possible to create arbitrary result sets using a mix of sources and filtering. Unfortunately this result type is not a simple row, nor is it a SQLite statement. This meant that neither of the existing types of cursors could hold the result of a procedure that used OUT UNION. -- CQL could not itself consume its own results. To address this hole, we need an additional cursor type. The syntax is exactly the same as the statement cursor cases described above but, instead of holding a SQLite statement, the cursor holds a result set pointer and the current and maximum row numbers. Stepping through the cursor simply increments the row number and fetches the next row out of the rowset instead of from SQLite. Example: -- reading the above create proc reader() begin declare C cursor for call bar(); loop fetch C begin call printf(&quot;%d %s\\n&quot;, C.id, C.stuff); -- or whatever fields you need end; end;  If bar had been created with a SELECT, UNION ALL, and ORDER BY to merge the results, the above would have worked with C being a standard statement cursor, iterating over the union. Since foo produces a result set, CQL transparently produces a suitable cursor implementation behind the scenes, but otherwise the usage is the same. Note this is a lousy way to simply iterate over rows; you have to materialize the entire result set so that you can just step over it. Re-consuming like this is not recommended at all for production code, but it is ideal for testing result sets that were made withOUT UNION which otherwise would require C/C++ to test. Testing CQL with CQL is generally a lot easier. "},{"title":"Reshaping Data, Cursor LIKE forms​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#reshaping-data-cursor-like-forms","content":"There are lots of cases where you have big rows with many columns, and there are various manipulations you might need to do. What follows is a set of useful syntactic sugar constructs that simplify handling complex rows. The idea is that pretty much anywhere you can specify a list of columns you can instead use the LIKE x construct to get the columns as they appear in the shape x -- which is usually a table or a cursor. It’s a lot easier to illustrate with examples, even though these are, again, a bit contrived. First we need some table with lots of columns -- usually the column names are much bigger which makes it all the more important to not have to type them over and over, but in the interest of some brevity, here's a big table: create table big ( id integer primary key, id2 integer unique, a integer, b integer, c integer, d integer, e integer, f integer);  This example showcases several of the cursor and shape slicing features by emitting two related rows: create proc foo(id_ integer not null) begin -- this is the shape of the result we want -- it's some of the columns of &quot;big&quot; -- note this query doesn't run, we just use its shape to create a cursor -- with those columns. declare result cursor like select id, b, c, d from big; -- fetch the main row, specified by id_ -- main row has all the fields, including id2 declare main_row cursor for select * from big where id = id_; fetch main_row; -- now fetch the result columns out of the main row -- 'like result' means &quot;the column names found in 'result'&quot; fetch result from cursor main_row(like result); -- this is our first result row out union result; -- now we want the related row, but we only need two columns -- from the related row, 'b' and 'c' declare alt_row cursor for select b, c from big where big.id2 = main_row.id2; fetch alt_row; -- update some of the fields 'result' from the the new cursor update cursor result(like alt_row) from cursor alt_row; -- and emit the 2nd row out union result; end;  Now let's briefly discuss what is above. The two essential parts are: fetch result from cursor main_row(like result); and update cursor result(like alt_row) from cursor alt_row; In the first case what we're saying is that we want to load the columns of result from main_row but we only want to take the columns that are actually present in result. So this is a narrowing of a wide row into a smaller row. In this case, the smaller row, result, is what we want to emit. We needed the other columns to compute alt_row. The second case, what we're saying is that we want to update result by replacing the columns found in alt_row with the values in alt_row. So in this case we're writing a smaller cursor into part of a wider cursor. Note that we used the update cursor form here because it preserves all other columns. If we used fetch we would be rewriting the entire row contents, using NULL if necessary, and that is not desired here. Here is the rewritten version of the above procedure; this is what ultimately gets compiled into C. CREATE PROC foo (id_ INTEGER NOT NULL) BEGIN DECLARE result CURSOR LIKE SELECT id, b, c, d FROM big; DECLARE main_row CURSOR FOR SELECT * FROM big WHERE id = id_; FETCH main_row; FETCH result(id, b, c, d) FROM VALUES(main_row.id, main_row.b, main_row.c, main_row.d); OUT UNION result; DECLARE alt_row CURSOR FOR SELECT b, c FROM big WHERE big.id2 = main_row.id2; FETCH alt_row; UPDATE CURSOR result(b, c) FROM VALUES(alt_row.b, alt_row.c); OUT UNION result; END;  Of course you could have typed the above directly but if there are 50 odd columns it gets old fast and is very error prone. The sugar form is going to be 100% correct and will require much less typing and maintenance. Finally, while I've shown both LIKE forms separately, they can also be used together. For instance:  update cursor C(like X) from cursor D(like X);  The above would mean, &quot;move the columns that are found in X from cursor D to cursor C&quot;, presuming X has columns common to both. "},{"title":"Fetch Statement Specifics​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#fetch-statement-specifics","content":"Many of the examples used the FETCH statement in a sort of demonstrative way that is hopefully self-evident but the statement has many forms and so it's worth going over them specifically. Below we'll use the letters C and D for the names of cursors. Usually C; Fetch with Statement or Result Set Cursors​ A cursor declared in one of these forms: declare C cursor for select * from foo;declare C cursor for call foo(); (foo might end with a select or use out union) is either a statement cursor or a result set cursor. In either case the cursor moves through the results. You load the next row with: FETCH C, orFETCH C into x, y, z; In the first form C is said to be automatic in that it automatically declares the storage needed to hold all its columns. As mentioned above, automatic cursors have storage for their row. Having done this fetch you can use C as a scalar variable to see if it holds a row, e.g. declare C cursor for select * from foo limit 1; fetch C; if C then -- bingo we have a row call printf(&quot;%s\\n&quot;, C.whatever); end if  You can easily iterate, e.g. declare C cursor for select * from foo; loop fetch C begin -- one time for every row call printf(&quot;%s\\n&quot;, C.whatever); end;  Automatic cursors are so much easier to use than explicit storage that explicit storage is rarely seen. Storing to out parameters is one case where explicit storage actually is the right choice, as the out parameters have to be declared anyway. Fetch with Value Cursors​ A value cursor is declared in one of these ways: declare C cursor fetch from call foo(args) foo must be a procedure that returns one row with OUT declare C cursor like select 1 id, &quot;x&quot; name; declare C cursor like X; where X is the name of a table, a view, another cursor, or a procedure that returns a structured result A value cursor is always automatic; it's purpose is to hold a row. It doesn't iterate over anything but it can be re-loaded in a loop. fetch C or fetch C into ... is not valid on such a cursor, because it doesn't have a source to step through. The canonical way to load such a cursor is: fetch C from call foo(args); foo must be a procedure that returns one row with OUT fetch C(a,b,c...) from values(x, y, z); The first form is in some sense the origin of the value cursor. Value cursors were added to the language initially to provide a way to capture the single row OUT statement results, much like result set cursors were added to capture procedure results from OUT UNION. In the first form, the cursor storage (a C struct) is provided by reference as a hidden out parameter to the procedure and the procedure fills it in. The procedure may or may not use the OUT statement in its control flow, as the cursor might not hold a row. You can use if C then ... as before to test for a row. The second form is more interesting as it allows the cursor to be loaded from arbitrary expressions subject to some rules: you should think of the cursor as a logical row: it's either fully loaded or it's not, therefore you must specify enough columns in the column list to ensure that all NOT NULL columns will get a valueif not mentioned in the list, NULL will be loaded where possibleif insufficient columns are named, an error is generatedif the value types specified are not compatible with the column types mentioned, an error is generatedlater in this chapter, we'll show that columns can also be filled with dummy data using a seed value With this form, any possible valid cursor values could be set, but many forms of updates that are common would be awkward. So there are various forms of syntactic sugar that are automatically rewritten into the canonical form. See the examples below: fetch C from values(x, y, z) if no columns are specified this is the same as naming all the columns, in declared order fetch C from arguments the arguments to the procedure in which this statement appears are used as the values, in orderin this case C was also rewritten into C(a,b,c,..) etc. fetch C from arguments like C the arguments to the procedure in which this statement appears are used, by name, as the values, using the names of of the indicated shapethe order in which the arguments appeared no longer matters, the names that match the columns of C are used if presentthe formal parameter name may have a single trailing underscore (this is what like C would generate)e.g. if C has columns a and b then there must exist formals named a or a_ and b or b_, in any position fetch C(a,b) from cursor D(a,b) the named columns of D are used as the valuesin this case the statement becomes: fetch C(a,b) from values(D.a, D.b); That most recent form doesn't seem like it saves much, but recall the first rewrite: fetch C from cursor D both cursors are expanded into all their columns, creating a copy from one to the otherfetch C from D can be used if the cursors have the exact same column names and types; it also generates slightly better code and is a common case It is very normal to want to use only some of the columns of a cursor; these like forms do that job. We saw some of these forms in an earlier example. fetch C from cursor D(like C) here D is presumed to be &quot;bigger&quot; than C, in that it has all of the C columns and maybe more. The like C expands into the names of the C columns so C is loaded from the C part of Dthe expansion might be fetch C(a, b, g) from values (D.a, D.b, D.g)D might have had fields c, d, e, f which were not used because they are not in C. The symmetric operation, loading some of the columns of a wider cursor can be expressed neatly: fetch C(like D) from cursor D the like D expands into the columns of D causing the cursor to be loaded with what's in D and NULL (if needed)when expanded, this might look like fetch C(x, y) from values(D.x, D.y) LIKE can be used in both places, for instance suppose E is a shape that has a subset of the rows of both C and D. You can write a form like this: fetch C(like E) from cursor D(like E) this means take the column names found in E and copy them from D to C.the usual type checking is done As is mentioned above, the fetch form means &quot;load an entire row into the cursor&quot;. This is important because &quot;half loaded&quot; cursors would be semantically problematic. However there are many cases where you might like to amend the values of an already loaded cursor. You can do this with the update form. update cursor C(a,b,..) from values(1,2,..); the update form is a no-op if the cursor is not already loaded with values (!!)the columns and values are type checked so a valid row is ensured (or no row)all the re-writes above are legal so update cursor C(like D) from D is possible; it is in fact the use-case for which this was designed. "},{"title":"Calling Procedures with Argument Bundles​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#calling-procedures-with-argument-bundles","content":"It's often desirable to treat bundles of arguments as a unit, or cursors as a unit, especially when calling other procedures. The shape patterns above are very helpful for moving data between cursors, and the database. These can be rounded out with similar constructs for procedure definitions and procedure calls as follows. First we'll define some shapes to use in the examples. Note that we made U using T. create table T(x integer not null, y integer not null, z integer not null); create table U(like T, a integer not null, b integer not null);  We haven't mentioned this before but the implication of the above is that you can use the LIKE construct inside a table definition to add columns from a shape. We can also use the LIKE construct to create procedure arguments. To avoid conflicts with column names, when used this way the procedure arguments all get a trailing underscore appended to them. The arguments will be x_, y_, and z_ as we can see if the following: create proc p1(like T) begin call printf(&quot;%d %d %d\\n&quot;, x_, y_, z_); end;  Shapes can also be used in a procedure call, as showed below. This next example is obviously contrived, but of course it generalizes. It is exactly equivalent to the above. create proc p2(like T) begin call printf(&quot;%d %d %d\\n&quot;, from arguments); end;  Now we might want to chain these things together. This next example uses a cursor to call p1. create proc q1() begin declare C cursor for select * from T; loop fetch C begin /* this is the same as call p(C.x, C.y, C.z) */ call p1(from C); end; end;  The like construct allows you to select some of the arguments, or some of a cursor to use as arguments. This next procedure has more arguments than just T. The arguments will be x_, y_, z_, a_, b_. But the call will still have the T arguments x_, y_, and z_. create proc q2(like U) begin /* just the args that match T: so this is still call p(x_, y_, z_) */ call p1(from arguments like T); end;  Or similarly, using a cursor. create proc q3(like U) begin declare C cursor for select * from U; loop fetch C begin /* just the columns that match T so this is still call p(C.x, C.y, C.z) */ call p1(from C like T); end; end;  Note that the from argument forms do not have to be all the arguments. For instance you can get columns from two cursors like so:  call something(from C, from D)  All the varieties can be combined but of course the procedure signature must match. And all these forms work in function expressions as well as procedure calls. e.g.  set x := a_function(from C);  Since these forms are simply syntatic sugar, they can also appear inside of function calls that are in SQL statements. The variables mentioned will be expanded and become bound variables just like any other variable that appears in a SQL statement. Note the form x IN (from arguments) is not supported at this time, though this would be a relatively easy addition. "},{"title":"Using Named Argument Bundles​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#using-named-argument-bundles","content":"There are many cases where stored procedures require complex arguments using data shapes that come from the schema, or from other procedures. As we have seen the LIKE construct for arguments can help with this, but it has some limitations. Let's consider a specific example to study: create table Person ( id text primary key, name text not null, address text not null, birthday real );  To manage this table we might need something like this: create proc insert_person(like Person) begin insert into Person from arguments; end;  As we have seen, the above expands into: create proc insert_person( id_ text not null, name_ text not null, address_ text not null, birthday_ real) begin insert into Person(id, name, address, birthday) values(id_, name_, address_, birthday_); end;  It's clear that the sugared version is a lot easier to reason about than the fully expanded version, and much less prone to errors as well. This much is already helpful, but just those forms aren't general enough to handle the usual mix of situations. For instance, what if we need a procedure that works with two people? A hypothetical insert_two_people procedure cannot be written with the forms we have so far. To generalize this the language adds the notion of named argument bundles. The idea here is to name the bundles which provides a useful scoping. Example: create proc insert_two_people(p1 like Person, p2 like Person) begin -- using a procedure that takes a Person args call insert_person(from p1); call insert_person(from p2); end;  or alternatively create proc insert_two_people(p1 like Person, p2 like Person) begin -- inserting a Person directly insert into Person from p1; insert into Person from p2; end;  The above expands into: create proc insert_two_people( p1_id text not null, p1_name text not null, p1_address text not null, p1_birthday real, p2_id text not null, p2_name text not null, p2_address text not null, p2_birthday real) begin insert into Person(id, name, address, birthday) values(p1_id, p1_name, p1_address, p1_birthday); insert into Person(id, name, address, birthday) values(p2_id, p2_name, p2_address, p2_birthday); end;  Or course different named bundles can have different types -- you can create and name shapes of your choice. The language allows you to use an argument bundle name in all the places that a cursor was previously a valid source. That includes insert,fetch, update cursor, and procedure calls. You can refer to the arguments by their expanded name such as p1_address or alternatively p1.address -- they mean the same thing. Here's another example showing a silly but illustrative thing you could do: create proc insert_lotsa_people(P like Person) begin -- make a cursor to hold the arguments declare C cursor like P; -- convert arguments to a cursor fetch C from P; -- set up to patch the cursor and use it 20 times let i := 0; while i &lt; 20 begin update cursor C(id) from values(printf(&quot;id_%d&quot;, i)); insert into Person from C; set i := i + 1; end; end;  The above shows that you can use a bundle as the source of a shape, and you can use a bundle as a source of data to load a cursor. After which you can do all the usual value cursor things. Of course in this case the value cursor was redundant, we could just as easily have done something like this:  set P_id := printf(&quot;id_%d&quot;, i); insert into Person from P; set i := i + 1;  Note: the CQL JSON output includes extra information about procedure arguments if they originated as part of a shape bundle do identify the shape source for tools that might need that information. "},{"title":"The COLUMNS/LIKE construct in the SELECT statement​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#the-columnslike-construct-in-the-select-statement","content":"The select list of a SELECT statement already has complex syntax and functionality, but it is a very interesting place to use shapes. To make it possible to use shape notations and not confuse them with standard SQL the COLUMNS construct was added to the language. This allows for a sugared syntax for extracting columns in bulk. The COLUMNS clause is like of a generalization of the select T.* with shape slicing and type-checking. The forms are discussed below: Columns from a join table or tables​ This is the simplest form, it's just like T.*: -- same as A.* select columns(A) from ...; -- same as A.*, B.* select columns(A, B) from ...;  Columns from a particular joined table that match a shape​ This allows you to choose some of the columns of one table of the FROM clause. -- the columns of A that match the shape Foo select columns(A like Foo) from ...; -- get the Foo shape from A and the Bar shape from B select columns(A like Foo, B like Bar) from ...;  Columns from any that match a shape, from anywhere in the FROM​ Here we do not specify a particular table that contains the columns, the could come from any of the tables in the FROM clause. --- get the Foo shape from anywhere in the join select columns(like Foo) from ...; -- get the Foo and Bar shapes, from anywhere in the join select columns(like Foo, like Bar) from ...;  Specific columns​ This form allows you to slice out a few columns without defining a shape, you simply list the exact columns you want. -- T1.x and T2.y plus the Foo shape select columns(T1.x, T2.y, like Foo) from ...;  Distinct columns​ Its often the case that there are duplicate column names in the FROM clause. For instance, you could join A to B with both having a column pk. The final result set can only have one column named pk, the distinct clause helps you to get distinct column names. In this context distinct is about column names, not values. -- removes duplicate column names -- e.g. there will be one copy of 'pk' select columns(distinct A, B) from A join B using(pk); -- if both Foo and Bar have an (e.g.) 'id' field you only get one copy select columns(distinct like Foo, like Bar) from ...;  If a specific column is mentioned it is always included, but later expressions that are not a specific column will avoid that column name. -- if F or B has an x it won't appear again, just T.x select columns(distinct T.x, F like Foo, B like Bar) from F, B ..;  Of course this is all just sugar, so it all compiles to a column list with table qualifications -- but the syntax is very powerful. You can easily narrowin a wide table, or fusing joins that share common keys. -- just the Foo columns select columns(like Foo) from Superset_Of_Foo_From_Many_Joins_Even; -- only one copy of 'pk' select columns(distinct A,B,C) from A join B using (pk) join C using (pk);  And of course you can define shapes however you like and then use them to slice off column chucks of your choice. There are many ways to build up shapes from other shapes. For instance, you can declare procedures that return the shape you want and never actually create the procedure -- a pattern is very much like a shape &quot;typedef&quot;. E.g. declare proc shape1() (x integer, y real, z text); declare proc shape2() (like shape1, u bool, v bool);  With this combination you can easily define common column shapes and slice them out of complex queries without having to type the columns names over and over. "},{"title":"Missing Data Columns, Nulls and Dummy Data​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#missing-data-columns-nulls-and-dummy-data","content":"What follows are the rules for columns that are missing in an INSERT, or FETCH statement. As with many of the other things discussed here, the forms result in automatic rewriting of the code to include the specified dummy data. So SQLite will never see these forms. Two things to note: First, the dummy data options described below are really only interesting in test code, it's hard to imagine them being useful in production code. Second, none of what follows applies to the update cursor statement because its purpose is to do partial updates on exactly the specified columns and we're about to talk about what happens with the columns that were not specified. When fetching a row all the columns must come from somewhere; if the column is mentioned or mentioned by rewrite then it must have a value mentioned, or a value mentioned by rewrite. For columns that are not mentioned, a NULL value is used if it is legal to do so. For example, fetch C(a) from values(1) might turn into fetch C(a,b,c,d) from values (1, NULL, NULL, NULL) In addition to the automatic NULL you may add the annotation @dummy_seed([long integer expression]). If this annotion is present then: the expression is evaluated and stored in the hidden variable seedall integers, and long integers get seed as their value (possibly truncated)booleans get 1 if and only if seed is non-zerostrings get the name of the string column an underscore and the value as text (e.g. &quot;myText7&quot; if _seed is 7)blobs are not currently supported for dummy data (CQL is missing blob conversions which are needed first) This construct is hugely powerful in a loop to create many complete rows with very little effort, even if the schema change over time. declare i integer not null; declare C like my_table; set i := 0; while (i &lt; 20) begin fetch C(id) from values(i+10000) @dummy_seed(i); insert into my_table from cursor C; end;  Now in this example we don't need to know anything about my_table other than that it has a column named id. This example shows several things: we got the shape of the cursor from the table we were inserting intoyou can do your own computation for some of the columns (those named) and leave the unnamed values to be defaultedthe rewrites mentioned above work for the insert statement as well as fetchin fact insert into my_table(id) values(i+10000) @dummy_seed(i) would have worked too with no cursor at all bonus, dummy blob data does work in insert statements because SQLite can do the string conversion easilythe dummy value for a blob is a blob that holds the text of the column name and the text of the seed just like a string column The @dummy_seed form can be modified with @dummy_nullables, this indicates that rather than using NULL for any nullable value that is missing, CQL should use the seed value. This overrides the default behavior of using NULL where columns are needed. Note the NULL filling works a little differently on insert statements. Since SQLite will provide a NULL if one is legal the column doesn't have to be added to the list with a NULL value during rewriting, it can simply be omitted, making the statement smaller. Finally for insert statement only, SQLite will normally use the default value of a column if it has one, so there is no need to add missing columns with default values to the insert statement. However if you specify @dummy_defaults then columns with a default value will instead be rewritten and they will get _seed_ as their value. Some examples. Suppose columns a, b, c are not null; m, n are nullable; and x, y have defaults. -- as written insert into my_table(a) values(7) @dummy_seed(1000) -- rewrites to insert into my_table(a, b, c) values(7, 1000, 1000);  -- as written insert into my_table(a) values(7) @dummy_seed(1000) @dummy_nullables -- rewrites to insert into my_table(a, b, c, m, n) values(7, 1000, 1000, 1000, 1000);  -- as written insert into my_table(a) values(7) @dummy_seed(1000) @dummy_nullables @dummy_defaults -- rewrites to insert into my_table(a, b, c, m, n, x, y) values(7, 1000, 1000, 1000, 1000, 1000, 1000);  The sugar features on fetch, insert, and update cursor are as symmetric as possible, but again, dummy data is generally only interesting in test code. Dummy data will continue to give you valid test rows even if columns are added or removed from the tables in question. "},{"title":"Generalized Cursor Lifetimes aka Cursor \"Boxing\"​","type":1,"pageTitle":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH","url":"/cql-guide/ch05#generalized-cursor-lifetimes-aka-cursor-boxing","content":"Generalized Cursor Lifetime refers to capturing a Statement Cursor in an object so that it can used more flexibly. Wrapping something in an object is often called &quot;boxing&quot;. Since Generalized Cursor Lifetime is a mouthful we'll refer to it as &quot;boxing&quot; from here forward. The symmetric operation &quot;unboxing&quot; refers to converting the boxed object back into a cursor. The normal cursor usage pattern is by far the most common, a cursor is created directly with something like these forms: declare C cursor for select * from shape_source; declare D cursor for call proc_that_returns_a_shape();  At this point the cursor can be used normally as follows: loop fetch C begin -- do stuff with C end;  Those are the usual patterns and they allow statement cursors to be consumed sort of &quot;up&quot; the call chain from where the cursor was created. But what if you want some worker procedures that consume a cursor? There is no way to pass your cursor down again with these normal patterns alone. To generalize the patterns, allowing, for instance, a cursor to be returned as an out parameter or accepted as an in parameter you first need to declare an object variable that can hold the cursor and has a type indicating the shape of the cursor. To make an object that can hold a cursor: declare obj object&lt;T cursor&gt;;  Where T is the name of a shape. It can be a table name, or a view name, or it can be the name of the canonical procedure that returns the result. T should be some kind of global name, something that could be accessed with #include in various places. Referring to the examples above, choices for T might be shape_source the table or proc_that_returns_a_shape the procedure. Note: it's always possible make a fake procedure that returns a result to sort of &quot;typedef&quot; a shape name. e.g. declare proc my_shape() (id integer not null, name text);  The procedure here my_shape doesn’t have to actually ever be created, in fact it’s better if it isn't. It won’t ever be called; its hypothetical result is just being as a shape. This can be useful if you have several procedures like proc_that_returns_a_shapethat all return results with the columns of my_shape. To create the boxed cursor, first declare the object variable that will hold it and then set object from the cursor. Note that in the following example the cursor C must have the shape defined by my_shape or an error is produced. The type of the object is crucial because, as we'll see, during unboxing that type defines the shape of the unboxed cursor. -- recap: declare the box that holds the cursor (T changed to my_shape for this example) declare box_obj object&lt;my_shape cursor&gt;; -- box the cursor into the object (the cursor shape must match the box shape) set box_obj from cursor C;  The variable box_obj can now be passed around as usual. It could be stored in a suitable out variable or it could be passed to a procedure as an in parameter. Then, later, you can &quot;unbox&quot; box_obj to get a cursor back. Like so -- unboxing a cursor from an object, the type of box_obj defines the type of the created cursor declare D cursor for box_obj;  These primitives will allow cursors to be passed around with general purpose lifetime. Example: -- consumes a cursor create proc cursor_user(box_obj object&lt;my_shape cursor&gt;) begin declare C cursor for box_obj; -- the cursors shape will be my_shape matching box loop fetch C begin -- do something with C end; end; -- captures a cursor and passes it on create proc cursor_boxer() begin declare C cursor for select * from something_like_my_shape; declare box_obj object&lt;my_shape cursor&gt; set box from cursor C; -- produces error if shape doesn't match call cursor_user(box_obj); end;  Importantly, once you box a cursor the underlying SQLite statement’s lifetime is managed by the box object with normal retain/release semantics. The box and underlying statement can be released simply by setting all references to it to null as usual. With this pattern it's possible to, for instance, create a cursor, box it, consume some of the rows in one procedure, do some other stuff, and then consume the rest of the rows in another different procedure. Important Notes: the underlying SQLite statement is shared by all references to it. Unboxing does not reset the cursor's position. It is possible, even desirable, to have different procedures advancing the same cursorthere is no operation for &quot;peeking&quot; at a cursor without advancing it; if your code requires that you inspect the row and then delegate it, you can do this simply by passing the cursor data as a value rather than the cursor statement. Boxing and unboxing are for cases where you need to stream data out of the cursor in helper proceduresdurably storing a boxed cursor (e.g. in a global) could lead to all manner of problems -- it is exactly like holding on to a sqlite3_stmt * for a long time with all the same problems because that is exactly is happening Summarizing, the main reason for using the boxing patterns is to allow for standard helper procedures that can get a cursor from a variety of places and process it. Boxing isn’t the usual pattern at all and returning cursors in a box, while possible, should be avoided in favor of the simpler patterns, if only because then then lifetime management is very simple in all those cases. "},{"title":"Chapter 6: Calling Procedures Defined Elsewhere","type":0,"sectionRef":"#","url":"/cql-guide/ch06","content":"","keywords":""},{"title":"Declaring Procedures Defined Elsewhere​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#declaring-procedures-defined-elsewhere","content":"Stored procedures defined in another file can be declared to CQL in various ways for each major type of stored procedure. These are covered in the sections below. "},{"title":"Simple Procedures (database free):​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#simple-procedures-database-free","content":"DECLARE PROCEDURE foo(id integer, out name text not null);  This introduces the symbol name without providing the body. This has important variations. "},{"title":"Procedures that use the database​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#procedures-that-use-the-database","content":"DECLARE PROCEDURE foo(id integer, out name text not null) USING TRANSACTION;  Most procedures you write will use SQLite in some fashion, maybe a select or something. The USING TRANSACTION annotation indicates that the proc in question uses the database and therefore the generated code will need a database connection in-argument and it will return a SQLite error code. "},{"title":"Procedures that create a result set​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#procedures-that-create-a-result-set","content":"If the procedure in question is going to use select or call to create a result set, the type of that result set has to be declared. An example might look like this: DECLARE PROC with_result_set () (id INTEGER NOT NULL, name TEXT, rate LONG INTEGER, type INTEGER, size REAL);  This says that the procedure takes no arguments (other than the implicit database connection) and it has an implicit out-argument that can be read to get a result set with the indicated columns: id, name, rate, type, and size. This form implies USING TRANSACTION. "},{"title":"Procedures that return a single row with a value cursor​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#procedures-that-return-a-single-row-with-a-value-cursor","content":"If the procedure emits a cursor with the OUT statement to produce a single row then it can be declared as follows: DECLARE PROC with_result_set () OUT (id INTEGER NOT NULL, name TEXT, rate LONG INTEGER, type INTEGER, size REAL);  This form can have USING TRANSACTION or not, since it is possible to emit a row with a value cursor and never use the database. See the previous chapter for details on the OUT statement. "},{"title":"Procedures that return a full result set​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#procedures-that-return-a-full-result-set","content":"If the procedure emits many rows with the OUT UNION statement to produce a full result set then it can be declared as follows: DECLARE PROC with_result_set () OUT UNION (id INTEGER NOT NULL, name TEXT, rate LONG INTEGER, type INTEGER, size REAL);  This form can have USING TRANSACTION or not, since it is possible to emit a rows with a value cursor and never use the database. See the previous chapter for details on the OUT UNION statement. "},{"title":"Exporting Declared Symbols Automatically​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#exporting-declared-symbols-automatically","content":"To avoid errors, the declarations for any given file can be automatically created by adding something like --generate_exports to the command line. This will require an additonal file name to be passed in the --cg portion to capture the exports. That file can then be used with #include when you combine the C pre-processor with CQL as is normally done. Nomenclature is perhaps a bit weird here. You use --generate_exports to export the stored procedure declarations from the translation units. Of course those exported symbols are what you then import in some other module. Sometimes this output file is called foo_imports.sql because those exports are of course exactly what you need to import foo. You can use whatever convention you like of course, CQL doesn't care. The full command line might look something like this: cql --in foo.sql --cg foo.h foo.c foo_imports.sql --generate_exports  Using the pre-processor you can get declarations from elsewhere with a directive like this: #include &quot;foo_imports.sql&quot;  "},{"title":"Declaration Examples​","type":1,"pageTitle":"Chapter 6: Calling Procedures Defined Elsewhere","url":"/cql-guide/ch06#declaration-examples","content":"Here are some more examples directly from the CQL test cases; these are all auto-generated with --generate_exports. DECLARE PROC test (i INTEGER NOT NULL); DECLARE PROC out_test (OUT i INTEGER NOT NULL, OUT ii INTEGER); DECLARE PROC outparm_test (OUT foo INTEGER NOT NULL) USING TRANSACTION; DECLARE PROC select_from_view () (id INTEGER NOT NULL, type INTEGER); DECLARE PROC make_view () USING TRANSACTION; DECLARE PROC copy_int (a INTEGER, OUT b INTEGER); DECLARE PROC complex_return () (_bool BOOL NOT NULL, _integer INTEGER NOT NULL, _longint LONG INTEGER NOT NULL, _real REAL NOT NULL, _text TEXT NOT NULL, _nullable_bool BOOL); DECLARE PROC outint_nullable ( OUT output INTEGER, OUT result BOOL NOT NULL) USING TRANSACTION; DECLARE PROC outint_notnull ( OUT output INTEGER NOT NULL, OUT result BOOL NOT NULL) USING TRANSACTION; DECLARE PROC obj_proc (OUT an_object OBJECT); DECLARE PROC insert_values ( id_ INTEGER NOT NULL, type_ INTEGER) USING TRANSACTION;  So far we've avoided discussing the generated C code in any details but here it seems helpful to show exactly what these declarations correspond to in the generated C to demystify all this. There is a very straightforward conversion. void test(cql_int32 i); void out_test( cql_int32 *_Nonnull i, cql_nullable_int32 *_Nonnull ii); cql_code outparm_test( sqlite3 *_Nonnull _db_, cql_int32 *_Nonnull foo); cql_code select_from_view_fetch_results( sqlite3 *_Nonnull _db_, select_from_view_result_set_ref _Nullable *_Nonnull result_set); cql_code make_view(sqlite3 *_Nonnull _db_); void copy_int(cql_nullable_int32 a, cql_nullable_int32 *_Nonnull b); cql_code complex_return_fetch_results( sqlite3 *_Nonnull _db_, complex_return_result_set_ref _Nullable *_Nonnull result_set); cql_code outint_nullable( sqlite3 *_Nonnull _db_, cql_nullable_int32 *_Nonnull output, cql_bool *_Nonnull result); cql_code outint_notnull( sqlite3 *_Nonnull _db_, cql_int32 *_Nonnull output, cql_bool *_Nonnull result); void obj_proc( cql_object_ref _Nullable *_Nonnull an_object); cql_code insert_values( sqlite3 *_Nonnull _db_, cql_int32 id_, cql_nullable_int32 type_);  As you can see, these declarations use exactly the normal SQLite types and so it is very easy to declare a procedure in CQL and then implement it yourself in straight C, simply by conforming to the contract. Importantly, SQLite does not know anything about CQL stored procedures, or anything at all about CQL really so CQL stored procedure names cannot be used in any way in SQL statements. CQL control flow like the call statement can be used to invoke other procedures and results can be captured by combing the OUT statement and a DECLARE CURSOR construct but SQLite is not involved in those things. This is another place where the inherent two-headed nature of CQL leaks out. Finally, this is a good place to reinforce that procedures with any of the structured result types (select, out, out union) can be used with a suitable cursor. create procedure get_stuff() begin select * from stuff; end;  Can be used in two interesting ways: create procedure meta_stuff(meta bool) begin if meta then call get_stuff(); else call get_other_stuff(); end if; end;  Assuming that get_stuff and get_other_stuff have the same shape, then this procedure simply passes on one or the other's result set unmodified as its own return value. But you could do more than simply pass on the result. create procedure meta_stuff(meta bool) begin declare C cursor for call get_stuff(); -- or get_meta_stuff(...) loop fetch C begin -- do stuff with C -- may be out union some of the rows after adjustment even end; end;  Here we can see that we used the procedure to get the results and then process them directly somehow. And of course the result of an OUT can similarly be processed using a value cursor, as previously seen. These combinations allow for pretty general composition of stored procedures so long as they are not recombined with SQLite statements. "},{"title":"Chapter 8: Functions","type":0,"sectionRef":"#","url":"/cql-guide/ch08","content":"","keywords":""},{"title":"Function Types​","type":1,"pageTitle":"Chapter 8: Functions","url":"/cql-guide/ch08#function-types","content":"Ordinary Scalar Functions​ These functions are written in regular C and provide for the ability to do operations on in-memory objects. For instance, you could create functions that allow you to read and write from a dictionary. You can declare these functions like so: declare function dict_get_value(dict object, key_ text not null) text;  Such a function is not known to SQLite and therefore cannot appear in SQL statements. CQL will enforce this. The above function returns a text reference, and, importantly, this is a borrowed reference. The dictionary is presumably holding on to the reference and as long as it is not mutated the reference is valid. CQL will retain this reference as soon as it is stored and release it automatically when it is out of scope. So, in this case, the dictionary continues to own the object. It is also possible to declare functions that create objects. Such as this example: declare function dict_create() create object;  This declaration tells CQL that the function will create a new object for our use. CQL does not retain the provided object, rather assuming ownership of the presumably one reference count the object already has. When the object goes out of scope it is released as usual. If we also declare this procedure: declare procedure dict_add( dict object not null, key_ text not null, value text not null);  then with this family of declarations we could write something like this: create proc create_and_init(out dict object not null) begin set dict := dict_create(); call dict_add(dict, &quot;k1&quot;, &quot;v1&quot;); call dict_add(dict, &quot;k2&quot;, &quot;v2&quot;); if (dict_get_value(dict, &quot;k1&quot;) == dict__get_value(dict, &quot;k2&quot;)) then call printf(&quot;insanity has ensued\\n&quot;); end if; end;  Note: Ordinary scalar functions may not use the database in any way. When they are invoked they will not be provided with the database pointer and so they will be unable to do any database operations. To do database operations, use regular procedures. You can create a function-like-procedure using the out convention discussed previously. SQL Scalar Functions​ SQLite includes the ability to add new functions to its expressions using sqlite3_create_function. In order to use this function in CQL, you must also provide its prototype definition to the compiler. You can do so following this example: declare select function strencode(t text not null) text not null;  This introduces the function strencode to the compiler for use in SQL constructs. With this done you could write a procedure something like this: create table foo(id integer, t text); create procedure bar(id_ integer) begin select strencode(T1.t) from foo T1 where T1.id = id_; end;  This presumably returns the &quot;encoded&quot; text, whatever that might be. Note that if sqlite3_create_functionis not called before this code runs, a run-time error will ensue. Just as CQL must assume that declared tables really are created, it also assumes that declared function really are created. This is another case of telling the compiler in advance what the situation will be at runtime. SQLite allows for many flexible kinds of user defined functions. CQL doesn't concern itself with the details of the implementation of the function, it only needs the signature so that it can validate calls. Note that SQL Scalar Functions cannot contain object parameters. To pass an object, you should instead pass the memory address of this object using a LONG INT parameter. To access the address of an object at runtime, you should use the ptr() function. See the notes section below for more information. See also: Create Or Redefine SQL Functions. SQL Table Valued Functions​ More recent versions of SQLite also include the ability to add table-valued functions to statements in place of actual tables. These functions can use their arguments to create a &quot;virtual table&quot; value for use in place of a table. For this to work, again SQLite must be told of the existence of the table. There are a series of steps to make this happen beginning with sqlite3_create_module which are described in the SQLite documents under &quot;The Virtual Table Mechanism Of SQLite.&quot; Once that has been done, a table-valued function can be defined for most object types. For instance it is possible to create a table-valued function like so: declare select function dict_contents(dict object not null) (k text not null, v text not null);  This is just like the previous type of select function but the return type is a table shape. Once the above has been done you can legally write something like this: create proc read_dict(dict object not null, pattern text) begin if pattern is not null then select k, v from dict_contents(dict) T1 where T1.k LIKE pattern; else select k, v from dict_contents(dict); end if; end;  This construct is very general indeed but the runtime set up for it is much more complicated than scalar functions and only more modern versions of SQLite even support it. "},{"title":"SQL Functions with Unchecked Parameter Types​","type":1,"pageTitle":"Chapter 8: Functions","url":"/cql-guide/ch08#sql-functions-with-unchecked-parameter-types","content":"Certain SQL functions like json_extract are variadic (they accept variable number of arguments). To use such functions within CQL, you can declare a SQL function to have untyped parameters by including the NO CHECK clause instead of parameter types. For example: declare select function json_extract no check text;  This is also supported for SQL table-valued functions: declare select function table_valued_function no check (t text, i int);  Note: currently the NO CHECK clause is not supported for non SQL Ordinary Scalar Functions. "},{"title":"Notes on Builtin Functions​","type":1,"pageTitle":"Chapter 8: Functions","url":"/cql-guide/ch08#notes-on-builtin-functions","content":"Some of the SQLite builtin functions are hard-coded; these are the functions that have semantics that are not readily captured with a simple prototype. Other SQLite functions can be declared with declare select function ... and then used. CQL's hard-coded builtin list includes: Aggregate Functions countmaxminsumtotalavggroup_concat Scalar Functions ifnullnullifuppercharabsinstrcoalescelast_insert_rowidprintfstrftimedatetimedatetimejuliandaysubstrreplaceroundtrimltrimrtrim Window Functions row_numberrankdense_rankpercent_rankcume_distntilelagleadfirst_valuelast_valuenth_value Special Functions nullablesensitiveptr Nullable casts an operand to the nullable version of its type and otherwise does nothing. This cast might be useful if you need an exact type match in a situation. It is stripped from any generated SQL and generated C so it has no runtime effect at all other than the indirect consequences of changing the storage class of its operand. Sensitive casts an operand to the sensitive version of its type and otherwise does nothing. This cast might be useful if you need an exact type match in a situation. It is stripped from any generated SQL and generated C so it has no runtime effect at all other than the indirect consequences of changing the storage class of its operand. Ptr is used to cause a reference type variable to be bound as a long integer to SQLite. This is a way of giving object pointers to SQLite UDFs. Not all versions of Sqlite support binding object variables, so passing memory addresses is the best we can do on all versions. "},{"title":"Chapter 9: Statements Summary and Error Checking","type":0,"sectionRef":"#","url":"/cql-guide/ch09","content":"","keywords":""},{"title":"The Primary SQL Statements​","type":1,"pageTitle":"Chapter 9: Statements Summary and Error Checking","url":"/cql-guide/ch09#the-primary-sql-statements","content":"These are, roughly, the statements that involve the database. The SELECT Statement​ Top level statement list processing for select. This is easily the hardest statement to process. Each clause has its own set of complex rules and the result of previous clauses constrains the next in a complex fashion. Among the things that are verified: the mentioned tables exist and have the mentioned columnsthe columns are type compatible in their contextany variables in the expressions are compatibleaggregate functions are used only in places where aggregation makes sensecolumn and table names are unambiguous, especially when self-joins are involvedcompound selects (e.g. with UNION) are type-consistent in all the fragmentsthe projection of a select has unique column labels if they are used The SELECT * Statement​ SELECT * is special in that it creates its own struct type by assembling all the columns of all the tables in the select's join result. CQL rewrites these column names into a new SELECT with the specific columns explicitly listed. While this makes the program slightly bigger it means that logically deleted columns are never present in results because SELECT * won't select them and attempting to use a logically deleted column results in an error. The CREATE TABLE Statement​ Unlike the other parts of DDL we actually deeply care about the tables. We have to grab all the columns and column types out of it and create the appropriate structure type for the table. Along the way we validate a bunch of stuff like: verify unique table nameno duplicate column namesrecursive correctness of constraints (see constraints discussion below) The UNIQUE KEY Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. Again, we use the usual helpers for name lookup within the context of the table that contains the constraint. The FOREIGN KEY Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. Again, we use the usual helpers for name lookup within the context of the table with the foreign key. Note that the foreign key has to be validated against two tables to fully validate it. The PRIMARY KEY Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. Again, we use the usual helpers for name lookup within the context of the table with the primary key. The CHECK Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. The CHECK clause is validated after the entire table has been processed so that even if it appears early in the table, the clause can use any columns defined later in the table. The CREATE INDEX Statement​ CQL doesn't really do anything with indices but we do validate that they make sense (so we lookup all the names of all the columns and so forth.) The CREATE VIEW Statement​ Create view analysis is very simple because the select analysis does the heavy lifting. All we have to do is validate that the view is unique, then validate the select statement. Additionally, views must not be allowed to have any NULL type columns; all nulls must be converted to some type with a CAST. e.g. create view foo as select NULL n is not valid. NULL is not a real storage type. The CREATE TRIGGER Statement​ The create trigger statement is quite a beast, and validations include: The trigger name must be uniqueFor insert the &quot;new.*&quot; table is available in expressions/statementFor delete the &quot;old.*&quot; table is available in expressions/statementsFor update both are available If optional columns present in the update, they must be unique/valid The when expression must evaluate to a numericThe statement list must be error free with the usual rules plus new/oldThe raise function may be used inside a trigger (NYI)The table name must be a table (not a view) UNLESS the trigger type is INSTEAD OFSelect statements inside the statement block do not count as returns for the procedure and that includes the create trigger The DROP TABLE Statement​ This is the basic checking for the drop table statement: the table must exist in some versionit has to be a table and not a view The DROP VIEW Statement​ This is the basic checking for the drop view statement: the view must exist in some versionit has to be a view and not a table The DROP INDEX Statement​ This is the basic checking for the drop index statement: the index must exist in some versionit could be deleted now, that's ok, but the name has to be valid The DROP TRIGGER Statement​ This is the basic checking for the drop trigger statement the trigger must exist in some versionit could be deleted now, that's ok, but the name has to be valid The RAISE Statement​ CQL validates that RAISE is being used in the context of a trigger and that it has the correct arguments The ALTER TABLE ADD COLUMN Statement​ To validate alter table add column we check the following: the table must exist and not be a view (in any version)the column definition of the new column must be self-consistentno auto-increment columns may be addedadded columns must be either nullable or have a default value Note: Alter statements are typically used in the context of migration, so it's possible the table that is mentioned is condemned in a future version. We still have to run the intervening upgrade steps so basically DDL gets to ignore the current deadness of the table as in context it might be &quot;not dead yet&quot;. This will be more obvious in the context of the schema maintenance features. (q.v.) The DELETE Statement​ The delete analyzer sets up a scope for the table being deleted and then validates the WHERE clause, if present, against that scope. Additionally, we verify that the table actually was defined and is not a view. The UPDATE Statement​ The update analyzer sets up the scope for the table(s) being updated. If there are optional clauses (e.g. LIMIT), they are evaluated just like in a select statement with those same helper methods. Expression fragments are evaluated similarly as in a select statement. The INSERT Statement​ We check that the table exists and then we walk the columns and the value list to make sure they are valid for the table. Also, we cannot insert into a view. Details: The column list specifies the columns we will provide; they must exist and be unique.The columns specified must suffice to insert a row (all not nulls and not default present.)The insert list specifies the values that are to be inserted.The type of each value must match the type of the column.Auto-increment columns may be specified as NULL.If there are too many or too few columns, that is considered an error.If no columns are specified, that is the same as if all columns had been specified, in table order. The THROW Statement​ Throw can literally go anywhere, so it's always ok. The BEGIN TRANSACTION Statement​ Begin transaction can go anywhere, so it's always ok. The sqlite documentation can be helpful here (CQL syntax is a subset). See: https://www.sqlite.org/lang_transaction.html The COMMIT TRANSACTION Statement​ Commit transaction can go anywhere, so it's always ok. The sqlite documentation can be helpful here (CQL syntax is a subset). See: https://www.sqlite.org/lang_transaction.html The ROLLBACK TRANSACTION Statement​ Rollback transaction can go anywhere but if you're using the format where you rollback to a particular save point, then we must have seen that name in a savepoint statement previously. Otherwise, it's an error. The sqlite documentation can be helpful here again (CQL syntax is a subset). See: https://www.sqlite.org/lang_transaction.html The SAVEPOINT Statement​ The savepoint statement can go anywhere but we do record this savepoint name as having been seen, so that we can verify it in rollback. So this is sort of a weak declaration of the savepoint name. The sqlite documentation can be helpful here (CQL syntax is a subset). https://www.sqlite.org/lang_savepoint.html The RELEASE SAVEPOINT Statement​ Release savepoint can go anywhere but we must have seen that name in a previous savepoint statement, otherwise it's an error. The sqlite documentation can be helpful here (CQL syntax is a subset). https://www.sqlite.org/lang_savepoint.html The PROCEDURE SAVEPOINT Statement​ A common pattern is to have a savepoint associated with a particular procedure. The savepoint's scope is the same as the procedure's scope. More precisely create procedure foo() begin proc savepoint begin -- your code end; end;  becomes: create procedure foo() begin savepoint @proc; -- @proc is always the name of the current procedure begin try -- your code release savepoint @proc; end try; begin catch rollback transaction to savepoint @proc; release savepoint @proc; throw; end catch; end;  This form is not quite syntactic sugar because there are some interesting rules: the proc savepoint form must be used at the top level of the procedure, hence no leave or continue may escape itwithin begin/end the return form may not be used; you must use rollback return or commit return (see below)throw may be used to return an error as usualproc savepoint may be used again, at the top level, in the same procedure, if there are, for instance, several sequential stagesa procedure using proc savepoint could call another such procedure, or a procedure that manipulates savepoints in some other way The ROLLBACK RETURN Statement​ This form may be used only inside of a proc savepoint block. It indicates that the savepoint should be rolled back and then the procedure should return. It is exactly equivalent to:  rollback transaction to savepoint @proc; release savepoint @proc; return; -- wouldn't actually be allowed inside of proc savepoint; see note below  Note: to avoid errors, the loose return above is not actually allowed inside of proc savepoint -- you must use rollback return or commit return. The COMMIT RETURN Statement​ This form may be used only inside of a proc savepoint block. It indicates that the savepoint should be released and then the procedure should return. It is exactly equivalent to:  release savepoint @proc; return; -- wouldn't actually be allowed inside of proc savepoint; see note below  Of course this isn't exactly a commit, in that there might be an outer savepoint or outer transaction that might still be rolled back, but it is commited at its level of nesting, if you will. Or, equivalently, you can think of it as merging the savepoint into the transaction in flight. Note: to avoid errors, the loose return above is not actually allowed inside of proc savepoint and you must use rollback return or commit return. The CREATE VIRTUAL TABLE Statement​ The SQLite CREATE VIRTUAL TABLE form (https://sqlite.org/lang_createvtab.html) is problematic from CQL because: it is not parseable, because the module arguments can be literally anything (or nothing), even a letter to your grandmathe arguments do not necessarily say anything about the table's schema at all So the CQL form departs from the standard syntax to this form: create virtual table virt_table using my_module [(module arguments)] as ( id integer not null, name text );  The part after the AS is used by CQL as a table declaration for the virtual table. The grammar for that is exactly the same as a normal CREATE TABLE statement. However, that part is not transmitted to SQLite; when the table is created, SQLite sees only the part it cares about, which is the part before the AS. In order to have strict parsing rules, the module arguments follow one of these forms: no arguments at alla list of identifiers, constants, and parenthesized sublists, just like in the @attribute formthe words arguments following Case 1 Example​ create virtual table virt_table using my_module as ( id integer not null, name text );  becomes (to SQLite) CREATE VIRTUAL TABLE virt_table USING my_module;  Note: empty arguments USING my_module() are not allowed in the SQLite docs but do seem to work in SQLite. We take the position that no args should be formatted with no parentheses, at least for now. Case 2 Example​ create virtual table virt_table using my_module(foo, 'goo', (1.5, (bar, baz))) as ( id integer not null, name text );  CREATE VIRTUAL TABLE virt_table USING my_module(foo, &quot;goo&quot;, (1.5, (bar, baz)));  This form allows for very flexible arguments but not totally arbitrary arguments, so it can still be parsed and validated. Case 3 Example​ This case recognizes the popular choice that the arguments are often the actual schema declaration for the table in question. So: create virtual table virt_table using my_module(arguments following) as ( id integer not null, name text );  becomes CREATE VIRTUAL TABLE virt_table USING my_module( id INTEGER NOT NULL, name TEXT );  The normalized text (keywords capitalized, whitespace normalized) of the table declaration in the as clause is used as the arguments. Other details​ Virtual tables go into their own section in the JSON and they include the module and moduleArgs entries; they are additionally marked isVirtual in case you want to use the same processing code for virtual tables as normal tables. The JSON format is otherwise the same, although some things can't happen in virtual tables (e.g. there is no TEMP option so &quot;isTemp&quot; must be false in the JSON.) For purposes of schema processing, virtual tables are on the @recreate plan, just like indices, triggers, etc. This is the only option since the alter table form is not allowed on a virtual table. Semantic validation enforces &quot;no alter statements on virtual tables&quot; as well as other things like no indices, and no triggers, since SQLite does not support any of those things. CQL supports the notion of eponymous virtual tables. If you intend to register the virtual table's module in this fashion, you can use create virtual table @eponymous ... to declare this to CQL. The only effect this has is to ensure that CQL will not try to drop this table during schema maintenance as dropping such a table is an invalid operation. In all other ways, the fact that the table is eponymous makes no difference. Finally, because virtual tables are on the @recreate plan, you may not have foreign keys that reference virtual tables. Such keys seem like a bad idea in any case. "},{"title":"The Primary Procedure Statements​","type":1,"pageTitle":"Chapter 9: Statements Summary and Error Checking","url":"/cql-guide/ch09#the-primary-procedure-statements","content":"These are the statements which form the language of procedures, and do not involve the database. The CREATE PROCEDURE Statement​ Semantic analysis of stored procedures is fairly easy at the core: check for duplicate namesvalidate the parameters are well formedset the current proc in flight (this not allowed to nest)recurse on the statement list and prop errorsrecord the name of the procedure for callers In addition, while processing the statement:we determine if it uses the database; this will change the emitted signature of the proc to include a sqlite3 *dbinput argument and it will return a sqlite error code (e.g. SQLITE_OK)select statements that are loose in the proc represent the &quot;return&quot; of that select; this changes the signature to include a sqlite3_stmt **pstmt parameter corresponding to the returned statement The IF Statement​ The top level if node links the initial condition with a possible series of else_if nodes and then the else node. Each condition is checked for validity. The conditions must be valid expressions that can each be converted to a boolean. The SET Statement​ The set statement is for variable assignment. We just validate that the target exists and is compatible with the source. Cursor variables cannot be set with simple assignment and CQL generates errors if you attempt to do so. The LET Statement​ Let combines a DECLARE and a SET. The variable is declared to be the exact type of the right hand side. All the validations for DECLAREand SET are applicable, but there is no chance that the variable will not be compatible with the expression. The expression could still be erroneous in the first place. The variable could be a duplicate. The SWITCH Statement​ The SWITCH form requires a number of conditions to successfully map down to a C switch statement. These are: the switch-expression must be a not-null integral type (integer not null or long integer not null) the WHEN expressions must be losslessly promotable to the type of the switch-expression the values in the WHEN clauses must be uniqueIf ALL VALUES is present then: the switch-expression must be of an enum typethe WHEN values must cover every value of the enum except those beginning with '_'there can be no extra WHEN values not in the enumthere can be no ELSE clause The DECLARE PROCEDURE Statement​ There are three forms of this declaration: a regular procedure with no DML e.g. declare proc X(id integer); a regular procedure that uses DML (it will need a db parameter and returns a result code) e.g. declare proc X(id integer) using transaction; a procedure that returns a result set, and you provide the result columns e.g. declare proc X(id integer) : (A bool not null, B text);The main validations here are that there are no duplicate parameter names, or return value columns. The DECLARE FUNCTION Statement​ Function declarations are similar to procedures; there must be a return type (use proc if there is none). The DECLARE SELECT FUNCTION form indicates a function visible to SQLite; other functions are usable in the call statement. The DECLARE Variable Statement​ This declares a new local or global variable that is not a cursor. The type is computed with the same helper that is used for analyzing column definitions. Once we have the type we walk the list of variable names, check them for duplicates and such (see above) and assign their type. The canonical name of the variable is defined here. If it is later used with a different casing the output will always be as declared. e.g. declare Foo integer; set foo = 1; is legal but the output will always contain the variable written as Foo. The DECLARE Cursor Statement​ There are two forms of the declare cursor, both of which allow CQL to infer the exact type of the cursor. declare foo cursor for select etc. the type of the cursor is the net struct type of the select list declare foo cursor for call proc(); proc must be statement that produces a result set via select (see above)the type of the cursor is the struct of the select returned by the procnote if there is more than one loose select in the proc they must match exactly cursor names have the same rules regarding duplicates as other variables With this in mind, both cases simply recurse on either the select or the call and then pull out the structure type of that thing and use it for the cursor's shape. If thecall is not semantically valid according to the rules for calls or the select is not semantically valid, then of course this declaration will generate errors. The DECLARE Value Cursor Statement​ This statement declares a cursor that will be based on the return type of a procedure. When using this form the cursor is also fetched, hence the name. The fetch result of the stored proc will be used for the value. At this point, we use its type only. the call must be semantically validthe procedure must return an OUT parameter (not a result set)the cursor name must be unique The WHILE Statement​ While semantic analysis is super simple. the condition must be numericthe statement list must be error-freeloop_depth is increased allowing the use of interior leave/continue The LOOP Statement​ Loop analysis is just as simple as &quot;while&quot; -- because the loop_stmt literally has an embedded fetch, you simply use the fetch helper to validate that the fetch is good and then visit the statement list. Loop depth is increased as it is with while. The CALL Statement​ There are three ways that a call can happen: signatures of procedures that we know in full: call foo();declare cursor for call foo(); some external call to some outside function we don't know e.g. call printf('hello, world\\n'); The cursor form can be used if and only if the procedure has a loose select or a call to a procedure with a loose select. In that case, the procedure will have a structure type, rather than just &quot;ok&quot; (the normal signature for a proc). If the user is attempting to do the second case, cursor_name will be set and the appropriate verification happens here. Note: Recursively calling fetch cursor is not really doable in general because at the point in the call we might not yet know that the method does in fact return a select. You could make it work if you put the select before the recursive call. Semantic rules: for all cases each argument must be error-free (no internal type conflicts)for known procs the call has to have the correct number of argumentsif the formal is an out parameter the argument must be a variable the type of the variable must be an exact type match for the formal non-out parameters must be type-compatible, but exact match is not required The DECLARE OUT CALL Statement​ This form is syntactic sugar and corresponds to declaring any OUT parameters of the CALL portion that are not already declared as the exact type of theOUT parameter. This is intended to save you from declaring a lot of variables just so that you can use them as OUT arguments. Since any variables that already exist are not re-declared, there are no additional semantic rules beyond the normal call except that it is an error to use this form if no OUT variables needed to be declared. The FETCH Statement​ The fetch statement has two forms: fetch C into var1, var2, var3 etc.fetch C; The second form is called the auto_cursor. In the first form the variables of the cursor must be assignment compatible with declared structure type of the cursor and the count must be correct. In the second form, the codegen will implicitly create local variables that are exactly the correct type, but we'll cover that later. Since no semantic error is possible in that case, we simply record that this is an auto_cursor and then later we will allow the use of C.field during analysis. Of course &quot;C&quot; must be a valid cursor. The CONTINUE Statement​ We just need to ensure that continue is inside a loop or while. The LEAVE Statement​ We only need to ensure that leave is inside a loop, while or switch. The TRY/CATCH Statements​ No analysis needed here other than that the two statement lists are ok. The CLOSE CURSOR Statement​ For close [cursor], we just validate that the name is in fact a cursor and it is not a boxed cursor. Boxed cursor lifetime is managed by the box object so manually closing it is not allowed. Instead, the usual reference-counting semantics apply; the boxed cursor variable typically falls out of scope and is released, or is perhaps set to NULL to release its reference early. The OUT CURSOR Statement​ For out [cursor], we first validate that the name is a cursor then we set the output type of the procedure we're in accordingly. "},{"title":"The \"Meta\" Statements​","type":1,"pageTitle":"Chapter 9: Statements Summary and Error Checking","url":"/cql-guide/ch09#the-meta-statements","content":"The program's control/ the overall meaning of the program / or may give the compiler specific directives as to how the program should be compiled. The @ECHO Statement​ Echo is valid in any top level contexts. The @PREVIOUS SCHEMA Statement​ Begins the region where previous schema will be compared against what has been declared before this directive for alterations that could not be upgraded. The @SCHEMA_UPGRADE_SCRIPT Statement​ When upgrading the DDL, it's necessary to emit create table statements for the original version of the schema. These create statements may conflict with the current version of the schema. This attribute tells CQL to 1) ignore DDL in stored procedures for declaration purposes; only DDL outside of a proc counts 2) do not make any columns &quot;hidden&quot; thereby allowing all annotations to be present so they can be used to validate other aspects of the migration script. The @SCHEMA_UPGRADE_VERSION Statement​ For sql stored procedures that are supposed to update previous schema versions you can use this attribute to put CQL into that mindset. This will make the columns hidden for the version in question rather than the current version. This is important because older schema migration procedures might still refer to old columns. Those columns truly exist at that schema version. The @ENFORCE_STRICT Statement​ Switch to strict mode for the indicated item. The choices and their meanings are: &quot;FOREIGN KEY ON DELETE&quot; indicates there must be some ON DELETE action in every FK&quot;FOREIGN KEY ON UPDATE&quot; indicates there must be some ON UPDATE action in every FK&quot;INSERT SELECT&quot; indicates that insert with SELECT for values may not include top level joins (avoiding a SQLite bug)&quot;IS TRUE&quot; indicates that IS TRUE IS FALSE IS NOT TRUE IS NOT FALSE may not be used (*)&quot;JOIN&quot; indicates only ANSI style joins may be used, and &quot;from A,B&quot; is rejected&quot;PROCEDURE&quot; indicates no calls to undeclared procedures (like loose printf calls)&quot;SELECT IF NOTHING&quot; indicates (select ...) expressions must include an IF NOTHING clause if they have a FROM part&quot;TABLE FUNCTIONS&quot; indicates table valued functions cannot be used on left/right joins (avoiding a SQLite bug)&quot;TRANSACTION&quot; indicates no transactions may be started, committed, or aborted&quot;UPSERT&quot; indicates no upsert statement may be used (*)&quot;WINDOW FUNCTION&quot; indicates no window functions may be used (*)&quot;WITHOUT ROWID&quot; indicates WITHOUT ROWID may not be used The items marked with * are present so that features can be disabled to target downlevel versions of SQLite that may not have those features. See the grammar details for exact syntax. The @ENFORCE_NORMAL Statement​ Turn off strict enforcement for the indicated item. The @ENFORCE_PUSH Statement​ Push the current strict settings onto the enforcement stack. This does not change the current settings. The @ENFORCE_POP Statement​ Pop the previous current strict settings from the enforcement stack. The @ENFORCE_RESET Statement​ Turns off all the strict modes. Best used immediately after @ENFORCE_PUSH. The @DECLARE_SCHEMA_REGION Statement​ A schema region is a partitioning of the schema such that it only uses objects in the same partition or one of its declared dependencies. One schema region may be upgraded independently from any others (assuming they happen such that dependents are done first.) Here we validate: the region name is uniquethe dependencies (if any) are unique and existthe directive is not inside a procedure The @BEGIN_SCHEMA_REGION Statement​ Entering a schema region makes all the objects that follow part of that region. It also means that all the contained objects must refer to only pieces of schema that are in the same region or a dependent region. Here we validate that region we are entering is in fact a valid region and that there isn't already a schema region. The @END_SCHEMA_REGION Statement​ Leaving a schema region puts you back in the default region. Here we check that we are in a schema region. The @EMIT_ENUMS Statement​ Declared enumarations can be voluminous and it is undesirable for every emitted .h file to contain every enumeration. To avoid this problem you can emit enumeration values of your choice using @emit_enums x, y, zwhich places the named enumerations into the .h file associated with the current translation unit. If no enumerations are listed, all enums are emitted. Note: generated enum definitions are protected by #ifndef X ... #endif so multiple definitions are harmless and hence you can afford to use @emit_enumsfor the same enum in several translations units, if desired. Note: Enumeration values also appear in the JSON output in their own section. The @EMIT_CONSTANTS Statement​ This statement is entirely analogous to the the @EMIT_ENUMS except that the parameters are one or more constant groups. In fact constants are put into groups precisely so that they can be emitted in logical bundles (and to encourage keeping related constants together). Placing @EMIT_CONSTANTScauses the C version of the named groups to go into the current .h file. Note: Global constants also appear in the JSON output in their own section. "},{"title":"Important Program Fragments​","type":1,"pageTitle":"Chapter 9: Statements Summary and Error Checking","url":"/cql-guide/ch09#important-program-fragments","content":"These items appear in a variety of places and are worthy of discussion. They are generally handled uniformly. Argument Lists​ In each case we walk the entire list and do the type inference on each argument. Note that this happens in the context of a function call, and depending on what the function is, there may be additional rules for compatibility of the arguments with the function. The generic code doesn't do those checks, there is per-function code that handles that sort of thing. At this stage the compiler computes the type of each argument and makes sure that, independently, they are not bogus. Procedures that return a Result Set​ If a procedure is returning a select statement then we need to attach a result type to the procedure's semantic info. We have to do some extra validation at this point, especially if the procedure already has some other select that might be returned. The compiler ensures that all the possible select results are are 100% compatible. General Name Lookups​ Every name is checked in a series of locations. If the name is known to be a table, view, cursor, or some other specific type of object then only those name are considered. If the name is more general a wider search is used. Among the places that are considered: columns in the current join if any (this must not conflict with #2)local or global variablesfields in an open cursorfields in enumerations and global constants Data Types with a Discriminator​ Discriminators can appear on any type, int, real, object, etc. Where there is a discriminator the compiler checks that (e.g.) object&lt;Foo&gt; only combines with object&lt;Foo&gt; or object. real&lt;meters&gt; only combines with real&lt;meters&gt; or real. In this way its not possible to accidentally add meters to kilograms or to store an int&lt;task_id&gt; where an int&lt;person_id&gt; is required. The CASE Expression​ There are two parts to this: the &quot;when&quot; expression and the &quot;then&quot; expression. We compute the aggregate type of the &quot;when&quot; expressions as we go, promoting it up to a larger type if needed (e.g. if one &quot;when&quot; is an int and the other is a real, then the result is a real). Likewise, nullability is computed as the aggregate. Note that if nothing matches, the result is null, so we always get a nullable resultm unless there is an &quot;else&quot; expression. If we started with case expression, then each &quot;when&quot; expression must be comparable to the case expression. If we started with case when xx then yy; then each case expression must be numeric (typically boolean). The BETWEEN EXPRESSIONS​ Between requires type compatibility between all three of its arguments. Nullability follows the usual rules: if any might be null then the result type might be null. In any case, the result's core type is BOOL. The CAST Expression​ For cast expressions we use the provided semantic type; the only trick is that we preserve the extra properties of the input argument. e.g. CAST does not remove NOT NULL. The COALESCE Function​ Coalesce requires type compatibility between all of its arguments. The result is a not null type if we find a not null item in the list. There should be nothing after that item. Note that ifnull and coalesce are really the same thing except ifnull must have exactly two arguments. The IN AND NOT IN Expressions​ The in predicate is like many of the other multi-argument operators. All the items must be type compatible. Note that in this case the nullablity of the items does not matter, only the nullability of the item being tested. Note that null in (null) is null, not true. Aggregate Functions​ Aggregate functions can only be used in certain places. For instance they may not appear in a WHERE clause. User Defined Functions​ User defined function - this is an external function. There are a few things to check: If this is declared without the select keyword then we can't use these in SQL, so this has to be a loose expression If this is declared with the select keyword then we can ONLY use these in SQL, not in a loose expression args have to be compatible with formals Calling a procedure as a function​ There are a few things to check: we can't use these in SQL, so this has to be a loose expressionargs have to be compatible with formals, exceptthe last formal must be an OUT arg and it must be a scalar typethat out arg will be treated as the return value of the &quot;function&quot;in code-gen we will create a temporary for it; semantic analysis doesn't care Root Expressions​ A top level expression defines the context for that evaluation. Different expressions can have constraints. e.g. aggregate functions may not appear in the WHERE clause of a statement. There are cases where expression nesting can happen. This nesting changes the evaluation context accordingly, e.g. you can put a nested select in a where clause and that nested select could legally have aggregates. Root expressions keep a stack of nested contexts to facilitate the changes. Table Factors​ A table factor is one of three things: a table name (a string) select * from Xa select subquery (select X,Y from..) as T2a list of table references select * from (X, Y, Z) Here we dispatch to the appropriate helper for each case. Joining with the USING Clause​ When specifying joins, one of the alternatives is to give the shared columns in the join e.g. select * from X inner join Y using (a,b). This method validates that all the columns are present on both sides of the join, that they are unique, and they are comparable. The return code tells us if any columns had SENSITIVE data. See Special Note on JOIN...USING below JOIN WITH THE ON Clause​ The most explicit join condition is a full expression in an ON clause this is like select a,b from X inner join Y on X.id = Y.id;The on expression should be something that can be used as a bool, so any numeric will do. The return code tells us if the ON condition used SENSITIVE data. TABLE VALUED FUNCTIONS​ Table valued functions can appear anywhere a table is allowed. The validation rules are: must be a valid function must return a struct type (i.e. a table-valued-function) must have valid arg expressions arg expressions must match formal parameters The name of the resulting table is the name of the function but it can be aliased later with &quot;AS&quot; Special Note on the select * and select T.* forms​ The select * construct is very popular in many codebases but it can be unsafe to use in production code because, if the schema changes, the code might get columns it does not expect. Note the extra columns could have appeared anywhere in the result set because the * applies to the entire result of the FROM clause, joins and all, so extra columns are not necessarily at the end and column ordinals are not preserved. CQL mitigates this situation somewhat with some useful constraints/features: in a select *, and indeed in any query, the column names of the select must be unique, this is because: they could form the field names of an automatically generated cursor (see the section on cursors)they could form the field names in a CQL result set (see section on result sets)it's weird/confusing to not have unique names generally when issuing a select * or a select T.* CQL will automatically expand the * into the actual logical columns that exist in the schema at the time the code was compiled this is important because if a column had been logically deleted from a table it would be unexpected in the result set even though it is still present in the database and would throw everything offlikewise if the schema were to change without updating the code, the code will still get the columns it was compiled with, not new columns Expanding the * at compile time means Sqlite cannot see anything that might tempt it to include different columns in the result. With this done we just have to look at the places a select * might appear so we can see if it is safe (or at least reasonably safe) to use * and, by extension of the same argument, T.*. In an EXISTS or NOT EXISTS clause like `where not exists (select from x)`* this is perfectly safe; the particular columns do not matter; select * is not even expanded in this case. In a statement that produces a result set like `select from table_or_view`* binding to a CQL result set is done by column name and we know those names are uniquewe won't include any columns that are logically deleted, so if you try to use a deleted column you'll get a compile time error In a cursor statement like declare C cursor for select * from table_or_view there are two cases here: Automatic Fetch fetch C; in this case you don't specify the column names yourself;2 they are inferredyou are therefore binding to the columns by name, so new columns in the cursor would be unused (until you choose to start using them)if you try to access a deleted column you get a compile-time error Manual Fetch: fetch C into a, b, c; In this case the number and type of the columns must match exactly with the specified variablesIf new columns are added, deleted, or changed, the above code will not compile So considering the cases above we can conclude that auto expanding the * into the exact columns present in the compile-time schema version ensures that any incompatible changes result in compile time errors. Adding columns to tables does not cause problems even if the code is not recompiled. This makes the * construct much safer, if not perfect, but no semantic would be safe from arbitrary schema changes without recompilation. At the very least here we can expect a meaningful runtime error rather than silently fetching the wrong columns. "},{"title":"Special Note on the JOIN...USING form​","type":1,"pageTitle":"Chapter 9: Statements Summary and Error Checking","url":"/cql-guide/ch09#special-note-on-the-joinusing-form","content":"CQL varies slightly from SQLite in terms of the expected results for joins if the USING syntax is employed. This is not the most common syntax (typically an ON clause is used) but Sqlite has special rules for this kind of join. Let's take a quick look. First some sample data: create table A( id integer, a text, b text); create table B( id integer, c text, d text); insert into A values(1, 'a1', 'b1'); insert into B values(1, 'c1', 'd1'); insert into A values(2, 'a2', 'b2'); insert into B values(2, 'c2', 'd2');  Now let's look at the normal join; this is our reference: select * from A T1 inner join B T2 on T1.id = T2.id; result: 1|a1|b1|1|c1|d1 2|a2|b2|2|c2|d2  As expected, you get all the columns of A, and all the columns of B. The 'id' column appears twice. However, with the USING syntax: select * T1 inner join B T2 using (id); result: 1|a1|b1|c1|d1 2|a2|b2|c2|d2  The id column is now appearing exactly once. However, the situation is not so simple as that. It seems that what hapened was that the * expansion has not included two copies of the id. The following cases show that both copies of id are still logically in the join. select T1.*, 'xxx', T2.* from A T1 inner join B T2 using (id); result: 1|a1|b1|xxx|1|c1|d1 2|a2|b2|xxx|2|c2|d2  The T2.id column is part of the join, it just wasn't part of the * In fact, looking further: select T1.id, T1.a, T1.b, 'xxx', T2.id, T2.c, T2.d from A T1 inner join B T2 using (id); result: 1|a1|b1|xxx|1|c1|d1 2|a2|b2|xxx|2|c2|d2  There is no doubt, T2.id is a valid column and can be used in expressions freely. That means the column cannot be removed from the type calculus. Now in CQL, the * and T.* forms are automatically expanded; SQLite doesn't see the *. This is done so that if any columns have been logically deleted they can be elided from the result set. Given that this happens, the * operator will expand to ALL the columns. Just the same as if you did T1.* and T2.*. As a result, in CQL, there is no difference between the USING form of a join and the ON form of a join. In fact, only the select * form could possibly be different, so in most cases this ends up being moot anyway. Typically, you don't need to use * in the presence of joins because of name duplication and ambiguity of the column names of the result set. CQL's automatic expansion means you have a much better idea exactly what columns you will get - those that were present in the schema you declared. "},{"title":"Chapter 7: CQL Result Sets","type":0,"sectionRef":"#","url":"/cql-guide/ch07","content":"","keywords":""},{"title":"Results Sets From OUT UNION​","type":1,"pageTitle":"Chapter 7: CQL Result Sets","url":"/cql-guide/ch07#results-sets-from-out-union","content":"The out keyword was added for writing procedures that produce a single row result set. With that, it became possible to make any single row result you wanted, assembling it from whatever sources you needed. That is an important case as single row results happen frequently and they are comparatively easy to create and pass around using C structures for the backing store. However, it's not everything; there are also cases where full flexibility is needed while producing a standard many-row result set. For this we have out union which was discussed fully in Chapter 5. Here we'll discuss the code generation behind that. Here’s an example from the CQL tests: create proc some_integers(start integer not null, stop integer not null) begin declare C cursor like select 1 v, 2 v_squared, &quot;xx&quot; some_text; declare i integer not null; set i := start; while (i &lt; stop) begin fetch C(v, v_squared, junk) from values (i, i*i, printf(&quot;%d&quot;, i)); out union C; set i := i + 1; end; end;  In this example the entire result set is made up out of thin air. Of course any combination of this computation or data-access is possible, so you can ultimately make any rows you want in any order using SQLite to help you as much or as little as you need. Virtually all the code pieces to do this already exist for normal result sets. The important parts of the output code look like this in your generated C. We need a buffer to hold the rows we are going to accumulate; We use cql_bytebuf just like the normal fetcher above. // This bit creates a growable buffer to hold the rows // This is how we do all the other result sets, too cql_bytebuf _rows_; cql_bytebuf_open(&amp;_rows_);  We need to be able to copy the cursor into the buffer and retain any internal references // This bit is what you get when you &quot;out union&quot; a cursor &quot;C&quot; // first we +1 any references in the cursor then we copy its bits cql_retain_row(C_); // a no-op if there is no row in the cursor if (C_._has_row_) cql_bytebuf_append(&amp;_rows_, (const void *)&amp;C_, sizeof(C_));  Finally, we make the rowset when the procedure exits. If the procedure is returning with no errors the result set is created, otherwise the buffer is released. The global some_integers_info has constants that describe the shape produced by this procedure just like the other cases that produce a result set. cql_results_from_data(_rc_, &amp;_rows_, &amp;some_integers_info, (cql_result_set_ref *)_result_set_);  The operations here are basically the same ones that will happen inside of the standard helpercql_fetch_all_results, the difference, of course, is that you write the loop manually and therefore have full control of the rows as they go in to the result set. In short, the overhead is pretty low. What you’re left with is pretty much the base cost of your algorithm. The cost here is very similar to what it would be for any other thing that make rows. Of course, if you make a million rows, well, that would burn a lot of memory. "},{"title":"A Working Example​","type":1,"pageTitle":"Chapter 7: CQL Result Sets","url":"/cql-guide/ch07#a-working-example","content":"Here's a fairly simple example illustrating some of these concepts including the reading of rowsets. -- hello.sql: create proc hello() begin create table my_data( pos integer not null primary key, txt text not null ); insert into my_data values(2, 'World'); insert into my_data values(0, 'Hello'); insert into my_data values(1, 'There'); select * from my_data order by pos; end;  And this main code to open the database and access the procedure: // main.c #include &lt;stdlib.h&gt; #include &lt;sqlite3.h&gt; #include &quot;hello.h&quot; int main(int argc, char **argv) { sqlite3 *db; int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db); if (rc != SQLITE_OK) { exit(1); /* not exactly world class error handling but that isn't the point */ } hello_result_set_ref result_set; rc = hello_fetch_results(db, &amp;result_set); if (rc != SQLITE_OK) { printf(&quot;error: %d\\n&quot;, rc); exit(2); } cql_int32 result_count = hello_result_count(result_set); for(cql_int32 row = 0; row &lt; result_count; row++) { cql_string_ref text = hello_get_txt(result_set, row); cql_alloc_cstr(ctext, text); printf(&quot;%d: %s\\n&quot;, row, ctext); cql_free_cstr(ctext, text); } cql_result_set_release(result_set); sqlite3_close(db); }  From these pieces you can make a working example like so: # ${cgsql} refers to the root directory of the CG-SQL sources # cql --in hello.sql --cg hello.h hello.c cc -o hello -I ${cgsql}/sources main.c hello.c ${cgsql}/sources/cqlrt.c -lsqlite3 ./hello  Additional demo code is available in Appendix 10. "},{"title":"Nested Result Sets (Parent/Child)​","type":1,"pageTitle":"Chapter 7: CQL Result Sets","url":"/cql-guide/ch07#nested-result-sets-parentchild","content":"There are many cases where you might want to nest one result set inside of another one. In order to do this ecomomically you must be able to run a parent query and a child query and then link the child rows to the parent rows. One way to do this is of course to run one query for each &quot;child&quot; but then you end up with O(n) child queries and if there are sub-children it would beO(n*m) and so forth. What you really want to do here is something more like a join, only without the cross-product part of the join. Many systems have such features, sometimes they are called &quot;chaptered rowsets&quot; but in any case there is a general need for such a thing. To reasonably support nested results sets the CQL language has to be extended a variety of ways, as discussed below. Here are some things that happened along the way that are interesting. Cursor Types and Result Types​ One of the first problems we run into thinking about how a CQL program might express pieces of a rowset and turn them into child results is that a program must be able to hash a row, append row data, and extract a result set from a key. These are the essential operations required. In order to do anything at all with a child rowset, a program must be able to describe its type. Result sets must appear in the type system as well as in the runtime. To address this we use an object type with a special &quot;kind&quot;, similar to how boxed statements are handled. A result set has a type that looks like this: object &lt;proc_name set&gt;. Here proc_name must the the name of a procedure that returns a result set and the object will represent a result set with the corresponding columns in it. Creating New Cursor Types From Existing Cursor Types​ In addition to creating result set types, the language must be able to express cursors that capture the necessary parent/child column. These are rows with all of the parent columns plus additional columns for the child rows (note that you can have more than one child result set per parent). So for instance you might have a list of people, and one child result might be the details of the schools they attended and another could be the details of the jobs they worked. To accomplish this kind of shape, the language must be able to describe a new output row is that is the same as the parent but includes columns for the the child results, too. This is done using a cursor declaration that comes from a typed name list. An example might be: declare C cursor like (id integer, name text);  Importantly, such constructs include the ability to reference existing shapes by name. So we might create a cursor we need like so: declare result cursor like (like parent_proc, child_result object&lt;child_proc set&gt;);  Where the above indicates all the parent columns plus a child result set. Or more than one child result set if needed. In addition, the language needs a way to conveniently declare a cursor that is only some of the columns of an existing cursor. In particular, nested result sets require us to extract the columns that link the parent and child result sets. The columns we will &quot;join&quot; on. To accomplish this the language extends the familiar notion: declare D cursor like C;  To the more general form: declare pks cursor like C(pk1, pk2);  Which chooses just the named fields from C and makes a cursor with only those. In this case this primary key fields, pk1 and pk2. Additionally, for completeness, we add this form: declare vals cursor like C(-pk1, -pk2);  To mean the cursor vals should have all the columns of C except pk1 and pk2 i.e. all the &quot;values&quot;. Using any number of intermediate construction steps, and maybe some declare X type ... statements, any type can be formed from existing shapes by adding and removing columns. Having done the above we can load a cursor that has just the primary keys with the usual form fetch pks from C(like pks);  Which says we want to load pks from the fields of C, but using only the columns of pks. That operation is of course going to be an exact type match by construction. Cursor Arguments​ In order to express the requisite parent/child join, the language must be able to express operations like &quot;hash a cursor&quot; (any cursor) or &quot;store this row into the appropriate partition&quot;. The language provides no way to write functions that can take any cursor and dynamically do things to it based on type information, but: we don't need very many of them,it's pretty easy to do that job in C (or lua if lua codegen is being used) The minimum requirement is that the language must be able to declare a functions that takes a generic cursor argument and to call such functions a generic cursor construct that has the necessary shape info. This form does the job: declare function cursor_hash(C cursor) long not null;  And it can be used like so: let hash := cursor_hash(C); -- C is any cursor  When such a call is made the C function cursor_hash is passed a so-called &quot;dynamic cursor&quot; pointer which includes: a pointer to the data for the cursorthe count of fieldsthe names of the fieldsthe type/offset of every field in the cursor With this information you can (e.g.) generically do the hash by applying a hash to each field and then combining all of those hashes. This kind of function works on any cursor and all the extra data about the shape that's needed to make the call is static, so really the cost of the call stays modest. Details of the dynamic cursor type are incqlrt_common.h and there are many example functions now in the cqlrt_common.c file. The Specific Parent/Child Functions​ Three helper functions are used to do the parent/child join, they are: DECLARE FUNC cql_partition_create () CREATE OBJECT&lt;partitioning&gt; NOT NULL; DECLARE FUNC cql_partition_cursor ( part OBJECT&lt;partitioning&gt; NOT NULL, key CURSOR, value CURSOR) BOOL NOT NULL; DECLARE FUNC cql_extract_partition ( part OBJECT&lt;partitioning&gt; NOT NULL, key CURSOR) CREATE OBJECT NOT NULL;  The first function makes a new partitioning. The second function hashes the key columns of a cursor (specified by the key argument) and appends the values provided in the second argument into a bucket for that key. By making a pass over the child rows a procedure can easily create a partitioning with each unique key combo having a buffer of all the matching rows. The third function is used once the partitioning is done. Given a key again, this time from the parent rows, a procedure can get the buffer it had accumulated and then make a result set out of it and return that. Note that the third function returns a vanilla object type because it could be returning a result set of any shape so a cast is required for correctness. Result Set Sugar​ Using the features mentioned above a developer could now join together any kind of complex parent and child combo as needed, but the result would be a lot of error-prone code, To avoid this CQL adds language sugar to do such partitionings automatically and type-safely, like so: -- parent and child defined elsewhere declare proc parent(x integer not null) (id integer not null, a integer, b integer); declare proc child(y integer not null) (id integer not null, u text, v text); -- join together parent and child using 'id' -- example x_, y_ arguments for illustration only create proc parent_child(x_ integer not null, y_ integer not null) begin out union call parent(x_) join call child(y_) using (id); end;  The generated code is simple enough, even though there's a good bit of it. But it's a useful exercise to look at it once. Comments added for clarity. CREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL) BEGIN DECLARE __result__0 BOOL NOT NULL; -- we need a cursor to hold just the key of the child row DECLARE __key__0 CURSOR LIKE child(id); -- we need our partitioning object (there could be more than one per function -- so it gets a number, likewise everything else gets a number LET __partition__0 := cql_partition_create(); -- we invoke the child and then iterate its rows DECLARE __child_cursor__0 CURSOR FOR CALL child(y_); LOOP FETCH __child_cursor__0 BEGIN -- we extract just the key fields (id in this case) FETCH __key__0(id) FROM VALUES(__child_cursor__0.id); -- we add this child to the partition using its key SET __result__0 := cql_partition_cursor(__partition__0, __key__0, __child_cursor__0); END; -- we need a shape for our result, it is the columns of the parent plus the child rowset DECLARE __out_cursor__0 CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER, child1 OBJECT&lt;child SET&gt; NOT NULL); -- now we call the parent and iterate it DECLARE __parent__0 CURSOR FOR CALL parent(x_); LOOP FETCH __parent__0 BEGIN -- we load the key values out of the parent this time, same key fields FETCH __key__0(id) FROM VALUES(__parent__0.id); -- now we create a result row using the parent columns and the child result set FETCH __out_cursor__0(id, a, b, child1) FROM VALUES(__parent__0.id, __parent__0.a, __parent__0.b, cql_extract_partition(__partition__0, __key__0)); -- and then we emit that row OUT UNION __out_cursor__0; END; END;  This code iterates the child once and the parent once and only has two database calls, one for the child and one for the parent. And this is enough to create parent/child result sets for the most common examples. Result Set Values​ While the above is probably the most common case, a developer might also want to make a procedure call for each parent row to compute the child. And, more generally, to work with result sets from procedure calls other than iterating them with a cursor. The iteration pattern: declare C cursor for call foo(args);  is very good if the data is coming from (e.g.) a select statement and we don't want to materialize all of the results if we can stream instead. However, when working with result sets the whole point is to create materialized results for use elsewhere. Since we can express a result set type with object&lt;proc_name set&gt; the language also includes the ability to call a procedure that returns a result set and capture that result. This yields these forms: declare child_result object&lt;child set&gt;; set child_result := child(args);  or better still: let child_result := child(args);  And more generally, this examples shows a manual iteration: declare proc parent(x integer not null) (id integer not null, a integer, b integer); declare proc child(id integer not null) (id integer not null, u text, v text); create proc parent_child(x_ integer not null, y_ integer not null) begin -- the result is like the parent with an extra column for the child declare result cursor like (like parent, child object&lt;child set&gt;); -- call the parent and loop over the results declare P cursor for call parent(x_); loop fetch P begin -- compute the child for each P and then emit it fetch result from values(from P, child(P.id)); out union result; end; end;  After the sugar is applied to expand the types out, the net program is the following: DECLARE PROC parent (x INTEGER NOT NULL) (id INTEGER NOT NULL, a INTEGER, b INTEGER); DECLARE PROC child (id INTEGER NOT NULL) (id INTEGER NOT NULL, u TEXT, v TEXT); CREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL) BEGIN DECLARE result CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER, child OBJECT&lt;child SET&gt;); DECLARE P CURSOR FOR CALL parent(x_); LOOP FETCH P BEGIN FETCH result(id, a, b, child) FROM VALUES(P.id, P.a, P.b, child(P.id)); OUT UNION result; END; END;  Note the LIKE and FROM forms are make it a lot easier to express this notion of just adding one more column to the result. The code for emitting the parent_childresult doesn't need to specify the columns of the parent or the columns of the child, only that the parent has at least the id column. Even that could have been removed. This call could have been used instead: fetch result from values(from P, child(from P like child arguments));  That syntax would result in using the columns of P that match the arguments of child -- justP.id in this case. But if there were many such columns the sugar would be easier to understand and much less error prone. Generated Code Details​ Normally all result sets that have an object type in them use a generic object cql_object_refas their C data type. This isn't wrong exactly but it would mean that a cast would be required in every use case on the native side, and it's easy to get the cast wrong. So the result type of column getters is adjusted to be a child_result_set_ref instead of just cql_object_refwhere child is the name of the child procedure. "},{"title":"Chapter 11: Previous Schema Validation","type":0,"sectionRef":"#","url":"/cql-guide/ch11","content":"","keywords":""},{"title":"Basic Usage​","type":1,"pageTitle":"Chapter 11: Previous Schema Validation","url":"/cql-guide/ch11#basic-usage","content":"The normal way that you do previous schema validation is to create an input file that provides both schema. This file may look something like this: -- prev_check.sql create table foo( id integer, new_field text @create(1) ); @previous_schema; create table foo( id integer );  So, here the old version of foo will be validated against the new version and all is well. A new nullable text field was added at the end. In practice these comparisons are likely to be done in a somewhat more maintainable way, like so: -- prev_check.sql #include &quot;table1.sql&quot; #include &quot;table2.sql&quot; #include &quot;table3.sql&quot; @previous_schema; #include &quot;previous.sql&quot;  Now importantly, in this configuration, everything that follows the @previous_schema directive does not actually contribute to the declared schema. This means the --rt schema result type will not see it. Because of this, you can do your checking operation like so: cc -E -x c prev_check.sql | cql --cg new_previous_schema.sql --rt schema  The above command will generate the schema in new_previous_schema and, if this command succeeds, it's safe to replace the existingprevious.sql with new_previous_schema. NOTE: you can bootstrap the above by leaving off the @previous_schema and what follows to get your first previous schema from the command above. Now, as you can imagine, comparing against the previous schema allows many more kinds of errors to be discovered. What follows is a large chunk of the CQL tests for this area taken from the test files themselves. For easy visibility I have brought each fragment of current and previous schema close to each other and I show the errors that are reported. We start with a valid fragment and go from there. Case 1 : No problemo​ create table foo( id integer not null, rate long int @delete(5, deletor), rate_2 long int @delete(4), id2 integer @create(4), name text @create(5), name_2 text @create(6) ); ------- create table foo( id integer not null, rate long int @delete(5, deletor), rate_2 long int @delete(4), id2 integer @create(4), name text @create(5), name_2 text @create(6) );  The table foo is the same! It doesn't get any easier than that. Case 2 : table create version changed​ create table t_create_version_changed(id integer) @create(1); ------- create table t_create_version_changed(id integer) @create(2); Error at sem_test_prev.sql:15 : in str : current create version not equal to previous create version for 't_create_version_changed'  You can't change the version a table was created in. Here the new schema says it appeared in version 1. The old schema says 2. Case 3 : table delete version changed​ create table t_delete_version_changed(id integer) @delete(1); ------- create table t_delete_version_changed(id integer) @delete(2); Error at sem_test_prev.sql:18 : in str : current delete version not equal to previous delete version for 't_delete_version_changed'  You can't change the version a table was deleted in. Here the new schema says it was gone in version 1. The old schema says 2. Case 4 : table not present in new schema​ -- t_not_present_in_new_schema is gone ------- create table t_not_present_in_new_schema(id integer); Error at sem_test_prev.sql:176 : in create_table_stmt : table was present but now it does not exist (use @delete instead) 't_not_present_in_new_schema'  So here t_not_present_in_new_schema was removed, it should have been marked with @delete. You don't remove tables. Case 5 : table is now a view​ create view t_became_a_view as select 1 id @create(6); ------- create table t_became_a_view(id integer); Error at sem_test_prev.sql:24 : in create_view_stmt : object was a table but is now a view 't_became_a_view'  Tables can't become views... Case 6 : table was in base schema, now created​ create table t_created_in_wrong_version(id integer) @create(1); ------- create table t_created_in_wrong_version(id integer); Error at sem_test_prev.sql:27 : in str : current create version not equal to previous create version for 't_created_in_wrong_version'  Here a version annotation is added after the fact. This item was already in the base schema. Case 7: table was in base schema, now deleted (ok)​ create table t_was_correctly_deleted(id integer) @delete(1); ------- create table t_was_correctly_deleted(id integer);  No errors here, just a regular delete. Case 8: column name changed​ create table t_column_name_changed(id_ integer); ------- create table t_column_name_changed(id integer); Error at sem_test_prev.sql:33 : in str : column name is different between previous and current schema 'id_'  You can't rename columns. We could support this but it's a bit of a maintenance nightmare and logical renames are possible easily without doing physical renames. Case 9 : column type changed​ create table t_column_type_changed(id real); ------- create table t_column_type_changed(id integer); Error at sem_test_prev.sql:36 : in str : column type is different between previous and current schema 'id'  You can't change the type of a column. Case 10 : column attribute changed​ create table t_column_attribute_changed(id integer not null); ------- create table t_column_attribute_changed(id integer); Error at sem_test_prev.sql:39 : in str : column type is different between previous and current schema 'id'  Change of column attributes counts as a change of type. Case 11: column version changed for delete​ create table t_column_delete_version_changed(id integer, id2 integer @delete(1)); ------- create table t_column_delete_version_changed(id integer, id2 integer @delete(2)); Error at sem_test_prev.sql:42 : in str : column current delete version not equal to previous delete version 'id2'  You can't change the delete version after it has been set. Case 12 : column version changed for create​ create table t_column_create_version_changed(id integer, id2 integer @create(1)); ------- create table t_column_create_version_changed(id integer, id2 integer @create(2)); Error at sem_test_prev.sql:45 : in str : column current create version not equal to previous create version 'id2'  You can't change the create version after it has been set. Case 13 : column default value changed​ create table t_column_default_value_changed(id integer, id2 integer not null default 2); ------- create table t_column_default_value_changed(id integer, id2 integer not null default 1); Error at sem_test_prev.sql:48 : in str : column current default value not equal to previous default value 'id2'  You can't change the default value after the fact. There's no alter statement that would allow this even though it does make some logical sense. Case 14 : column default value did not change (ok)​ create table t_column_default_value_ok(id integer, id2 integer not null default 1); ------- create table t_column_default_value_ok(id integer, id2 integer not null default 1);  No change. No error here. Case 15 : create table with additional attribute present and matching (ok)​ create table t_additional_attribute_present(a int not null, b int, primary key (a,b)); ------- create table t_additional_attribute_present(a int not null, b int, primary key (a,b));  No change. No error here. Case 16 : create table with additional attribute (doesn't match)​ create table t_additional_attribute_mismatch(a int not null, primary key (a)); ------- create table t_additional_attribute_mismatch(a int not null, b int, primary key (a,b)); Error at sem_test_prev.sql:57 : in pk_def : a table facet is different in the previous and current schema  This is an error because the additional attribute does not match the previous schema. Case 17 : column removed​ create table t_columns_removed(id integer); ------- create table t_columns_removed(id integer, id2 integer); Error at sem_test_prev.sql:255 : in col_def : items have been removed from the table rather than marked with @delete 't_columns_removed'  You can't remove columns from tables. You have to mark them with @delete instead. Case 18 : create table with added facet not present in the previous​ create table t_attribute_added(a int not null, primary key (a)); ------- create table t_attribute_added(a int not null); Error at sem_test_prev.sql:63 : in pk_def : table has a facet that is different in the previous and current schema 't_attribute_added'  Table facets like primary keys cannot be added after the fact. There is no way to do this in sqlite. Case 19 : create table with additional column and no @create​ create table t_additional_column(a int not null, b int); ------- create table t_additional_column(a int not null); Error at sem_test_prev.sql:66 : in col_def : table has columns added without marking them @create 't_additional_column'  If you add a new column like b above you have to mark it with @create in a suitable version. Case 20 : create table with additional column and `@create (ok)​ create table t_additional_column_ok(a int not null, b int @create(2), c int @create(6)); ------- create table t_additional_column_ok(a int not null, b int @create(2));  Column properly created. No errors here. Case 21 : create table with different flags (like TEMP)​ create TEMP table t_becomes_temp_table(a int not null, b int); ------- create table t_becomes_temp_table(a int not null, b int); Error at sem_test_prev.sql:72 : in create_table_stmt : table create statement attributes different than previous version 't_becomes_temp_table'  Table became a TEMP table, there is no way to generate an alter statement for that. Not allowed. Case 22 : create table and apply annotation (ok)​ create table t_new_table_ok(a int not null, b int) @create(6); ------- -- no previous version  No errors here; this is a properly created new table. Case 23 : create new table without annotation (error)​ create table t_new_table_no_annotation(a int not null, b int); ------- -- no previous version Error at sem_test_prev.sql:85 : in create_table_stmt : new table must be added with @create(6) or later 't_new_table_no_annotation'  This table was added with no annotation. It has to have an @create and be at least version 6, the current largest. Case 24 : create new table stale annotation (error)​ create table t_new_table_stale_annotation(a int not null, b int) @create(2); ------- -- no previous version Error at sem_test_prev.sql:91 : in create_table_stmt : new table must be added with @create(6) or later 't_new_table_stale_annotation'  The schema is already up to version 6. You can't then add a table in the past at version 2. Case 25 : add columns to table, marked @create and @delete​ create table t_new_table_create_and_delete(a int not null, b int @create(6) @delete(7)); ------- create table t_new_table_create_and_delete(a int not null); Error at sem_test_prev.sql:96 : in col_def : table has newly added columns that are marked both @create and @delete 't_new_table_create_and_delete'  Adding a column in the new version and marking it both create and delete is ... weird... don't do that. Technically you can do it (sigh) but it must be done one step at a time. Case 26 : add columns to table, marked @create correctly​ create table t_new_legit_column(a int not null, b int @create(6)); ------- create table t_new_legit_column(a int not null);  No errors here; new column added in legit version. Case 27 : create table with a create migration proc where there was none​ create table with_create_migrator(id integer) @create(1, ACreateMigrator); ------- create table with_create_migrator(id integer) @create(1); Error at sem_test_prev.sql:104 : in str : @create procedure changed in object 'with_create_migrator'  You can't add a create migration proc after the fact. Case 28 : create table with a different create migration proc​ create table with_create_migrator(id integer) @create(1, ACreateMigrator); ------- create table with_create_migrator(id integer) @create(1, ADifferentCreateMigrator); Error at sem_test_prev.sql:104 : in str : @create procedure changed in object 'with_create_migrator'  You can't change a create migration proc after the fact. Case 29 : create table with a delete migration proc where there was none​ create table with_delete_migrator(id integer) @delete(1, ADeleteMigrator); ------- create table with_delete_migrator(id integer) @delete(1); Error at sem_test_prev.sql:107 : in str : @delete procedure changed in object 'with_delete_migrator'  You can't add a delete migration proc after the fact. Case 30 : create table with a different delete migration proc​ create table with_delete_migrator(id integer) @delete(1, ADeleteMigrator); ------- create table with_delete_migrator(id integer) @delete(1, ADifferentDeleteMigrator); Error at sem_test_prev.sql:107 : in str : @delete procedure changed in object 'with_delete_migrator'  You can't change a delete migration proc after the fact. Case 31 : create a table which was a view in the previous schema​ create table view_becomes_a_table(id int); ------- create view view_becomes_a_table as select 1 X; Error at sem_test_prev.sql:110 : in create_table_stmt : object was a view but is now a table 'view_becomes_a_table'  Converting views to tables is not allowed. Case 32 : delete a view without marking it deleted​ --- no matching view in current schema ------- create view view_was_zomg_deleted as select 1 X; Error at sem_test_prev.sql:333 : in create_view_stmt : view was present but now it does not exist (use @delete instead) 'view_was_zomg_deleted'  Here the view was deleted rather than marking it with @delete, resulting in an error. Case 33 : create a new version of this view that is not temp​ create view view_was_temp_but_now_it_is_not as select 1 X; ------- create temp view view_was_temp_but_now_it_is_not as select 1 X; Error at sem_test_prev.sql:339 : in create_view_stmt : TEMP property changed in new schema for view 'view_was_temp_but_now_it_is_not'  A temp view became a view. This flag is not allowed to change. Side note: temp views are weird. Case 34 : create a new version of this view that was created in a different version​ create view view_with_different_create_version as select 1 X @create(3); ------- create view view_with_different_create_version as select 1 X @create(2); Error at sem_test_prev.sql:116 : in str : current create version not equal to previous create version for 'view_with_different_create_version'  You can't change the create version of a view after the fact. Case 35 : create an index that is now totally gone in the new schema​ --- no matching index in current schema ------- create index this_index_was_deleted_with_no_annotation on foo(id); Error at sem_test_prev.sql:349 : in create_index_stmt : index was present but now it does not exist (use @delete instead) 'this_index_was_deleted_with_no_annotation'  You have to use @delete on indices to remove them correctly. Case 36 : create a view with no annotation that is not in the previous schema​ create view view_created_with_no_annotation as select 1 X; ------- --- there is no previous version Error at sem_test_prev.sql:122 : in create_view_stmt : new view must be added with @create(6) or later 'view_created_with_no_annotation'  You have to use @create on views to create them correctly. Case 37 : index created in different version​ create index this_index_has_a_changed_attribute on foo(id) @create(2); ------- create index this_index_has_a_changed_attribute on foo(id) @create(1); Error at sem_test_prev.sql:125 : in str : current create version not equal to previous create version for 'this_index_has_a_changed_attribute'  You can't change the @create version of an index. Case 38 : create a new index but with no @create annotation​ create index this_index_was_created_with_no_annotation on foo(id); ------- --- there is no previous version Error at sem_test_prev.sql:130 : in create_index_stmt : new index must be added with @create(6) or later 'this_index_was_created_with_no_annotation'  You have to use @create on indices to make new ones. Case 39 : create a table with a column def that has a different create migrator proc​ create table create_column_migrate_test( id int, id2 int @create(2, ChangedColumnCreateMigrator) ); ------- create table create_column_migrate_test( id int, id2 int @create(2, PreviousColumnCreateMigrator) ); Error at sem_test_prev.sql:136 : in str : column @create procedure changed 'id2'  You can't change the @create migration stored proc on columns. Case 40 : create a table with a column def that has a different delete migrator proc​ create table delete_column_migrate_test( id int, id2 int @delete(2, ChangedColumnDeleteMigrator) ); ------- create table delete_column_migrate_test( id int, id2 int @delete(2, PreviousColumnDeleteMigrator) ); Error at sem_test_prev.sql:142 : in str : column @delete procedure changed 'id2'  You can't change the @delete migration stored proc on columns. NOTE: in addition to these errors, there are many more that do not require the previous schema which are also checked (not shown here). These comprise things like making sure the delete version is greater than the create version on any item. There is a lot of sensibility checking that can happen without reference to the previous schema. "},{"title":"Chapter 10: Schema Management Features","type":0,"sectionRef":"#","url":"/cql-guide/ch10","content":"","keywords":""},{"title":"Annotations​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#annotations","content":"There are three basic flavors of annotation @create(version [, migration proc])@delete(version [, migration proc])@recreate They have various constraints: @create and @delete can only be applied to tables and columns@recreate can only be applied to tables (nothing else needs it anyway)@recreate cannot mix with @create or @delete@recreate can include a group name as in @recreate(musketeers); if a group name is specified then all the tables in that group are recreated if any of them change Indices, Views, and Triggers are always &quot;recreated&quot; (just like tables can be) and so neither the @recreate nor the @create annotations are needed (or allowed). However when an Index, View, or Trigger is retired it must be marked with @delete so that it isn't totally forgotten but can be deleted anywhere it might still exist. Note that when one of these items is deleted, the definition is not used as it will only be dropped anyway. The simplest creation of the object with the correct name will do the job as a tombstone. e.g. create view used_to_be_fabulous as select 1 x @delete(12); suffices to drop the used_to_be_fabulous view in version 12 no matter how complicated it used to be. Its CREATE VIEW will not be emitted into the upgrade procedure in any case. Similarly, trivial indices and triggers of the correct name can be used for the tombstone. In addition, if there is some data migration that needs to happen at a particular schema version that isn't associated with any particular change in schema, you can run an ad hoc migrator at any time. The syntax for that is @schema_ad_hoc_migration(version, migration proc);. Ad hoc migrations are the last to run in any given schema version; they happen after table drop migrations. "},{"title":"Semantics​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#semantics","content":"@create declares that the annotated object first appeared in the indicated version, and at that time the migration proc needs to be executed to fill in default values, denormalize values, or whatever the case may be. @delete declares that the annotated object disappeared in the indicated version, and at that time the migration proc needs to be executed to clean up the contents, or potentially move them elsewhere. @recreate declares that the annotated object can be dropped and recreated when it changes because there is no need to preserve its contents during an upgrade. Such objects may be changed arbitrarily from version to version. no columns in a @recreate table may have @create or @delete (these aren't needed anyway) therefore tables with @recreate never have deprecated columns (since @delete isn't allowed on their columns) NOTE: all annotations are suppressed from generated SQL. SQLite never sees them. NOTE: looking at the annotations it is possible to compute the logical schema at any version, especially the original schema -- it's what you get if you disregard all @delete entirely (don't delete) and then remove anything marked with @create directives. "},{"title":"Allowable changes​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#allowable-changes","content":"Not all migrations are possible in a sensible fashion, therefore CQL enforces certain limitations: the &quot;original&quot; schema has no annotations or just delete annotationsnew tables may be added (with @create)tables may be deleted (with @delete)columns may be added to a table, but only at the end of the tableadded columns must be nullable or have a default value (otherwise all existing insert statements would break for sure)columns may not be renamedcolumns may be deleted but this is only a logical delete, SQLite has no primitive to remove columns; once deleted you may no longer refer to that column in queriesdeleted columns must be nullable or have a default value (otherwise all existing and future insert statements would break for sure, the column isn't really gone)views, indices, and triggers may be added (no annotation required) and removed (with @delete) like tablesviews, indices, and triggers may be altered completely from version to versionno normal code is allowed to refer to deleted columns, tables, etc. This includes views, indices, and triggersschema migration stored procs see the schema as it existed in their annotation (so an older version). They are also forbidden from using views (see below)recreated objects (tables marked with @recreate, views, tables, and indices) have no change restrictions "},{"title":"Prosecution​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#prosecution","content":"Moving from one schema version to another is done in an orderly fashion with the migration proc taking these essential steps in this order: the cql_schema_facets table is created if needed -- this records the current state of the schema the last known schema hash is read from the cql_schema_facets tables (it is zero by default) if the overall schema hash code matches what is stored, processing stops; otherwise an upgrade ensues all known views are dropped (hence migration procs won't see them!) any index that needs to change is dropped (this includes items marked @delete or indices that are different than before) change is detected by hash (crc64) of the previous index definition vs. the current all known triggers are dropped (hence they will not fire during migration!) the current schema version is extracted from cql_schema_facets (it is zero by default) if the current schema version is zero, then the original versions of all the tables are created if the current schema version is &lt;= 1 then any tables that need to be created at schema version 1 are created as they exist at schema version 1any columns that need to be created at schema version 1 are created as they exist at schema version 1migration procedures schema version 1 are run in this order: create table migrationcreate column migrationdelete trigger migration (these are super rare and supported for uniformity)delete index migration (these are super rare and supported for uniformity)delete view migration (these are super rare and supported for uniformity)delete column migrationdelete table migrationad hoc migrationeach proc is run exactly one time any tables that need to be dropped at schema version 1 are droppedthe schema version is marked as 1 in cql_schema_facetseach sub-step in the above is recorded in cql_schema_facets as it happens so it is not repeated all that checking not shown for brevity the above process is repeated for all schema versions up to the current version all tables that are marked with @recreate are re-created if necessary i.e. if the checksum of the table definition has changed for any table (or group) then drop it and create the new version. all indices that changed and were not marked with @delete are re-created all views not marked with @delete are re-created all triggers not marked with @delete are re-installed the current schema hash is written to the cql_schema_facets table "},{"title":"Example Migration​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#example-migration","content":"Here's an example of a schema directly from the test cases: -- crazy amount of versioning here create table foo( id integer not null, rate long integer @delete(5), rate_2 long integer @delete(4, DeleteRate2Proc), id2 integer default 12345 @create(4, CreateId2Proc), name text @create(5), name_2 text @create(6) ); -- much simpler table, lots of stuff added in v2. -- note v1 is the first new version and v0 is base version create table table2( id integer not null, name1 text @create(2, CreateName1Proc), name2 text @create(2, CreateName2Proc), name3 text @create(2), -- no proc name4 text @create(2) -- no proc ); create table added_table( id integer not null, name1 text, name2 text @create(4) ) @create(3) @delete(5); -- this view is present in the output create view live_view as select * from foo; -- this view is also present in the output create view another_live_view as select * from foo; -- this view is not present in the output create view dead_view as select * from foo @delete(2); -- this index is present create index index_still_present on table2(name1, name2); -- this index is going away create index index_going_away on table2(name3) @delete(3); -- this is a simple trigger, and it's a bit silly but that doesn't matter create trigger trigger_one after insert on foo begin delete from table2 where table2.id = new.id; end;  This schema has a LOT of versioning... you can see tables and columns appearing in versions 2 through 6. There is a lot of error checking happening. things with no create annotation were present in the base schemaonly things with no delete annotation are visible to normal codecreated columns have to be at the end of their table (required by SQLite)they have to be in ascending schema version order (but you can add several columns in one version)there may or may not be a proc to run to populate data in that column when it's added or to remove data when it's deleted proc names must be unique you can't delete a table or column in a version before it was createdyou can't delete a column in a table in a version before the table was createdyou can't create a column in a table in a version after the table was deletedthere may be additional checks not listed here "},{"title":"Sample Upgrade Script​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#sample-upgrade-script","content":"With just those annotations you can automatically create the following upgrade script which is itself CQL (and hence has to be compiled). Notice that this code is totally readable! The script has been split into logical pieces to make it easier to explain what's going on. Preamble​ -- ...copyright notice... possibly generated source tag... elided to avoid confusion -- no columns will be considered hidden in this script -- DDL in procs will not count as declarations @SCHEMA_UPGRADE_SCRIPT;  Schema upgrade scripts need to see all the columns even the ones that would be logically deleted in normal mode. This is so that things like alter table add column can refer to real columns and drop table can refer to a table that shouldn't even be visible. Remember in CQL the declarations tell you the logical state of the universe and DLL mutations are expected to create that condition, so you should be dropping tables that are marked with @deleteCQL stores the current state of the universe in this table. -- schema crc -7714030317354747478  The schema crc is computed by hashing all the schema declarations in canonical form. That's everything in this next section. Facet Helpers​ CQL uses a set of four functions to manage a dictionary. The implementation is in cqlrt_common.c but it's really just a simple hash table that maps from a string key to a number. This functionality was added because over time the facets table can get pretty big and running a SQL query every time to read a single integer is not economical. -- declare facet helpers-- DECLARE facet_data TYPE LONG&lt;facet_data&gt; not null; DECLARE test_facets facet_data; DECLARE FUNCTION cql_facets_new() facet_data; DECLARE PROCEDURE cql_facets_delete(facets facet_data); DECLARE FUNCTION cql_facet_add(facets facet_data, facet TEXT NOT NULL, crc LONG NOT NULL) BOOL NOT NULL; DECLARE FUNCTION cql_facet_find(facets facet_data, facet TEXT NOT NULL) LONG NOT NULL;  Declaration Section​ Wherein all the necessary objects are declared... -- declare sqlite_master -- CREATE TABLE sqlite_master ( type TEXT NOT NULL, name TEXT NOT NULL, tbl_name TEXT NOT NULL, rootpage INTEGER NOT NULL, sql TEXT NOT NULL );  The sqlite_master table is built-in but it has to be introduced to CQL so that we can query it. Like all the other loose DDL declarations here there is no code generated for this. We are simply declaring tables. To create code you have to put the DDL in a proc. Normally DDL in procs also declares the table but since we may need the original version of a table created and the final version declared we have @schema_upgrade_script to help avoid name conflicts. -- declare full schema of tables and views to be upgraded -- CREATE TABLE foo( id INTEGER NOT NULL, rate LONG INT @DELETE(5), rate_2 LONG INT @DELETE(4, DeleteRate2Proc), id2 INTEGER DEFAULT 12345 @CREATE(4, CreateId2Proc), name TEXT @CREATE(5), name_2 TEXT @CREATE(6) ); CREATE TABLE table2( id INTEGER NOT NULL, name1 TEXT @CREATE(2, CreateName1Proc), name2 TEXT @CREATE(2, CreateName2Proc), name3 TEXT @CREATE(2), name4 TEXT @CREATE(2) ); CREATE TABLE added_table( id INTEGER NOT NULL, name1 TEXT, name2 TEXT @CREATE(4) ) @CREATE(3) @DELETE(5);  NOTE: all the tables are emitted including all the annotations. This lets us do the maximum validation when we compile this script. CREATE VIEW live_view AS SELECT * FROM foo; CREATE VIEW another_live_view AS SELECT * FROM foo; CREATE VIEW dead_view AS SELECT * FROM foo @DELETE(2);  These view declarations do very little. We only need the view names so we can legally drop the views. We create the views elsewhere. CREATE INDEX index_still_present ON table2 (name1, name2); CREATE INDEX index_going_away ON table2 (name3) @DELETE(3);  Just like views, these declarations introduce the index names and nothing else. CREATE TRIGGER trigger_one AFTER INSERT ON foo BEGIN DELETE FROM table2 WHERE table2.id = new.id; END;  We have only the one trigger; we declare it here. -- facets table declaration -- CREATE TABLE IF NOT EXISTS test_cql_schema_facets( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL );  This is where we will store everything we know about the current state of the schema. Below we define a few helper procs for reading and writing that table and reading sqlite_master -- saved facets table declaration -- CREATE TEMP TABLE test_cql_schema_facets_saved( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL );  We will snapshot the facets table at the start of the run so that we can produce a summary of the changes at the end of the run. This table will hold that snapshot. NOTE: the prefix &quot;test&quot; was specified when this file was built so all the methods and tables begin with test_. Helper Procedures​ -- helper proc for testing for the presence of a column/type CREATE PROCEDURE test_check_column_exists(table_name TEXT NOT NULL, decl TEXT NOT NULL, OUT present BOOL NOT NULL) BEGIN SET present := (SELECT EXISTS(SELECT * FROM sqlite_master WHERE tbl_name = table_name AND sql GLOB decl)); END;  check_column_exists inspects sqlite_master and returns true if a column matching decl exists. -- helper proc for creating the schema version table CREATE PROCEDURE test_create_cql_schema_facets_if_needed() BEGIN CREATE TABLE IF NOT EXISTS test_cql_schema_facets( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL ); END;  Here we actually create the cql_schema_facets table with DDL inside a proc. In a non-schema-upgrade script the above would give a name conflict. -- helper proc for saving the schema version table CREATE PROCEDURE test_save_cql_schema_facets() BEGIN DROP TABLE IF EXISTS test_cql_schema_facets_saved; CREATE TEMP TABLE test_cql_schema_facets_saved( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL ); INSERT INTO test_cql_schema_facets_saved SELECT * FROM test_cql_schema_facets; END;  The save_sql_schema_facets procedure simply makes a snapshot of the current facets table. Later we use this snapshot to report the differences by joining these tables. -- helper proc for setting the schema version of a facet CREATE PROCEDURE test_cql_set_facet_version(_facet TEXT NOT NULL, _version LONG INTEGER NOT NULL) BEGIN INSERT OR REPLACE INTO test_cql_schema_facets (facet, version) VALUES(_facet, _version); END; -- helper proc for getting the schema version of a facet CREATE PROCEDURE test_cql_get_facet_version(_facet TEXT NOT NULL, out _version LONG INTEGER NOT NULL) BEGIN BEGIN TRY SET _version := (SELECT version FROM test_cql_schema_facets WHERE facet = _facet LIMIT 1 IF NOTHING -1); END TRY; BEGIN CATCH SET _version := -1; END CATCH; END;  The two procedures cql_get_facet_version and cql_set_facet_version do just what you would expect. Note the use of try and catch to return a default value if the select fails. There are two additional helper procedures that do essentially the same thing using a schema version index. These two methods exist only to avoid unnecessary repeated string literals in the output file which cause bloat. -- helper proc for getting the schema version CRC for a version index CREATE PROCEDURE test_cql_get_version_crc(_v INTEGER NOT NULL, out _crc LONG INTEGER NOT NULL) BEGIN SET _crc := cql_facet_find(test_facets, printf('cql_schema_v%d', _v)); END; -- helper proc for setting the schema version CRC for a version index CREATE PROCEDURE test_cql_set_version_crc(_v INTEGER NOT NULL, _crc LONG INTEGER NOT NULL) BEGIN INSERT OR REPLACE INTO test_cql_schema_facets (facet, version) VALUES('cql_schema_v'||_v, _crc); END;  As you can see, these procedures are effectively specializations of cql_get_facet_version and cql_set_facet_version where the facet name is computed from the integer. Triggers require some special processing. There are so-called &quot;legacy&quot; triggers that crept into the system. These begin with tr__ and they do not have proper tombstones. In fact some are from early versions of CQL before they were properly tracked. To fix any old databases that have these in them, we delete all triggers that start with tr__. Note we have to use the GLOB operator to do this, because _ is the LIKE wildcard. -- helper proc to reset any triggers that are on the old plan -- DECLARE PROCEDURE cql_exec_internal(sql TEXT NOT NULL) USING TRANSACTION; CREATE PROCEDURE test_cql_drop_legacy_triggers() BEGIN DECLARE C CURSOR FOR SELECT name from sqlite_master WHERE type = 'trigger' AND name GLOB 'tr__*'; LOOP FETCH C BEGIN call cql_exec_internal(printf('DROP TRIGGER %s;', C.name)); END; END;  Baseline Schema​ The 'baseline' or 'v0' schema is unannotated (no @create or @recreate). The first real schema management procedures are for creating and dropping these tables. CREATE PROCEDURE test_cql_install_baseline_schema() BEGIN CREATE TABLE foo( id INTEGER NOT NULL, rate LONG_INT, rate_2 LONG_INT ); CREATE TABLE table2( id INTEGER NOT NULL ); END;  -- helper proc for dropping baseline tables before installing the baseline schema CREATE PROCEDURE test_cql_drop_baseline_tables() BEGIN DROP TABLE IF EXISTS foo; DROP TABLE IF EXISTS table2; END;  Migration Procedures​ The next section declares the migration procedures that were in the schema. These are expected to be defined elsewhere. -- declared upgrade procedures if any DECLARE proc CreateName1Proc() USING TRANSACTION; DECLARE proc CreateName2Proc() USING TRANSACTION; DECLARE proc CreateId2Proc() USING TRANSACTION; DECLARE proc DeleteRate2Proc() USING TRANSACTION;  The code below will refer to these migration procedures. We emit a declaration so that we can use the names in context. NOTE: USING TRANSACTION when applied to a proc declaration simply means the proc will access the database so it needs to be provided with a sqlite3 *db parameter. Views​ -- drop all the views we know CREATE PROCEDURE test_cql_drop_all_views() BEGIN DROP VIEW IF EXISTS live_view; DROP VIEW IF EXISTS another_live_view; DROP VIEW IF EXISTS dead_view; END; -- create all the views we know CREATE PROCEDURE test_cql_create_all_views() BEGIN CREATE VIEW live_view AS SELECT * FROM foo; CREATE VIEW another_live_view AS SELECT * FROM foo; END;  View migration is done by dropping all views and putting all views back. NOTE: dead_view was not created, but we did try to drop it if it existed. Indices​ -- drop all the indices that are deleted or changing CREATE PROCEDURE test_cql_drop_all_indices() BEGIN IF cql_facet_find(test_facets, 'index_still_present_index_crc') != -6823087563145941851 THEN DROP INDEX IF EXISTS index_still_present; END IF; DROP INDEX IF EXISTS index_going_away; END; -- create all the indices we need CREATE PROCEDURE test_cql_create_indices() BEGIN IF cql_facet_find(test_facets, 'index_still_present_index_crc') != -6823087563145941851 THEN CREATE INDEX index_still_present ON table2 (name1, name2); CALL test_cql_set_facet_version('index_still_present_index_crc', -6823087563145941851); END IF; END;  Indices are processed similarly to views, however we do not want to drop indices that are not changing. Therefore we compute the CRC of the index definition. At the start of the script any indices that are condemned (e.g. index_going_away) are dropped as well as any that have a new CRC. At the end of migration, changed or new indices are (re)created using cql_create_indices. Triggers​ - drop all the triggers we know CREATE PROCEDURE test_cql_drop_all_triggers() BEGIN CALL test_cql_drop_legacy_triggers(); DROP TRIGGER IF EXISTS trigger_one; END; -- create all the triggers we know CREATE PROCEDURE test_cql_create_all_triggers() BEGIN CREATE TRIGGER trigger_one AFTER INSERT ON foo BEGIN DELETE FROM table2 WHERE table2.id = new.id; END; END;  Triggers are always dropped before migration begins and are re-instated quite late in the processing as we will see below. Caching the state of the facets​ To avoid selecting single rows out of the facets table repeatedly we introduce this procedure whose job is to harvest the facets table and store it in a dictionary. The helpers that do this were declared above. You've already seen usage of the facets in the code above. CREATE PROCEDURE test_setup_facets() BEGIN BEGIN TRY SET test_facets := cql_facets_new(); DECLARE C CURSOR FOR SELECT * from test_cql_schema_facets; LOOP FETCH C BEGIN LET added := cql_facet_add(test_facets, C.facet, C.version); END; END TRY; BEGIN CATCH -- if table doesn't exist we just have empty facets, that's ok END CATCH; END;  Main Migration Script​ The main script orchestrates everything. There are inline comments for all of it. The general order of events is: create schema facets table if neededcheck main schema crc; if it matches we're done here, otherwise continue... These operations are done in test_perform_needed_upgrades drop all viewsdrop condemned indicesfetch the current schema versionif version 0 then install the baseline schema (see below)for each schema version with changes do the following: create any tables that need to be created in this versionadd any columns that need to be added in this versionrun migration procs in this order: create tablecreate columndelete triggerdelete viewdelete indexdelete columndelete table drop any tables that need to be dropped in this versionmark schema upgraded to the current version so far, and proceed to the next versioneach partial step is also marked as completed so that it can be skipped if the script is run again create all the views(re)create any indices that changed and are not deadset the schema CRC to the current CRC That's it... the details are below. CREATE PROCEDURE test_perform_upgrade_steps() BEGIN DECLARE column_exists BOOL NOT NULL; DECLARE schema_version LONG INTEGER NOT NULL; -- dropping all views -- CALL test_cql_drop_all_views(); -- dropping condemned or changing indices -- CALL test_cql_drop_all_indices(); -- dropping condemned or changing triggers -- CALL test_cql_drop_all_triggers(); ---- install baseline schema if needed ---- CALL test_cql_get_version_crc(0, schema_version); IF schema_version != -9177754326374570163 THEN CALL test_cql_install_baseline_schema(); CALL test_cql_set_version_crc(0, -9177754326374570163); END IF; ---- upgrade to schema version 2 ---- CALL test_cql_get_version_crc(2, schema_version); IF schema_version != -6840158498294659234 THEN -- altering table table2 to add column name1 TEXT; CALL test_check_column_exists('table2', '*[( ]name1 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name1 TEXT; END IF; -- altering table table2 to add column name2 TEXT; CALL test_check_column_exists('table2', '*[( ]name2 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name2 TEXT; END IF; -- altering table table2 to add column name3 TEXT; CALL test_check_column_exists('table2', '*[( ]name3 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name3 TEXT; END IF; -- altering table table2 to add column name4 TEXT; CALL test_check_column_exists('table2', '*[( ]name4 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name4 TEXT; END IF; -- data migration procedures IF cql_facet_find(test_facets, 'CreateName1Proc') = -1 THEN CALL CreateName1Proc(); CALL test_cql_set_facet_version('CreateName1Proc', 2); END IF; IF cql_facet_find(test_facets, 'CreateName2Proc') = -1 THEN CALL CreateName2Proc(); CALL test_cql_set_facet_version('CreateName2Proc', 2); END IF; CALL test_cql_set_version_crc(2, -6840158498294659234); END IF; ---- upgrade to schema version 3 ---- CALL test_cql_get_version_crc(3, schema_version); IF schema_version != -4851321700834943637 THEN -- creating table added_table CREATE TABLE IF NOT EXISTS added_table( id INTEGER NOT NULL, name1 TEXT ); CALL test_cql_set_version_crc(3, -4851321700834943637); END IF; ---- upgrade to schema version 4 ---- CALL test_cql_get_version_crc(4, schema_version); IF schema_version != -6096284368832554520 THEN -- altering table added_table to add column name2 TEXT; CALL test_check_column_exists('added_table', '*[( ]name2 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE added_table ADD COLUMN name2 TEXT; END IF; -- altering table foo to add column id2 INTEGER; CALL test_check_column_exists('foo', '*[( ]id2 INTEGER*', column_exists); IF NOT column_exists THEN ALTER TABLE foo ADD COLUMN id2 INTEGER DEFAULT 12345; END IF; -- logical delete of column rate_2 from foo; -- no ddl -- data migration procedures IF cql_facet_find(test_facets, 'CreateId2Proc') = -1 THEN CALL CreateId2Proc(); CALL test_cql_set_facet_version('CreateId2Proc', 4); END IF; IF cql_facet_find(test_facets, 'DeleteRate2Proc') = -1 THEN CALL DeleteRate2Proc(); CALL test_cql_set_facet_version('DeleteRate2Proc', 4); END IF; CALL test_cql_set_version_crc(4, -6096284368832554520); END IF; ---- upgrade to schema version 5 ---- CALL test_cql_get_version_crc(5, schema_version); IF schema_version != 5720357430811880771 THEN -- altering table foo to add column name TEXT; CALL test_check_column_exists('foo', '*[( ]name TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE foo ADD COLUMN name TEXT; END IF; -- logical delete of column rate from foo; -- no ddl -- dropping table added_table DROP TABLE IF EXISTS added_table; CALL test_cql_set_version_crc(5, 5720357430811880771); END IF; ---- upgrade to schema version 6 ---- CALL test_cql_get_version_crc(6, schema_version); IF schema_version != 3572608284749506390 THEN -- altering table foo to add column name_2 TEXT; CALL test_check_column_exists('foo', '*[( ]name_2 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE foo ADD COLUMN name_2 TEXT; END IF; CALL test_cql_set_version_crc(6, 3572608284749506390); END IF; CALL test_cql_create_all_views(); CALL test_cql_create_all_indices(); CALL test_cql_create_all_triggers(); CALL test_cql_set_facet_version('cql_schema_version', 6); CALL test_cql_set_facet_version('cql_schema_crc', -7714030317354747478); END;  We have one more helper that will look for evidence that we're trying to move backwards to a previous schema version. This is not supported. This procedure also arranges for the original facet versions to be saved and it proceduces a difference in facets after the upgrade is done. CREATE PROCEDURE test_perform_needed_upgrades() BEGIN -- check for downgrade -- IF cql_facet_find(test_facets, 'cql_schema_version') &gt; 6 THEN SELECT 'downgrade detected' facet; ELSE -- save the current facets so we can diff them later -- CALL test_save_cql_schema_facets(); CALL test_perform_upgrade_steps(); -- finally produce the list of differences SELECT T1.facet FROM test_cql_schema_facets T1 LEFT OUTER JOIN test_cql_schema_facets_saved T2 ON T1.facet = T2.facet WHERE T1.version is not T2.version; END IF; END;  This is the main function for upgrades, it checks only the master schema version. This function is separate so that the normal startup path doesn't have to have the code for the full upgrade case in it. This lets linker order files do a superior job (since full upgrade is the rare case). CREATE PROCEDURE test() BEGIN DECLARE schema_crc LONG INTEGER NOT NULL; -- create schema facets information table -- CALL test_create_cql_schema_facets_if_needed(); -- fetch the last known schema crc, if it's different do the upgrade -- CALL test_cql_get_facet_version('cql_schema_crc', schema_crc); IF schema_crc &lt;&gt; -7714030317354747478 THEN BEGIN TRY CALL test_setup_facets(); CALL test_perform_needed_upgrades(); END TRY; BEGIN CATCH CALL cql_facets_delete(test_facets); SET test_facets := 0; THROW; END CATCH; CALL cql_facets_delete(test_facets); SET test_facets := 0; ELSE -- some canonical result for no differences -- SELECT 'no differences' facet; END IF; END;  Temp Tables​ We had no temporary tables in this schema, but if there were some they get added to the schema after the upgrade check. A procedure like this one is generated: CREATE PROCEDURE test_cql_install_temp_schema() BEGIN CREATE TEMP TABLE tempy( id INTEGER ); END;  This entry point can be used any time you need the temp tables. But normally it is automatically invoked.  ---- install temp schema after upgrade is complete ---- CALL test_cql_install_temp_schema();  That logic is emitted at the end of the test procedure. "},{"title":"Schema Regions​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#schema-regions","content":"Schema Regions are designed to let you declare your schema in logical regions whose dependencies are specified. It enforces the dependencies you specify creating errors if you attempt to break the declared rules. Schema regions allow you to generate upgrade scripts for parts of your schema that can compose and be guaranteed to remain self-consistent. Details​ In many cases schema can be factored into logical and independent islands. This is desireable for a number of reasons: so that the schema can go into different databasesso that the schema can be upgraded on a different scheduleso that &quot;not relevant&quot; schema can be omitted from distributionsso that parts of your schema that have no business knowing about each other can be prevented from taking dependencies on each other These all have very real applications: E.g. Your Application has an on-disk and an in-memory database​ This creates basically three schema regions: on disk: which cannot refer to the in-memory at allin-memory: which cannot refer to the on-disk schema at allcross-db: which refers to both, also in memory (optional) Your Application Needs To Upgrade Each of the Above​ There must be a separate upgrade script for both the island databases and yet a different one for the &quot;cross-db&quot; database Your Customer Doesn't Want The Kitchen Sink of Schema​ If you're making a library with database support, your customers likely want to be able to create databases that have only features they want; you will want logical parts within your schema that can be separated for cleanliness and distribution. Declaring Regions and Dependencies​ Schema Regions let you create logical groupings, you simply declare the regions you want and then start putting things into those regions. The regions form a directed acyclic graph -- just like C++ base classes. You create regions like this: @declare_schema_region root; @declare_schema_region extra using root;  The above simply declares the regions -- it doesn't put anything into them. In this case we now have a root region and an extra region. The root schema items will not be allowed to refer to anything in extra. Without regions, you could also ensure that the above is true by putting all the extra items afer the root in the input file but things can get more complicated than that in general, and the schema might also be in several files, complicating ordering as the option. Also, relying on order could be problematic as it is quite easy to put things in the wrong place (e.g. add a new root item after the extra items). Making this a bit more complicated, we could have: @declare_schema_region feature1 using extra; @declare_schema_region feature2 using extra; @declare_schema_region everything using feature1, feature2;  And now there are many paths to root from the everything region; that's ok but certainly it will be tricky to do all that with ordering. Using Regions​ An illustrative example, using the regions defined above: @begin_schema_region root; create table main( id integer, name text ); create view names as select name from main order by name; @end_schema_region; @begin_schema_region extra; create table details( id integer references main(id), details text ); create proc get_detail(id_ integer) begin select T1.id, T1.details, T2.name from details T1 inner join main T2 on T1.id = T2.id where T1.id = id_; end; @end_schema_region; @begin_schema_region feature1; create table f1( id integer references details(id), f1_info text ); create proc get_detail(id_ integer) begin select T1.id, T1.details, T2.name, f1_info from details T1 inner join f T2 on T1.id = T2.id inner join f1 on f1.id = T1.id where T1.id = id_; end; @end_schema_region; @begin_schema_region feature2; -- you can use details, and main but not f1 @end_schema_region;  With the structure above specified, even if a new contribution to the root schema appears later, the rules enforce that this region cannot refer to anything other than things in root. This can be very important if schema is being included via #include and might get pulled into the compilation in various orders. A feature area might also have a named public region that others things can depend on (e.g. some views) and private regions (e.g. some tables, or whatever). Region Visibility​ Schema regions do not provide additional name spaces -- the names of objects should be unique across all regions. In other words, regions do not hide or scope entity names; rather they create errors if inappropriate names are used. Case 1: The second line will fail semantic validation because table A already exists -- obvious standard name conflict create table A (id integer); create table A (id integer, name text);  Case 2: This fails for the same reason as case #1. Table A already exists @declare_region root; -- table A is in no region create table A (id integer); @begin_region root: -- this table A is in the root region, still an error create table A (id integer, name text); @end_region;  Case 3: Again fails for the same reason as case #1. Table A already exist in region extra, and you cannot define another table with the same name in another region. @declare_region root; @declare_region extra; @begin_region extra; -- so far so good create table A (id integer); @end_region; @begin_region root; -- no joy, this A conflicts with the previous A create table A (id integer, name text); @end_region;  Really the visibility rules couldn't be anything other than the above, as SQLite has no knowledge of regions at all and so any exotic name resolution would just doom SQLite statements to fail when they finally run. Exception for &quot;... LIKE &lt;table&gt;&quot; statement​ The rules above are enforced for all constructs except for where the syntactic sugar ... LIKE &lt;table&gt; forms, which can happen in a variety of statements. This form doesn't create a dependence on the table (but does create a dependence on its shape). When CQL generates output, the LIKE construct is replaced with the actual names of the columns it refers to. But these are independent columns, so this is simply a keystroke saver. The table (or view, cursor, etc.) reference will be gone. These cases below will succeed. @declare_region root; create table A (...); create view B (....); create procedure C {...} @begin_region root; create table AA(LIKE A); create table BB(LIKE B); create table CC(LIKE C); @end_region;  Note: this exception may end up causing maintenance problems and so it might be revisited in the future. Maintaining Schema in Pieces​ When creating upgrade scripts, using the --rt schema_upgrade flags you can add region options --include_regions a b c and --exclude_regions d e f per the following: Included regions: must be valid region names -- the base types are walked to compute all the regions that are &quot;in&quot;declarations are emitted in the upgrade for all of the &quot;in&quot; objects -- &quot;exclude&quot; does not affect the declarations Excluded regions: must be valid region names and indicate parts of schema that are upgraded elsewhere, perhaps with a seperate CQL run, a different automatic upgrade, or even a manual mechanismupgrade code will be generated for all the included schema, but not for the excluded regions and their contents Example: Referring to the regions above you might do something like this  # All of these also need a --global_proc param for the entry point but that's not relevant here cql --in schema.sql --cg shared.sql --rt schema_upgrade --include_regions extra cql --in schema.sql --cg f1.cql --rt schema_upgrade --include_regions feature1 --exclude_regions extra cql --in schema.sql --cg f2.cql --rt schema_upgrade --include_regions feature2 --exclude_regions extra  The first command generates all the shared schema for regions root and extra because extra contains root The second command declares all of root and extra so that the feature1 things can refer to them, however the upgrade code for these shared regions is not emitted. Only the upgrade for schema in feature1 is emitted. feature2 is completely absent. This will be ok because we know feature1 cannot depend on feature2 and extra is assumed to be upgraded elsewhere (such as in the previous line). The third command declares all of root and extra so that the feature2 things can refer to them, however the upgrade code for these shared regions is not emitted. Only the upgrade for schema in feature2 is emitted. feature1 is completely absent. Note that in the above examples, CQL is generating more CQL to be compiled again (a common pattern). The CQL upgrade scripts need to be compiled as usual to produce executable code. Thus the output of this form includes the schema declarations and executable DDL. Schema Not In Any Region​ For schema that is not in any region you might imagine that it is a special region &lt;none&gt; that depends on everything. So basically you can put anything there. Schema that is in any region cannot ever refer to schema that is in &lt;none&gt;. When upgrading, if any include regions are specified then &lt;none&gt; will not be emitted at all. If you want an upgrader for just &lt;none&gt; this is possible with an assortment of exclusions. You can always create arbitrary grouping regions to make this easier. A region named any that uses all other regions would make this simple. In general, best practice is that there is no schema in &lt;none&gt;, but since most SQL code has no regions some sensible meaning has to be given to DDL before it gets region encodings. Deployable Regions​ Given the above we note that some schema regions correspond to the way that we will deploy the schema. We want those bundles to be safe to deploy but to in order to be so we need a new notion -- a deployable region. To make this possible CQL includes the following: You can declare a region as deployable using @declare_deployable_regionCQL computes the covering of a deployable region: its transitive closure up to but not including any deployable regions it referencesNo region is allowed to depend on a region that is within the interior of a different deployable region, but you can depend on the deployable region itself Because of the above, each deployable region is in fact a well defined root for the regions it contains. The deployable region becomes the canonical way in which a bundle of regions (and their content) is deployed and any given schema item can be in only one deployable region. Motivation and Examples​ As we saw above, regions are logical groupings of tables/views/etc such that if an entity is in some region R then it is allowed to only refer to the things that R declared as dependencies D1, D2, etc. and their transitive closures. You can make as many logical regions as you like and you can make them as razor thin as you like; they have no physical reality but they let you make as many logical groups of things as you might want. Additionally, when we’re deploying schema you generally need to do it in several pieces. E.g. if we have tables that go in an in-memory database then defining a region that holds all the in-memory tables makes it easy to, say, put all those in-memory tables into a particular deployment script. Now we come to the reason for deployable regions. From CQL’s perspective, all regions are simply logical groups; some grouping is then meaningful to programmers but has no physical reality. This means you’re free to reorganize tables etc. as you see fit into new or different regions when things should move. Only, that’s not quite true. The fact that we deploy our schema in certain ways means while most logical moves are totally fine, if you were to move a table from, say, the main database region to the in-memory region you would be causing a major problem. Some installations may already have the table in the main area and there would be nothing left in the schema to tell CQL to drop the table from the main database -- the best you can hope for is the new location gets a copy of the table the old location keeps it and now there are name conflicts forever. So, the crux of the problem is this: We want to let you move schema freely between logical regions in whatever way makes sense to you, but once you pick the region you are going to deploy in, you cannot change that. To accomplish this, CQL needs to know that some of the regions are deployable regions and there have to be rules to make it all makes sense. Importantly, every region has to be contained in at most one deployable region. Since the regions form a DAG we must create an error if any region could ever roll up to two different deployable regions. The easiest way to describe this rule is “no peeking” – the contents of a deployable region are “private” they can refer to each other in any DAG shape but outside of the deployable region you can only refer to its root. So you can still compose them but each deployable region owns a well-defined covering. Note that you can make as many fine-grained deployable regions as you want; you don’t actually have to deploy them separately, but you get stronger rules about the sharing when you do. Here’s an example: Master Deployment 1 Feature 1 (Deployable) logical regions for feature 1 Core (Deployable) logical regions for core Feature 2 (Deployable) logical regions for feature 2 Core ... Master Deployment 2 Feature 1 (Deployable) ... Feature 3 (Deployable) logical regions for feature 3  In the above: none of the logical regions for feature 1, 2, 3 are allowed to refer to logical regions in any other feature, though any of them could refer to Core (but not directly to what is inside Core)within those regions you can make any set of groupings that makes sense and you can change them over time as you see fit, with some restrictionsany such regions are not allowed to move to a different Feature group (because those are deployment regions)the Master Deployment regions just group features in ways we’d like to deploy them; in this case there are two deployments: one that includes Feature 1 &amp; 2 and another that includes Feature 1 &amp; 3the deployable region boundaries are preventing Feature 1 regions from using Feature 2 regions in an ad hoc way (i.e. you can't cheat by taking a direct dependency on something inside a different feature), but both Features can use CoreFeature 3 doesn’t use Core but Core will still be in Master Deployment 2 due to Feature 1 Note that the deployable regions for Feature 1, 2, and 3 aren't actually deployed alone, but they are adding enforcement that makes the features cleaner Because of how upgrades work, “Core” could have its own upgrader. Then when you create the upgrader for Master Deployment 1 and 2, you can specify “exclude Core” in which case those tables are assumed to be updated independently. You could create as many or as few independently upgrade-able things with this pattern. Because regions are not allowed to &quot;peek&quot; inside of a deployable region, you can reorganize your logical regions without breaking other parts of the schema. Private Regions​ The above constructs create a good basis for creating and composing regions, but a key missing aspect is the ability to hide internal details in the logical groups. This becomes increasingly important as your desire to modularize schema grows; you will want to have certain parts that can change without worrying about breaking others and without fear that there are foreign keys and so forth referring to them. To accomplish this, CQL provides the ability to compose schema regions with the optional private keyword. In the following example there will be three regions creatively named r1, r2, and r3. Region r2 consumes r1 privately and therefore r3 is not allowed to use things in r1 even though it consumes r2. When creating an upgrade script for r3 you will still need (and will get) all of r2 and r1, but from a visibility perspective r3 can only directly depend on r2. @declare_schema_region r1; @declare_schema_region r2 using r1 private; @declare_schema_region r3 using r2; @begin_schema_region r1; create table r1_table(id integer primary key); @end_schema_region; @begin_schema_region r2; create table r2_table(id integer primary key references r1_table(id)); @end_schema_region; @begin_schema_region r3; -- this is OK create table r3_table_2(id integer primary key references r2_table(id)); -- this is an error, no peeking into r1 create table r3_table_1(id integer primary key references r1_table(id)); @end_schema_region;  As expected r2 is still allowed to use r1 because your private regions are not private from yourself. So you may think it’s easy to work around this privacy by simply declaring a direct dependency on r1 wherever you need it. @declare_schema_region my_sneaky_region using r1, other_stuff_I_need;  That would seem to make it all moot. However, this is where deployable regions come in. Once you bundle your logical regions in a deployable region there’s no more peeking inside the the deployable region. So we could strengthen the above to: @declare_deployable_region r2 using r1 private;  Once this is done it becomes an error to try to make new regions that peek into r2; you have to take all of r2 or none of it -- and you can’t see the private parts. Of course you can do region wrapping at any level so you can have as many subgroups as you like, whatever is useful. You can even add additional deployable regions that aren’t actually deployed to get the &quot;hardened&quot; grouping at no cost. So, in summary, to get true privacy, first make whatever logical regions you like that are helpful. Put privacy where you need/want it. Import logical regions as much as you want in your own bundle of regions. Then wrap that bundle up in a deployable region (they nest) and then your private regions are safe from unwanted usage. "},{"title":"Unsubscription and Resubscription Features​","type":1,"pageTitle":"Chapter 10: Schema Management Features","url":"/cql-guide/ch10#unsubscription-and-resubscription-features","content":"Any significant library that is centered around a database is likely to accrue significant amounts of schema to support its features. Often users of the library don’t want all its features and therefore don’t want all of its schema. CQL’s primary strategy is to allow the library author to divide the schema into regions and then the consumer of the library may generate a suitable schema deployer that deploys only the desired regions. You simply subscribe to the regions you want. The @unsub construct deals with the unfortunate situation of over-subscription. In the event that a customer has subscribed to regions that it turns out they don’t need, or if indeed the regions are not fine-grained enough, they may wish to (possibly much later) unsubscribe from particular tables or entire regions that they previously had included. Unfortunately it’s not so trivial as to simply remove the regions after the fact. The problem is that there might be billions of devices that already have the undesired tables and are paying the initialization costs for them. Affirmatively removing the tables is highly desirable and that means a forward-looking annotation is necessary to tell the upgrader to generate DROP statements at some point. Furthermore, a customer might decide at some point later that now is the time they need the schema in question, so resubcription also has to be possible. Unsubscription and Resubscription​ To accomplish this we add the following construct: @unsub(table_name);  The effects of a valid @unsub are as follows: The table is no longer accessible by statementsIf the table is marked @create, then “DROP IF EXISTS tablename” is emitted into the upgrade steps for _version_numberIf the table is @recreate the table is unconditionally dropped as though it had been deletedThe JSON includes the unsub details in a new subscriptions section The compiler ensures that the directives are valid and stay valid. Validations for @unsub(table):​ table must be a valid table nametable must not be already unsubscribedIf table must not be marked with @delete unsubscribing from a table after it’s been outright deleted is clearly a mistake For every child table -- those that mention this table using REFERENCES The child must be already deleted or unsubscribedThe deletion or unsubscription must have happened at a version &lt;= version table is marked unsubscribed for purposes of further analysis caution The legacy form @unsub(version, table) is supported but deprecated, and will soon be an error. The version is ignored. The legacy @resub directive is now an error; Resubscription is accomplished by simply removing the relevant @unsubdirective(s). Previous Schema validations for @unsub​ Unsubscriptions may be removed when they are no longer desired in order to resubscribe as long as this results in a valid chain of foreign keys. These validations are sufficient to guarantee a constistent logical history for unsubscriptions. "},{"title":"Chapter 12: Testability Features","type":0,"sectionRef":"#","url":"/cql-guide/ch12","content":"","keywords":""},{"title":"Dummy Data​","type":1,"pageTitle":"Chapter 12: Testability Features","url":"/cql-guide/ch12#dummy-data","content":"Test code can be needlessly brittle, especially when creating dummy data; any column changes typically cause all sorts of data insertion code to need to be repaired. In many cases the actual data values are completely uninteresting to the test -- any values would do. There are several strategies you can use to get good dummy data into your database in a more maintainable way. Simple Inserts With Dummy Data​ The simplest form uses a variant of the insert statement that fills in any missing columns with a seed value. An example might be something like the below: create proc dummy_user() begin insert into users () values () @dummy_seed(123) @dummy_nullables @dummy_defaults; end;  This statement causes all values including columns that are nullable or have a default value to get the value 123 for any numeric type and'column_name_123' for any text. If you omit the @dummy_nullables then any nullable fields will be null as usual. And likewise if you omit @dummy_defaults then any fields with a default value will use that value as usual. You might want any combination of these for your tests (null values are handy in your tests and default behavior is also handy.) The @dummy_seed expression provided can be anything that resolves to a non-null integer value, so it can be pretty flexible. You might use a while loop to insert a bunch of rows with the seed value being computed from the while loop variable. The form above is sort of like insert * into table in that it is giving dummy values for all columns but you can also specify some of the columns while using the seed value for others. Importantly, you can specify values you particularly want to control either for purposes of creating a more tailored test or because you need them to match existing or created rows in a table referenced by a foreign key. As an example: insert into users (id) values (1234) @dummy_seed(123) @dummy_nullables @dummy_defaults;  will provide dummy values for everything but the id column. Using WITH RECURSIVE​ Sometimes what you want to do is create a dummy result set without necessarily populating the database at all. If you have code that consumes a result set of a particular shape, it's easy enough to create a fake result set with a pattern something like this: create procedure dummy_stuff(lim integer not null) begin WITH RECURSIVE dummy(x) AS ( SELECT 1 UNION ALL SELECT x+1 FROM dummy WHERE x &lt; lim) SELECT x id, printf(&quot;name_%d&quot;, x) name, cast(x % 2 as bool) is_cool, x * 1.3 as rate, x etc1, x etc2 FROM dummy; end;  The first part of the above creates a series of numbers from 1 to lim. The second uses those values to create dummy columns. Any result shape can be generated in this fashion. You get data like this from the above: 1|name_1|1|1.3|1|1 2|name_2|0|2.6|2|2 3|name_3|1|3.9|3|3 4|name_4|0|5.2|4|4 5|name_5|1|6.5|5|5 6|name_6|0|7.8|6|6 7|name_7|1|9.1|7|7 8|name_8|0|10.4|8|8 9|name_9|1|11.7|9|9 10|name_10|0|13.0|10|10  The result of the select statement is itself quite flexible and if more dummy data is what you wanted, this form can be combined withINSERT ... FROM SELECT... to create dummy data in real tables. And of course once you have a core query you could use it in a variety of ways, combined with cursors or any other strategy to select out pieces and insert them into various tables. Using Temporary Tables​ If you need an API to create very flexible dummy data with values of your choice you can use temporary tables and a series of helper procedures. First, create a table to hold the results. You can of course make this table however you need to but the like construct in the table creation is especially helpful; it creates columns in the table that match the name and type of the named object. For instance like my_proc is shorthand for the column names ands of the shape that my_proc returns. This is perfect for emulating the results of my_proc. create proc begin_dummy() begin drop table if exists my_dummy_data; -- the shape of my_dummy_data matches the columns -- returned by proc_I_want_to_emulate create temp table my_dummy_data( like proc_I_want_to_emulate; ); end;  Next, you will need a procedure that accepts and writes a single row to your temp table. You can of course write this all explicitly but the testing support features provide more support to make things easier; In this example, arguments of the procedure will exactly match the output of the procedure we emulating, one argument for each column the proc returns. The insert statement gets its values from the arguments. create proc add_dummy(like proc_I_want_to_emulate) begin insert into my_dummy_data from arguments; end;  This allows you to create the necessary helper methods automatically even if the procedure changes over time. Next we need a procedure to get our result set. create proc get_dummy() begin select * from my_dummy_data; end;  And finally, some cleanup. create proc cleanup_dummy() begin drop table if exists my_dummy_data; end;  Again the temp table could be combined with INSERT INTO ...FROM SELECT... to create dummy data in real tables. Other Considerations​ Wrapping your insert statements in try/catch can be very useful if there may be dummy data conflicts. In test code searching for a new suitable seed is pretty easy. Alternatively set seed := 1 + (select max(id) from foo);  could be very useful. Many alternatives are also possible. The dummy data features are not suitable for use in production code, only tests. But the LIKE features are generally useful for creating contract-like behavior in procs and there are reasonable uses for them in production code. Complex Result Set Example​ Here's a more complicated example that can be easily rewritten using the sugar features. This method is designed to return a single-row result set that can be used to mock a method. I've replaced the real fields with 'f1, 'f2' etc. CREATE PROCEDURE test_my_subject( f1_ LONG INTEGER NOT NULL, f2_ TEXT NOT NULL, f3_ INTEGER NOT NULL, f4_ LONG INTEGER NOT NULL, f5_ TEXT, f6_ TEXT, f7_ TEXT, f8_ BOOL NOT NULL, f9_ TEXT, f10_ TEXT ) BEGIN DECLARE data_cursor CURSOR LIKE my_subject; FETCH data_cursor() FROM VALUES (f1_, f2_, f3_, f4_, f5_, f6_, f7_, f8_, f9_, f10); OUT data_cursor; END;  This can be written much more maintainably as: CREATE PROCEDURE test_my_subject(like my_subject) BEGIN DECLARE C CURSOR LIKE my_subject; FETCH C FROM ARGUMENTS; OUT C; END;  Naturally, real columns have much longer names and there are often many more than 10. "},{"title":"Autotest Attributes​","type":1,"pageTitle":"Chapter 12: Testability Features","url":"/cql-guide/ch12#autotest-attributes","content":"Some of the patterns described above are so common that CQL offers a mechanism to automatically generate those test procedures. Temporary Table Pattern​ The attributes dummy_table, dummy_insert, and dummy_select can be used together to create and populate temp tables. Example: To create a dummy row set for sample_proc, add the cql:autotest attribute with dummy_table, dummy_insert, and dummy_select values. create table foo( id integer not null, name text not null ); @attribute(cql:autotest=(dummy_table, dummy_insert, dummy_select)) create proc sample_proc(foo int) begin select * from Foo; end;  dummy_table generates procedures for creating and dropping a temp table with the same shape as sample_proc. CREATE PROC open_sample_proc() BEGIN CREATE TEMP TABLE test_sample_proc(LIKE sample_proc); END; CREATE PROC close_sample_proc() BEGIN DROP test_sample_proc; END;  The dummy_insert attribute generates a procedure for inserting into the temp table. CREATE PROC insert_sample_proc(LIKE sample_proc) BEGIN INSERT INTO test_sample_proc FROM ARGUMENTS; END;  The dummy_select attribute generates procedures for selecting from the temp table. CREATE PROC select_sample_proc() BEGIN SELECT * FROM test_sample_proc; END;  It's interesting to note that the generated test code does not ever need to mention the exact columns it is emulating because it can always use like, *, and from arguments in a generic way. When compiled, the above will create C methods that can create, drop, insert, and select from the temp table. They will have the following signatures: CQL_WARN_UNUSED cql_code open_sample_proc( sqlite3 *_Nonnull _db_); CQL_WARN_UNUSED cql_code close_sample_proc( sqlite3 *_Nonnull _db_); CQL_WARN_UNUSED cql_code insert_sample_proc( sqlite3 *_Nonnull _db_, cql_int32 id_, cql_string_ref _Nonnull name_); CQL_WARN_UNUSED cql_code select_sample_proc_fetch_results( sqlite3 *_Nonnull _db_, select_sample_proc_result_set_ref _Nullable *_Nonnull result_set);  Single Row ResultSet​ In some cases, using four APIs to generate fake data can be verbose. In the case that only a single row of data needs to be faked, the dummy_result_set attribute can be more convenient. Example: @attribute(cql:autotest=(dummy_result_set)) create proc sample_proc() begin select id from Foo; end;  Will generate the following procedure CREATE PROC generate_sample_proc_row(LIKE sample_proc) BEGIN DECLARE curs CURSOR LIKE sample_proc; FETCH curs FROM ARGUMENTS; OUT curs; END;  Which generates this C API: void generate_sample_proc_row_fetch_results( generate_sample_proc_row_rowset_ref _Nullable *_Nonnull result_set, string_ref _Nonnull foo_, int64_t bar_);  These few test helpers are useful in a variety of scenarios and can save you a lot of typing and maintenance. They evolve automatically as the code changes, always matching the signature of the attributed procedure. Generalized Dummy Test Pattern​ The most flexible test helper is the dummy_test form. This is far more advanced than the simple helpers above. While the choices above were designed to help you create fake result sets pretty easily, dummy_test goes much further letting you set up arbitrary schema and data so that you can run your procedure on actual data. The dummy_test code generator uses the features above to do its job and like the other autotest options, it works by automatically generating CQL code from your procedure definition. However, you get a lot more code in this mode. It's easiest to study an example so let's begin there. To understand dummy_test we'll need a more complete example, so we start with this simple two-table schema with a trigger and some indices. To this we add a very small procedure that we might want to test. create table foo( id integer not null primary key, name text ); create table bar( id integer not null primary key references foo(id), data text ); create index foo_index on foo(name); create index bar_index on bar(data); create temp trigger if not exists trigger1 before delete on foo begin delete from foo where name = 'this is so bogus'; end; @attribute(cql:autotest=( dummy_table, dummy_insert, dummy_select, dummy_result_set, (dummy_test, (bar, (data), ('plugh')))) ) create proc the_subject() begin select * from bar; end;  As you can see, we have two tables, foo and bar; the foo table has a trigger; both foo and bar have indices. This schema is very simple, but of course it could be a lot more complicated, and real cases typically are. The procedure we want to test is creatively called the_subject. It has lots of test attributes on it. We've already discussed dummy_table, dummy_insert, dummy_select, and dummy_result_set above but as you can see they can be mixed in with dummy_test. Now let's talk about dummy_test. First you'll notice that annotation has additional sub-attributes; the attribute grammar is sufficiently flexible such that, in principle, you could represent an arbitrary LISP program, so the instructions can be very detailed. In this case, the attribute provides table and column names, as well as sample data. We'll discuss that when we get to the population code. First let's dispense with the attributes we already discussed -- since we had all the attributes, the output will include those helpers, too. Here they are again: -- note that the code does not actually call the test subject -- this declaration is used so that CQL will know the shape of the result DECLARE PROC the_subject () (id INTEGER NOT NULL, data TEXT); CREATE PROC open_the_subject() BEGIN CREATE TEMP TABLE test_the_subject(LIKE the_subject); END; CREATE PROC close_the_subject() BEGIN DROP TABLE test_the_subject; END; CREATE PROC insert_the_subject(LIKE the_subject) BEGIN INSERT INTO test_the_subject FROM ARGUMENTS; END; CREATE PROC select_the_subject() BEGIN SELECT * FROM test_the_subject; END; CREATE PROC generate_the_subject_row(LIKE the_subject) BEGIN DECLARE curs CURSOR LIKE the_subject; FETCH curs FROM ARGUMENTS; OUT curs; END;  That covers what we had before, so, what's new? Actually, quite a bit. We'll begin with the easiest: CREATE PROC test_the_subject_create_tables() BEGIN CREATE TABLE IF NOT EXISTS foo( id INTEGER NOT NULL PRIMARY KEY, name TEXT ); CREATE TABLE IF NOT EXISTS bar( id INTEGER NOT NULL PRIMARY KEY REFERENCES foo (id), data TEXT ); END;  Probably the most important of all the helpers, test_the_subject_create_tables will create all the tables you need to run the procedure. Note that in this case, even though the subject code only references bar, CQL determined that foo is also needed because of the foreign key. The symmetric drop procedure is also generated: CREATE PROC test_the_subject_drop_tables() BEGIN DROP TABLE IF EXISTS bar; DROP TABLE IF EXISTS foo; END;  Additionally, in this case there were triggers and indices. This caused the creation of helpers for those aspects. CREATE PROC test_the_subject_create_indexes() BEGIN CREATE INDEX bar_index ON bar (data); CREATE INDEX foo_index ON foo (name); END; CREATE PROC test_the_subject_create_triggers() BEGIN CREATE TEMP TRIGGER IF NOT EXISTS trigger1 BEFORE DELETE ON foo BEGIN DELETE FROM foo WHERE name = 'this is so bogus'; END; END; CREATE PROC test_the_subject_drop_indexes() BEGIN DROP INDEX IF EXISTS bar_index; DROP INDEX IF EXISTS foo_index; END; CREATE PROC test_the_subject_drop_triggers() BEGIN DROP TRIGGER IF EXISTS trigger1; END;  If there are no triggers or indices, the corresponding create/drop methods will not be generated. With these helpers available, when writing test code you can then choose if you want to create just the tables, or the tables and indices, or tables and indices and triggers by invoking the appropriate combination of helper methods. Since all the implicated triggers and indices are automatically included, even if they change over time, maintenance is greatly simplified. Note that in this case the code simply reads from one of the tables, but in general the procedure under test might make modifications as well. Test code frequently has to read back the contents of the tables to verify that they were modified correctly. So these additional helper methods are also included: CREATE PROC test_the_subject_read_foo() BEGIN SELECT * FROM foo; END; CREATE PROC test_the_subject_read_bar() BEGIN SELECT * FROM bar; END;  These procedures will allow you to easily create result sets with data from the relevant tables which can then be verified for correctness. Of course if more tables were implicated, those would have been included as well. As you can see, the naming always follows the convention test_[YOUR_PROCEDURE]_[helper_type] Finally, the most complicated helper is the one that used that large annotation. Recall that we provided the fragment (dummy_test, (bar, (data), ('plugh')))) to the compiler. This fragment helped to produce this last helper function: CREATE PROC test_the_subject_populate_tables() BEGIN INSERT OR IGNORE INTO foo(id) VALUES(1) @dummy_seed(123); INSERT OR IGNORE INTO foo(id) VALUES(2) @dummy_seed(124) @dummy_nullables @dummy_defaults; INSERT OR IGNORE INTO bar(data, id) VALUES('plugh', 1) @dummy_seed(125); INSERT OR IGNORE INTO bar(id) VALUES(2) @dummy_seed(126) @dummy_nullables @dummy_defaults; END;  In general the populate_tables helper will fill all implicated tables with at least two rows of data. It uses the dummy data features discussed earlier to generate the items using a seed. Recall that if @dummy_seed is present in an insert statement then any missing columns are generated using that value, either as a string, or as an integer (or true/false for a boolean). Note that the second of the two rows that is generated also specifies @dummy_nullables and @dummy_defaults. This means that even nullable columns, and columns with a default value will get the non-null seed instead. So you get a mix of null/default/explicit values loaded into your tables. Of course blindly inserting data doesn't quite work. As you can see, the insert code used the foreign key references in the schema to figure out the necessary insert order and the primary key values for foo were automatically specified so that they could then be used again in bar. Lastly, the autotest attribute included explicit test values for the table bar, and in particular the data column has the value 'plugh'. So the first row of data for table bar did not use dummy data for the data column but rather used 'plugh'. In general, the dummy_test annotation can include any number of tables, and for each table you can specify any of the columns and you can have any number of tuples of values for those columns. NOTE: if you include primary key and/or foreign key columns among the explicit values, it's up to you to ensure that they are valid combinations. SQLite will complain as usual if they are not, but the CQL compiler will simply emit the data you asked for. Generalizing the example a little bit, we could use the following: (dummy_test, (foo, (name), ('fred'), ('barney'), ('wilma'), ('betty')), (bar, (id, data), (1, 'dino'), (2, 'hopparoo'))))  to generate this population: CREATE PROC test_the_subject_populate_tables() BEGIN INSERT OR IGNORE INTO foo(name, id) VALUES('fred', 1) @dummy_seed(123); INSERT OR IGNORE INTO foo(name, id) VALUES('barney', 2) @dummy_seed(124) @dummy_nullables @dummy_defaults; INSERT OR IGNORE INTO foo(name, id) VALUES('wilma', 3) @dummy_seed(125); INSERT OR IGNORE INTO foo(name, id) VALUES('betty', 4) @dummy_seed(126) @dummy_nullables @dummy_defaults; INSERT OR IGNORE INTO bar(id, data) VALUES(1, 'dino') @dummy_seed(127); INSERT OR IGNORE INTO bar(id, data) VALUES(2, 'hopparoo') @dummy_seed(128) @dummy_nullables @dummy_defaults; END;  And of course if the annotation is not flexible enough, you can write your own data population. The CQL above results in the usual C signatures. For instance: CQL_WARN_UNUSED cql_code test_the_subject_populate_tables(sqlite3 *_Nonnull _db_);  So, it's fairly easy to call from C/C++ test code or from CQL test code. Cross Procedure Limitations​ Generally it's not possible to compute table usages that come from called procedures. This is because to do so you need to see the body of the called procedure and typically that body is in a different translation -- and is therefore not available. A common workaround for this particular problem is to create a dummy procedure that explicitly uses all of the desired tables. This is significantly easier than creating all the schema manually and still gets you triggers and indices automatically. Something like this: @attribute(cql:autotest=(dummy_test)) create proc use_my_stuff() begin let x := select 1 from t1, t2, t3, t4, t5, t6, etc..; end;  The above can be be done as a macro if desired. But in any case use_my_stuff simply and directly lists the desired tables. Using this approach you can have one set of test helpers for an entire unit rather than one per procedure. This is often desirable and the maintenance is not too bad. You just use the use_my_stuff test helpers everywhere. "},{"title":"Chapter 14: CQL Query Fragments","type":0,"sectionRef":"#","url":"/cql-guide/ch14","content":"","keywords":""},{"title":"Base Query Fragments​","type":1,"pageTitle":"Chapter 14: CQL Query Fragments","url":"/cql-guide/ch14#base-query-fragments","content":"The base fragment might look something like this: @attribute(cql:base_fragment=base_frag) create proc base_frag_template(id_ integer not null) begin with base_frag(*) as (select * from my_table where my_table.id = id_) select * from base_frag; end;  Here are the essential aspects: the base fragment is given a name, it can be anything, probably something that describes the purpose of the fragmentsthe procedure name can be anything at allthe procedure must consist of exactly one with...select statementthe fragment name must be the one and only CTE in the select statementyou must select all the columns from the CTE Note the syntax helper base_frag(*) is just shorthand to avoid retyping all the column names of my_table. The interesting part is (select * from my_table where my_table.id = id_) which could have been any select statement of your choice. Everything else in the procedure must follow the designated format, and the format is enforced due to the presence of @attribute(cql:base_fragment=base_frag). The point of putting everything on rails like this is that all base fragments will look the same and it will be clear how to transform any base fragment into the final query when it is assembled with its extensions. Note: the base fragment produces no codegen at all. There is no base_frag_template procedure in the output. This is just a template. Also, the name of the procedure cannot be base_frag this name will be used by the assembly fragment later. Really any descriptive unique name will do since the name does not appear in the output at all. "},{"title":"Extension Query Fragments​","type":1,"pageTitle":"Chapter 14: CQL Query Fragments","url":"/cql-guide/ch14#extension-query-fragments","content":"Adding Columns​ The most common thing that an extension might want to do is add columns to the result. There can be any number of such extensions in the final assembly. Here's a simple example that adds one column. @attribute(cql:extension_fragment=base_frag) create proc adds_columns(id_ integer not null) begin with base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate), col_adder_frag(*) as ( select base_frag.*, added_columns.data from base_frag left outer join added_columns on base_frag.id = added_columns.id) select * from col_adder_frag; end;  Again there are some important features to this extension and they are largely completely constrained, i.e. you must follow the pattern. the attribute indicates extension_fragment and the name (here base_frag) must have been previously declared in a base_fragmentthe procedure name can be any unique name other than base_frag - it corresponds to this particular extension's purposethe procedure arguments must be identical to those in the base fragmentthe first CTE must match the base_fragment attribute value, base_frag in this caseyou do not need to repeat the full select statement for base_frag, any surrogate with the same column names and types will do the base fragment code might include a #define to make this easier e.g. #define base_frags_core as base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate) doing so will make maintenance easier if new columns are added to the base fragment there must be exactly one additional CTE it may have any unique descriptive name you likeit must begin with select base_frags.* with the appropriate CTE name matching the base fragment CTEit must add at least one column (or it would be uninteresting)it may not have any clause other than the first from (e.g. no where, having, limit etc.) if any of these were allowed they would remove or re-order rows in the base query which is not allowedthe from clause often includes nested selects which have no restrictions it must select from the base fragment name and left outer join to wherever it likes to get optional additional columns because of this the additional column(s) will certainly be a nullable type in the projection the final select must be of the form select * from col_adder_frag with the appropriate namekeeping all this in mind, the interesting bit happens here: left outer join added_columns on base_frag.id = added_columns.id this is where you get the data for your additional column using values in the core columns This fragment can be (and should be) compiled in its own compiland while using #include to get the base fragment only. This will result in code gen for the accessor functions for a piece of the overall query -- the part this extension knows about. Importantly code that uses this extension's data does not need or want to know about any other extensions that may be present, thereby keeping dependencies under control. The C signatures generated would look like this: extern cql_int32 adds_columns_get_id( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nonnull adds_columns_get_name( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_double adds_columns_get_rate( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nullable adds_columns_get_data( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 adds_columns_result_count( base_frag_result_set_ref _Nonnull result_set);  Even if there were dozens of other extensions, the functions for reading those columns would not be declared in the header for this extension. Any given extension &quot;sees&quot; only the core columns plus any columns it added. Adding Rows​ Query extensions also frequently want to add additional rows to the main result set, based on the data that is already present. The second form of extension allows for this; it is similarly locked in form. Here is an example: @attribute(cql:extension_fragment=base_frag) create proc adds_rows(id_ integer not null) begin with base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate), row_adder_frag(*) as ( select * from base_frag union all select * from added_rows) select * from row_adder_frag; end;  Let's review the features of this second template form: there is a surrogate for the core querythere is a mandatory second CTEthe second CTE is a compound query with any number of branches, all union allthe first branch must be select * from base_frag (the base fragment) to ensure that the original rows remain this is also why all the branches must be union all this form cannot add new columnsthe extension CTE may not include order by or limit because that might reorder or remove rows of the baseany extensions of this form must come before those of the left outer join form for a given base fragment which ironically means row_adder_frag has to come before col_adder_frag the usual restrictions are in place on compound selects (same type and number of columns) to ensure a consistent resultthe final select after the CTE section must be exactly in the form select * from row_adder_frag which is the name of the one and only additional CTE with no other clauses or options in practice only the CTE will be used to create the final assembly, so even if you did change the final select to something else it would be moot The signatures generated for this will look something like so: extern cql_int32 adds_rows_get_id( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nonnull adds_rows_get_name( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_double adds_rows_get_rate( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 adds_rows_result_count( base_frag_result_set_ref _Nonnull result_set);  which gives you access to the core columns. Again this fragment can and should be compiled standalone with only the declaration for the base fragment in the same translation unit to get the cleanest possible output. This is so that consumers of this extension do not &quot;see&quot; other extensions which may or may not be related and may or may not always be present. Assembling the Fragments​ With all the fragments independently declared they need to be unified to create one final query. This is where the major rewriting happens. The assembly_fragment looks something like this: @attribute(cql:assembly_fragment=base_frag) create proc base_frag(id_ integer not null) begin with base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate) select * from base_frag; end;  It will always be as simple as this; all the complexity is in the fragments. the assembly_fragment name must match the core fragment namethe procedure arguments must be identical to the base fragment argumentsthe procedure must have the same name as the assembly fragment (base_frag in this case) the code that was generated for the previous fragments anticipates this and makes reference to what will be generated herethis is enforced the assembled query is what you run to get the result set, this has real code behind it the other fragments only produce result set readers that call into the helper methods to get columns there is a surrogate for the core fragment as usualall of CTE section will ultimately be replaced with the fragments chained togetherthe final select should be of the form select * from your_frags but it can include ordering and/or filtering, this statement will be present in final codegen, the final order is usually defined here When compiling the assembly fragment, you should include the base, and all the other fragments, and the assembly template. The presence of the assembly_fragment will cause codegen for the extension fragments to be suppressed. The assembly translation unit only contains the assembly query as formed from the fragments. Now let's look at how the query is rewritten, the process is pretty methodical. After rewriting the assembly looks like this: CREATE PROC base_frag (id_ INTEGER NOT NULL) BEGIN WITH base_frag (id, name, rate) AS (SELECT * FROM my_table WHERE my_table.id = id_), row_adder_frag (id, name, rate) AS (SELECT * FROM base_frag UNION ALL SELECT * FROM added_rows), col_adder_frag (id, name, rate, data) AS (SELECT row_adder_frag.*, added_columns.data FROM row_adder_frag LEFT OUTER JOIN added_columns ON row_adder_frag.id = added_columns.id) SELECT * FROM col_adder_frag; END;  Let's dissect this part by part. Each CTE serves a purpose: the core CTE was replaced by the CTE in the base_fragment, and it appears directlynext, the first extension was added as a CTE referring to the base fragment just as before recall that the first extension has to be row_adder_frag, as that type must come firstlooking at the chain you can see why it would be hard to write a correct fragment if it came after columns were added next the second extension was added as a CTE all references to the base fragment were replaced with references to row_adder_fragthe extra column names in the CTE were added such that all previous column names are introduced this process continues until all extensions are exhaustedthe final select statement reads all the columns from the last extension CTE and includes and ordering and so forth that was present in the assembly query The result of all this is a single query that gets all the various columns that were requested in all the extensions and all the union all operations play out as written. The extensions are emitted in the order that they appear in the translation unit with the assembly, which again must have the row adding extensions first. This facility provides considerable ability to compose a large query, but each fragment can be independently checked for errors so that nobody ever has to debug the (possibly monstrous) overall result. Fragments can be removed simply by excluding them from the final assembly (with e.g. #ifdefs, or build rules) With the rewrite of the assembly_fragment complete, the codegen for that procedure is the normal codegen for a procedure with a single select. As always, Java and Objective C codegen on these pieces will produce suitable wrappers for the C. The output code for the assembly fragment generates these reading functions: extern cql_int32 base_frag_get_id( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nonnull base_frag_get_name( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_double base_frag_get_rate( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); // used by adds_columns_get_data() to read its data extern cql_string_ref _Nullable __PRIVATE__base_frag_get_data( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 base_frag_result_count( base_frag_result_set_ref _Nonnull result_set);  These are exactly what you would get for a normal query except that the pieces that came from extensions are marked PRIVATE. Those methods should not be used directly but instead the methods generated for each extension proc should be used. Additionally, to create the result set, as usual. extern CQL_WARN_UNUSED cql_code base_frag_fetch_results( sqlite3 *_Nonnull _db_, base_frag_result_set_ref _Nullable *_Nonnull result_set, cql_int32 id_);  With the combined set of methods you can create a variety of assembled queries from extensions in a fairly straightforward way. "},{"title":"Shared Fragments​","type":1,"pageTitle":"Chapter 14: CQL Query Fragments","url":"/cql-guide/ch14#shared-fragments","content":"Shared fragments do not have the various restrictions that the &quot;extension&quot; style fragments have. While extensions were created to allow a single query to be composed by authors that did not necessarily work with each other, and therefore they are full of restrictions on the shape, shared queries instead are designed to give you maximum flexibility in how the fragments are re-used. You can think of them as being somewhat like a parameterized view, but the parameters are both value parameters and type parameters. In Java or C#, a shared fragments might have had an invocation that looked something like this: `my_fragment(1,2)&lt;table1, table2&gt;. As with the other fragment types the common table expression (CTE) is the way that they plug in. It's helpful to consider a real example: split_text(tok) AS ( WITH RECURSIVE splitter(tok,rest) AS ( SELECT '' tok, IFNULL( some_variable_ || ',', '') rest UNION ALL SELECT substr(rest, 1, instr(rest, ',') - 1) tok, substr(rest, instr(rest, ',') + 1) rest FROM splitter WHERE rest != '' ) SELECT tok from splitter where tok != '' )  This text might appear in dozens of places where a comma separated list needs to be split into pieces and there is no good way to share the code between these locations. CQL is frequently used in conjunction with the C-pre-processor so you could come up with something using the #define construct but this is problematic for several reasons: the compiler does not then know that the origin of the text really is the same thus it has no clue that sharing the text of the string might be a good idea any error messages happen in the context of the use of the macro not the definitionbonus: a multi-line macro like the above gets folded into one line so any error messages are impenetrableif you try to compose such macros it only gets worse; it's more code duplication and harder error casesany IDE support for syntax coloring and so forth will be confused by the macro as it's not part of the language None of this is any good but the desire to create helpers like this is real both for correctness and for performance. To make these things possible, we introduce the notion of shared fragments. We need to give them parameters and the natural way to create a select statement that is bindable in CQL is the procedure. So the shape we choose looks like this: @attribute(cql:shared_fragment) CREATE PROC split_text(value TEXT) BEGIN WITH RECURSIVE splitter(tok,rest) AS ( SELECT '' tok, IFNULL( value || ',', '') rest UNION ALL SELECT substr(rest, 1, instr(rest, ',') - 1) tok, substr(rest, instr(rest, ',') + 1) rest FROM tokens WHERE rest != '' ) SELECT tok from splitter where tok != '' END;  The introductory attribute @attribute(cql:shared_fragment) indicates that the procedure is to produce no code, but rather will be inlined as a CTE in other locations. To use it, we introduce the ability to call a procedure as part of a CTE declaration. Like so: WITH result(v) as (call split_text('x,y,z')) select * from result;  Once the fragment has been defined, the statement above could appear anywhere, and of course the text 'x,y,z' need not be constant. For instance: CREATE PROC print_parts(value TEXT) BEGIN DECLARE C CURSOR FOR WITH result(v) as (CALL split_text('x,y,z')) SELECT * from result; LOOP FETCH C BEGIN CALL printf(&quot;%s\\n&quot;, C.v); END; END;  Fragments are also composable, so for instance, we might also want some shared code that extracts comma separated numbers. We could do this: @attribute(cql:shared_fragment) CREATE PROC ids_from_string(value TEXT) BEGIN WITH result(v) as (CALL split_text(value)) SELECT CAST(v as LONG) as id from result; END;  Now we could write: CREATE PROC print_ids(value TEXT) BEGIN DECLARE C CURSOR FOR WITH result(id) as (CALL ids_from_string('1,2,3')) SELECT * from result; LOOP FETCH C BEGIN CALL printf(&quot;%ld\\n&quot;, C.id); END; END;  Of course these are very simple examples but in principle you can use the generated tables in whatever way is necessary. For instance, here's a silly but illustrative example: /* This is a bit silly */ CREATE PROC print_common_ids(value TEXT) BEGIN DECLARE C CURSOR FOR WITH v1(id) as (CALL ids_from_string('1,2,3')), v2(id) as (CALL ids_from_string('2,4,6')) SELECT * from v1 INTERSECT SELECT * from v2; LOOP FETCH C BEGIN CALL printf(&quot;%ld\\n&quot;, C.id); END; END;  With a small amount of dynamism in the generation of the SQL for the above, it's possible to share the body of v1 and v2. SQL will of course see the full expansion but your program only needs one copy no matter how many times you use the fragment anywhere in the code. So far we have illustrated the &quot;parameter&quot; part of the flexibility. Now let's look at the &quot;generics&quot; part; even though it's overkill for this example, it should still be illustrative. You could imagine that the procedure we wrote above ids_from_string might do something more complicated, maybe filtering out negative ids, ids that are too big, or that don't match some pattern, whatever the case might be. You might want these features in a variety of contexts, maybe not just starting from a string to split. We can rewrite the fragment in a &quot;generic&quot; way like so: @attribute(cql:shared_fragment) CREATE PROC ids_from_string_table() BEGIN WITH source(v) LIKE (select &quot;x&quot; v) SELECT CAST(v as LONG) as id from source; END;  Note the new construct for a CTE definition: inside a fragment we can use &quot;LIKE&quot; to define a plug-able CTE. In this case we used a select statement to describe the shape the fragment requires. We could also have used a name source(*) LIKE shape_name just like we use shape names when describing cursors. The name can be any existing view, table, a procedure with a result, etc. Any name that describes a shape. Now when the fragment is invoked, you provide the actual data source (some table, view, or CTE) and that parameter takes the role of &quot;values&quot;. Here's a full example: CREATE PROC print_ids(value TEXT) BEGIN DECLARE C CURSOR FOR WITH my_data(*) as (CALL split_text(value)), my_numbers(id) as (CALL ids_from_string_table() USING my_data AS source) SELECT id from my_numbers; LOOP FETCH C BEGIN CALL printf(&quot;%ld\\n&quot;, C.id); END; END;  We could actually rewrite the previous simple id fragment as follows: @attribute(cql:shared_fragment) CREATE PROC ids_from_string(value TEXT) BEGIN WITH tokens(v) as (CALL split_text(value)) ids(id) as (CALL ids_from_string_table() USING tokens as source) SELECT * from ids; END;  And actually we have a convenient name we could use for the shape we need so we could have used the shape syntax to define ids_from_string_table. @attribute(cql:shared_fragment) CREATE PROC ids_from_string_table() BEGIN WITH source(*) LIKE split_text SELECT CAST(tok as LONG) as id from source; END;  These examples have made very little use of the database but of course normal data is readily available, so shared fragments can make a great way to provide access to complex data with shareable, correct code. For instance, you could write a fragment that provides the ids of all open businesses matching a name from a combination of tables. This is similar to what you could do with a VIEW plus a WHERE clause but: such a system can give you well controlled combinations known to work wellthere is no schema required, so your database load time can still be fastparameterization is not limited to filtering VIEWs after the fact&quot;generic&quot; patterns are available, allowing arbitrary data sources to be filtered, validated, augmentedeach fragment can be tested separately with its own suite rather than only in the context of some larger thingcode generation can be more economical because the compiler is aware of what is being shared In short, shared fragments can help with the composition of any complicated kinds of queries. If you're producing an SDK to access a data set, they are indispensible. Creating and Using Valid Shared Fragments​ When creating a fragment the following rules are enforced: the fragment many not have any out argumentsit must consist of exactly one valid select statement (but see future forms below)it may use the LIKE construct in CTE definitions to create placeholder shapes this form is illegal outside of shared fragments (otherwise how would you bind it) the LIKE form may only appear in top level CTE expressions in the fragmentthe fragment is free to use other fragments, but it may not call itself calling itself would result in infinite inlining Usage of a fragment is always introduced by a &quot;call&quot; to the fragment name in a CTE body. When using a fragment the following rules are enforced. the provided parameters must create a valid procedure call just like normal procedure calls i.e. the correct number and type of arguments the provided parameters may not use nested (SELECT ...) expressions this could easily create fragment building within fragment building which seems not worth the complexityif database access is required in the parameters simply wrap it in a helper procedure the optional USING clause must specify each required table parameter exactly once and no other tables a fragment that requires table parameters be invoked without a USING clause every actual table provided must match the column names of the corresponding table parameter i.e. in USING my_data AS values the actual columns in my_data must be the same as in the values parameterthe columns need not be in the same order each column in any actual table must be &quot;assignment compatible&quot; with its corresponding column in the parameters i.e. the actual type could be converted to the formal type using the same rules as the := operatorthese are the same rules used for procedure calls, for instance, where the call is kind of like assigning the actual parameter values to the formal parameter variables the provided table values must not conflict with top level CTEs in the shared fragment exception: the top level CTEs that were parameters do not create conflictse.g. it's common to do values(*) as (CALL something() using source as source) - here the caller's &quot;source&quot; takes the value of the fragment's &quot;source&quot;, which is not a true conflicthowever, the caller's source might itself have been a parameter in which case the value provided could create an inner conflict all these problems are easily avoided with a simple naming convention for parameters so that real arguments never look like parameter names and parameter forwarding is apparente.g. USING _source AS _source makes it clear that a parameter is being forwarded and _source is not likely to conflict with real table or view names Note that when shared fragments are used, the generated SQL has the text split into parts, with each fragment and its surroundings separated, therefore the text of shared fragments is shared(!) between usages if normal linker optimizations for text folding are enabled (common in production code.) "},{"title":"Shared Fragments with Conditionals​","type":1,"pageTitle":"Chapter 14: CQL Query Fragments","url":"/cql-guide/ch14#shared-fragments-with-conditionals","content":"Shared fragments use dynamic assembly of the text to do the sharing but it is also possible to create alternative texts. There are many instances where it is desirable to not just replace parameters but use, for instance, an entirely different join sequence. Without shared fragments, the only way to accomplish this is to fork the desired query at the topmost level (because SQLite has no internal possibility of &quot;IF&quot; conditions.) This is expensive in terms of code size and also cognitive load because the entire alternative sequences have to be kept carefully in sync. Macros can help with this but then you get the usual macro maintenance problems, including poor diagnostics. And of course there is no possibility to share the common parts of the text of the code if it is forked. However, conditional shared fragments allow forms like this: @attribute(cql:shared_fragment) CREATE PROC ids_from_string(val TEXT) BEGIN IF val IS NULL OR val IS '' THEN SELECT 0 id WHERE 0; -- empty result ELSE WITH tokens(v) as (CALL split_text(val)) ids(id) as (CALL ids_from_string_table() USING tokens as source) SELECT * from ids; END IF; END;  Now we can do something like:  ids(*) AS (CALL ids_from_string(str))  In this case, if the string val is empty then SQLite will not see the complex comma splitting code, and instead will see the trivial case select 0 id where 0. The code in a conditional fragment might be entirely different between the branches removing unnecessary code, or swapping in a new experimental cache in your test environment, or anything like that. The generalization is simply this: instead of just one select statement there is one top level &quot;IF&quot; statementeach statement list of the IF must be exactly one select statementthere must be an ELSE clausethe select statements must be type compatible, just like in a normal procedureany table parameters with the same name in different branches must have the same type otherwise it would be impossible to provide a single actual table for those table parameters With this additional flexibility a wide variety of SQL statements can be constructed economically and maintainability. Importantly, consumers of the fragments need not deal with all these various alternate possibilities but they can readily create their own useful combinations out of building blocks. Ultimately, from SQLite's perspective, all of these shared fragment forms result in nothing more complicated than a chain of CTE expressions. See Appendix 8 for an extensive section on best practices around fragments and common table expressions in general. tip If you use CQL's query planner on shared fragments with conditionals, the query planner will only analyze the first branch by default. You need to use @attribute(cql:query_plan_branch=[integer]) to modify the behaviour. Read Query Plan Generation for details. "},{"title":"Shared Fragments as Expressions​","type":1,"pageTitle":"Chapter 14: CQL Query Fragments","url":"/cql-guide/ch14#shared-fragments-as-expressions","content":"The shared fragment system also has the ability to create re-usable expression-style fragments giving you something like SQL inline functions. These do come with some performance cost so they should be used for larger fragments. In many systems a simple shared fragment would not compete well with an equivalent #define. Expression fragments shine when: the fragment is quite largeits used frequently (hence providing significant space savings)the arguments are complex, potentially used many times in the expression From a raw performance perspective, the best you can hope for with any of the fragment approaches is a &quot;tie&quot; on speed compared do directly inlining equivalent SQL or using a macro to do the same. However, from a correctness and space perspective it is very possible to come out ahead. It's fair to say that expression fragments have the greatest overhead compared to the other types and so they are best used in cases where the size benefits are going to be important. Syntax​ An expression fragment is basically a normal shared fragment with no top-level FROM clause that generates a single column. A typical one might look like this: -- this isn't very exciting because regular max would do the job @attribute(cql:shared_fragment) create proc max_func(x integer, y integer) begin select case when x &gt;= y then x else y end; end;  The above can be used in the context of a SQL statement like so: select max_func(T1.column1, T1.column2) the_max from foo T1;  Discussion​ The consequence of the above is that the body of max_func is inlined into the generated SQL. However, like the other shared fragments, this is done in such a way that the text can be shared between instances so you only pay for the cost of the text of the SQL in your program one time, no matter how many time you use it. In particular, for the above, the compiler will generate the following SQL: select ( select case when x &gt;= y then x else y end from (select T1.column1 x, column2 y))  But each line will be its own string literal, so, more accurately, it will concatenate the following three strings: &quot;select (&quot;, // string1 &quot; select case when x &gt;= y then x else y end&quot;, // string2 &quot; from (select T1.column1 x, column2 y))&quot; // string3  Importantly, string2 is fixed for any given fragment. The only thing that changes is string3, i.e., the arguments. In any modern C compilation system, the linker will unify the string2 literal across all translation units so you only pay for the cost of that text one time. It also means that the text of the arguments appears exactly one time, no matter how complex they are. For these benefits, we pay the cost of the select wrapper on the arguments. If the arguments are complex that &quot;cost&quot; is negative. Consider the following: select max_func((select max(T.m) from T), (select max(U.m) from U))  A direct expansion of the above would result in something like this: case when (select max(T.m) from T) &gt;= (select max(U.m) from U) then (select max(T.m) from T) else (select max(U.m) from U) end;  The above could be accomplished with a simple #define style macro. However, the expression fragment generates the following code: select ( select case when x &gt;= y then x else y end from select (select max(T.m) from T) x, (select max(U.m) from U) y))  Expression fragments can nest, so you could write: @attribute(cql:shared_fragment) create proc max3_func(x integer, y integer, z integer) begin select max_func(x, max_func(y, z)); end;  Again, this particular example is a waste because regular max would already do the job better. To give another example, common mappings from one kind of code to another using case/when can be written and shared this way: -- this sort of thing happens all the time @attribute(cql:shared_fragment) create proc remap(x integer not null) begin select case x when 1 then 1001 when 2 then 1057 when 3 then 2010 when 4 then 2011 else 9999 end; end;  In the following: select remap(T1.c), remap(T2.d), remap(T3.e) from T1, T2, T3... etc.  The text for remap will appear three times in the generated SQL query but only one time in your binary. Restrictions​ the fragment must consist of exactly one simple select statement no FROM, WHERE, HAVING, etc. -- the result is an expression the select list must have exactly one value Note: the expression can be a nested SELECT which could then have all the usual SELECT elements the usual shared fragment rules apply, e.g. no out-parameters, exactly one statement, etc. Additional Notes​ A simpler syntax might have been possible but expression fragments are only interesting in SQL contexts where (among other things) normal procedure and function calls are not available. So the select keyword makes it clear to the coder and the compiler that the expression will be evaluated by SQLite and the rules for what is allowed to go in the expression are the SQLite rules. The fragment has no FROM clause because we're trying to produce an expression, not a table-value with one column. If you want a table-value with one column, the original shared fragments solution already do exactly that. Expression fragments give you a solution for sharing code in, say, the WHERE clause of a larger select statement. Commpared to something like #define max_func(x,y) case when (x) &gt;= (y) then x else y end;  The macro does give you a ton of flexibility, but it has many problems: if the macro has an error, you see the error in the call site with really bad diagnostic infothe compiler doesn't know that the sharing is going on so it won't be able to share text between call sitesthe arguments can be evaluated many times each which could be expensive, bloaty, or wrongthere is no type-checking of arguments to the macro so you may or may not get compilation errors after expansionyou have to deal with all the usual pre-processor hazards In general, macros can be used (as in C and C++) as an alternative to expression fragments, especially for small fragments. But, this gets to be a worse idea as such macros grow. For larger cases, C and C++ provide inline functions -- CQL provides expression fragments. "},{"title":"Chapter 13: JSON Output","type":0,"sectionRef":"#","url":"/cql-guide/ch13","content":"","keywords":""},{"title":"Tables​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#tables","content":"The &quot;tables&quot; section has zero or more tables, each table is comprised of these fields: name : the table namecrc : the schema CRC for the entire table definition, including columns and constraintsisTemp : true if this is a temporary tableifNotExists : true if the table was created with &quot;if not exists&quot;withoutRowid : true if the table was created using &quot;without rowid&quot;isAdded : true if the table has an @create directive addedVersion : optional, the schema version number in the @create directive isDeleted : true if the table was marked with @delete or is currently unsubscribed deletedVersion : optional, the schema version number in the @delete directive isRecreated : true if the table is marked with @recreate recreateGroupName : optional, if the @recreate attribute specifies a group name, it is present here unsubscribedVersion : optional, if the table was last unsubscribed, the version number when this happenedresubscribedVersion : optional, if the table was last resubscribed, the version number when this happenedregion information : optional, see the section on Region Infoindices : optional, a list of the names of the indices on this table, see the indices sectionattributes : optional, see the section on attributes, they appear in many placescolumns : an array of column definitions, see the section on columnsprimaryKey : a list of column names, possibly empty if no primary keyprimaryKeySortOrders : a list of corresponding sort orders, possibly empty, for each column of the primary key if specifiedprimaryKeyName : optional, the name of the primary key, if it has oneforeignKeys : a list of foreign keys for this table, possibly empty, see the foreign keys sectionuniqueKeys : a list of unique keys for this table, possibly empty, see the unique keys sectioncheckExpressions : a list of check expressions for this table, possibly empty, see the check expression section Example: @attribute(an_attribute=(1,('foo', 'bar'))) CREATE TABLE foo( id INTEGER, name TEXT );  generates:  { &quot;name&quot; : &quot;foo&quot;, &quot;CRC&quot; : &quot;-1869326768060696459&quot;, &quot;isTemp&quot; : 0, &quot;ifNotExists&quot; : 0, &quot;withoutRowid&quot; : 0, &quot;isAdded&quot; : 0, &quot;isDeleted&quot; : 0, &quot;isRecreated&quot;: 0, &quot;indices&quot; : [ &quot;foo_name&quot; ], &quot;attributes&quot; : [ { &quot;name&quot; : &quot;an_attribute&quot;, &quot;value&quot; : [1, [&quot;foo&quot;, &quot;bar&quot;]] } ], &quot;columns&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0, &quot;isAdded&quot; : 0, &quot;isDeleted&quot; : 0, &quot;isPrimaryKey&quot; : 0, &quot;isUniqueKey&quot; : 0, &quot;isAutoIncrement&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0, &quot;isAdded&quot; : 0, &quot;isDeleted&quot; : 0, &quot;isPrimaryKey&quot; : 0, &quot;isUniqueKey&quot; : 0, &quot;isAutoIncrement&quot; : 0 } ], &quot;primaryKey&quot; : [ ], &quot;primaryKeySortOrders&quot; : [ ], &quot;foreignKeys&quot; : [ ], &quot;uniqueKeys&quot; : [ ], &quot;checkExpressions&quot; : [ ] }  "},{"title":"Region Information​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#region-information","content":"Region Information can appear on many entities, it consists of two optional elements: region : optional, the name of the region in which the entity was defineddeployedInRegion : optional, the deployment region in which that region is located "},{"title":"Attributes​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#attributes","content":"Miscellaneous attributes can be present on virtual every kind of entity. They are optional. The root node introduces the attributes: attributes : a list at least one attribute Each attribute is a name and value pair: name : any string attribute names are often compound like &quot;cql:shared_fragment&quot;they are otherwise simple identifiers value : any attribute value Each attribute value can be: any literalan array of attribute values Since the attribute values can nest its possible to represent arbitrarily complex data types in an attribute. You can even represent a LISP program. "},{"title":"Global attributes​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#global-attributes","content":"While the most common use case for attributes is to be attached to other entities (e.g., tables, columns), CQL also lets you define &quot;global&quot; attributes, which are included in the top level attributes section of the JSON output. To specify global attributes you can declare a variable ending with the suffix &quot;database&quot; and attach attributes to it. CQL will merge together all the attributes from all the variables ending with &quot;database&quot; and place them in the attributes section of the JSON output. The main usage of global attributes is as a way to propagate configurations across an entire CQL build. You can, for instance, include these attributes in some root file that you #include in the rest of your CQL code, and by doing this these attributes will be visible everywhere else. Example: @attribute(attribute_1 = &quot;value_1&quot;) @attribute(attribute_2 = &quot;value_2&quot;) declare database object; @attribute(attribute_3 = &quot;value_3&quot;) declare some_other_database object;  Generates:  { &quot;attributes&quot;: [ { &quot;name&quot;: &quot;attribute_1&quot;, &quot;value&quot;: &quot;value_1&quot; }, { &quot;name&quot;: &quot;attribute_2&quot;, &quot;value&quot;: &quot;value_2&quot; }, { &quot;name&quot;: &quot;attribute_3&quot;, &quot;value&quot;: &quot;value_3&quot; } ] }  "},{"title":"Foreign Keys​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#foreign-keys","content":"Foreign keys appear only in tables, the list of keys contains zero or more entries of this form: name : optional, the name of the foreign key if specifiedcolumns : the names of the constrained columns in the current table (the &quot;child&quot; table)referenceTable : the name of the table that came after REFERENCES in the foreign keyreferenceColumns : the constraining columns in the referenced tableonUpdate : the ON UPDATE action (e.g. &quot;CASCADE&quot;, &quot;NO ACTION&quot;, etc.)onDelete : the ON DELETE action (e.g. &quot;CASCADE&quot;, &quot;NO ACTION&quot;, etc.)isDeferred : boolean, indicating the deferred or not deferred setting for this foreign key "},{"title":"Unique Keys​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#unique-keys","content":"Unique keys appear only in tables, the list of keys contains zero or more entries of this form: name: optional, the name of the unique key if specifiedcolumns: a list of 1 or more constrained column namessortOrders: a list of corresponding sort orders for the columns "},{"title":"Check Expressions​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#check-expressions","content":"Check Expressions appear only in tables, the list of keys contains zero or more entries of this form: name : optional, the name of the unique key if specifiedcheckExpr : the check expression in plain textcheckExprArgs: an array of zero or more local variables that should be bound to the ? items in the check expression The checkExprArgs will almost certainly be the empty list []. In the exceedingly rare situation that the table in question was defined in a procedure and some of parts of the check expression were arguments to that procedure then the check expression is not fully known until that procedure runs and some of its literals will be decided at run time. This is an extraordinary choice but technically possible. "},{"title":"Columns​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#columns","content":"Columns are themselves rather complex, there are 1 or more of them in each table. The table will have a list of records of this form: name : the name of the columnsattributes : optional, see the section on attributes, they appear in many placestype : the column type (e.g. bool, real, text, etc.)kind : optional, if the type is qualified by a discriminator such as int&lt;task_id&gt; it appears hereisSensitive : optional, indicates a column that holds sensitive information such as PIIisNotNull : true if the column is not nullisAdded : true if the column has an @create directive addedVersion : optional, the schema version number in the @create directive isDeleted : true if the column was marked with @delete deletedVersion : optional, the schema version number in the @delete directive defaultValue : optional, can be any literal, the default value of the columncollate : optional, the collation string (e.g. nocase)checkExpr : optional, the check expression for this column (see the related section)isPrimaryKey : true if the column was marked with PRIMARY KEYisUniqueKey : true if the column was marked with UNIQUEisAutoIncrement : true if the column was marked with AUTOINCREMENT "},{"title":"Virtual Tables​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#virtual-tables","content":"The &quot;virtualTables&quot; section is very similar to the &quot;tables&quot; section with zero or more virtual table entries. Virtual table entries are the same as table entries with the following additions: module : the name of the module that manages this virtual tableisEponymous : true if the virtual table was declared eponymousisVirtual : always true for virtual tables The JSON schema for these items was designed to be as similar as possible so that typically the same code can handle both with possibly a few extra tests of the isVirtual field. "},{"title":"Views​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#views","content":"The views section contains the list of all views in the schema, it is zero or more view entires of this form. name : the view namecrc : the schema CRC for the entire view definitionisTemp : true if this is a temporary viewisDeleted : true if the view was marked with @delete deletedVersion : optional, the schema version number in the @delete directive region information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placesprojection : an array of projected columns from the view, the view result if you will, see the section on projectionsselect : the text of the select statement that defined the viewselectArgs : the names of arguments any unbound expressions (&quot;?&quot;) in the viewdependencies : several lists of tables and how they are used in the view, see the section on dependencies Note that the use of unbound expressions in a view truly extraordinary so selectArgs is essentially always going to be an empty list. Example: CREATE VIEW MyView AS SELECT * FROM foo  Generates:  { &quot;name&quot; : &quot;MyView&quot;, &quot;CRC&quot; : &quot;5545408966671198580&quot;, &quot;isTemp&quot; : 0, &quot;isDeleted&quot; : 0, &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;select&quot; : &quot;SELECT id, name FROM foo&quot;, &quot;selectArgs&quot; : [ ], &quot;fromTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ] }  "},{"title":"Projections​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#projections","content":"A projection defines the output shape of something that can return a table-like value such as a view or a procedure. The projection consists of a list of one or more projected columns, each of which is: name : the name of the result column (e.g. in select 2 as foo) the name is &quot;foo&quot;type : the type of the column (e.g. text, real, etc.)kind : optional, the discriminator of the type if it has one (e.g. if the result is an int&lt;job_id&gt; the kind is &quot;job_id&quot;)isSensitive : optional, true if the result is sensitive (e.g. PII or something like that)isNotNull : true if the result is known to be not null "},{"title":"Dependencies​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#dependencies","content":"The dependencies section appears in many entities, it indicates things that were used by the object and how they were used. Most of the fields are optional, some fields are impossible in some contexts (e.g. inserts can happen inside of views). insertTables : optional, a list of tables into which values were insertedupdateTables : optional, a list of tables whose values were updateddeleteTables : optional, a list of tables which had rows deletedfromTables : optional, a list of tables that appeared in a FROM clause (maybe indirectly inside a VIEW or CTE)usesProcedures : optional, a list of procedures that were accessed via CALL (not shared fragments, those are inlined)usesViews : optional, a list of views which were accessed (these are recursively visited to get to tables)usesTables : the list of tables that were used in any way at all by the current entity (i.e. the union of the previous table sections) "},{"title":"Indices​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#indices","content":"The indices section contains the list of all indices in the schema, it is zero or more view entires of this form: name : the index namecrc : the schema CRC for the entire index definitiontable : the name of the table with this indexisUnique : true if this is a unique indexifNotExists : true if this index was created with IF NOT EXISTSisDeleted : true if the view was marked with @delete deletedVersion : optional, the schema version number in the @delete directive region information : optional, see the section on Region Infowhere : optional, if this is partial index then this has the partial index where expressionattributes : optional, see the section on attributes, they appear in many placescolumns : the list of column names in the indexsortOrders : the list of corresponding sort orders Example: create index foo_name on foo(name);  Generates:  { &quot;name&quot; : &quot;foo_name&quot;, &quot;CRC&quot; : &quot;6055860615770061843&quot;, &quot;table&quot; : &quot;foo&quot;, &quot;isUnique&quot; : 0, &quot;ifNotExists&quot; : 0, &quot;isDeleted&quot; : 0, &quot;columns&quot; : [ &quot;name&quot; ], &quot;sortOrders&quot; : [ &quot;&quot; ] }  "},{"title":"Procedures​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#procedures","content":"The next several sections: QueriesInsertsGeneral InsertsUpdatesDeletesGeneral All provide information about various types of procedures. Some &quot;simple&quot; procedures that consist only of the type of statement correspond to their section (and some other rules) present additional information about their contents. This can sometimes be useful. All the sections define certain common things about procedures so that basic information is available about all procedures. This is is basically the contents of the &quot;general&quot; section which deals with procedures that have a complex body of which little can be said. "},{"title":"Queries​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#queries","content":"The queries section corresponds to the stored procedures that are a single SELECT statement with no fragments. The fields of a query record are: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placesprojection : an array of projected columns from the procedure, the view if you will, see the section on projectionsstatement : the text of the select statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statement Example: create proc p(name_ text) begin select * from foo where name = name_; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;name_&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;fromTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;statement&quot; : &quot;SELECT id, name FROM foo WHERE name = ?&quot;, &quot;statementArgs&quot; : [ &quot;name_&quot; ] }  "},{"title":"Procedure Arguments​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#procedure-arguments","content":"Procedure arguments have several generalities that don't come up very often but are important to describe. The argument list of a procedure is 0 or more arguments of the form: name : the argument name, any valid identifierargOrigin : either the name repeated if it's just a name or a 3 part string if it came from a bundle, see belowtype : the type of the argument (e.g. text, real, etc.)kind : optional, the discriminated type if any e.g. in int&lt;job_id&gt; it's &quot;job_id&quot;isSensitive : optional, true if the argument is marked with @sensitive (e.g. it has PII etc.)isNotNull : true if the argument is declared not null An example of a simple argument was shown above, if we change the example a little bit to use the argument bundle syntax (even though it's overkill) we can see the general form of argOrigin. Example: create proc p(a_foo like foo) begin select * from foo where name = a_foo.name or id = a_foo.id; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;a_foo_id&quot;, &quot;argOrigin&quot; : &quot;a_foo foo id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;a_foo_name&quot;, &quot;argOrigin&quot; : &quot;a_foo foo name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;fromTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;statement&quot; : &quot;SELECT id, name FROM foo WHERE name = ? OR id = ?&quot;, &quot;statementArgs&quot; : [ &quot;a_foo_name&quot;, &quot;a_foo_id&quot; ] }  Note the synthetic names a_foo_id and a_foo_name the argOrigin indicates that the bundle name is a_foowhich could have been anything, the shape was foo and the column in foo was id or name as appropriate. The JSON is often used to generate glue code to call procedures from different languages. The argOrigin can be useful if you want to codegen something other normal arguments in your code. "},{"title":"General Inserts​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#general-inserts","content":"The general insert section corresponds to the stored procedures that are a single INSERT statement with no fragments. The fields of a general insert record are: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placestable : the name of the table the procedure inserts intostatement : the text of the select statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statementstatementType : there are several insert forms such as &quot;INSERT&quot;, &quot;INSERT OR REPLACE&quot;, &quot;REPLACE&quot;, etc. the type is encoded here General inserts does not include the inserted values because they are not directly extractable in general. This form is used if one of these is true: insert from multiple value rowsinsert from a select statementinsert using a WITH clauseinsert using the upsert clause If fragments are in use then even &quot;generalInsert&quot; cannot capture everything and &quot;general&quot; must be used (see below). Example: create proc p() begin insert into foo values (1, &quot;foo&quot;), (2, &quot;bar&quot;); end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;args&quot; : [ ], &quot;insertTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;INSERT INTO foo(id, name) VALUES(1, 'foo'), (2, 'bar')&quot;, &quot;statementArgs&quot; : [ ], &quot;statementType&quot; : &quot;INSERT&quot;, &quot;columns&quot; : [ &quot;id&quot;, &quot;name&quot; ] }  "},{"title":"Simple Inserts​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#simple-inserts","content":"The vanilla inserts section can be used for procedures that just insert a single row. This is a very common case and if the JSON is being used to drive custom code generation it is useful to provide the extra information. The data in this section is exactly the same as the General Inserts section except that includes the inserted values. The &quot;values&quot; property has this extra information. Each value in the values list corresponds 1:1 with a column and has this form: value : the expression for this valuevalueArgs: the array of procedure arguments that should replace the &quot;?&quot; entries in the value Example: create proc p(like foo) begin insert into foo from arguments; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;foo name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;id_&quot;, &quot;argOrigin&quot; : &quot;foo id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 } ], &quot;insertTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;INSERT INTO foo(id, name) VALUES(?, ?)&quot;, &quot;statementArgs&quot; : [ &quot;id_&quot;, &quot;name_&quot; ], &quot;statementType&quot; : &quot;INSERT&quot;, &quot;columns&quot; : [ &quot;id&quot;, &quot;name&quot; ], &quot;values&quot; : [ { &quot;value&quot; : &quot;?&quot;, &quot;valueArgs&quot; : [ &quot;id_&quot; ] }, { &quot;value&quot; : &quot;?&quot;, &quot;valueArgs&quot; : [ &quot;name_&quot; ] } ] }  "},{"title":"Updates​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#updates","content":"The updates section corresponds to the stored procedures that are a single UPDATE statement with no fragments. The fields of an update record are: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placestable : the name of the table the procedure inserts intostatement : the text of the update statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statement Example: create proc p(like foo) begin update foo set name = name_ where id = id_; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;foo name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;id_&quot;, &quot;argOrigin&quot; : &quot;foo id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 } ], &quot;updateTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;UPDATE foo SET name = ? WHERE id = ?&quot;, &quot;statementArgs&quot; : [ &quot;name_&quot;, &quot;id_&quot; ] }  "},{"title":"Deletes​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#deletes","content":"The deletes section corresponds to the stored procedures that are a single DELETE statement with no fragments. The fields of a delete record are exactly the same as those of update. Those are the basic fields needed to bind any statement. Example: create proc delete_proc (name_ text) begin delete from foo where name like name_; end;  Generates:  { &quot;name&quot; : &quot;delete_proc&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;name_&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;deleteTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;DELETE FROM foo WHERE name LIKE ?&quot;, &quot;statementArgs&quot; : [ &quot;name_&quot; ] }  "},{"title":"General​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#general","content":"And finally the section for procedures that were encountered that are not one of the simple prepared statement forms. The principle reasons for being in this category are: the procedure has out argumentsthe procedure uses something other than a single DML statementthe procedure has no projection (no result of any type)the procedure uses shared fragments and hence has complex argument binding The fields of a general procedure are something like a union of update and delete and query but with no statement info. The are as follows: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : complex procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placesprojection : optional, an array of projected columns from the procedure, the view if you will, see the section on projectionsresult_contract : optional,table : the name of the table the procedure inserts intostatement : the text of the update statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statementusesDatabase : true if the procedure requires you to pass in a sqlite connection to call it The result contract is at most one of these: hasSelectResult : true if the procedure generates its projection using SELECThasOutResult: true if the procedure generates its projection using OUThasOutUnionResult: true if the procedure generates its projection using OUT UNION A procedure that does not produce a result set in any way will set none of these and have no projection entry. Example: create proc with_complex_args (inout arg real) begin set arg := (select arg+1 as a); select &quot;foo&quot; bar; end;  Generates:  { &quot;name&quot; : &quot;with_complex_args&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 1, &quot;args&quot; : [ { &quot;binding&quot; : &quot;inout&quot;, &quot;name&quot; : &quot;arg&quot;, &quot;argOrigin&quot; : &quot;arg&quot;, &quot;type&quot; : &quot;real&quot;, &quot;isNotNull&quot; : 0 } ], &quot;usesTables&quot; : [ ], &quot;projection&quot; : [ { &quot;name&quot; : &quot;bar&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 1 } ], &quot;hasSelectResult&quot; : 1, &quot;usesDatabase&quot; : 1 }  Complex Procedure Arguments​ The complex form of the arguments allows for an optional &quot;binding&quot; binding : optional, if present it can take the value &quot;out&quot; or &quot;inout&quot; if absent then binding is the usual &quot;in&quot; Note that atypical binding forces procedures into the &quot;general&quot; section. "},{"title":"Interfaces​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#interfaces","content":"name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredattributes : optional, see the section on attributes, they appear in many placesprojection: An array of projections. See the section on projections Example declare interface interface1 (id integer);  Generates:  { &quot;name&quot; : &quot;interface1&quot;, &quot;definedInFile&quot; : &quot;x.sql&quot;, &quot;definedOnLine&quot; : 1, &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 } ] }  "},{"title":"Procecdure Declarations​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#procecdure-declarations","content":"The declareProcs section contains a list of procedure declaractions. Each declaration is of the form: name : the name of the procedureargs : procedure arguments see the relevant sectionattributes : optional, see the section on attributes, they appear in many placesprojection : An array of projections. See the section on projectionsusesDatabase : true if the procedure requires you to pass in a sqlite connection to call it "},{"title":"Function Declarations​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#function-declarations","content":"The declareFuncs section contains a list of function declarations, Each declaration is of the form: name : the name of the functionargs : see the relevant sectionattributes : optional, see the section on attributes, they appear in many placesreturnType : see the relevant section below.createsObject : true if the function will create a new object (e.g. declare function dict_create() create object;) "},{"title":"Return Type​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#return-type","content":"type : base type of the return value (e.g. INT, LONG)kind : optional, if the type is qualified by a discriminator such as int&lt;task_id&gt; it appears hereisSensitive : optional, true if the result is sensitive (e.g. PII)isNotNull : true if the result is known to be not null "},{"title":"Regions​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#regions","content":"The regions section contains a list of all the region definitions. Each region is of the form: name : the name of the regionisDeployableRoot : is this region itself a deployment region (declared with @declare_deployable_region)deployedInRegion : name, the deployment region that contains this region or &quot;(orphan)&quot; if none note that deploymentRegions form a forest using : a list of zero or more parent regionsusingPrivately: a list of zero more more booleans, one corresponding to each region the boolean is true if the inheritance is private, meaning that sub-regions cannot see the contents of the inherited region There are more details on regions and the meaning of these terms in Chapter 10. "},{"title":"Ad Hoc Migrations​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#ad-hoc-migrations","content":"This section lists all of the declared ad hoc migrations. Each entry is of the form: name : the name of the procedure to be called for the migration stepcrc : the CRC of this migration step, a hash of the callattributes : optional, see the section on attributes, they appear in many places Exactly one of: version: optional, any positive integer, the version at which the migration runs, ORonRecreateOf: optional, if present indicates that the migration runs when the indicated group is recreated There are more details on ad hoc migrations in Chapter 10. "},{"title":"Enums​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#enums","content":"This section list all the enumeration types and values. Each entry is of the form: name : the name of the enumerationtype : the base type of the enumeration (e.g. INT, LONG)isNotNull: always true, all enum values are not null (here for symmetry with other uses of &quot;type&quot;)values: a list of legal enumeration values Each enumeration value is of the form: name : the name of the valuevalue : a numeric literal Example: declare enum an_enumeration integer ( x = 5, y = 12 );  Generates:  { &quot;name&quot; : &quot;an_enumeration&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 1, &quot;values&quot; : [ { &quot;name&quot; : &quot;x&quot;, &quot;value&quot; : 5 }, { &quot;name&quot; : &quot;y&quot;, &quot;value&quot; : 12 } ] }  "},{"title":"Constant Groups​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#constant-groups","content":"This section list all the constant groups and values. Each entry is of the form: name : the name of the constant groupvalues: a list of declared constant values, this can be of mixed type Each constant value is of the form: name : the name of the constanttype : the base type of the constant (e.g. LONG, REAL, etc.)kind : optional, the type kind of the constant (this can be set with a CAST on a literal, e.g. CAST(1 as int&lt;job_id&gt;))isNotNull : true if the constant type is not null (which is anything but the NULL literal)value : the numeric or string literal value of the constant Example: declare const group some_constants ( x = cast(5 as integer&lt;job_id&gt;), y = 12.0, z = 'foo' );  Generates:  { &quot;name&quot; : &quot;some_constants&quot;, &quot;values&quot; : [ { &quot;name&quot; : &quot;x&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;kind&quot; : &quot;job_id&quot;, &quot;isNotNull&quot; : 1, &quot;value&quot; : 5 }, { &quot;name&quot; : &quot;y&quot;, &quot;type&quot; : &quot;real&quot;, &quot;isNotNull&quot; : 1, &quot;value&quot; : 1.200000e+01 }, { &quot;name&quot; : &quot;z&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 1, &quot;value&quot; : &quot;foo&quot; } ] }  "},{"title":"Subscriptions​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#subscriptions","content":"This section list all the schema subscriptions in order of appearance. Each entry is of the form: type : always &quot;unsub&quot; at this timetable : the target of the subscription directiveversion : the version at which this operation is to happen (always 1 at this time) This section is a little more complicated than it needs to be becasue of the legacy/deprecated @resub directive. At this point only the table name is relevant. The version is always 1 and the type is always &quot;unsub&quot;. Example: @unsub(foo);  Generates:  { &quot;type&quot; : &quot;unsub&quot;, &quot;table&quot; : &quot;foo&quot;, &quot;version&quot; : 1 }  "},{"title":"Summary​","type":1,"pageTitle":"Chapter 13: JSON Output","url":"/cql-guide/ch13#summary","content":"These sections general provide all the information about everything that was declared in a translation unit. Typically not the full body of what was declared but its interface. The schema information provide the core type and context while the procedure information illuminates the code that was generated and how you might call it. "},{"title":"Chapter 15: Query Plan Generation","type":0,"sectionRef":"#","url":"/cql-guide/ch15","content":"","keywords":""},{"title":"Query Plan Generation Compilation Steps​","type":1,"pageTitle":"Chapter 15: Query Plan Generation","url":"/cql-guide/ch15#query-plan-generation-compilation-steps","content":"tip The following steps are used in ./repl/go_query_plan.sh, you can run it to get a quick demonstration of this feature in action. The rest of the section explains how query plan generation works and some of its quirks. To execute query plans for a given CQL file, the following commands need to be run: CQL_FILE= # The CQL file to compile CQL_ROOT_DIR= # Path to cql directory CQL=$CQL_ROOT_DIR/out/cql # Generate Query Plan Script $CQL --in $CQL_FILE --rt query_plan --cg go-qp.sql # Generate UDF stubs $CQL --in $CQL_FILE --rt udf --cg go-qp-udf.h go-qp-udf.c # Compile and link CQL artifacts, with a main C file query_plan_test.c $CQL --in go-qp.sql --cg go-qp.h go-qp.c --dev cc -I$CQL_ROOT_DIR -I. -c $CQL_ROOT_DIR/query_plan_test.c go-qp.c go-qp-udf.c cc -I$CQL_ROOT_DIR -I. -O -o go_query_plan go-qp.o go-qp-udf.o query_plan_test.o $CQL_ROOT_DIR/cqlrt.c -lsqlite3 # Run and generate query plans ./go_query_plan  Contrary to what one might expect, rather than providing query plans directly, CQL uses --rt query_plan to generate a second CQL script that returns query plans for each SQL statement used. A separate command, --rt udf is required to generate any stubbed user defined functions that are used in the original script. Afterwards, the generated query plan script, udf stubs needs to compiled like any CQL file and run by a &quot;main&quot; function that needs to be created separately. The CQL repository provides the file query_plan_test.c that can be used as the &quot;main&quot; function, otherwise you can make your own. ::note When compiling the CQL file generated by --rt query_plan, the --dev flag is required. ::: "},{"title":"Special Handling of CQL features in Query Plan Generation​","type":1,"pageTitle":"Chapter 15: Query Plan Generation","url":"/cql-guide/ch15#special-handling-of-cql-features-in-query-plan-generation","content":"CQL's query planner generator modifies the usage of the following features to allow SQLite run EXPLAIN QUERY PLAN successfully: VariablesUser Defined FunctionsConditionals in Shared Fragments caution Generating query plans of CQL files that use table valued functions, or virtual tables is not supported. Variables​ Variables used in SQL statements are stubbed into constants. The exact value varies depending on the type of the variable, but it is always equivalent to some form of &quot;1&quot;. original.sql ... SELECT * FROM my_table WHERE id = x; ...  query_plan.sql ... EXPLAIN QUERY PLAN SELECT * FROM my_table WHERE my_table.id = nullable(1); ...  User Defined Functions​ Read Functions on details about Function Types. Since the implementation of UDFs in a CQL file do not affect SQLite query plans, CQL's query plan script expects stubs generated by cql --rt udf to be used instead. Conditionals in Shared Fragments​ Read CQL Query Fragments on details about shared fragments Only one branch of a conditional is chosen for query plan analysis. By default this will be the first branch, which is the initial SELECT statement following the IF conditional. The branch to analyze can be configured with the cql:query_plan_branch @attribute. Here's an example of cql:query_plan_branch being used: original.sql @attribute(cql:shared_fragment) @attribute(cql:query_plan_branch=1) CREATE PROC frag2(y int) BEGIN IF y == 2 THEN SELECT 10 b; ELSE IF y == -1 THEN SELECT 20 b; ELSE SELECT 30 b; END IF; END;  query_plan.sql EXPLAIN QUERY PLAN SELECT 20 b;  Setting cql:query_plan_branch=1 selects the second branch. Providing cql:query_plan_branch=2 instead would yield the ELSE clause SELECT 30 b. cql:query_plan_branch=0 would yield SELECT 10 b, which is the same as the default behaviour. "},{"title":"internal","type":0,"sectionRef":"#","url":"/cql-guide/generated/internal","content":"","keywords":""},{"title":"Part 1: Lexing, Parsing, and the AST​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-1-lexing-parsing-and-the-ast","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface","content":"The following is a summary of the implementation theory of the CQL compiler. This is an adjunct to the Guide proper, which describes the language, and to a lesser extent the code that the compiler generates. The actual code is heavily commented, so it's better to read the code to see the details of how any particular operation happens rather than try to guess from the language specification or from this overview. However, some things, like general principles, really are nowhere (or everywhere) in the codebase and it's important to understand how things hang together. If you choose to go on adventures in the source code, especially if you aren't already familiar with compilers and how they are typically built, this document is a good place to start. "},{"title":"General Structure​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#general-structure","content":"The CQL compiler uses a very standard lex+yacc parser, though to be more precise it's flex+bison. The grammar is a large subset of the SQLite dialect of SQL augmented with control flow and compiler directives. As a consequence, it's a useful asset in-and-of-itself. If you're looking for an economical SQL grammar, you could do a lot worse than start with the one CQL uses. The grammar is of course in the usual .y format that bison consumes but it's also extracted into more readable versions for use in the railroad diagram and the Guide documentation. Any of those sources would be a good starting place for a modest SQL project in need of a grammar. "},{"title":"Lexical Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#lexical-analysis","content":"Inside of cql.l you'll find the formal definition of all the tokens. These of course correspond to the various tokens needed to parse the SQL language, plus a few more of the CQL control flow extensions. There's no need to discuss the approximately 150 such tokens, but the following points are of general interest: the lexer expects plain text files, and all the tokens are defined in plain ASCII only, however the presence of UTF8 characters in places where any text is legal (such as string literals) should just work all of the tokens are case-insensitive this means only vanilla ASCII insensitivity; no attempt is made to understand more complex UNICODE code-points multi-word tokens typically are defined with an expression like this: IS[ \\t]+NOT[ \\t]+FALSE/[^A-Z_] in most cases, to avoid ambiguity, and to get order of operations correct, the entire word sequence is one tokenonly spaces and tabs are allowed between the wordsthe token ends on non-identifier characters, so the text &quot;X IS NOT FALSEY&quot; must become the tokens { X, IS_NOT, FALSEY } and not { X, IS_NOT_FALSE, Y } the second option is actually the longest token, so without the trailing qualifier it would be preferredhence, where a continuation is possible, the trailing context must be specified in multi-word tokens there is special processing needed to lex /* ... */ comments correctlythere are token types for each of the sorts of literals that can be encountered special care is taken to keep the literals in string form so that no precision is lostinteger literals are compared against 0x7fffffff and if greater they automatically become long literals even if they are not marked with the trailing L as in 1Lstring literals include the quotation marks in the token text which distinguishes them from identifiers; they are otherwise encoded similarly the character class [-+&amp;~|^/%*(),.;!&lt;&gt;:=] produces single character tokens for operators; other non-matching single characters (e.g. '$') produce an errorline directives ^#line\\ [0-9]+\\ \\&quot;[^&quot;]*\\&quot;.* or ^#\\ [0-9]+\\ \\&quot;[^&quot;]*\\&quot;.* get special processing so that pre-processed input does not lose file and line number fidelity "},{"title":"Parsing and the Abstract Syntax Tree​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#parsing-and-the-abstract-syntax-tree","content":"Inside of cql.y you will find the token declarations, precedence rules, and all of the productions in the overall grammar. The grammar processing does as little as possible in that stage to create an abstract syntax tree (AST). The AST itself is a simple binary tree; where nodes might require more than just left and right children to specify the syntax fully, additional nodes are used in the tree shape rather than introduce n-ary nodes. This means the tree is sometimes bigger, but generally not very much bigger. The benefit of this choice is that the AST can always be walked generically as a binary tree, so if you need to find all the table_factor nodes it is easy to do so without having to worry about how every kind of node expands. If new node types come along the generic walkers can go through those new nodes as well. All of the grammar productions simply make one or more AST nodes and link them together so that in the end there is a single root for the entire program in a binary tree. There are 4 kinds of AST nodes, they all begin with the following five fields. These represent the AST &quot;base type&quot;, if you like.  const char *_Nonnull type; struct sem_node *_Nullable sem; struct ast_node *_Nullable parent; int32_t lineno; const char *_Nonnull filename;  type : a string literal that uniquely identifies the node type the string literal is compared for identity (it's an exact pointer match: you don't strcmp types) sem : begins as NULL this is where the semantic type goes once semantic processing happensparent : the parent node in the AST (not often used but sometimes indispensible)lineno : the line number of the file that had the text that led to this AST (useful for errors)filename : the name of the file that had the text that led to this AST (useful for errors) this string is durable, should not be mutated, and is shared between MANY nodes The Generic Binary AST node ast_node​ typedef struct ast_node { ... the common fields struct ast_node *_Nullable left; struct ast_node *_Nullable right; } ast_node;  This node gives the tree its shape and is how all the expression operators and statements get encoded. An example shows this more clearly: SET X := 1 + 3; {assign} | {name X} | {add} | {int 1} | {int 3}  In the above, &quot;assign&quot; and &quot;add&quot; are the generic nodes. Note that this node type can be a leaf but usually is not. The other types are always leaves. Note that in the above output, the node type was directly printed (because it's a meaningful name). Likewise, the type needs no decoding when viewing the AST in a debugger. Simply printing the node with something like p *ast in lldb will show you all the node fields and the type in human-readable form. The Grammar Code Node int_ast_node​ typedef struct int_ast_node { ... the common fields int64_t value; } int_ast_node;  This kind of node holds an integer that quantifies some kind of choice in the grammar. Note that this does NOT hold numeric literals (see below). The file ast.h includes many #define constants for this purpose such as: define JOIN_INNER 1 define JOIN_CROSS 2 define JOIN_LEFT_OUTER 3 define JOIN_RIGHT_OUTER 4 define JOIN_LEFT 5 define JOIN_RIGHT 6  The integer for this fragment will be one of those defined values. It can be a bitmask, or an enumeration. In this statement: SELECT x FROM a LEFT OUTER JOIN b;  a part of the AST will look like this: | {join_clause} | | {table_or_subquery} | | | {name a} | | {join_target_list} | | {join_target} | | {int 3} | | {table_join} | | {table_or_subquery} | | {name b}  The {int 3} above is an int_ast_node and it corresponds to JOIN_LEFT_OUTER. This node type is always a leaf. The String Node str_ast_node​ typedef struct str_ast_node { ... the common fields const char *_Nullable value; bool_t cstr_literal; } str_ast_node;  This node type holds: string literals blob literals identifiers value : the text of the string cstr_literal : true if the string was specified using &quot;C&quot; syntax (see below) CQL supports C style string literals with C style escapes such as &quot;foo\\n&quot;. These are normalized into the SQL version of the same literal so that SQLite will see a literal it understands. However, if the origin of the string was the C string form (i.e. like &quot;foo&quot; rather than 'bar') then the cstr_literal boolean flag will be set. When echoing the program back as plain text, the C string will be converted back to the C form for display to a user. But when providing the string to Sqlite, it's in SQL format. Identifiers can be distinguished from string literals because the quotation marks (always '') are still in the string. This node type is always a leaf. The Number Node num_ast_node​ typedef struct num_ast_node { ... the common fields int32_t num_type; const char *_Nullable value; } num_ast_node;  num_type : the kind of numericvalue : the text of the number All numerics are stored as strings so that there is no loss of precision. This is important because it is entirely possible that the CQL compiler is built with a different floating point library, than the target system, or different integer sizes. As a result CQL does not evaluate anything outside of an explicit const(...) expression. This policy avoids integer overflows at compile time or loss of floating point precision. Constants in the text of the output are emitted byte-for-byte as they appeared in the source code. This node type is always a leaf. "},{"title":"Examples​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#examples","content":"Example 1: A LET statement and expression​ LET x := 1 + (3 - 2); {let_stmt} | {name x} | {add} | {int 1} | {sub} | {int 3} | {int 2}  Note that there are no parentheses in the AST but it exactly and authoritatively captures the precedence with its shape. This means, among other things, that when CQL echos its input, any redundant parentheses will be gone. Example 2: An IF/ELSE construct​ IF x THEN LET x := 1.5e7; ELSE IF y THEN LET y := 'that'; ELSE LET z := &quot;this&quot;; END IF; {if_stmt} | {cond_action} | | {name x} | | {stmt_list} | | {let_stmt} | | {name x} | | {dbl 1.5e7} | {if_alt} | {elseif} | | {cond_action} | | {name y} | | {stmt_list} | | {let_stmt} | | {name y} | | {strlit 'that'} | {else} | {stmt_list} | {let_stmt} | {name z} | {strlit 'this'}  Note that the string &quot;this&quot; was normalized to 'this' (which was trivial in this case) but rest assured thatcstr_literal was set. This is shown because the text of the statement came out with double quotes. The text above was not the input to the compiler, the compiler was actually given this text: if x then let x := 1.5e7; else if y then let y := 'that'; else let z := &quot;this&quot;; end if;  And it was normalized into what you see as part of the output. We'll talk about this output echoing in coming sections. As you can see, the compiler can be used as a SQL normalizer/beautifier. Example 3: A SELECT statement​ SELECT * FROM foo INNER JOIN bar WHERE foo.x = 1 LIMIT 3; {select_stmt} | {select_core_list} | | {select_core} | | {select_expr_list_con} | | {select_expr_list} | | | {star} | | {select_from_etc} | | {join_clause} | | | {table_or_subquery} | | | | {name foo} | | | {join_target_list} | | | {join_target} | | | {int 1} | | | {table_join} | | | {table_or_subquery} | | | {name bar} | | {select_where} | | {opt_where} | | | {eq} | | | {dot} | | | | {name foo} | | | | {name x} | | | {int 1} | | {select_groupby} | | {select_having} | {select_orderby} | {select_limit} | {opt_limit} | | {int 3} | {select_offset}  As you can see the trees rapidly get more complex. The SELECT statement has many optional pieces and so the AST actually has places in its skeleton where these could go but are absent (e.g. GROUP BY,HAVING, ORDER BY, and OFFSET are all missing). The shape of the AST is largely self-evident from the above, but you can easily cross check it against what's in cql.y for details and then look at gen_sql.c for decoding tips (discussed below). The compiler can produce these diagrams in 'dot' format which makes pretty pictures, but the reality is that for non-trivial examples those pictures are so large as to be unreadable whereas the simple text format remains readable even up to several hundred lines of output. The text is also readily searchable, and diffable. The test suites for semantic analysis do pattern matching on the text of the AST to verify correctness. We'll discuss semantic analysis in Part 2. "},{"title":"AST definitions​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#ast-definitions","content":"ast.h defines all the tree types mentioned above. There are helper methods to create AST nodes with type safety. It includes helper functions for the various leaf types mentioned above but also for the various &quot;normal&quot; types. These are specified using the AST macros AST, AST1, and AST0. Examples: AST0(star) AST1(not) AST(or)  This says that: the star AST node (used in select *) is a leaf, it has 0 children this means the left and right nodes will always be NULL the not AST node (used in select NOT x) is unary this means only the left node is populated, the right is always NULLnode many unary nodes have optional children, so the left node might still be NULL the or AST node (used in select x OR y) is binary this means both its left and right children are populatednote that some binary nodes have optional children, so left or right still might be NULL At present there are about 300 unique AST node types. "},{"title":"Echoing the AST​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#echoing-the-ast","content":"The first set of features that were built (after parsing) provided the ability to echo back the parse tree as SQL again. This all happens in gen_sql.c. Since this code has to be able to echo back any tree, it often has the best and simplest examples of how to crack the AST for any particular type of node you might be interested in. There are several reasons why we might want to echo the SQL, but the inescapable one is this: any hunk of SQL that appears as part of a CQL program (i.e. DDL/DML rather than control flow like IF/WHILE) has to go to SQLite and SQLite expects that code to be plain text. So the AST must be reformatted as plain text that is exactly equivalent to the original input. The process of parsing removes extra white space and parentheses, so to get something that looks reasonable, some standard formatting (including indenting) is applied to the output text. This has the effect of normalizing the input and potentially beautifying it as well (especially if it was poorly formatted initially). To see these features, run cql with the --echo flag. For example: out/cql --echo &lt; your_file.sql  or out/cql --echo --in your_file.sql  By default, it reads stdin, makes the AST, and then emits the normalized, formatted text. If there are no syntax errors, the input and the output should be equivalent. Standard formatting is essential, but CQL also has a number of extra demands. CQL includes a lot of versioning directives like @create(...) @delete(...) and so forth. SQLite should never see these things when the DDL for SQLite is emitted. But when echoing the input they should be included. Additionally, any local or global variables in a SQL statement should be replaced with ? in the text that goes to SQLite and then followed up with binding instructions. We'll cover the binding more in the section code generation, but importantly this also has to significantly alter the output. As a result the standard formatter includes extensive configurability to get these various results. "},{"title":"Configuring the Output with Callbacks and Flags​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#configuring-the-output-with-callbacks-and-flags","content":"Some of these features, like variable binding, require a callback to the formatter's client. The client gets a notification, along with a few control variables, and it can then decide exactly what goes in the output. The control structure is struct gen_sql_callbacks, and it is described below. This structure includes the various callbacks (all of which are optional) and each callback gets a 'context' pointer of its choice. The context pointer is some arbitrary void * value that you provide, which will be given to your function along with the AST pointer relevant to the call. The callback also gets the current output buffer so it can choose to emit something (like '?') into the stream. // signature for a callback, you get your context plus the ast // if you return true then the normal output is suppressed // in any case the output you provide is emitted typedef bool_t (*_Nullable gen_sql_callback)( struct ast_node *_Nonnull ast, void *_Nullable context, charbuf *_Nonnull output );  The meaning of the bool_t return value varies depend on which callback it is. The coarsest control is provided by the generation mode. It is one of these values: // These modes control the overall style of the output enum gen_sql_mode { gen_mode_echo, // Prints everything in the original, with standard whitespace and parentheses gen_mode_sql, // Prints the AST formatted for SQLite consumption, omits anything CQL specific gen_mode_no_annotations // Equivalent to gen_mode_echo without versioning attributes or generic attributes // * @create, @delete, @recreate, and @attribute are removed // * statements like @echo are not affected, nor is the type specifier @sensitive };  The actual callbacks structure is optional, if it is NULL then a full echo of the AST with no changes will be produced. Otherwise the callbacks and flags alter the behavior of the echoer somewhat. // Callbacks allow you to significantly alter the generated sql, see the particular flags below. typedef struct gen_sql_callbacks { // Each time a local/global variable is encountered in the AST, this callback is invoked // this is to allow the variable reference to be noted and replaced with ? in the generated SQL gen_sql_callback _Nullable variables_callback; void *_Nullable variables_context; // Each time a column definition is emitted this callback is invoked, it may choose to // suppress that column. This is used to remove columns that were added in later schema // versions from the baseline schema. gen_sql_callback _Nullable col_def_callback; void *_Nullable col_def_context; // This callback is used to explain the * in select * or select T.* gen_sql_callback _Nullable star_callback; void *_Nullable star_context; // This callback is used to force the &quot;IF NOT EXISTS&quot; form of DDL statements when generating // schema upgrade steps. e.g. a &quot;CREATE TABLE Foo declarations get &quot;IF NOT EXISTS&quot; added // to them in upgrade steps. gen_sql_callback _Nullable if_not_exists_callback; void *_Nullable if_not_exists_context; // If true, hex literals are converted to decimal. This is for JSON which does not support hex literals. bool_t convert_hex; // If true casts like &quot;CAST(NULL as TEXT)&quot; are reduced to just NULL. The type information is not needed // by SQLite so it just wasts space. bool_t minify_casts; // If true then unused aliases in select statements are elided to save space. This is safe because // CQL always binds the top level select statement by ordinal anyway. bool_t minify_aliases; // mode to print cql statement: gen_mode_echo, gen_mode_sql, gen_mode_no_annotations. // gen_mode_sql mode causes the AS part of virtual table to be suppressed enum gen_sql_mode mode; // If CQL finds a column such as 'x' below' // // create table foo( // x long_int primary key autoincrement // ); // // that column must be converted to this form: // // create table foo( // x integer primary key autoincrement // ); // // This is because SQLite mandates that autoincrement must be exactly // in the second example above however, it is also the case that in SQLite // an integer can store a 64 bit value. So sending &quot;integer&quot; to SQLite while // keeping the sense that the column is to be treated as 64 bits in CQL works // just fine. // // However, when we are emitting CQL (rather than SQL) we want to keep // the original long_int type so as not to lose fidelity when processing // schema for other semantic checks (such as matching FK data types). // // This flag is for that purpose: It tells us that the target isn't SQLite // and we don't need to do the mapping (yet). Indeed, we shouldn't, or the // types will be messed up. // // In short, if CQL is going to process the output again, use this flag // to control the autoincrement transform. It might be possible to fold // this flag with the mode flag but it's sufficiently weird that this // extra documentation and special handling is probably worth the extra // boolean storage. bool_t long_to_int_conv; } gen_sql_callbacks;  Each callback can be best understood by reading the source, so we'll avoid trying to precisely define it here. But it is helpful to give the gist of these options. mode : one of the three enum modes that control overall behaviorvariables_callback : invoked when a variable appears in the SQL, the caller can record the specific variable and then use it for bindingcol_def_callback : when creating the &quot;baseline&quot; schema you don't want column definitions from later schema to be included, this gives you a chance to suppress themstar_callback : normally the * in select * or select T.* is expanded when emitting for SQLite, this callback does the expansion when appropriateif_not_exists_callback : when generating DDL for schema upgrade you typically want to force IF NOT EXISTS to be added to the schema even if it wasn't present in the declaration; this callback lets you do thatconvert_hex : if true, hex constants are converted to decimal; used when emitting JSON because JSON doesn't understand hex constantsminify_casts : minification converts casts like CAST(NULL AS TEXT) to just NULL -- the former is only useful for type information, SQLite does need to see itminify_aliases : unused column aliases as in select foo.x as some_really_long_alias can be removed from the output when targeting SQLite to save space "},{"title":"Invoking the Generator​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#invoking-the-generator","content":"There are several generation functions but they all follow a similar pattern, the differences are essentially what fragment of the AST they expect to begin on. We'll just cover one here. cql_noexport void gen_statement_with_callbacks(ast_node *_Nonnull ast, gen_sql_callbacks *_Nullable _callbacks);  This has the typical signature for all these generators: ast : the part of the tree to print_callbacks : the optional callbacks described above To use these you'll need to these functions as well: cql_noexport void gen_init(void); cql_noexport void gen_cleanup(void);  You'll want to call gen_init() one time before doing any generation. That sets up the necessary tables. When you're done use gen_cleanup() to release any memory that was allocated in setup. You don't have to do the cleanup step if the process is going to exit anyway, however, because of the amalgam options, cql_main() assumes it might be called again and so it tidies things up rather than risk leaking. With the one time initialization in place there are these preliminaries: cql_noexport void init_gen_sql_callbacks(gen_sql_callbacks *_Nullable callbacks);  Use init_gen_sql_callbacks to fill in your callback structure with the normal defaults. This give you normal echo for SQL by default. To get a full echo, a NULL callback may be used. And of course other options are possible. Finally, cql_noexport void gen_set_output_buffer(struct charbuf *_Nonnull buffer);  Use this before the call to gen_&lt;something&gt;_with_callbacks to redirect the output into a growable character buffer of your choice. The buffers can then be written where they are needed. Maybe further processed into a C string literal for compiler output, or into a C style comment, or just right back to stdout. There are a few simplified versions of this sequence like this one: cql_noexport void gen_stmt_list_to_stdout(ast_node *_Nullable ast);  This uses NULL for the callbacks and emits directly to stdout with no extra steps. The extra wiring is done for you. "},{"title":"Generator Internals​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#generator-internals","content":"The generator has to be able to walk the entire tree and emit plain text, and in many areas the tree is very flexible so we want a simple dynamic dispatch mechanism that can call the right formatting function from anywhere in the tree. It turns out two different signatures are needed to do this properly, one for formatting statements and the other for expressions -- the difference being that expressions have to concern themselves with the precedence of the various operators so that parentheses can be correctly (re)inserted into the output. To do this there are two symbol tables that map from an AST node type string to a formatting function. They are initialized with a series of statements similar to these: Generating Expressions​ cql_noexport void gen_init() { gen_stmts = symtab_new(); gen_exprs = symtab_new(); STMT_INIT(if_stmt); ... EXPR_INIT(mul, gen_binary, &quot;*&quot;, EXPR_PRI_MUL); EXPR_INIT(div, gen_binary, &quot;/&quot;, EXPR_PRI_MUL); EXPR_INIT(mod, gen_binary, &quot;%&quot;, EXPR_PRI_MUL); EXPR_INIT(add, gen_binary, &quot;+&quot;, EXPR_PRI_ADD); EXPR_INIT(sub, gen_binary, &quot;-&quot;, EXPR_PRI_ADD); EXPR_INIT(not, gen_unary, &quot;NOT &quot;, EXPR_PRI_NOT); EXPR_INIT(tilde, gen_unary, &quot;~&quot;, EXPR_PRI_TILDE); ... }  These statements populate the symbol tables. For statements, the entry maps if_stmt to the function gen_if_stmtFor expressions, the entry maps mul to gen_binary including the metadata &quot;*&quot; and EXPR_PRI_MUL As you can see, nearly all binary operators are handled identically as are all unary operators. Let's look at those two in detail. static void gen_binary(ast_node *ast, CSTR op, int32_t pri, int32_t pri_new) { // We add parens if our priority is less than the parent priority // meaning something like this: // * we're a + node, our parent is a * node // * we need parens because the tree specifies that the + happens before the * // // Also, grouping of equal operators is left to right // so for so if our right child is the same precedence as us // that means there were parens there in the original expression // e.g. 3+(4-7); // effectively it's like we're one binding strength higher for our right child // so we call it with pri_new + 1. If it's equal to us it must emit parens if (pri_new &lt; pri) gen_printf(&quot;(&quot;); gen_expr(ast-&gt;left, pri_new); gen_printf(&quot; %s &quot;, op); gen_expr(ast-&gt;right, pri_new + 1); if (pri_new &lt; pri) gen_printf(&quot;)&quot;); }  The convention gives us: ast : pointer to the current AST nodeop : the text of the operator (CSTR is simply const char *)pri : the binding strength of the node above uspri_new : the binding strength of this node (the new node) So generically, if the binding strength of the current operator pri_new is weaker than the context it is contained in pri, then parentheses are required to preserve order of operations. See the comment for more details. With parens taken care of, we emit the left expression, the operator, and the right expression. And as you can see below, unary operators are much the same. static void gen_unary(ast_node *ast, CSTR op, int32_t pri, int32_t pri_new) { if (pri_new &lt; pri) gen_printf(&quot;(&quot;); gen_printf(&quot;%s&quot;, op); gen_expr(ast-&gt;left, pri_new); if (pri_new &lt; pri) gen_printf(&quot;)&quot;); }  There are special case formatters for some of the postfix operators and other cases that are special like CASE... WHEN... THEN... ELSE... END but they operate on the same principles down to the leaf nodes. Generating Statements​ With no binding strength to worry about, statement processing is quite a bit simpler. Here's the code for the IF statement mentioned above. static void gen_if_stmt(ast_node *ast) { Contract(is_ast_if_stmt(ast)); EXTRACT_NOTNULL(cond_action, ast-&gt;left); EXTRACT_NOTNULL(if_alt, ast-&gt;right); EXTRACT(elseif, if_alt-&gt;left); EXTRACT_NAMED(elsenode, else, if_alt-&gt;right); gen_printf(&quot;IF &quot;); gen_cond_action(cond_action); if (elseif) { gen_elseif_list(elseif); } if (elsenode) { gen_printf(&quot;ELSE\\n&quot;); EXTRACT(stmt_list, elsenode-&gt;left); gen_stmt_list(stmt_list); } gen_printf(&quot;END IF&quot;); }  There is a general boilerplate sort of recursive form to all of these; they follow the same basic shape. These patterns are designed to make it impossible to walk the tree incorrectly. If the tree shape changes because of a grammar change, you get immediate concrete failures where the tree walk has to change. Since there are test cases to cover every tree shape you can always be sure you have it exactly right if the macros do not force assertion failures. The steps were: use Contract to assert that the node we are given is the type we expectuse EXTRACT macros (detailed below) to get the tree parts you want starting from your rootuse gen_printf to emit the constant pieces of the statementuse recursion to print sub-fragments (like the IF condition in this case)test the tree fragments where optional pieces are present, emit them as needed It might be instructive to include gen_cond_action; it is entirely unremarkable: static void gen_cond_action(ast_node *ast) { Contract(is_ast_cond_action(ast)); EXTRACT(stmt_list, ast-&gt;right); gen_root_expr(ast-&gt;left); gen_printf(&quot; THEN\\n&quot;); gen_stmt_list(stmt_list); }  A cond_action node has an expression on the left and a statement list on the right; it can appear in the base IF x THEN y part of the IF or as ELSE IF x THEN y. Either case is formatted the same. Extraction Macros​ These macros are used by all the parts of CQL that walk the AST. They are designed to make it impossible for you to get the tree shape wrong without immediately failing. We do not ever want to walk off the tree in some exotic way and then continue to several levels of recursion before things go wrong. CQL locks this down by checking the node type at every step -- any problems are found immediately, exactly at the extraction site, and can be quickly corrected. Again 100% coverage of all the tree shapes makes this rock solid, so CQL never compromises on 100% code coverage. The most common macros all appear in this example: EXTRACT_NOTNULL(cond_action, ast-&gt;left); read ast-&gt;left, assert that it is of type cond_action, it must not be NULLdeclare a local variable named cond_action to hold the result EXTRACT_NOTNULL(if_alt, ast-&gt;right); read ast-&gt;right, assert that it is of type if_alt, it must not be NULLdeclare a local variable named if_alt to hold the result EXTRACT(elseif, if_alt-&gt;left); read if_alt-&gt;left, assert that it is either NULL or else of type elseifdeclare a variable named elseif to hold the result EXTRACT_NAMED(elsenode, else, if_alt-&gt;right); read if_alt-&gt;right, assert that it is either NULL or else of type elsedeclare a variable named elsenode to hold the resultnote that we can't use a variable named else because else is a keyword in C Other options: EXTRACT_NAMED_NOTNULL : like the NAMED variantEXTRACT_ANY : if the tree type is not known (e.g. expr-&gt;left could be any expression type)EXTRACT_ANY_NOTNULL : as above but not optionalEXTRACT_NUM_TYPE : extracts the num_type field from a numeric AST node The ANY variants are usually re-dispatched with something like gen_expr that uses the name table again (and that will check the type) or else the extracted value is checked with ad hoc logic immediately after extraction if it's perhaps one of two or three variations. In all cases the idea is to force a failure very quickly. gen_root_expr() for instance in the if_cond example will fail immediately if the node it gets is not an expression type. Because of the clear use of EXTRACT, the gen_ family of functions are often the best/fastest way to understand the shape of the AST. You can dump a few samples and look at the gen_ function and quickly see exactly what the options are authoritatively. As a result it's very normal to paste the extraction code from a gen_ function into a new/needed semantic analysis or code-generation function. "},{"title":"Part 2: Semantic Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-2-semantic-analysis","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-1","content":"Part 2 continues with a discussion of the essentials of the semantic analysis pass of the CQL compiler. As in the previous sections, the goal here is not to go over every single rule but rather to give a sense of how semantic analysis happens in general -- the core strategies and implementation choices -- so that when reading the code you will have an idea of how smaller pieces fit into the whole. To accomplish this, various key data structures will be explained in detail and selected examples of their use are included. "},{"title":"Semantic Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#semantic-analysis","content":"The overall goal of the semantic analysis pass is to verify that a correct program has been submitted to the compiler. The compiler does this by &quot;decorating&quot; the AST with semantic information. This information is mainly concerned with the &quot;types&quot; of the various things in the program. A key function of the semantic analyzer, the primary &quot;weapon&quot; in computing these types, if you will, is name resolution. The semantic analyzer decides what any given name means in any context and then uses that meaning, which is itself based on the AST constructs that came before, to compute types and then check those types for errors. Broadly speaking, the errors that can be discovered are of these forms: mentioned names do not exist e.g. using a variable or table or column without declaring it mentioned names are not unique, or are ambiguous e.g. every view must have a unique namee.g. table names need to be unique, or aliased when joining tables operands are not compatible with each other or with the intended operation e.g. you can't add a string to a reale.g. you can't do the % operation on a reale.g. the expression in a WHERE clause must result in a numerice.g. the first argument to printf must be a string literale.g. you can't assign a long value to an integer variablee.g. you can't assign a possibly null result to a not-null variable there are too many or two few operands for an operation e.g. an INSERT statement must include sufficiently many columns and no extrase.g. a function or procedure call must have the correct number of operands an operation is happening in a context where it is not allowed e.g. use of aggregate functions in the WHERE clausee.g. use of unique SQLite functions outside of a SQL statement There are several hundred possible errors, and no attempt will be made to cover them all here but we will talk about how errors are created, recorded, and reported. "},{"title":"Decorated AST examples​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#decorated-ast-examples","content":"Recalling the AST output from Part 1, this is what that same tree looks like with semantic information attached: LET X := 1 + 3; {let_stmt}: X: integer notnull variable | {name X}: X: integer notnull variable | {add}: integer notnull | {int 1}: integer notnull | {int 3}: integer notnull  And here's an example with some structure types: SELECT 1 AS x, 3.2 AS y; {select_stmt}: select: { x: integer notnull, y: real notnull } | {select_core_list}: select: { x: integer notnull, y: real notnull } | | {select_core}: select: { x: integer notnull, y: real notnull } | | {select_expr_list_con}: select: { x: integer notnull, y: real notnull } | | {select_expr_list}: select: { x: integer notnull, y: real notnull } | | | {select_expr}: x: integer notnull | | | | {int 1}: integer notnull | | | | {opt_as_alias} | | | | {name x} | | | {select_expr_list} | | | {select_expr}: y: real notnull | | | {dbl 3.2}: real notnull | | | {opt_as_alias} | | | {name y} | | {select_from_etc}: ok | | {select_where} | | {select_groupby} | | {select_having} | {select_orderby} | {select_limit} | {select_offset}  These can be generated by adding --sem --ast to the CQL command line along with --in your_file.sql. Keep these shapes in mind as we discuss the various sources of type information. "},{"title":"The Base Data Structures​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-base-data-structures","content":"First recall that every AST node has this field in it: struct sem_node *_Nullable sem;  This is the pointer to the semantic information for that node. Semantic analysis happens immediately after parsing and before any of the code-generators run. Importantly, code generators never run if semantic analysis reported any errors. Before we get into the shape of the semantic node, we should start with the fundamental unit of type info sem_t which is usually stored in a variable called sem_type. typedef uint64_t sem_t;  The low order bits of a sem_t encode the core type and indeed there is a helper function to extract the core type from a sem_t. // Strips out all the flag bits and gives you the base/core type. cql_noexport sem_t core_type_of(sem_t sem_type) { return sem_type &amp; SEM_TYPE_CORE; }  The core bits are as follows: #define SEM_TYPE_NULL 0 // the subtree is a null literal (not just nullable) #define SEM_TYPE_BOOL 1 // the subtree is a bool #define SEM_TYPE_INTEGER 2 // the subtree is an integer #define SEM_TYPE_LONG_INTEGER 3 // the subtree is a long integer #define SEM_TYPE_REAL 4 // the subtree is a real #define SEM_TYPE_TEXT 5 // the subtree is a text type #define SEM_TYPE_BLOB 6 // the subtree is a blob type #define SEM_TYPE_OBJECT 7 // the subtree is any object type #define SEM_TYPE_STRUCT 8 // the subtree is a table/view #define SEM_TYPE_JOIN 9 // the subtree is a join #define SEM_TYPE_ERROR 10 // marks the subtree as having a problem #define SEM_TYPE_OK 11 // sentinel for ok but no type info #define SEM_TYPE_PENDING 12 // sentinel for type calculation in flight #define SEM_TYPE_REGION 13 // the ast is a schema region #define SEM_TYPE_CURSOR_FORMAL 14 // this is used for the cursor parameter type uniquely #define SEM_TYPE_CORE 0xff // bit mask for the core types #define SEM_TYPE_MAX_UNITARY (SEM_TYPE_OBJECT+1) // the last unitary type  These break into a few categories: NULL to OBJECT are the &quot;unitary&quot; types -- these are the types that a single simple variable can be a column can be any of these except OBJECT or NULLthe NULL type comes only from the NULL literal which has no typeinstances of, say, a TEXT column might have a NULL value but they are known to be TEXT STRUCT indicates that the object has many fields, like a table, or a cursorJOIN indicates that the object is the concatenation of many STRUCT types e.g. T1 inner join T2 is a JOIN type with T1 and T2 being the partsa JOIN could be flattened to STRUCT, but this is typically not donethe type of a SELECT statement will be a STRUCT representing the expressions that were selectedthose expressions in turn used columns from the JOIN that was the FROM clause ERROR indicates that the subtree had an error the error will have been already reportedthe error type generally cascades up the AST to the root OK indicates that there is no type information but there was no problem e.g. a correct IF statement will resolve to simply OK (no error) PENDING is used sometimes while a type computation is in progress this type doesn't appear in the AST, but has its own unique value so as to not conflict with any others REGION is used to identify AST fragments that correspond to schema regions see Chapter 10 of the Guide for more information on regions CORE is the mask for the core parts, 0xf would do the job but for easy reading in the debugger we use 0xff new core types are not added very often, adding a new one is usually a sign that you are doing something wrong The core type can be modified by various flags. The flags, in principle, can be combined in any way but in practice many combinations make no sense. For instance, HAS_DEFAULT is for table columns and CREATE_FUNC is for function declarations. There is no one object that could require both of these. The full list as of this writing is as follows: #define SEM_TYPE_NOTNULL _64(0x0100) // set if and only if null is not possible #define SEM_TYPE_HAS_DEFAULT _64(0x0200) // set for table columns with a default #define SEM_TYPE_AUTOINCREMENT _64(0x0400) // set for table columns with autoinc #define SEM_TYPE_VARIABLE _64(0x0800) // set for variables and parameters #define SEM_TYPE_IN_PARAMETER _64(0x1000) // set for in parameters (can mix with below) #define SEM_TYPE_OUT_PARAMETER _64(0x2000) // set for out parameters (can mix with above) #define SEM_TYPE_DML_PROC _64(0x4000) // set for stored procs that have DML/DDL #define SEM_TYPE_HAS_SHAPE_STORAGE _64(0x8000) // set for a cursor with simplified fetch syntax #define SEM_TYPE_CREATE_FUNC _64(0x10000) // set for a function that returns a created object +1 ref #define SEM_TYPE_SELECT_FUNC _64(0x20000) // set for a sqlite UDF function declaration #define SEM_TYPE_DELETED _64(0x40000) // set for columns that are not visible in the current schema version #define SEM_TYPE_VALIDATED _64(0x80000) // set if item has already been validated against previous schema #define SEM_TYPE_USES_OUT _64(0x100000) // set if proc has a one rowresult using the OUT statement #define SEM_TYPE_USES_OUT_UNION _64(0x200000) // set if proc uses the OUT UNION form for multi row result #define SEM_TYPE_PK _64(0x400000) // set if column is a primary key #define SEM_TYPE_FK _64(0x800000) // set if column is a foreign key #define SEM_TYPE_UK _64(0x1000000) // set if column is a unique key #define SEM_TYPE_VALUE_CURSOR _64(0x2000000) // set only if SEM_TYPE_HAS_SHAPE_STORAGE is set and the cursor has no statement #define SEM_TYPE_SENSITIVE _64(0x4000000) // set if the object is privacy sensitive #define SEM_TYPE_DEPLOYABLE _64(0x8000000) // set if the object is a deployable region #define SEM_TYPE_BOXED _64(0x10000000) // set if a cursor's lifetime is managed by a box object #define SEM_TYPE_HAS_CHECK _64(0x20000000) // set for table column with a &quot;check&quot; clause #define SEM_TYPE_HAS_COLLATE _64(0x40000000) // set for table column with a &quot;collate&quot; clause #define SEM_TYPE_INFERRED_NOTNULL _64(0x80000000) // set if inferred to not be nonnull (but was originally nullable) #define SEM_TYPE_VIRTUAL _64(0x100000000) // set if and only if this is a virtual table #define SEM_TYPE_HIDDEN_COL _64(0x200000000) // set if and only if hidden column on a virtual table #define SEM_TYPE_TVF _64(0x400000000) // set if and only table node is a table valued function #define SEM_TYPE_IMPLICIT _64(0x800000000) // set if and only the variable was declare implicitly (via declare out) #define SEM_TYPE_CALLS_OUT_UNION _64(0x1000000000) // set if proc calls an out union proc for  Note: _64(x) expands to either a trailing L or a trailing LL depending on the bitness of the compiler, whichever yields an int64_t. Going over the meaning of all of the above is again beyond the scope of this document; some of the flags are very specialized and essentially the validation just requires a bit of storage in the tree to do its job so that storage is provided with a flag. However two flag bits are especially important and are computed almost everywhere sem_t is used. These are SEM_TYPE_NOTNULL and SEM_TYPE_SENSITIVE. SEM_TYPE_NOTNULL indicates that the marked item is known to be NOT NULL, probably because it was declared as such, or directly derived from a not null item Typically when two operands are combined both must be marked NOT NULL for the result to still be NOT NULL (there are exceptions like COALESCE)Values that might be null cannot be assigned to targets that must not be null SEM_TYPE_SENSITIVE indicates that the marked item is some kind of PII or other sensitive data. Any time a sensitive operand is combined with another operand the resulting type is sensitiveThere are very few ways to &quot;get rid&quot; of the sensitive bit -- it corresponds to the presence of @sensitive in the data type declarationValues that are sensitive cannot be assigned to targets that are not marked sensitive The semantic node sem_node carries all the possible semantic info we might need, and the sem_type holds the flags above and tells us how to interpret the rest of the node. There are many fields -- we'll talk about some of the most important ones here to give you a sense of how things hang together. Note that CSTR is simply an alias for const char *. CSTR is used extensively in the codebase for brevity. typedef struct sem_node { sem_t sem_type; // core type plus flags CSTR name; // for named expressions in select columns, etc. CSTR kind; // the Foo in object&lt;Foo&gt;, not a variable or column name CSTR error; // error text for test output, not used otherwise struct sem_struct *sptr; // encoded struct if any struct sem_join *jptr; // encoded join if any int32_t create_version; // create version if any (really only for tables and columns) int32_t delete_version; // delete version if any (really only for tables and columns) bool_t recreate; // for tables only, true if marked @recreate CSTR recreate_group_name; // for tables only, the name of the recreate group if they are in one CSTR region; // the schema region, if applicable; null means unscoped (default) symtab *used_symbols; // for select statements, we need to know which of the ids in the select list was used, if any list_item *index_list; // for tables we need the list of indices that use this table (so we can recreate them together if needed) struct eval_node *value; // for enum values we have to store the evaluated constant value of each member of the enum } sem_node;  sem_type : already discussed above, this tells you how to interpret everything elsename : variables, columns, etc. have a canonical name -- when a name case-insensitivity resolves, the canonical name is stored here typically later passes emit the canonical variable name everywheree.g. because FoO and fOO might both resolve to an object declared as foo, we always emit foo in codegen kind : in CQL any type can be discriminated as in declare foo real&lt;meters&gt;, the kind here is meters two expressions of the same core type (e.g. real) are incompatible if they have a kind and the kind does not matche.g. if you have bar real&lt;liters&gt; then set foo := bar; this is an error even though both are real because foo above is real&lt;meters&gt; sptr : if the item's core type is SEM_TYPE_STRUCT then this is populated (see below)jptr : if the item's core type is SEM_TYPE_JOIN then this is populated (see below) If the object is a structure type then this is simply an array of names, kinds, and semantic types. In fact the semantic types will be all be unitary, possibly modified by NOT_NULL or SENSITIVE but none of the other flags apply. A single sptr directly corresponds to the notion of a &quot;shape&quot; in the analyzer. Shapes come from anything that looks like a table, such as a cursor, or the result of a SELECT statement. // for tables and views and the result of a select typedef struct sem_struct { CSTR struct_name; // struct name uint32_t count; // count of fields CSTR *names; // field names CSTR *kinds; // the &quot;kind&quot; text of each column, if any, e.g. integer&lt;foo&gt; foo is the kind sem_t *semtypes; // typecode for each field } sem_struct;  If the object is a join type (such as the parts of the FROM clause) then the jptr field will be populated. This is nothing more than a named list of struct types. // for the data type of (parts of) the FROM clause // sometimes I refer to as a &quot;joinscope&quot; typedef struct sem_join { uint32_t count; // count of table/views in the join CSTR *names; // names of the table/view struct sem_struct **tables; // struct type of each table/view } sem_join;  With these building blocks we can represent the type of anything in the CQL language. "},{"title":"Initiating Semantic Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#initiating-semantic-analysis","content":"The semantic analysis pass runs much the same way as the AST emitter. In sem.c there is the essential function sem_main. It suffices to call sem_main on the root of the AST. That root node is expected to be a stmt_list node. // This method loads up the global symbol tables in either empty state or // with the appropriate tokens ready to go. Using our own symbol tables for // dispatch saves us a lot of if/else string comparison verbosity. cql_noexport void sem_main(ast_node *ast) { // restore all globals and statics we own sem_cleanup(); eval_init(); ... }  As you can see, sem_main begins by resetting all the global state. You can of course do this yourself after calling sem_main (when you're done with the results). sem_main sets a variety of useful and public global variables that describe the results of the analysis. The ones in sem.h are part of the contract and you should feel free to use them in a downstream code-generator. Other items are internal and should be avoided. The internal items are typically defined statically in sem.c. The essential outputs will be described in the last section of this part. The cleanup has this structure: // This method frees all the global state of the semantic analyzer cql_noexport void sem_cleanup() { eval_cleanup(); BYTEBUF_CLEANUP(deployable_validations); BYTEBUF_CLEANUP(recreate_annotations); BYTEBUF_CLEANUP(schema_annotations); SYMTAB_CLEANUP(funcs); SYMTAB_CLEANUP(globals); SYMTAB_CLEANUP(indices); SYMTAB_CLEANUP(locals); ... // these are getting zeroed so that leaksanitizer will not count those objects as reachable from a global root. all_ad_hoc_list = NULL; all_functions_list = NULL; ...  This basically deallocates everything and resets all the globals to NULL. sem_main of course has to walk the AST and it does so in much the same way as we saw in gen_sql.c. There is a series of symbol tables whose key is an AST type and whose value is a function plus arguments to dispatch (effectively a lambda.) The semantic analyzer doesn't have to think about things like &quot;should I emit parentheses?&quot; so the signature of each type of lambda can be quite a bit simpler. We'll go over each kind with some examples. First we have the non-SQL statements, these are basic flow control or other things that SQLite will never see directly.  symtab *syms = non_sql_stmts; STMT_INIT(if_stmt); STMT_INIT(while_stmt); STMT_INIT(switch_stmt); STMT_INIT(leave_stmt); ...  Here STMT_INIT creates a binding between (e.g.) the AST type if_stmt and the function sem_if_stmt. This lets us dispatch any part of the AST to its handler directly. Next we have the SQL statements. These get analyzed in the same way as the others, and with functions that have the same signature, however, if you use one of these it means that procedure that contained this statement must get a database connection in order to run. Use of the database will require the procedure's signature to change; this is recorded by the setting the SEM_TYPE_DML_PROC flag bit to be set on the procedure's AST node.  syms = sql_stmts; STMT_INIT(create_table_stmt); STMT_INIT(drop_table_stmt); STMT_INIT(create_index_stmt); STMT_INIT(create_view_stmt); STMT_INIT(select_stmt); STMT_INIT(delete_stmt); STMT_INIT(update_stmt); STMT_INIT(insert_stmt); ...  Again STMT_INIT creates a binding between (e.g.) the AST type delete_stmt and the function sem_delete_stmt so we can dispatch to the handler. Next we have expression types. These are set up with EXPR_INIT. Many of the operators require exactly the same kinds of verification, so in order to be able to share the code, the expression analysis functions get an extra argument for the operator in question. Typically the string of the operator is only needed to make a good quality error message with validation being otherwise identical. Here are some samples...  EXPR_INIT(num, sem_expr_num, &quot;NUM&quot;); EXPR_INIT(str, sem_expr_str, &quot;STR&quot;); EXPR_INIT(blob, sem_expr_blob, &quot;BLB&quot;); EXPR_INIT(null, sem_expr_null, &quot;NULL&quot;); EXPR_INIT(dot, sem_expr_dot, &quot;DOT&quot;); EXPR_INIT(const, sem_expr_const, &quot;CONST&quot;); EXPR_INIT(mul, sem_binary_math, &quot;*&quot;); EXPR_INIT(mod, sem_binary_integer_math, &quot;%&quot;); EXPR_INIT(not, sem_unary_logical, &quot;NOT&quot;); EXPR_INIT(is_true, sem_unary_is_true_or_false, &quot;IS TRUE&quot;); EXPR_INIT(tilde, sem_unary_integer_math, &quot;~&quot;); EXPR_INIT(uminus, sem_unary_math, &quot;-&quot;);  Looking at the very first entry as an example, we see that EXPR_INIT creates a mapping between the AST type numand the analysis function sem_expr_num and that function will get the text &quot;NUM&quot; as an extra argument. As it happens sem_expr_num doesn't need the extra argument, but sem_binary_math certainly needs the &quot;*&quot;as that function handles a large number of binary operators. Let's quickly go over this list as these are the most important analyzers: sem_expr_num : analyzes any numeric constantsem_expr_str : analyzes any string literal or identifiersem_expr_blob : analyzes any blob literalsem_expr_null : analyzes the NULL literal (and nothing else)sem_expr_dot : analyzes a compound name like T1.idsem_expr_const : analyzes a const(...) expression, doing the constant evaluationsem_binary_math : analyzes any normal binary math operator like '+', '-', '/' etc.sem_binary_integer_math : analyzes any binary math operator where the operands must be integers like '%' or '|'sem_unary_logical : analyzes any unary logical operator (the result is a bool) -- this is really only NOTsem_unary_is_true_or_false : analyzes any of the IS TRUE, IS FALSE family of postfix unary operatorssem_unary_integer_math : analyzes any unary operator where the operand must be an integer -- this is really only ~sem_unary_math : analyzes any any math unary operator, presently only negation (but in the future unary + too) The last group of normal associations are for builtin functions, like these:  FUNC_INIT(changes); FUNC_INIT(printf); FUNC_INIT(strftime); FUNC_INIT(date); FUNC_INIT(time);  Each of these is dispatched when a function call is found in the tree. By way of example FUNC_INIT(changes)causes the changes function to map to sem_func_changes for validation. There are a few other similar macros for more exotic cases but the general pattern should be clear now. With these in place it's very easy to traverse arbitrary statement lists and arbitrary expressions with sub expressions and have the correct function invoked without having large switch blocks all over. "},{"title":"Semantic Errors​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#semantic-errors","content":"Some of the following examples will show the handling of semantic errors more precisely but the theory is pretty simple. Each of the analyzers that has been registered is responsible for putting an appropriate sem_node into the AST it is invoked on. The caller will look to see if that sem_nodeis of type SEM_TYPE_ERROR using is_error(ast). If it is, the caller will mark its own AST as errant using record_error(ast) and this continues all the way up the tree. The net of this is that wherever you begin semantic analysis, you can know if there were any problems by checking for an error at the top of the tree you provided. At the point of the initial error, the analyzer is expected to also call report_error providing a suitable message. This will be logged to stderr. In test mode it is also stored in the AST so that verification steps can confirm that errors were reported at exactly the right place. If there are no errors, then a suitable sem_node is created for the resulting type or else, at minimum, record_ok(ast) is used to place the shared &quot;OK&quot; type on the node. The &quot;OK&quot; type indicates no type information, but no errors either. &quot;OK&quot; is helpful for statements that don't involve expressions like DROP TABLE Foo. "},{"title":"The Primitive Types​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-primitive-types","content":"Perhaps the simplest analysis of all happens at the leaves of the AST. By way of example, here is the code for expression nodes of type num, the numeric literals. // Expression type for numeric primitives static void sem_expr_num(ast_node *ast, CSTR cstr) { Contract(is_ast_num(ast)); EXTRACT_NUM_TYPE(num_type, ast); switch (num_type) { case NUM_BOOL: ast-&gt;sem = new_sem(SEM_TYPE_BOOL | SEM_TYPE_NOTNULL); break; case NUM_INT: ast-&gt;sem = new_sem(SEM_TYPE_INTEGER | SEM_TYPE_NOTNULL); break; case NUM_LONG: ast-&gt;sem = new_sem(SEM_TYPE_LONG_INTEGER | SEM_TYPE_NOTNULL); break; default: // this is all that's left Contract(num_type == NUM_REAL); ast-&gt;sem = new_sem(SEM_TYPE_REAL | SEM_TYPE_NOTNULL); break; } }  As you can see the code simply looks at the AST node, confirming first that it is a num node. Then it extracts the num_type. Then ast-&gt;sem is set to a semantic node of the matching type adding in SEM_TYPE_NOTNULL because literals are never null. The new_sem function is used to make an empty sem_node with the sem_type filled in as specified. Nothing can go wrong creating a literal so there are no failure modes. It doesn't get much simpler unless maybe... // Expression type for constant NULL static void sem_expr_null(ast_node *ast, CSTR cstr) { Contract(is_ast_null(ast)); // null literal ast-&gt;sem = new_sem(SEM_TYPE_NULL); }  It's hard to get simpler than doing semantic analysis of the NULL literal. Its code should be clear with no further explanation needed. "},{"title":"Unary Operators​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#unary-operators","content":"Let's dive in to a simple case that does require some analysis -- the unary operators. There are comparatively few and there isn't much code required to handle them all. Here's the code for the unary math operators: // The only unary math operators are '-' and '~' // Reference types are not allowed static void sem_unary_math(ast_node *ast, CSTR op) { sem_t core_type, combined_flags; if (!sem_unary_prep(ast, &amp;core_type, &amp;combined_flags)) { return; } if (!sem_validate_numeric(ast, core_type, op)) { return; } // The result of unary math promotes to integer. Basically this converts // bool to integer. Long integer and Real stay as they are. Text is // already ruled out. sem_t sem_type_result = sem_combine_types( (SEM_TYPE_INTEGER | SEM_TYPE_NOTNULL), (core_type | combined_flags)); ast-&gt;sem = new_sem(sem_type_result); ast-&gt;sem-&gt;kind = ast-&gt;left-&gt;sem-&gt;kind; // note ast-&gt;sem-&gt;name is NOT propagated because SQLite doesn't let you refer to // the column 'x' in 'select -x' -- the column name is actually '-x' which is useless // so we have no name once you apply unary math (unless you use 'as') // hence ast-&gt;sem-&gt;name = ast-&gt;left-&gt;sem-&gt;name is WRONG here and it is not missing on accident }  Unary Prep OK already we need to pause because there is a &quot;prep&quot; pattern here common to most of the shared operators that we should discuss. The prep step takes care of most of the normal error handling which is the same for all the unary operators and the same pattern happens in binary operators. Let's take a look at sem_unary_prep. // The unary operators all have a similar prep to the binary. We need // to visit the left side (it's always the left node even if the operator goes on the right) // if that's ok then we need the combined_flags and core type. There is only // the one. Returns true if everything is ok. static bool_t sem_unary_prep(ast_node *ast, sem_t *core_type, sem_t *combined_flags) { // op left | left op sem_expr(ast-&gt;left); if (is_error(ast-&gt;left)) { *core_type = SEM_TYPE_ERROR; *combined_flags = 0; record_error(ast); return false; } sem_node *sem = ast-&gt;left-&gt;sem; sem_t sem_type = sem-&gt;sem_type; *core_type = core_type_of(sem_type); *combined_flags = not_nullable_flag(sem_type) | sensitive_flag(sem_type); Invariant(is_unitary(*core_type)); return true; }  Reviewing the steps: first we analyze the operand, it will be in ast-&gt;leftif that's an error, we just return the error code from the prep stepsnow that it's not an error, we pull the core type out of the operandthen we pull the not nullable and sensitive flag bits out of the operandfinally return a boolean indicating the presence of an error (or not) for convenience This is useful setup for all the unary operators, and as we'll see, the binary operators have a similar prep step. Back to Unary Processing Looking at the overall steps we see: sem_unary_prep : verifies that the operand is not an error, and gets its core type and flag bitssem_validate_numeric : verifies that the operand is a numeric type recall these are the math unary operators, so the operand must be numeric sem_combine_types : creates the smallest type that holds two compatible types by combining with &quot;integer not null&quot; we ensure that the resulting type is at least as big as an integerif the argument is of type long or real then it will be the bigger type and the resulting type will be long or real as appropriatein short, bool is promoted to int, everything else stays the samesem_combine_types also combines the nullability and sensitivity appropriately a new sem_node of the combined type is created the type &quot;kind&quot; of the operand is preserved (e.g. the meters in real&lt;meters&gt;)any column alias or variable name is not preserved, the value is now anonymous These primitives are designed to combine well, for instance, consider sem_unary_integer_math static void sem_unary_integer_math(ast_node *ast, CSTR op) { sem_unary_math(ast, op); sem_reject_real(ast, op); }  The steps are: sem_unary_math : do the sequence we just discussedsem_reject_real : report/record an error if the result type is real otherwise do nothing Note that in all cases the op string simply gets pushed down to the place where the errors happen. Let's take a quick look at one of the sources of errors in the above. Here's the numeric validator: static bool_t sem_validate_numeric(ast_node *ast, sem_t core_type, CSTR op) { if (is_blob(core_type)) { report_error(ast-&gt;left, &quot;CQL0045: blob operand not allowed in&quot;, op); record_error(ast); return false; } if (is_object(core_type)) { report_error(ast-&gt;left, &quot;CQL0046: object operand not allowed in&quot;, op); record_error(ast); return false; } if (is_text(core_type)) { report_error(ast-&gt;left, &quot;CQL0047: string operand not allowed in&quot;, op); record_error(ast); return false; } return true; }  That function is pretty much dumb as rocks. The non-numeric types are blob, object, and text. There is a custom error for each type (it could have been shared but specific error messages seem to help users.) This code doesn't know its context, but all it needs is op to tell it what the numeric-only operator was and it can produce a nice error message. It leaves an error in the AST using record_error. Its caller can then simply returnif anything goes wrong. It's not hard to guess how sem_reject_real works: // Some math operators like &lt;&lt; &gt;&gt; &amp; | % only make sense on integers // This function does the extra checking to ensure they do not get real values // as arguments. It's a post-pass after the normal math checks. static void sem_reject_real(ast_node *ast, CSTR op) { if (!is_error(ast)) { sem_t core_type = core_type_of(ast-&gt;sem-&gt;sem_type); if (core_type == SEM_TYPE_REAL) { report_error(ast, &quot;CQL0001: operands must be an integer type, not real&quot;, op); record_error(ast); } } }  if the AST node isn't already an error, and the node is of type &quot;real&quot;, report an errorit assumes the type is already known to be numericthe pre-check for errors is to avoid double reporting; if something has already gone wrong, the core type will be SEM_TYPE_ERROR no new error recording is needed in that case, as obviously an error was already recorded "},{"title":"Binary Operators​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#binary-operators","content":"Binary Prep​ With the knowledge we have so far, this code pretty much speaks for itself, but we'll walk through it. // All the binary ops do the same preparation -- they evaluate the left and the // right expression, then they check those for errors. Then they need // the types of those expressions and the combined_flags of the result. This // does exactly that for its various callers. Returns true if all is well. static bool_t sem_binary_prep(ast_node *ast, sem_t *core_type_left, sem_t *core_type_right, sem_t *combined_flags) { EXTRACT_ANY_NOTNULL(left, ast-&gt;left); EXTRACT_ANY_NOTNULL(right, ast-&gt;right); // left op right sem_expr(left); sem_expr(right); if (is_error(left) || is_error(right)) { record_error(ast); *core_type_left = SEM_TYPE_ERROR; *core_type_right = SEM_TYPE_ERROR; *combined_flags = 0; return false; } *core_type_left = core_type_of(left-&gt;sem-&gt;sem_type); *core_type_right = core_type_of(right-&gt;sem-&gt;sem_type); *combined_flags = combine_flags(left-&gt;sem-&gt;sem_type, right-&gt;sem-&gt;sem_type); Invariant(is_unitary(*core_type_left)); Invariant(is_unitary(*core_type_right)); return true; }  sem_expr : used to recursively walk the left and right nodesis_error : checks if either side had errors, and, if so, simply propagates the errorextract the left and right core typescombine nullability and sensitivity flags And that's it! These are the standard prep steps for all binary operators. With this done, the caller has the core types of the left and right operands plus combined flags on a silver platter and one check is needed to detect if anything went wrong. Example: Is or Is Not​ This analyzer is the simplest of all the binaries // IS and IS NOT are special in that they return a not null boolean. static void sem_binary_is_or_is_not(ast_node *ast, CSTR op) { sem_t core_type_left, core_type_right, combined_flags; if (!sem_binary_prep(ast, &amp;core_type_left, &amp;core_type_right, &amp;combined_flags)) { return; } if (!sem_verify_compat(ast, core_type_left, core_type_right, op)) { return; } // the result of is or is not is always a bool and never null ast-&gt;sem = new_sem(SEM_TYPE_BOOL | SEM_TYPE_NOTNULL | sensitive_flag(combined_flags)); }  sem_binary_prep : checks for errors in the left or rightsem_verify_compat : ensures that left and right operands are type compatible (discussed later)the result is always of type bool not null If either step goes wrong the error will naturally propagate. Example: Binary Math​ This is the general worker for binary math operations, the most common operations like '+', '-', '*' and so forth. // For all math operations, we combine the types and yield the type that // holds both using the helper. If any text, that's an error. static void sem_binary_math(ast_node *ast, CSTR op) { sem_t core_type_left, core_type_right, combined_flags; if (!sem_binary_prep(ast, &amp;core_type_left, &amp;core_type_right, &amp;combined_flags)) { return; } if (error_any_object(ast, core_type_left, core_type_right, op)) { return; } if (error_any_blob_types(ast, core_type_left, core_type_right, op)) { return; } if (error_any_text_types(ast, core_type_left, core_type_right, op)) { return; } sem_t core_type = sem_combine_types(core_type_left, core_type_right); CSTR kind = sem_combine_kinds(ast-&gt;right, ast-&gt;left-&gt;sem-&gt;kind); if (is_error(ast-&gt;right)) { record_error(ast); return; } ast-&gt;sem = new_sem(core_type | combined_flags); ast-&gt;sem-&gt;kind = kind; }  Let's have a look at those steps: sem_binary_prep : checks for errors on the left or righterror_any_object : reports an error if the left or right is of type objecterror_any_blob_types : reports an error if the left or right is of type bloberror_any_text_types : reports an error if the left or right is of type textsem_combine_type : computes the combined type, the smallest numeric type that holds both left and right note the operands are now known to be numericthe three type error checkers give nice tight errors about the left or right operand sem_combine_kinds : tries to create a single type kind for both operands if their kind is incompatible, records an error on the right new_sem : creates a sem_node with the combined type, flags, and then the kind is set At this point it might help to look at a few more of the base validators -- they are rather unremarkable. Example Validator: error_any_object​ // If either of the types is an object, then produce an error on the ast. static bool_t error_any_object(ast_node *ast, sem_t core_type_left, sem_t core_type_right, CSTR op) { if (is_object(core_type_left)) { report_error(ast-&gt;left, &quot;CQL0002: left operand cannot be an object in&quot;, op); record_error(ast); return true; } if (is_object(core_type_right)) { report_error(ast-&gt;right, &quot;CQL0003: right operand cannot be an object in&quot;, op); record_error(ast); return true; } return false; }  is_object : checks a sem_type against SEM_TYPE_OBJECT if the left or right child is an object, an appropriate error is generated there is no strong convention for returning true if ok, or true if error; it's pretty ad hoc this doesn't seem to cause a lot of problems Example Validator: sem_combine_kinds​ // Here we check that type&lt;Foo&gt; only combines with type&lt;Foo&gt; or type. // If there is a current object type, then the next item must match // If there is no such type, then an object type that arrives becomes the required type // if they ever don't match, record an error static CSTR sem_combine_kinds_general(ast_node *ast, CSTR kleft, CSTR kright) { if (kright) { if (kleft) { if (strcmp(kleft, kright)) { CSTR errmsg = dup_printf(&quot;CQL0070: expressions of different kinds can't be mixed: '%s' vs. '%s'&quot;, kright, kleft); report_error(ast, errmsg, NULL); record_error(ast); } } return kright; } return kleft; } // helper to crack the ast nodes first and then call the normal comparisons static CSTR sem_combine_kinds(ast_node *ast, CSTR kright) { CSTR kleft = ast-&gt;sem-&gt;kind; return sem_combine_kinds_general(ast, kleft, kright); }  sem_combine_kinds : uses the worker sem_combine_kinds_general after extracting the kind from the left node usually you already have one kind and you want to know if another kind is compatible, hence this helper sem_combine_kinds_general : applies the general rules for &quot;kind&quot; strings: NULL + NULL =&gt; NULLNULL + x =&gt; xx + NULL =&gt; xx + x =&gt; xx + y =&gt; error (if x != y) this is one of the rare functions that creates a dynamic error message Example Validator : is_numeric_compat​ This helper is frequently called several times in the course of other semantic checks. This one produces no errors, that's up to the caller. Often there is a numeric path and a non-numeric path so this helper can't create the errors as it doesn't yet know if anything bad has happened. Most of the is_something functions are the same way. cql_noexport bool_t is_numeric_compat(sem_t sem_type) { sem_type = core_type_of(sem_type); return sem_type &gt;= SEM_TYPE_NULL &amp;&amp; sem_type &lt;= SEM_TYPE_REAL; }  is_numeric_compat operates by checking the core type for the numeric range. Note that NULL is compatible with numerics because expressions like NULL + 2have meaning in SQL. The type of that expression is nullable integer and the result is NULL. Example Validator : sem_combine_types​ // The second workhorse of semantic analysis, given two types that // are previously known to be compatible, it returns the smallest type // that holds both. If either is nullable, the result is nullable. // Note: in the few cases where that isn't true, the normal algorithm for // nullability result must be overridden (see coalesce, for instance). static sem_t sem_combine_types(sem_t sem_type_1, sem_t sem_type_2) { ... too much code ... summary below }  This beast is rather lengthy but unremarkable. It follows these rules: text is only compatible with textobject is only compatible with objectblob is only compatible with blobnumerics are only compatible with other numerics and NULL NULL promotes the other operand, whatever it is (might still be NULL)bool promotes to integer if neededinteger promotes to long integer if neededlong integer promotes to real if neededthe combined type is the smallest numeric type that holds left and right according to the above rules Some examples might be helpful: 1 + 2L -&gt; longfalse + 3.1 -&gt; real2L + 3.1 -&gt; realtrue + 2 -&gt; integer'x' + 1 -&gt; not compatible Note that sem_combine_types assumes the types have already been checked for compatibility and will use Contract to enforce this. You should be using other helpers like is_numeric_compat and friends to ensure the types agree before computing the combined type. A list of values that must be compatible with each other (e.g. in needle IN (haystack)) can be checked using sem_verify_compat repeatedly. Example Validator : sem_verify_assignment​ The sem_verify_assignment function is used any time there is something like a logical assignment going on. There are two important cases: SET x := y : an actual assignmentcall foo(x) : the expression x must be &quot;assignable&quot; to the formal variable for the argument of foo This is a lot like normal binary operator compatibility with one extra rule: the source expression must not be a bigger type than the target. e.g. you cannot assign a long to an integer, nor pass a long expression to a function that has an integer parameter. // This verifies that the types are compatible and that it's ok to assign // the expression to the variable. In practice that means: // * the variable type core type and kind must be compatible with the expression core type and kind // * the variable must be nullable if the expression is nullable // * the variable must be sensitive if the assignment is sensitive // * the variable type must be bigger than the expression type // Here ast is used only to give a place to put any errors. static bool_t sem_verify_assignment(ast_node *ast, sem_t sem_type_needed, sem_t sem_type_found, CSTR var_name) { if (!sem_verify_compat(ast, sem_type_needed, sem_type_found, var_name)) { return false; } if (!sem_verify_safeassign(ast, sem_type_needed, sem_type_found, var_name)) { return false; } if (is_nullable(sem_type_found) &amp;&amp; is_not_nullable(sem_type_needed)) { report_error(ast, &quot;CQL0013: cannot assign/copy possibly null expression to not null target&quot;, var_name); return false; } if (sensitive_flag(sem_type_found) &amp;&amp; !sensitive_flag(sem_type_needed)) { report_error(ast, &quot;CQL0014: cannot assign/copy sensitive expression to non-sensitive target&quot;, var_name); return false; } return true; }  sem_verify_compat : checks for standard type compatibility between the left and the rightsem_verify_safeassign : checks that if the types are different the right operand is the smaller of the twonullability checks ensure you aren't trying to assign a nullable value to a not null variablesensitivity checks ensure you aren't trying to assign a sensitive value to a not sensitive variable "},{"title":"Simple Statement Validation​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#simple-statement-validation","content":"With the expression building blocks, most of the usual kind of language statements become quite simple to check for correctness. It's probably easiest to illustrate this with an example. Let's look at validation for the WHILE statement: // While semantic analysis is super simple. // * the condition must be numeric // * the statement list must be error-free // * loop_depth is increased allowing the use of interior leave/continue static void sem_while_stmt(ast_node *ast) { Contract(is_ast_while_stmt(ast)); EXTRACT_ANY_NOTNULL(expr, ast-&gt;left); EXTRACT(stmt_list, ast-&gt;right); // WHILE [expr] BEGIN [stmt_list] END sem_numeric_expr(expr, ast, &quot;WHILE&quot;, SEM_EXPR_CONTEXT_NONE); if (is_error(expr)) { record_error(ast); return; } if (stmt_list) { loop_depth++; sem_stmt_list(stmt_list); loop_depth--; if (is_error(stmt_list)) { record_error(ast); return; } } record_ok(ast); }  EXTRACT* : pulls out the tree parts we needsem_numeric_expr : verifies the loop expression is numericsem_stmt_list : recursively validates the body of the loop Note: the while expression is one of the loop constructs which means that LEAVE and CONTINUE are legal inside it. The loop_depth global tracks the fact that we are in a loop so that analysis for LEAVE and CONTINUE can report errors if we are not. It's not hard to imagine that sem_stmt_list will basically walk the AST, pulling out statements and dispatching them using the STMT_INIT tables previously discussed. You might land right back in sem_while_stmt for a nested WHILE -- it's turtles all the way down. If SEM_EXPR_CONTEXT_NONE is a mystery, don't worry, it's covered in the next section. "},{"title":"Expression Contexts​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#expression-contexts","content":"It turns out that in the SQL language some expression types are only valid in some parts of a SQL statement (e.g. aggregate functions can't appear in a LIMIT clause) and so there is always a context for any numeric expression. When a new root expression is being evaluated, it sets the expression context per the caller's specification. The expression contexts are as follows: #define SEM_EXPR_CONTEXT_NONE 0x0001 #define SEM_EXPR_CONTEXT_SELECT_LIST 0x0002 #define SEM_EXPR_CONTEXT_WHERE 0x0004 #define SEM_EXPR_CONTEXT_ON 0x0008 #define SEM_EXPR_CONTEXT_HAVING 0x0010 #define SEM_EXPR_CONTEXT_ORDER_BY 0x0020 #define SEM_EXPR_CONTEXT_GROUP_BY 0x0040 #define SEM_EXPR_CONTEXT_LIMIT 0x0080 #define SEM_EXPR_CONTEXT_OFFSET 0x0100 #define SEM_EXPR_CONTEXT_TABLE_FUNC 0x0200 #define SEM_EXPR_CONTEXT_WINDOW 0x0400 #define SEM_EXPR_CONTEXT_WINDOW_FILTER 0x0800 #define SEM_EXPR_CONTEXT_CONSTRAINT 0x1000  The idea here is simple: when calling a root expression, the analyzer provides the context value that has the bit that corresponds to the current context. For instance, the expression being validated in is the WHERE clause -- the code will provide SEM_EXPR_CONTEXT_WHERE. The inner validators check this context, in particular anything that is only available in some contexts has a bit-mask of that is the union of the context bits where it can be used. The validator can check those possibilities against the current context with one bitwise &quot;and&quot; operation. A zero result indicates that the operation is not valid in the current context. This bitwise &quot;and&quot; is performed by one of these two helper macros which makes the usage a little clearer: #define CURRENT_EXPR_CONTEXT_IS(x) (!!(current_expr_context &amp; (x))) #define CURRENT_EXPR_CONTEXT_IS_NOT(x) (!(current_expr_context &amp; (x)))  Expression Context Example : Concat​ The concatenation operator || is challenging to successfully emulate because it does many different kinds of numeric to string conversions automatically. Rather than perennially getting this wrong, we simply do not support this operator in a context where SQLite isn't going to be doing the concatenation. So typically users use &quot;printf&quot; instead to get formatting done outside of a SQL context. The check for invalid use of || is very simple and it happens, of course, in sem_concat.  if (CURRENT_EXPR_CONTEXT_IS(SEM_EXPR_CONTEXT_NONE)) { report_error(ast, &quot;CQL0241: CONCAT may only appear in the context of a SQL statement&quot;, NULL); record_error(ast); return; }  Expression Context Example : IN​ A slightly more complex example happens processing the IN operator. This operator has two forms: the form with an expression list, which can be used anywhere, and the form with a SELECT statement. The latter form can only appear in some sections of SQL, and not at all in loose expressions. For instance, that form may not appear in the LIMIT or OFFSET sections of a SQLite statement. We use this construct to do the validation:  uint32_t valid = SEM_EXPR_CONTEXT_SELECT_LIST |SEM_EXPR_CONTEXT_WHERE |SEM_EXPR_CONTEXT_ON |SEM_EXPR_CONTEXT_HAVING |SEM_EXPR_CONTEXT_TABLE_FUNC; if (CURRENT_EXPR_CONTEXT_IS_NOT(valid)) { report_error( ast, &quot;CQL0078: [not] in (select ...) is only allowed inside &quot; &quot;of select lists, where, on, and having clauses&quot;, NULL); record_error(ast); return; }  If the reader is interested in a simple learning exercise, run down the purpose of SEM_EXPR_CONTEXT_TABLE_FUNC -- it's simple, but important, and it only has one use case so it's easy to find. "},{"title":"Name Resolution​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#name-resolution","content":"We've gotten pretty far without talking about the elephant in the room: name resolution. Like SQL, many statements in CQL have names in positions where the type of the name is completely unambiguous. For instance nobody could be confused what sort of symbol Foo is in DROP INDEX Foo. This type, with a clear name category, is the easiest name resolutions, and there are a lot in this form. Let's do an example. Example: Index Name Resolution​ // This is the basic checking for the drop index statement // * the index must exist (have been declared) in some version // * it could be deleted now, that's ok, but the name has to be valid static void sem_drop_index_stmt(ast_node *ast) { Contract(is_ast_drop_index_stmt(ast)); EXTRACT_ANY_NOTNULL(name_ast, ast-&gt;right); EXTRACT_STRING(name, name_ast); ast_node *index_ast = find_usable_index(name, name_ast, &quot;CQL0112: index in drop statement was not declared&quot;); if (!index_ast) { record_error(ast); return; } record_ok(ast); }  Well, this is interesting. But what's going on with find_usable_index? What is usable? Why aren't we just looking up the index name in some name table? Let's have a look at the details of find_usable_index. // returns the node only if it exists and is not restricted by the schema region. static ast_node *find_usable_index(CSTR name, ast_node *err_target, CSTR msg) { ast_node *index_ast = find_index(name); if (!index_ast) { report_error(err_target, msg, name); return NULL; } if (!sem_validate_object_ast_in_current_region(name, index_ast, err_target, msg)) { return NULL; } return index_ast; }  We haven't discussed schema regions yet but what you need to know about them for now is this: any object can be in a region.a region may depend on other regions If an object is in a region, then it may only use schema parts that are in the same region, or the region's dependencies (transitively). The point of this is that you might have a rather large schema and you probably don't want any piece of code to use just any piece of schema. You can use regions to ensure that the code for feature &quot;X&quot; doesn't try to use schema designed exclusively for feature &quot;Y&quot;. That &quot;X&quot; code probably has no business even knowing of the existence of &quot;Y&quot; schema. So now usable simply means this: find_index can find the name in the symbol table for indicesthe found index is accessible in the current region If we had used an example that was looking up a table name, the same region considerations would apply, however, additionally tables can be deprecated with @delete so there would be additional checks to make sure we're talking about a live table and not a table's tombstone. In short, these simple cases just require looking up the entity and verifying that it's accessible in the current context. Flexible Name Resolution​ The &quot;hard case&quot; for name resolution is where the name is occurring in an expression. Such a name can refer to all manner of things. It could be a global variable, a local variable, an argument, a table column, a field in a cursor, and others. The general name resolver goes through several phases looking for the name. Each phase can either report an affirmative success or error (in which case the search stops), or it may simply report that the name was not found but the search should continue. We can demystify this a bit by looking at the most common way to get name resolution done. // Resolves a (potentially qualified) identifier, writing semantic information // into `ast` if successful, or reporting and recording an error for `ast` if // not. static void sem_resolve_id(ast_node *ast, CSTR name, CSTR scope) { Contract(is_id(ast) || is_ast_dot(ast)); Contract(name); // We have no use for `type` and simply throw it away. sem_t *type = NULL; sem_resolve_id_with_type(ast, name, scope, &amp;type); }  The name resolver works on either a vanilla name (e.g. x) or a scoped name (e.g. T1.x). The name and scope are provided. The ast parameter is used only as a place to report errors; there is no further cracking of the AST needed to resolve the name. As you can see sem_resolve_id just calls the more general function sem_resolve_id_with_type and is used in the most common case where you don't need to be able to mutate the semantic type info for the identifier. That's the 99% case. So let's move on to the &quot;real&quot; resolver. // This function is responsible for resolving both unqualified identifiers (ids) // and qualified identifiers (dots). It performs the following two roles: // // - If an optional `ast` is provided, it works the same way most semantic // analysis functions work: semantic information will be written into the // ast, errors will be reported to the user, and errors will be recorded in // the AST. // // - `*type_ptr` will be set to mutable type (`sem_t *`) in the current // environment if the identifier successfully resolves to a type. (There are, // unfortunately, a few exceptions in which a type will be successfully // resolved and yet `*type_ptr` will not be set. These include when a cursor is // in an expression position, when the expression is `rowid` (or similar), and // when the id resolves to an enum case. The reason no mutable type is // returned in these cases is that a new type is allocated as part of semantic // analysis, and there exists no single, stable type in the environment to // which a pointer could be returned. This is a limitation of this function, // albeit one that's currently not problematic.) // // Resolution is attempted in the order that the `sem_try_resolve_*` functions // appear in the `resolver` array. Each takes the same arguments: An (optional) // AST, a mandatory name, an optional scope, and mandatory type pointer. If the // identifier provided to one of these resolvers is resolved successfully, *or* // if the correct resolver was found but there was an error in the program, // `SEM_RESOLVE_STOP` is returned and resolution is complete, successful or not. // If a resolver is tried and it determines that it is not the correct resolver // for the identifier in question, `SEM_RESOLVE_CONTINUE` is returned and the // next resolver is tried. // // This function should not be called directly. If one is interested in // performing semantic analysis, call `sem_resolve_id` (or, if within an // expression, `sem_resolve_id_expr`.) Alternatively, if one wants to get a // mutable type from the environment, call `find_mutable_type`. static void sem_resolve_id_with_type(ast_node *ast, CSTR name, CSTR scope, sem_t **type_ptr) { Contract(name); Contract(type_ptr); *type_ptr = NULL; sem_resolve (*resolver[])(ast_node *ast, CSTR, CSTR, sem_t **) = { sem_try_resolve_arguments, sem_try_resolve_column, sem_try_resolve_rowid, sem_try_resolve_cursor_as_expression, sem_try_resolve_variable, sem_try_resolve_enum, sem_try_resolve_cursor_field, sem_try_resolve_arg_bundle, }; for (uint32_t i = 0; i &lt; sizeof(resolver) / sizeof(void *); i++) { if (resolver[i](ast, name, scope, type_ptr) == SEM_RESOLVE_STOP) { return; } } report_resolve_error(ast, &quot;CQL0069: name not found&quot;, name); record_resolve_error(ast); }  This function is well described in its own comments. We can easily see the &quot;mini-resolvers&quot; which attempt to find the name in order: sem_try_resolve_arguments : an argument in the argument listsem_try_resolve_column : a column name (possibly scoped)sem_try_resolve_rowid : the virtual rowid column (possibly scoped)sem_try_resolve_cursor_as_expression : use of a cursor as a boolean -- the bool is true if the cursor has datasem_try_resolve_variable : local or global variablessem_try_resolve_enum : the constant value of an enum (must be scoped)sem_try_resolve_cursor_field : a field in a cursor (must be scoped)sem_try_resolve_arg_bundle : a field in an argument bundle (must be scoped) These all use this enum to communicate progress: // All `sem_try_resolve_*` functions return either `SEM_RESOLVE_CONTINUE` to // indicate that another resolver should be tried, or `SEM_RESOLVE_STOP` to // indicate that the correct resolver was found. Continuing implies that no // failure has (yet) occurred, but stopping implies neither success nor failure. typedef enum { SEM_RESOLVE_CONTINUE = 0, SEM_RESOLVE_STOP = 1 } sem_resolve;  Each of these mini-resolvers will have a series of rules, for example sem_try_resolve_cursor_field is going to have to do something like this: if there is no scope, it can't be a cursor field, return CONTINUEif the scope is not the name of a cursor, return CONTINUEif the name is a field in the cursor, return STOP with successelse, report that the name is not a valid member of the cursor, and return STOP with an error All the mini-resolvers are similarly structured, generically: if it's not my case, return CONTINUEif it is my case return STOP (maybe with an error) Some of the mini-resolvers have quite a few steps, but any one mini-resolver is only about a screenful of code and it does one job. "},{"title":"Flow Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#flow-analysis","content":"CQL implements a basic form of control flow analysis in &quot;flow.c&quot;. The header &quot;flow.h&quot; exposes a small set of primitives used by &quot;sem.c&quot; during semantic analysis. Flow analysis in CQL involves two important concepts: flow contexts andimprovements. These are rather entangled concepts — one is useless without the other — and so the approach to describing them here will alternate between giving a bit of background on one and then the other, with a greater level of detail about the specific types of improvements being supplied later on. A flow context is used, in essence, to create a boundary around a portion of a user's program. At the moment, there are four types of contexts. The first type of context is called, rather boringly, a normal context. Normal contexts are used for portions of a user's code that may be entered conditionally. A good example of this is in SELECT expressions: When a WHEREclause is present, the expression list is only evaluated when the WHERE clause is true. If we look at sem_select_expr_list_con, we can get an idea of how this works in terms of flow contexts: static void sem_select_expr_list_con(ast_node *ast) { ... // Analyze the FROM portion (if it exists). sem_select_from(select_from_etc); error = is_error(select_from_etc); // Push a flow context to contain improvements made via the WHERE clause that // will be in effect for the SELECT expression list. FLOW_PUSH_CONTEXT_NORMAL(); if (!error) { ... sem_sensitive = sem_select_where_etc(select_from_etc); ... sem_set_improvements_for_true_condition(where_expr); ... } ... if (!error) { ... sem_select_expr_list(select_expr_list); ... } ... FLOW_POP_CONTEXT_NORMAL(); ... }  While very much simplified above, it can be seen that the steps are essentially as follows: Analyze the FROM clause.Push a new normal context.Analyze the WHERE clause.Set improvements given the WHERE clause (ultimately by callingflow_set_flag_for_type); we'll come back to this part shortly.Analyze the expression list with the improvements from the WHERE in effect.Pop the context, un-setting the improvements from the WHERE. This, of course, only begins to make sense once one understands what we mean by improvements. CQL, at the moment, supports two forms of improvements: nullability improvements and initialization improvements. Both of these will be discussed in more detail later, but the basic idea is that an improvement upgrades the type of some value within a particular flow context. For example, in the expression SELECT x + x FROM t WHERE x IS NOT NULL, we can reason that x + x can safely be given a nonnull type because of the WHERE clause. This is exactly what we do insem_select_expr_list_con: We make a context to hold the improvements that may come from the WHERE, analyze the WHERE, set the appropriate improvements given the WHERE, analyze the expression list, and then pop the context to unset the improvements (as they must not affect any enclosing expressions). In addition to normal contexts, there are also branch contexts and branch group contexts. These two context types are designed to work together for handling IF, CASE, IIF, SWITCH, et cetera. Like normal contexts, branch contexts assume that they are entered when some condition is true. The difference is that branch contexts lie within a branch group context, and branch groups know that at most one branch of a given set of branches will be entered. A great example of this can be found insem_if_stmt: static void sem_if_stmt(ast_node *ast) { ... // Each branch gets its own flow context in `sem_cond_action` where its // condition is known to be true. We also create one more context for the // entire set of branches. In addition to grouping the branches together, this // outer context holds all of the negative improvements that result from the // knowledge that, if a given branch's statements are being evaluated, all // previous branches' conditions must have been false. FLOW_PUSH_CONTEXT_BRANCH_GROUP(); // IF [cond_action] sem_cond_action(cond_action); ... if (elseif) { sem_elseif_list(elseif); ... } ... if (elsenode) { // ELSE [stmt_list] flow_set_context_branch_group_covers_all_cases(true); EXTRACT(stmt_list, elsenode-&gt;left); if (stmt_list) { FLOW_PUSH_CONTEXT_BRANCH(); sem_stmt_list_in_current_flow_context(stmt_list); FLOW_POP_CONTEXT_BRANCH(); ... } else { flow_context_branch_group_add_empty_branch(); } record_ok(elsenode); } ... FLOW_POP_CONTEXT_BRANCH_GROUP(); ... }  It's instructive to look at sem_cond_action as well: static void sem_cond_action(ast_node *ast) { ... // [expr] THEN stmt_list sem_expr(expr); ... if (stmt_list) { FLOW_PUSH_CONTEXT_BRANCH(); // Add improvements for `stmt_list` where `expr` must be true. sem_set_improvements_for_true_condition(expr); sem_stmt_list_in_current_flow_context(stmt_list); FLOW_POP_CONTEXT_BRANCH(); ... } else { flow_context_branch_group_add_empty_branch(); } // If a later branch will be taken, `expr` must be false. Add its negative // improvements to the context created in `sem_if_stmt` so that all later // branches will be improved by the OR-linked spine of IS NULL checks in // `expr`. sem_set_improvements_for_false_condition(expr); ... }  Putting all of this together, we can see that the basic steps for analyzing anIF statement are as follows: Push a new branch group context to hold all of the branch contexts that are to come.Analyze the condition in the IF condition THEN portion of the statement.Push a new branch context to hold the nullability improvements from the condition (e.g., in IF x IS NOT NULL THEN, we can improve x to have a nonnull type in the statement list after the THEN).Set the improvements.Anaylze the statement list after the THEN.Pop the branch context.Set the negative improvements resulting from the knowledge that conditionmust have been false if the previous branch wasn't entered (e.g., in IF y IS NULL THEN, we know that y must be nonnull from just after the end of the branch until the end of the current branch group).Repeat for the ELSE IF and ELSE branches (if any).Pop the branch group context. What makes all of this work are the following: When a branch context is popped, it resets all improvements such that they become exactly what they were before the branch was analyzed. This is done to reflect the fact that, because at most one branch will be entered, neither adding improvements (via flow_set_flag_for_type) nor removing existing improvements (via flow_unset_flag_for_type) in a branch should affect any of the other branches in the group. When a branch group context is popped, it merges the effects of all of its branches. This is a key step that allows CQL to retain an improvement after a branch group is popped whenever the same improvement is made within every one of its branches and when the branches given cover all possible cases (which is indicated by the call to flow_set_context_branch_group_covers_all_casesin the code above). The final type of context is called a jump context. Jump contexts are a maximally pessimistic form of context that assume every improvement that might be unset within them will be unset and that every improvement that might be set within them will not be set. Jump contexts are used to make semantic analysis safe in the possible presence of control flow statements like CONTINUE,LEAVE, and THROW, and so jump contexts are used for the analysis of statements like LOOP, WHILE, and TRY. Take the following line-numbered code as an example: 001 DECLARE x TEXT; 002 SET x := &quot;foo&quot;; 003 WHILE some_condition 004 BEGIN 005 IF another_condition THEN 006 SET x := NULL; 007 IF yet_another_condition THEN 008 LEAVE; 009 END IF; 010 SET x := &quot;bar&quot;; 011 ELSE 012 -- do nothing 013 END IF; 014 END; 015 CALL requires_text_notnull(x);  Here, even though the outer IF makes no change overall to the nullability improvement to x from line 2 -- it unsets it on line 6 and then re-sets it on line 10 and the ELSE does nothing—there is no guarantee that line 10 will ever be evaluated because we may jump straight from line 8 to line 15. As a result, it is necessary that x be un-improved after the WHILE loop; a normal context would not accomplish this, but a jump context does. See the comments within_flow_push_context_branch for additional discussion. While jump contexts are necessary for the safety of improvements in the presence of loops, they are not sufficient: It's actually necessary to analyze loopstwice. This is because execution of a loop might repeat, and so a statement that results in the unsetting of an improvement later in a loop must affect improvements earlier in that loop. For example: DECLARE x INT; SET x := 1; WHILE some_condition BEGIN -- okay on the first analysis pass, but not the second CALL requires_int_notnull(x); -- must negatively affect the call on the line above SET x := NULL; END;  Semantic analysis keeps track of whether or not it is currently reanalyzing the statement list of a loop via the current_loop_analysis_state variable: // The analysis of loops like LOOP and WHILE is done in two passes. First, we // analyze the loop to conservatively figure out every improvement that the loop // could possibly unset. After that, we reanalyze it with said improvements // unset to ensure that everything is safe. See `sem_stmt_list_within_loop` for // more information on why this is necessary. typedef enum { LOOP_ANALYSIS_STATE_NONE, LOOP_ANALYSIS_STATE_ANALYZE, LOOP_ANALYSIS_STATE_REANALYZE } loop_analysis_state; ... // Keeps tracks of the current loop analysis state. If this is equal to // `LOOP_ANALYSIS_STATE_ANALYZE`, we are analyzing with a non-final set of // improvements. This is useful for two reasons: // // 1. Procedures that perform rewrites based on improvements (e.g., // `sem_resolve_id_expr`) can use this to verify whether a rewrite is safe to // perform (`LOOP_ANALYSIS_STATE_NONE` or `LOOP_ANALYSIS_STATE_REANALYZE`) or // whether they should wait because they do not yet have definitive // information (`LOOP_ANALYSIS_STATE_ANALYZE`). // // 2. Analyses that would otherwise fail if called during reanalysis (e.g., // `sem_verify_legal_variable_name`) can use this to check whether the // current state is `LOOP_ANALYSIS_STATE_REANALYZE` and adjust their // behaviors accordingly. static loop_analysis_state current_loop_analysis_state = LOOP_ANALYSIS_STATE_NONE;  As indicated in the first comment above, the comments withinsem_stmt_list_within_loop go into further detail. At this point, we've only scratched the surface of control flow analysis in CQL. Fortunately, the files &quot;flow.h&quot; and &quot;flow.c&quot; are heavily commented and can be studied to deepen one's understanding. "},{"title":"Nullability Improvements​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#nullability-improvements","content":"Via a form of occurrence typing (also known as flow typing), CQL has the ability to determine that, due to a prior conditional check, a nullable variable or cursor field cannot be null within a particular context, and CQL will improve its type in that context. Unlike most forms of semantic analysis performed by CQL, the analysis for nullability improvements, as is the case for all types of improvements, makes heavy use of the find_mutable_type function: // Returns the *mutable* type (`sem_t *`) for a given (potentially qualified) // identifier if one exists in the environment. See the documentation for // `sem_resolve_id_with_type` for limitations. static sem_t *find_mutable_type(CSTR name, CSTR scope);  This function allows us to look up the type of the original binding referred to by a particular name/scope pair. In essence, it provides access to the current type environment for whichever part of the program we are analyzing. It also allows us to mutate that environment by virtue of the fact that it returns a pointer to the type of the binding, not merely the type itself. By using find_mutable_type to get a type pointer and toggling theSEM_TYPE_INFERRED_NOTNULL flag via flow_set_flag_for_type andflow_unset_flag_for_type, the procedures sem_set_notnull_improved andsem_unset_notnull_improved are able to record that a nullable identifier or cursor field is either temporarily nonnull or no longer nonnull respectively: // Enables a nonnull improvement, if possible. static void sem_set_notnull_improved(CSTR name, CSTR scope); // This needs to be called for everything that is no longer safe to consider NOT // NULL due to a mutation. It is fine to call this for something not currently // subject to improvement, but it must only be called with a name/scope pair // referring to something that has a mutable type (e.g., it must not be an unbound // variable, a cursor used as an expression, an enum case, et cetera). static void sem_unset_notnull_improved(CSTR name, CSTR scope);  Similarly, sem_is_notnull_improved uses find_mutable_type to check whether or not something is currently improved: // Returns true if currently improved to be nonnull, else false. static bool_t sem_is_notnull_improved(CSTR name, CSTR scope);  Why does nullability inference use this approach? The reason is that the alternative would be maintaining some sort of set of currently improved identifiers and cursor fields and checking it whenever resolving an identifier or cursor field. The problem would be that merely knowing that some identifier &quot;x&quot; is improved would not be sufficient, however: We'd have to know which &quot;x&quot;. Is it the local variable &quot;x&quot;? Is it the column &quot;x&quot; of the table from which we're currently selecting? In essence, correctly maintaining an independent set of all currently active improvements would involve re-implementing all of the scoping rules of the language. By using find_mutable_type, we can simply piggyback on the existing name resolution logic and avoid all of these issues. A nullability improvement is always created within a particular flow context. When an improvement is added via sem_set_notnull_improved, a record of that improvement is recorded in the current context. When that context ends, that same record is used to remove the improvement. It is also the case thatsem_unset_notnull_improved may be used to remove an improvement before a context has ended due to a SET, FETCH, or call to a procedure or function with an OUT argument resulting in the improvement no longer being safe. Improvements can be introduced into the current context viasem_set_notnull_improved directly (when a variable is SET to a value of a nonnull type), but more commonly they are introduced via one of the following two functions: // Given a conditional expression `ast` possibly containing AND-linked // subexpressions, set all of the applicable nullability and has-row // improvements within the current flow context. Generally speaking, calls to // this function should be bounded by a new flow context corresponding to the // portion of the program for which the condition `ast` must be be true. static void sem_set_improvements_for_true_condition(ast_node *expr); // Improvements for known-false conditions are dual to improvements for // known-true conditions. // // For nullability, known-false conditions improve ids and dots verified to be // NULL via `IS NULL` along the outermost spine of `OR` expressions, whereas // known-true conditions improve ids and dots verified to be nonnull via `IS NOT // NULL` along the outermost spine of `AND` expressions. For example, the // following two statements introduce the same improvements: // // IF a IS NOT NULL AND b IS NOT NULL THEN // -- `a` and `b` are improved here because we know the condition is true // END IF; // // IF a IS NULL OR b IS NULL RETURN; // -- `a` and `b` are improved here because we know the condition is false // -- since we must not have returned if we got this far // // ... static void sem_set_improvements_for_false_condition(ast_node *ast);  These functions introduce improvements by gathering up all of the IS NOT NULLchecks (in the true case) or IS NULL checks (in the false case) and introducing improvements appropriately. The true version is used when we enter a context that will only be evaluated at runtime when some particular condition is true; the false version, conversely, is used when we enter a context that will only be evaluated at runtime when some particular condition is false: IF some_condition THEN -- &quot;true&quot; improvements from `some_condition` are in -- effect here ELSE IF another_condition THEN -- &quot;false&quot; improvements from `some_condition` and true -- improvements from `another_condition` are in effect -- here ELSE -- &quot;false&quot; improvements from both `some_condition` and -- `another_condition` are in effect here END IF;  Global variables in CQL require special treatment when it comes to nullability improvements. This is because any procedure call could potentially mutate any number of global variables, and so all currently improved globals must be un-improved at every such call. The following list keeps track of which global variables are currently improved: typedef struct global_notnull_improvement_item { sem_t *type; struct global_notnull_improvement_item *next; } global_notnull_improvement_item; ... // Keeps track of all global variables that may currently be improved to be NOT // NULL. We need this because we must un-improve all such variables after every // procedure call (because we don't do interprocedural analysis and cannot know // which globals may have been set to NULL). static global_notnull_improvement_item *global_notnull_improvements;  The fact that we don't do interprocedural analysis (as the comment above indicates) is not a deficiency. Programmers should be able to reason locally about nullability improvements, and an analysis that depended upon the details of how other procedures were implemented would make that impossible. So far, we have talked a lot about how improvements are set and unset, but we haven't talked about how the improvement actually happens in terms of code generation. Since CQL represents values of nullable and nonnull types differently (at least in the case of non-reference types), we cannot simply treat a value of a nullable type as though it were of a nonnull type: We need to actually change its representation. The way this works is that, whenever we resolve a name/scope pair viasem_resolve_id_expr, we check whether the pair is currently improved viasem_is_notnull_improved. If it is, we call rewrite_nullable_to_notnull to wrap the id or dot we're resolving with a call to the functioncql_inferred_notnull (for which we generate code incg_func_cql_inferred_notnull): // Wraps an id or dot in a call to cql_inferred_notnull. cql_noexport void rewrite_nullable_to_unsafe_notnull(ast_node *_Nonnull ast); // The `cql_inferred_notnull` function is not used by the programmer directly, // but rather inserted via a rewrite during semantic analysis to coerce a value // of a nullable type to be nonnull. The reason for this approach, as opposed to // just changing the type directly, is that there are also representational // differences between values of nullable and nonnull types; some conversion is // required. static void cg_func_cql_inferred_notnull(ast_node *call_ast, charbuf *is_null, charbuf *value);  As the comment for cg_func_cql_inferred_notnull indicates, programmers do not use cql_inferred_notnull directly: It is only inserted as a product of the above-mentioned rewrite. In fact, we explicitly disallow its use by programmers in the parser: // We insert calls to `cql_inferred_notnull` as part of a rewrite so we expect // to see it during semantic analysis, but it cannot be allowed to appear in a // program. It would be unsafe if it could: It coerces a value from a nullable // type to a nonnull type without any runtime check. #define YY_ERROR_ON_CQL_INFERRED_NOTNULL(x) \\ EXTRACT_STRING(proc_name, x); \\ if (!strcmp(proc_name, &quot;cql_inferred_notnull&quot;)) { \\ yyerror(&quot;Call to internal function is not allowed 'cql_inferred_notnull'&quot;); \\ }  One subtle aspect of the rewrite is that the rewrite itself performs analysis to validate the product of the rewrite (as do other many other rewrites). To avoid going into a loop of rewriting, analyzing the result (which ultimately happens in sem_special_func_cql_inferred_notnull), rewriting again because the result contains a name that is improved, et cetera, we keep track of whether or not we're currently analyzing a subexpression under a call to cql_inferred_notnulland avoid re-rewriting appropriately: // This is true if we are analyzing a call to `cql_inferred_notnull`. This can // happen for three reasons: // // * We just did a rewrite that produced a `cql_inferred_notnull` call and now // we're computing its type. // * We're analyzing an expression that was already analyzed (e.g., in a CTE). // * We're analyzing the output of a previous CQL run within which calls to // `cql_inferrred_notnull` may occur. // // Regardless of the cause, if `is_analyzing_notnull_rewrite` is true, we do not // want to rewrite again. static bool_t is_analyzing_notnull_rewrite; static void sem_special_func_cql_inferred_notnull(ast_node *ast, uint32_t arg_count, bool_t *is_aggregate) { ... // Since we're checking a call to `cql_inferred_notnull`, its arguments have // already been rewritten and we don't want to do it again. Setting // `is_analyzing_notnull_rewrite` prevents that. is_analyzing_notnull_rewrite = true; sem_arg_list(arg_list, IS_NOT_COUNT); is_analyzing_notnull_rewrite = false; ... } // Like `sem_resolve_id`, but specific to expression contexts (where nullability // improvements are applicable). static void sem_resolve_id_expr(ast_node *ast, CSTR name, CSTR scope) { ... if (is_analyzing_notnull_rewrite) { // If we're analyzing the product of a rewrite and we're already inside of a // call to `cql_inferred_notnull`, do not expand again. // forever. return; } ... }  At this point, you should have a decent understanding of how nullability improvements function, both in terms of semantic analysis and in terms of code generation. The implementation is heavily commented, so reading the code and searching for calls to the core functions listed below should be sufficient to fill in any gaps: bool_t sem_is_notnull_improved(CSTR name, CSTR scope); void sem_set_notnull_improved(CSTR name, CSTR scope); void sem_unset_notnull_improved(CSTR name, CSTR scope); void sem_unset_global_notnull_improvements(); void sem_set_improvements_for_true_condition(ast_node *ast); void sem_set_improvements_for_false_condition(ast_node *ast); void sem_special_func_cql_inferred_notnull(ast_node *ast, uint32_t arg_count, bool_t *is_aggregate) void rewrite_nullable_to_unsafe_notnull(ast_node *_Nonnull ast);  "},{"title":"Initialization Improvements​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#initialization-improvements","content":"Compared to nullability improvements, initialization improvements are relatively simple. The idea behind initialization improvements is that, if one declares a variable of a reference type (BLOB, OBJECT, or TEXT) that is also NOT NULL, it is not safe to use the variable until it has been given a value. For example: DECLARE x TEXT NOT NULL; IF some_condition THEN SET x := some_text_notnull_value; -- `x` is safe to use here ELSE -- `x` is NOT safe to use here (it might be uninitialized) END IF; -- `x` is NOT safe to use here either (it might be uninitialized)  As with nullability improvements, initialization improvements rely heavily on flow contexts. The function sem_set_initialization_improved, similarly tosem_set_notnull_improved for nullability, is used to enable an initialization improvement. (There is nothing analogous to sem_unset_notnull_improved for initialization because nothing can ever be uninitialized once it has been given a value.) Unlike nullability improvements, initialization improvements use two flags:SEM_TYPE_INIT_REQUIRED and SEM_TYPE_INIT_COMPLETE. Rather than assuming everything is uninitalized by default and requiring the presence of someSEM_TYPE_INITIALIZED flag before anything can be used, we explicitly tag things that are not initialized but need to be with SEM_TYPE_INIT_REQUIRED and later tag them with SEM_TYPE_INIT_COMPLETE once they've been initialized. Doing it in this way has two benefits: It reduces the amount of noise in the AST output significantly: Code likeLET x := 10; can remain {let_stmt}: x: integer notnull variable in the AST without the need of the extra noise of some initialized flag. More importantly, it means we only have to deal with initialization in a tiny portion of &quot;sem.c&quot;. For example, we must handle it in sem_declare_vars_typeto add the SEM_TYPE_INIT_REQUIRED flag and in sem_assign to addSEM_TYPE_INIT_COMPLETE, but sem_let_stmt can remain blissfully ignorant of initialization altogether. There are only three places in which a variable may be initialized: sem_assign(as mentioned), sem_fetch_stmt (for the FETCH...INTO form), andsem_arg_for_out_param (as passing a variable to a procedure requiring an OUTargument of a NOT NULL type can initialize it). Regarding sem_arg_for_out_param, we can only set initialization improvements when a variable is passed as an OUT argument because we require that all procedures initialize all of their OUT parameters of a nonnull reference type. This is handled in two places: In sem_param, we set the SEM_TYPE_INIT_REQUIRED flag whenparam_should_require_initialization is true. In sem_validate_current_proc_params_are_initialized, which is called both after analyzing the statement list of a procedure and for each return statement within the procedure, we ensure that SEM_TYPE_INIT_COMPLETE is present on all parameters that have SEM_TYPE_INIT_REQUIRED. There is only one wrinkle in all of this: the cql:try_is_proc_body attribute. If cql:try_is_proc_body is present on a TRY statement, we callsem_validate_current_proc_params_are_initialized at the end of the TRY andnot at the end of the procedure. The rationale for this is explained thoroughly in the comments forsem_find_ast_misc_attr_trycatch_is_proc_body_callback. That's all there is to it: &quot;flow.c&quot; does most of the hard work for us. "},{"title":"Structure types and the notion of Shapes​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#structure-types-and-the-notion-of-shapes","content":"Earlier we discussed SEM_TYPE_STRUCT briefly. Recall the basic notion of the structure type: // for tables and views and the result of a select typedef struct sem_struct { CSTR struct_name; // struct name uint32_t count; // count of fields CSTR *names; // field names CSTR *kinds; // the &quot;kind&quot; text of each column, if any, e.g. integer&lt;foo&gt; foo is the kind sem_t *semtypes; // typecode for each field } sem_struct;  The structure is nothing more than an array of names, types and kinds with a count. But it creates the notion of what's usually called a &quot;shape&quot; in the codebase. Shapes can be used in a variety of ways as is described inChapter 5 of the CQL Guide. But before we get into shapes, let's look at an example of how a structure type is created. The code that follows is the back end of sem_create_table_stmt. At this point the bulk of the analysis is done and the columns all have their types. We're about to build the struct type for the table. Let's see how that goes.  // now create a struct type with the correct number of columns // the types have already been computed so all we have to do is // check for duplicates sem_struct *sptr = new_sem_struct(name, cols); symtab *columns = symtab_new(); int32_t col = 0; for (ast_node *item = col_key_list; item; item = item-&gt;right) { Contract(is_ast_col_key_list(item)); EXTRACT_ANY_NOTNULL(def, item-&gt;left); if (is_ast_col_def(def)) { Invariant(def-&gt;sem-&gt;name); Invariant(col &lt;= cols); // it's possible that the rest are deleted and we're at the end. // columns must be unique, including deleted columns if (!symtab_add(columns, def-&gt;sem-&gt;name, NULL)) { EXTRACT_NOTNULL(col_def_type_attrs, def-&gt;left); EXTRACT_NOTNULL(col_def_name_type, col_def_type_attrs-&gt;left); EXTRACT_ANY_NOTNULL(col_def_ast, col_def_name_type-&gt;left); report_error(col_def_ast, &quot;CQL0142: duplicate column name&quot;, def-&gt;sem-&gt;name); record_error(ast); symtab_delete(columns); goto cleanup;; } if (is_deleted(def-&gt;sem-&gt;sem_type)) { continue; } Invariant(col &lt; cols); sptr-&gt;names[col] = def-&gt;sem-&gt;name; sptr-&gt;semtypes[col] = def-&gt;sem-&gt;sem_type; sptr-&gt;kinds[col] = def-&gt;sem-&gt;kind; col++; } } symtab_delete(columns); Invariant(col == cols); ast-&gt;sem = new_sem(SEM_TYPE_STRUCT); ast-&gt;sem-&gt;sptr = sptr; ast-&gt;sem-&gt;jptr = sem_join_from_sem_struct(sptr); ast-&gt;sem-&gt;region = current_region;  new_sem_struct : makes a struct to hold the result, we already have the count of columns and the table namesymtab_new : is going to give us a scratch symbol table so we can check for duplicate column nameswe walk all the items in the table and use is_ast_col_def(def) to find the column definitionsInvariant(def-&gt;sem-&gt;name) : claims that we must have already computed the semantic info for the column and it has its name populated this was done earlier symtab_add(columns, def-&gt;sem-&gt;name, NULL) : adds a nil entry under the column name -- if this fails we have a duplicate column, in which case we report errors and stop is_deleted : tells us if the column was marked with @delete in which case it no longer counts as part of the tableif all this is good we set the names, kinds, and semtypes from the column definition's semantic infosymtab_delete : cleans up the temporary symbol tablenew_sem : creates a sem_node of type SEM_TYPE_STRUCT which is filled in sem_join_from_sem_struct will be discussed shortly, but it creates a jptr with one table in it Structure types often come from the shape of a table, but other things can create a structure type. For instance, the columns of a view, or any select statement, are also described by a structure type and are therefore valid &quot;shapes&quot;. The return type of a procedure usually comes from a SELECT statement, so the procedure too can be the source of a shape. The arguments of a procedure form a shape. The fields of a cursor form a shape. You can even have a named subset of the arguments of a procedure and use them like a shape. All of these things are described by structure types. Shapes and the LIKE construct​ There are many cases where you want to be able to capture or re-use something with a known shape and you don't want to have to fully re-declare the thing. CQL uses the LIKE construct to do these sorts of things. This is more fully explained in Chapter 5 of the Guide, but for now let's look at two different cases that are of interest. First, a cursor: DECLARE C CURSOR LIKE Foo; -- Foo something with a shape  So, in the above, Foo could be a table, a view, a procedure with a result, another cursor, and so forth. How might we do this? This is the business of sem_declare_cursor_like_name // Here we're going to make a new value cursor using the indicated name for the shape. // The name has to be &quot;likeable&quot; meaning it refers to some named thing with a shape // such as a table, a view, another cursor, or a procedure that returns a result set. // These are the so called &quot;value cursors&quot; in that they have no underlying statement // that they move through. You can just load them up with a row and pass them around. static void sem_declare_cursor_like_name(ast_node *ast) { Contract(is_ast_declare_cursor_like_name(ast)); EXTRACT_ANY_NOTNULL(new_cursor_ast, ast-&gt;left); EXTRACT_STRING(new_cursor_name, new_cursor_ast); EXTRACT_ANY_NOTNULL(shape_def, ast-&gt;right); // no duplicates allowed if (!sem_verify_legal_variable_name(ast, new_cursor_name)) { record_error(new_cursor_ast); record_error(ast); return; } // must be a valid shape ast_node *found_shape = sem_find_shape_def(shape_def, LIKEABLE_FOR_VALUES); if (!found_shape) { record_error(ast); return; } // good to go, make our cursor, with storage. shape_def-&gt;sem = found_shape-&gt;sem; new_cursor_ast-&gt;sem = new_sem(SEM_TYPE_STRUCT | SEM_TYPE_VARIABLE | SEM_TYPE_VALUE_CURSOR | SEM_TYPE_HAS_SHAPE_STORAGE); new_cursor_ast-&gt;sem-&gt;sptr = found_shape-&gt;sem-&gt;sptr; new_cursor_ast-&gt;sem-&gt;name = new_cursor_name; ast-&gt;sem = new_cursor_ast-&gt;sem; symtab_add(current_variables, new_cursor_name, new_cursor_ast); }  EXTRACT : gets the pieces we need from the ASTsem_verify_legal_variable_name : makes sure the cursor name is unique and doesn't hide a table namesem_find_shape_def : searches for something with a suitable name that has a shapewe populate the name node with the semantic type that we foundnew_sem : makes a new sem_node for the cursor variable with SEM_TYPE_STRUCT set the sptr field using the discovered shape Note: name_ast-&gt;sem isn't actually used for anything but it is helpful for debugging. If the AST is printed it shows the original unmodified semantic type which can be helpful. Briefly sem_find_shape_def does these steps: if the right of the LIKE refers to procedure arguments (e.g. C LIKE Foo ARGUMENTS), get the args of the named procedure and use them as a shapeif the right is a local or global, and its a cursor, use the shape of that cursor for the new cursorif the right is the name of an argument bundle, use the shape of the bundle e.g. in CREATE PROC Foo(p1 like Person, p2 like Person) p1 and p2 are the names of argument bundles shaped like Person if the right is the name of a table or view, use that shapeif the right is the name of a procedure with a structure result, use that shapeif it's none of these, produce an error This is the primary source of shape reuse. Let's look at how we might use that. Suppose we want to write a procedure that inserts a row into the table Foo, we could certainly list the columns of Foo as arguments like this: CREATE PROC InsertIntoFoo(id integer, t text, r real, b blob) BEGIN INSERT INTO Foo(id, t, r, b) VALUES(id, t, r, b); END;  But that approach is going to get a lot less exciting when there are lots of columns and it will be increasingly a maintenance headache. Compare that with the following: CREATE PROC InsertIntoFoo(row LIKE Foo) BEGIN INSERT INTO Foo FROM row; END;  Those two versions of InsertIntoFoo compile into the same code. The semantic analyzer expands the (row LIKE Foo) into(row_id integer, row_t text, row_r real, row_b blob) and then replaces FROM row with(row_id, row_t, row_r, row_b). In both case it simply looked up the shape using sem_find_shape_defand then altered the AST to the canonical pattern. This kind of &quot;shape sugar&quot; is all over CQL and greatly increases maintainability while eliminating common errors. The most common operation is simply to expland a &quot;shape&quot; into a list of arguments or columns (maybe with or without type names). SQLite doesn't know any of this shape magic so by the time SQLite sees the code it has to look &quot;normal&quot; -- the shapes are all resolved. "},{"title":"Join Types​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#join-types","content":"The last of the type building data structure is the join type. Recall that we have this shape: // for the data type of (parts of) the FROM clause // sometimes I refer to as a &quot;joinscope&quot; typedef struct sem_join { uint32_t count; // count of table/views in the join CSTR *names; // names of the table/view struct sem_struct **tables; // struct type of each table/view } sem_join;  This is an array of named structure types, which is exactly what you get when you do something like this: select * from T1 INNER JOIN T2;  The result has all of the columns of T1 and all of the columns of T2. They can be referred to with scoped names like T1.x which means &quot;find the sptr corresponding to the name T1 then within that structure find the column named x&quot;. In general, when we join, we take a jptr on the left and concatenate it with a jptr on the right. For all this to work we have to start somewhere, usually single tables. As we saw when we make a table we use sem_join_from_sem_struct to make its initial jptr. Let's have a look at that now: // Create a base join type from a single struct. static sem_join *sem_join_from_sem_struct(sem_struct *sptr) { sem_join *jptr = new_sem_join(1); jptr-&gt;names[0] = sptr-&gt;struct_name; jptr-&gt;tables[0] = new_sem_struct_strip_table_flags(sptr); return jptr; }  It doesn't get much simpler than the above, here are the steps briefly: new_sem_join : gives us an empty sem_join with room for 1 tablewe use the struct name for the name and the table's sptr for the shapenew_sem_struct_strip_table_flags : copies the table's sptr keeping only the essential flags SEM_TYPE_HIDDEN_COLSEM_FLAG_NOTNULLSEM_FLAG_SENSITIVE The other flags (e.g. SEM_TYPE_PK) have no value in doing type checking and were only needed to help validate the table itself. Those extra flags would be harmless but they would also contaminate all of the debug output, so they are stripped. As a result the type of columns as they appear in say SELECT statements is simpler than how they appear in a CREATE TABLE statement. When we need to create a new join type we simply (*) make a new sem_join that is the concatenation of the left and right sides of the join operation. some join types change the nullability of columns like LEFT JOIN, so we have to handle that toothe names of the tables in the new concatenated joinscope have to be unambiguous so there is also error checking to dobut basically it's just a concatenation Importantly, we call the thing a &quot;joinscope&quot; because it creates a namespace. When we are evaluating names inside of the FROM clause or even later in, say, a WHERE clause, the joinscope that we have created so far controls the table.column combinations that you can use in expressions. This changes again when there is a subquery, so the joinscopes can be pushed and popped as needed. By way of example, you'll see these two patterns in the code:  PUSH_JOIN(from_scope, select_from_etc-&gt;sem-&gt;jptr); error = sem_select_orderby(select_orderby); POP_JOIN();  PUSH_JOIN : use the jptr from the FROM clause to put things back in scope for the ORDER BY clause  PUSH_JOIN_BLOCK(); sem_numeric_expr(ast-&gt;left, ast, &quot;LIMIT&quot;, SEM_EXPR_CONTEXT_LIMIT); POP_JOIN();  PUSH_JOIN_BLOCK : causes the name search to stop -- nothing deeper in the stack is searchedin this case we do not allow LIMIT expressions to see any joinscopes, as they may not use any columns even if the LIMIT clause is appearing in a subquery it can't refer to columns in the parent query "},{"title":"Schema Regions​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#schema-regions","content":"We touched briefly on schema regions earlier in this section. The purpose and language for regions is described more fully in Chapter 10 of the Guide. In this section we'll deal with how they are implemented and what you should expect to see in the code. When a region declaration is found this method is used: // A schema region is an partitioning of the schema such that it // only uses objects in the same partition or one of its declared // dependencies. One schema region may be upgraded independently // from any others (assuming they happen such that dependents are done first). // Here we validate: // * the region name is unique // * the dependencies (if any) are unique and exist static void sem_declare_schema_region_stmt(ast_node *ast) { ... }  The general rules are described in the comment, but effectively it accumulates the list of the declared region's dependencies. Sometimes these are called the antecedent regions. Since a region can only depend on regions that have already been declared, it's not possible to make any cycles. Regions are declared before you put anything into them. Pieces of schema or procedures (or anything really) can go into a region by putting that code inside a begin/end pair for the named region. Like so: @begin_schema_region your_region; -- your stuff @end_schema_region;  Now whatever happens to be in &quot;your stuff&quot; is: limited to seeing only the things that your_region is allowed to see, andcontributes its contents to your_region thereby limiting how others will be able to use &quot;your stuff&quot; To see how this happens, let's have a look at sem_begin_schema_region_stmt. // Entering a schema region makes all the objects that follow part of that // region. It also means that all the contained objects must refer to // only pieces of schema that are in the same region or a dependent region. // Here we validate that region we are entering is in fact a valid region // and that there isn't already a schema region. static void sem_begin_schema_region_stmt(ast_node * ast) { Contract(is_ast_begin_schema_region_stmt(ast)); EXTRACT_STRING(name, ast-&gt;left); // @BEGIN_SCHEMA_REGION name if (!verify_schema_region_out_of_proc(ast)) { record_error(ast); return; } if (current_region) { report_error(ast, &quot;CQL0246: schema regions do not nest; end the current region before starting a new one&quot;, NULL); record_error(ast); return; } ast_node *region = find_region(name); if (!region) { report_error(ast-&gt;left, &quot;CQL0244: unknown schema region&quot;, name); record_error(ast); return; } // Get the canonical name of the region (case adjusted) Contract(is_region(region)); EXTRACT_STRING(region_name, region-&gt;left); // we already know we are not in a region Invariant(!current_region_image); current_region_image = symtab_new(); sem_accumulate_public_region_image(current_region_image, region_name); // this is the one and only text pointer value for this region current_region = region_name; record_ok(ast); }  We see these basic steps: EXTRACT : gets the region nameverify_schema_region_out_of_proc : makes sure we are out of any procedure (we have to be at the top level) errors if in a procedure current_region : is tested to make sure we are not already in a region (no nesting) errors if already in a region find_region : is used to find the region AST by name errors if the region name isn't valid EXTRACT : is used again to get the canonical name of the region you could write @begin_schema_region YoUr_ReGION; but we want the canonical name your_region, as it was declared symtab_new : creates a new symbol table current_region_imagesem_accumulate_public_region_image : populates current_region_image by recursively walking this region adding the names of all the regions we find along the way note the regions form a DAG so we might find the same name twice; we can stop if we find a region that is already in the image symbol table current_region : set it to the now new current region Now we're all set up. We can use current_region to set the region in the sem_node of anything we encounterWe can use current_region_image to quickly see if we are allowed to use any given region if it's in the symbol table we can use it Recall that at the end of sem_create_table_stmt we do this:  ast-&gt;sem = new_sem(SEM_TYPE_STRUCT); ast-&gt;sem-&gt;sptr = sptr; ast-&gt;sem-&gt;jptr = sem_join_from_sem_struct(sptr); ast-&gt;sem-&gt;region = current_region;  That should make a lot more sense now. When doing the symmetric check in sem_validate_object_ast_in_current_region we see this pattern: // Validate whether or not an object is usable with a schema region. The object // can only be a table, view, trigger or index. static bool_t sem_validate_object_ast_in_current_region( CSTR name, ast_node *table_ast, ast_node *err_target, CSTR msg) { // We're in a non-region therefore no validation needed because non-region stmt // can reference schema in any region. if (!current_region) { return true; } if (table_ast-&gt;sem &amp;&amp; table_ast-&gt;sem-&gt;region) { // if we have a current region then the image is always computed! Invariant(current_region_image); if (!symtab_find(current_region_image, table_ast-&gt;sem-&gt;region)) { // The target region is not accessible from this region ... return false; } } else { // while in schema region '%s', accessing an object that isn't in a region is invalid ... return false; } return true; }  I've elided some of the code here, but only the part that generates error messages. The essential logic is: if we are not in a region we can access anythingif we're in a region then... the thing we're trying to access must also be in a region, andthat region must be in current_region_imageotherwise, we can't access it This is enough to do all the region validation we need. "},{"title":"Results of Semantic Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#results-of-semantic-analysis","content":"Semantic Analysis leaves a lot of global state ready for the remaining stages to harvest. If the state is defined in sem.h then it's ok to harvest. Here we'll highlight some of the most important things you can use in later passes. These are heavily used in the code generators. cql_data_decl( struct list_item *all_tables_list ); cql_data_decl( struct list_item *all_functions_list ); cql_data_decl( struct list_item *all_views_list ); cql_data_decl( struct list_item *all_indices_list ); cql_data_decl( struct list_item *all_triggers_list ); cql_data_decl( struct list_item *all_regions_list ); cql_data_decl( struct list_item *all_ad_hoc_list ); cql_data_decl( struct list_item *all_select_functions_list ); cql_data_decl( struct list_item *all_enums_list );  These linked lists are authoritiative; they let you easily enumerate all the objects of the specified type. For instance, if you wanted to do some validation of all indices, you could simply walk all_indices_list. cql_noexport ast_node *find_proc(CSTR name); cql_noexport ast_node *find_region(CSTR name); cql_noexport ast_node *find_func(CSTR name); cql_noexport ast_node *find_table_or_view_even_deleted(CSTR name); cql_noexport ast_node *find_enum(CSTR name);  These functions give you access to the core name tables (which are still valid!) so that you can look up procedures, functions, tables, etc. by name. Finally, information about all the schema annotations is invaluable for building schema upgraders. These two buffers hold dense arrays of annotation records as shown below. cql_data_decl( bytebuf *schema_annotations ); cql_data_decl( bytebuf *recreate_annotations ); typedef struct recreate_annotation { CSTR target_name; // the name of the target CSTR group_name; // group name or &quot;&quot; if no group (not null, safe to sort) ast_node *target_ast; // top level target (table, view, or index) ast_node *annotation_ast; // the actual annotation int32_t ordinal; // when sorting we want to use the original order (reversed actually) within a group } recreate_annotation; typedef struct schema_annotation { int32_t version; // the version number (always &gt; 0) ast_node *target_ast; // top level target (table, view, or index) CSTR target_name; // the name of the target uint32_t annotation_type; // one of the codes below for the type of annotation ast_node *annotation_ast; // the actual annotation int32_t column_ordinal; // -1 if not a column ast_node *column_ast; // a particular column if column annotation } schema_annotation; // Note: schema annotations are processed in the indicated order: the numbers matter! #define SCHEMA_ANNOTATION_INVALID 0 #define SCHEMA_ANNOTATION_FIRST 1 #define SCHEMA_ANNOTATION_UNSUB 1 #define SCHEMA_ANNOTATION_CREATE_TABLE 2 #define SCHEMA_ANNOTATION_CREATE_COLUMN 3 #define SCHEMA_ANNOTATION_DELETE_TRIGGER 4 #define SCHEMA_ANNOTATION_DELETE_VIEW 5 #define SCHEMA_ANNOTATION_DELETE_INDEX 6 #define SCHEMA_ANNOTATION_DELETE_COLUMN 7 #define SCHEMA_ANNOTATION_DELETE_TABLE 8 #define SCHEMA_ANNOTATION_AD_HOC 9 #define SCHEMA_ANNOTATION_RESUB 10 #define SCHEMA_ANNOTATION_LAST 10  And of course, each &quot;back end&quot; is provided with the root of the AST so that it can also search and/or walk the AST in its own manner. We will see examples of this in later sections. "},{"title":"Recap​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#recap","content":"At present, there are nearly 20000 lines in sem.c and it would no doubt take more than 20000 lines of text to explain what they all do, and that would be more imprecise than the source code, and probably less readable. sem.c includes over 4000 lines of comments, and probably should have more. While there is a lot of code there, it's very readable and I encourage you to do just that -- read it -- to get your answers. The point of this part of the Internals Guide isn't to fully explain all 400+ error checks in about as many semantic error checking functions, it is to showcase the key concepts shared by all of them. Things like: errors are reported largely in the AST and percolate upexpressions and statements have general purpose dispatch logic for continuing a statement walkEXTRACT macros are used to keep the tree walk on track and correct in the face of changesregions are used for visibilityversioning contributes to visibilitynullability and sensitivity are tracked throughout using type bitstype &quot;kind&quot; is managed by a simple string in the sem_node payloadthe three main payloads are sem_node for basic info, andsem_struct or sem_join for the non-unitary types This isn't everything but it should leave you well armed to begin your own exploration of sem.c. Note: details on unsub/resub are forthcoming. This code is under development. "},{"title":"Part 3: C Code Generation​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-3-c-code-generation","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-2","content":"Part 3 continues with a discussion of the essentials of the C code generation pass of the CQL compiler. As in the previous sections, the goal here is not to go over every detail of code generation but rather to give a sense of how codegen happens in general -- the core strategies and implementation choices -- so that when reading the code you will have an idea how smaller pieces would fit into the whole. To accomplish this, various key data structures will be explained in detail as well as selected examples of their use. "},{"title":"C Code Generation​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#c-code-generation","content":"There are several key pieces of C code that we have to generate to make working CQL procedures using C functions. This all happens in cg_c.c. From a big picture perspective, these are the essential problems: we have to compile SQL expressions into C including expressions with variables that are nullableincluding SQL expressions that are highly complex like CASE..WHEN..THEN..END and IN (..) we have to generate control flow for things like IF, WHILE and, SWITCHwe have to make result sets including the code to slurp up all the rows from a SQL statement into an array of valueswe want to do this very economically we have to be able to create the text for every SQLite statement and bind any variables to itwe have to check every SQLite API for errors and throw exceptions consistently and deal with them including constructs that allow users to handle exceptions, such as TRY/CATCH we have to track any reference types carefully so that retain/release pairs are done consistently even in the presence of SQLite errors or other exceptions we have to produce a .h and a .c file for the C compiler contributions to these files could come from various places, not necessarily in orderthe .c file will itself have various sections and we might need to contribute to them at various points in the compilation we want to do this all in one pass over the ASTwe get to assume that the program is error-free -- codegen never runs unless semantic analysis reports zero errors so nothing can be wrong by the time the codegen pass runs, we never detect errors heresometimes we add Contract and Invariant statements to cg.c to make our assumptions clear and to prevent regressions There are some very important building blocks used to solve these problems: we will start with those, then move to a discussion of each of the essential kinds of code generation that we have to do to get working programs. "},{"title":"Launching the Code Generator​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#launching-the-code-generator","content":"Once semantic analysis is done, all of the code generators have the same contract: they have a main function like cg_c_main for the C code generator. It gets the root of the AST and it can use the public interface of the semantic analyzer to get additional information. See Part 2 for those details. // Main entry point for code-gen. This will set up the buffers for the global // variables and any loose calls or DML. Any code that needs to run in the // global scope will be added to the global_proc. This is the only codegen // error that is possible. If you need global code and you don't have a global // proc then you can't proceed. Semantic analysis doesn't want to know that stuff. // Otherwise all we do is set up the most general buffers for the global case and // spit out a function with the correct name. cql_noexport void cg_c_main(ast_node *head) { ... }  In addition to initializing its scratch storage, the main entry point also sets up a symbol table for AST dispatch just like the gen_ and sem_ functions do. Here are some samples from that table with the most common options:  DDL_STMT_INIT(drop_table_stmt); DDL_STMT_INIT(drop_view_stmt); DDL_STMT_INIT(create_table_stmt); DDL_STMT_INIT(create_view_stmt);  The DDL (Data Definition Language) statements all get the same handling: The text of the statement is generated from the AST. Any variables are bound and then the statement is executed. The work is done with cg_bound_sql_statement which will be discussed later. // Straight up DDL invocation. The ast has the statement, execute it! // We don't minify the aliases because DDL can have views and the view column names // can be referred to in users of the view. Loose select statements can have // no external references to column aliases. static void cg_any_ddl_stmt(ast_node *ast) { cg_bound_sql_statement(NULL, ast, CG_EXEC|CG_NO_MINIFY_ALIASES); }  DML (Data Manipulation Language) statements are declared similarly:  STD_DML_STMT_INIT(begin_trans_stmt); STD_DML_STMT_INIT(commit_trans_stmt); STD_DML_STMT_INIT(rollback_trans_stmt); STD_DML_STMT_INIT(savepoint_stmt); STD_DML_STMT_INIT(delete_stmt);  The DML statements are handled by cg_std_dml_exec_stmt; the processing is identical to DDL except CG_MINIFY_ALIASES is specified. This allows the code generator to remove unused column aliases in SELECT statements to save space. // Straight up DML invocation. The ast has the statement, execute it! static void cg_std_dml_exec_stmt(ast_node *ast) { cg_bound_sql_statement(NULL, ast, CG_EXEC|CG_MINIFY_ALIASES); }  Note that this flag difference only matters for the CREATE VIEW statement but for symmetry all the DDL is handled with one macro and all the DML with the second macro. Next, the easiest case... there are a bunch of statements that create no code-gen at all. These statements are type definitions that are interesting only to the semantic analyzer, or other control statements. Some examples:  NO_OP_STMT_INIT(declare_enum_stmt); NO_OP_STMT_INIT(declare_named_type);  Next, the general purpose statement handler. STMT_INIT creates mappings such as the if_stmt AST node mapping to cg_if_stmt.  STMT_INIT(if_stmt); STMT_INIT(switch_stmt); STMT_INIT(while_stmt); STMT_INIT(assign);  The next group of declarations are the expressions, with precedence and operator specified. There is a lot of code sharing between AST types as you can see from this sample:  EXPR_INIT(num, cg_expr_num, &quot;num&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(str, cg_expr_str, &quot;STR&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(null, cg_expr_null, &quot;NULL&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(dot, cg_expr_dot, &quot;DOT&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(mul, cg_binary, &quot;*&quot;, C_EXPR_PRI_MUL); EXPR_INIT(div, cg_binary, &quot;/&quot;, C_EXPR_PRI_MUL); EXPR_INIT(mod, cg_binary, &quot;%&quot;, C_EXPR_PRI_MUL); EXPR_INIT(add, cg_binary, &quot;+&quot;, C_EXPR_PRI_ADD); EXPR_INIT(sub, cg_binary, &quot;-&quot;, C_EXPR_PRI_ADD); EXPR_INIT(not, cg_unary, &quot;!&quot;, C_EXPR_PRI_UNARY); EXPR_INIT(tilde, cg_unary, &quot;~&quot;, C_EXPR_PRI_UNARY); EXPR_INIT(uminus, cg_unary, &quot;-&quot;, C_EXPR_PRI_UNARY);  Most (not all) of the binary operators are handled with one function cg_binary and likewise most unary operators are handled with cg_unary. Note: the precedence constants are the C_EXPR_PRI_* flavor because, naturally, parentheses will be generated based on the C rules during C codegen. Importantly, the AST still, and always, authoritatively encodes the user-specified order of operations -- there's no change there. The only thing that changes is where parentheses are needed to get the desired result. Some parens may need to be added, and some that were present in the original text might no longer be needed. Here are some helpful examples: CREATE PROC p () BEGIN /* NOT is weaker than + */ LET x := (NOT 1) + (NOT 2); SET x := NOT 1 + 2; END;  void p(void) { cql_bool x = 0; /* ! is stronger than + */ x = ! 1 + ! 2; x = ! (1 + 2); }  Finally, many built-in functions need special codegen, such as:  FUNC_INIT(coalesce); FUNC_INIT(printf);  FUNC_INIT(coalesce) creates a mapping between the function name coalesce and the generator cg_func_coalesce. "},{"title":"Character Buffers and Byte Buffers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#character-buffers-and-byte-buffers","content":"The first kind of text output that CQL could produce was the AST echoing. This was originally done directly with fprintf but that was never going to be flexible enough -- we have to be able to emit that output into other places like comments, or the text of SQL statements. This need forces that pass to use character buffers, which we touched on in Part 1. C Code generation has a more profound dependency on character buffers -- they are literally all over cg_c.c and we need to go over how they are used if we're going to understand the codegen passes. The public interface for charbuf is in charbuf.h and it's really quite simple. You allocate a charbuf and then you canbprintf into it. Let's be a bit more specific: #define CHARBUF_INTERNAL_SIZE 1024 #define CHARBUF_GROWTH_SIZE 1024 typedef struct charbuf { char *ptr; // pointer to stored data, if any uint32_t used; // bytes used in current buffer uint32_t max; // max bytes in current buffer // builtin buffer storage char internal[CHARBUF_INTERNAL_SIZE]; } charbuf; cql_data_decl( int32_t charbuf_open_count ); cql_noexport void bopen(charbuf* b); cql_noexport void bclose(charbuf *b); cql_noexport void bprintf(charbuf *b, const char *format, ...);  The typical pattern goes something like this:  charbuf foo; bopen(&amp;foo); bprintf(&amp;foo, &quot;Hello %s\\n&quot;, &quot;World&quot;); // do something with foo.ptr bclose(&amp;foo);  Note that charbuf includes CHARBUF_INTERNAL_SIZE of storage that does not have to be allocated with malloc and it doesn't grow very aggressively. This economy reflects that fact that most charbuf instances are very small. Of course a charbuf could go on the heap if it needs to outlive the function it appears in, but this is exceedingly rare. To make sure buffers are consistently closed -- and this is a problem because there are often a lot of them -- they are allocated with these simple helper macros: #define CHARBUF_OPEN(x) \\ int32_t __saved_charbuf_count##x = charbuf_open_count; \\ charbuf x; \\ bopen(&amp;x) #define CHARBUF_CLOSE(x) \\ bclose(&amp;x); \\ Invariant(__saved_charbuf_count##x == charbuf_open_count)  The earlier example would be written more properly:  CHARBUF_OPEN(foo); bprintf(&amp;foo, &quot;Hello %s\\n&quot;, &quot;World&quot;); // do something with foo.ptr CHARBUF_CLOSE(foo);  If you forget to close a buffer the count will get messed up and the next close will trigger an assertion failure. It's normal to create several buffers in the course of doing code generation. In fact some of these buffers become &quot;globally&quot; visible and get swapped out as needed. For instance, the kind of chaining we see inside of cg_create_proc_stmt is normal, here is the sequence: Make new buffers...  CHARBUF_OPEN(proc_fwd_ref); CHARBUF_OPEN(proc_body); CHARBUF_OPEN(proc_locals); CHARBUF_OPEN(proc_cleanup);  Save the current buffer pointers...  charbuf *saved_main = cg_main_output; charbuf *saved_decls = cg_declarations_output; charbuf *saved_scratch = cg_scratch_vars_output; charbuf *saved_cleanup = cg_cleanup_output; charbuf *saved_fwd_ref = cg_fwd_ref_output;  Switch to the new buffers...  cg_fwd_ref_output = &amp;proc_fwd_ref; cg_main_output = &amp;proc_body; cg_declarations_output = &amp;proc_locals; cg_scratch_vars_output = &amp;proc_locals; cg_cleanup_output = &amp;proc_cleanup;  And of course the code puts the original values back when it's done and then closes what it opened. This means that while processing a procedure the codegen that declares say scratch variables, which would go to cg_scratch_vars_output, is going to target the proc_locals buffer which will be emitted before the proc_body. By the time cg_stmt_list is invoked thecg_main_output variable will be pointing to the procedure body, thus any statements will go into there rather than being accumulated at the global level. Note: it's possible to have code that is not in a procedure (see --global_proc). In general, it's very useful to have different buffers open at the same time. New local variables or scratch variables can be added to their own buffer. New cleanup steps that are necessary can be added tocg_cleanup_output which will appear at the end of a procedure. The final steps of procedure codegen combines all of these pieces plus a little glue to make a working procedure. All codegen works like this -- statements, expressions, all of it. One interesting but unexpected feature of charbuf is that it provides helper methods for indenting a buffer by whatever amount you like. This turns out to be invaluable in creating well formatted C code because of course we want (e.g.) the body of an if statement to be indented. CQL tries to create well formatted code that is readable by humans as much as possible. Byte Buffers​ The byte buffers type, creatively called bytebuf is less commonly used. It is a peer to charbufand provides a growable binary buffer. bytebuf is often used to hold arrays of structures. Interestingly, cg_c.c doesn't currently consume byte buffers, the presence of bytebuf.c actually came late to the CQL compiler. However the CQL runtime cqlrt.c (and cqlrt_common.c) providecql_bytebuf_open, cql_bytebuf_alloc and, cql_bytebuf_close which are akin to the charbuf methods. These functions are used in the generated code to create result sets at runtime. bytebuf was so useful that it found its way back from the runtime into the compiler itself, and is used by other code-generators like the schema upgraded. The semantic analyzer also uses it to help with query fragments and to track the various upgrade annotations. Both charbuf and bytebuf are simple enough that they don't need special discussion. Surveying their code and comments is an excellent exercise for the reader. "},{"title":"Expressions​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#expressions","content":"Many of the output needs of CQL stemmed from the base case of creating the code for CQL expressions. A simple CQL expression like:  SET x := x + y;  seems innocuous enough, and we'd like that expression to compile to this code:  x = x + y;  And indeed, it does. Here's some actual output from the compiler: /* CREATE PROC p () BEGIN DECLARE x, y INTEGER NOT NULL; SET x := x + y; END; */ #define _PROC_ &quot;p&quot; void p(void) { cql_int32 x = 0; cql_int32 y = 0; x = x + y; } #undef _PROC_  (*) the output above was created by using out/cql --in x --cg x.h x.c --nolines to avoid all the # directives That expression looks easy enough. And indeed if all expressions were like this, we could do expression compilation pretty simply -- every binary operator would look something like this: recurse leftemit infix operatorrecurse right This would sort of build up your expressions inside out and your final buffer after all the recursion was done would have the whole expression. This doesn't work at all. To illustrate what goes wrong, we only have to change the test case a tiny bit. The result is telling: /* CREATE PROC p () BEGIN DECLARE x, y INTEGER; SET x := x + y; END; */ #define _PROC_ &quot;p&quot; void p(void) { cql_nullable_int32 x; cql_set_null(x); cql_nullable_int32 y; cql_set_null(y); cql_combine_nullables(x, x.is_null, y.is_null, x.value + y.value); } #undef _PROC_  In this new example above, x and y became nullable variables i.e. the NOT NULL was removed from their declarations -- this makes all the difference in the world. Let's take a quick look at cql_nullable_int32 and we'll see the crux of the problem immediately: typedef struct cql_nullable_int32 { cql_bool is_null; cql_int32 value; } cql_nullable_int32;  The problem is that nullable value types like cql_nullable_int32 have both their value field and a boolean is_null and these don't flow into expressions that use operators like +, -, / and so forth. This means that even simple expressions involving nullable types actually expand into several statements. And, in general, these statements need a place to put their temporary results to accumulate the correct answer, so scratch variables are required to make all this work. Here's a more realistic example: /* CREATE PROC combine (x INTEGER, y INTEGER, OUT result INTEGER) BEGIN SET result := 5 * x + 3 * y; END; */ #define _PROC_ &quot;combine&quot; void combine(cql_nullable_int32 x, cql_nullable_int32 y, cql_nullable_int32 *_Nonnull result) { cql_contract_argument_notnull((void *)result, 3); cql_nullable_int32 _tmp_n_int_1; cql_set_null(_tmp_n_int_1); cql_nullable_int32 _tmp_n_int_2; cql_set_null(_tmp_n_int_2); cql_set_null(*result); // set out arg to non-garbage cql_set_nullable(_tmp_n_int_1, x.is_null, 5 * x.value); cql_set_nullable(_tmp_n_int_2, y.is_null, 3 * y.value); cql_combine_nullables(*result, _tmp_n_int_1.is_null, _tmp_n_int_2.is_null, _tmp_n_int_1.value + _tmp_n_int_2.value); } #undef _PROC_ #pragma clang diagnostic pop  _tmp_n_int_1 : holds the product of x and 5, it's null if x.is_null is true_tmp_n_int_2 : holds the product of y and 3, it's null if y.is_null is true*result : holds the answer, it's null if either of _tmp_n_int_1.is_null, _tmp_n_int_2.is_null is true otherwise it's _tmp_n_int_1.value + _tmp_n_int_2.value So, in general, we need to emit arbitrarily many statements in the course of evaluating even simple looking expressions and we need good mechanisms to manage that. This is what we'll talk about in the coming sections. Managing Scratch Variables​ The function that actually assigns scratch variables is cg_scratch_var // The scratch variable helper uses the given sem_type and the current // stack level to create a temporary variable name for that type at that level. // If the variable does not already have a declaration (as determined by the masks) // then a declaration is added to the scratch_vars section. This is one of the root // ways of getting an .is_null and .value back. Note that not null variables always // have a .is_null of &quot;0&quot; which becomes important when deciding how to assign // one result to another. Everything stays uniform. static void cg_scratch_var(ast_node *ast, sem_t sem_type, charbuf *var, charbuf *is_null, charbuf *value)  The signature is a bit unexpected so we'll go over it, some of below will make more sense as we learn about expressions generally, but this is as good an introduction as any. ast : holds a reference to a variable we want to assign to this argument is normally NULL for scratch variablesast is not null for the RESULT macros which we'll study laterfor now, we can basically ignore this argument sem_type : holds the type of the variable we need it must be a unitary type, optionally with SEM_TYPE_NOTNULL set var : a character buffer that will get the name of the variableis_null : a character buffer that will get the is_null expression for this variable (more below)value : a character buffer that will get the value expression for this variable (more below) And this is a good time to talk about is_null and value because they will be everywhere. The codegen for expressions in the C code generator produces two results: the text that corresponds to the current value so far (e.g. &quot;(1+2)*3&quot;), and,the text that will tell you if the current value is null this could be as simple as &quot;0&quot; for an expression that is known to be not null Let's make this a little more concrete: Suppose we ask for a scratch &quot;not null integer&quot;, we get results like this: var: &quot;_tmp_n_int_1&quot;is_null: &quot;0&quot;value: &quot;_tmp_n_int_1&quot; Meaning: if we want the value, use the text &quot;_tmp_n_int_1&quot; if we want to know if the variable is null, we use the text &quot;0&quot; Note: many parts of cg_c.c special case an is_null value of &quot;0&quot; to make better code because such a thing is known to be not null at compile time. Now let's suppose we ask for a scratch nullable integer, we get results like this: var: &quot;_tmp_int_1&quot;is_null: &quot;_tmp_int_1.is_null&quot;value: &quot;_tmp_int_1.value&quot; So again, we have exactly the text we need to test for null, and the test we need to get the value. Additional notes: scratch variables can be re-used, they are on a &quot;stack&quot;a bitmask is used to track which scratch variables have already had a declaration emitted, so they are only declared oncethe variable name is based on the current value of the stack_level variable which is increased in a push/pop fashion as temporaries come in and out of scope this strategy isn't perfect, but the C compiler can consolidate locals even if the CQL codegen is not perfect so it ends up being not so badimportantly, there is one stack_level variable for all temporaries not one stack_level for every type of temporary, this seemed like a reasonable simplification Allocating Scratch Variables​ The most common reason to create a &quot;scratch&quot; variable is that a temporary variable is needed for some part of the computation. The most common reason for a temporary variable is to hold an intermediate result of a computation involving nullable arithmetic. These temporaries are created with CG_PUSH_TEMP which simply creates the three charbuf variables needed and then asks for a scratch variable of the required type. The variables follow a simple naming convention. The stack level is increased after each temporary is allocated. // Create buffers for a temporary variable. Use cg_scratch_var to fill in the buffers // with the text needed to refer to the variable. cg_scratch_var picks the name // based on stack level-and type. #define CG_PUSH_TEMP(name, sem_type) \\ CHARBUF_OPEN(name); \\ CHARBUF_OPEN(name##_is_null); \\ CHARBUF_OPEN(name##_value); \\ cg_scratch_var(NULL, sem_type, &amp;name, &amp;name##_is_null, &amp;name##_value); \\ stack_level++;  Symmetrically, CG_POP_TEMP closes the charbuf variables and restores the stack level. // Release the buffers for the temporary, restore the stack level. #define CG_POP_TEMP(name) \\ CHARBUF_CLOSE(name##_value); \\ CHARBUF_CLOSE(name##_is_null); \\ CHARBUF_CLOSE(name); \\ stack_level--;  As with the other PUSH/POP OPEN/CLOSE macro types, these macros are designed to make it impossible to forget to free the buffers, or to get the stack level wrong. The stack level can be (and is) checked at strategic places to ensure it's back to baseline -- this is easy because the code can always just snapshot stack_level, do some work that should be clean, and then check that stack_level is back to where it's supposed to be with an Invariant. Recursing Sub-expressions​ Now that we understand that we can create scratch variables as needed, it's time to take a look at the typical evaluation patterns and how the evaluation works within that pattern. This is everywhere in cg_c.c. So let's look at an actual evaluator, the simplest of them all, this one does code generation for the NULL literal. static void cg_expr_null( ast_node *expr, CSTR op, charbuf *is_null, charbuf *value, int32_t pri, int32_t pri_new) { Contract(is_ast_null(expr)); // null literal bprintf(value, &quot;NULL&quot;); bprintf(is_null, &quot;1&quot;); }  Now this may be looking familiar: the signature of the code generator is something very much like the signature of the the gen_ functions in the echoing code. That's really because in some sense the echoing code is a very simple code generator itself. expr : the AST we are generating code forop : the relevant operator if any (operators share code)is_null : a charbuf into which we can write the is_null expression textvalue : a charbuf into which we can write the value expression textpri : the binding strength of the node above this onepri_new : the binding strength of this node This particular generator is going to produce &quot;NULL&quot; for the value and &quot;1&quot; for the is_null expression. is_null and value are the chief outputs, and the caller will use these to create its own expression results with recursive logic. But the expression logic can also write into the statement stream, the cleanup stream, even into the header file stream, and as we'll see, it does. pri and pri_new work exactly like they did in the echoing code (see Part 1), they are used to allow the code generator to decide if it needs to emit parentheses. But recall that the binding strengths now will be the C binding strengths NOT the SQL binding strengths (discussed above). Let's look at one of the simplest operators: the IS NULL operator handled by cg_expr_is_null Note: this code has a simpler signature because it's actually part of codegen for cg_expr_is which has the general contract. // The code-gen for is_null is one of the easiest. The recursive call // produces is_null as one of the outputs. Use that. Our is_null result // is always zero because IS NULL is never, itself, null. static void cg_expr_is_null(ast_node *expr, charbuf *is_null, charbuf *value) { sem_t sem_type_expr = expr-&gt;sem-&gt;sem_type; // expr IS NULL bprintf(is_null, &quot;0&quot;); // the result of is null is never null // The fact that this is not constant not null for not null reference types reflects // the weird state of affairs with uninitialized reference variables which // must be null even if they are typed not null. if (is_not_nullable(sem_type_expr) &amp;&amp; !is_ref_type(sem_type_expr)) { // Note, sql has no side-effects so we can fold this away. bprintf(value, &quot;0&quot;); } else { CG_PUSH_EVAL(expr, C_EXPR_PRI_ROOT); bprintf(value, &quot;%s&quot;, expr_is_null.ptr); CG_POP_EVAL(expr); } }  So walking through the above: the result of IS NULL is never null, so we can immediately put &quot;0&quot; into the is_null bufferif the operand is a not-null numeric type then the result of IS NULL is 0if the operand might actually be null then use CG_PUSH_EVAL to recursively do codegen for itcopy its expr_is_null text into our value text Note: the code reveals one of the big CQL secrets -- that not null reference variables can be null... C has the same issue with _Nonnull globals. Now let's look at those helper macros, they are pretty simple: // Make a temporary buffer for the evaluation results using the canonical // naming convention. This might exit having burned some stack slots // for its result variables, that's normal. #define CG_PUSH_EVAL(expr, pri) \\ CHARBUF_OPEN(expr##_is_null); \\ CHARBUF_OPEN(expr##_value); \\ cg_expr(expr, &amp;expr##_is_null, &amp;expr##_value, pri);  The push macro simply creates buffers to hold the is_null and value results, then it calls cg_expr to dispatch the indicated expression. The pri value provided to this macro represents the binding strength that the callee should assume its parent has. Usually this is the pri_new of the caller. but often C_EXPR_PRI_ROOT can be used if the current context implies that the callee will never need parentheses. How do we know that parens are not needed here? It seems like the operand of IS NULL could be anything, surely it might need parentheses? Let's consider: if the operand is of not null numeric type then we aren't even going to evaluate it, we're on the easy &quot;no it's not null&quot; path no parens there if the operand is nullable then the only place the answer can be stored is in a scratch variable and its is_null expression will be exactly like var.is_null no parens there if the operand is a reference type, there are no operators that combine reference types to get more reference types, so again the result must be in a variable, and is is_null expression will be like !var no parens there So, none of these require further wrapping regardless of what is above the IS NULL node in the tree because of the high strength of the . and ! operators. Other cases are usually simpler, such as &quot;no parentheses need to be added by the child node because it will be used as the argument to a helper function so there will always be parens hard-coded anyway&quot;. However these things need to be carefully tested hence the huge variety of codegen tests. Note that after calling cg_expr the temporary stack level might be increased. We'll get to that in the next section. For now, looking at POP_EVAL we can see it's very straightforward: // Close the buffers used for the above. // The scratch stack is not restored so that any temporaries used in // the evaluation of expr will not be re-used prematurely. They // can't be used again until either the expression is finished, // or they have been captured in a less-nested result variable. #define CG_POP_EVAL(expr) \\ CHARBUF_CLOSE(expr##_value); \\ CHARBUF_CLOSE(expr##_is_null);  CG_POP_EVAL simply closes the buffers, leaving the stack level unchanged. More on this in the coming section. Result Variables​ When recursion happens in the codegen, a common place that the result will be found is in a temporary variable i.e. the generated code will use one or more statements to arrange for the correct answer to be in a variable. To do this, the codegen needs to first get the name of a result variable of a suitable type. This is the &quot;other&quot; reason for making scratch variables. There are three macros that make this pretty simple. The first is CG_RESERVE_RESULT_VAR // Make a scratch variable to hold the final result of an evaluation. // It may or may not be used. It should be the first thing you put // so that it is on the top of your stack. This only saves the slot. // If you use this variable you can reclaim other temporaries that come // from deeper in the tree since they will no longer be needed. #define CG_RESERVE_RESULT_VAR(ast, sem_type) \\ int32_t stack_level_reserved = stack_level; \\ sem_t sem_type_reserved = sem_type; \\ ast_node *ast_reserved = ast; \\ CHARBUF_OPEN(result_var); \\ CHARBUF_OPEN(result_var_is_null); \\ CHARBUF_OPEN(result_var_value); \\ stack_level++;  If this looks a lot like PUSH_TEMP that shouldn't be surprising. The name of the variable and the expression parts always go into charbuf variables named result_var, result_var_is_null, and result_var_valuebut the scratch variable isn't actually allocated! However -- we burn the stack_level as though it had been allocated. The name of the macro provides a clue: this macro reserves a slot for the result variable, it's used if the codegen might need a result variable, but it might not. If/when the result variable is needed, it we can artificially move the stack level back to the reserved spot, allocate the scratch variable, and then put the stack level back. When the name is set we know that the scratch variable was actually used. The CG_USE_RESULT_VAR macro does exactly this operation. // If the result variable is going to be used, this writes its name // and .value and .is_null into the is_null and value fields. #define CG_USE_RESULT_VAR() \\ int32_t stack_level_now = stack_level; \\ stack_level = stack_level_reserved; \\ cg_scratch_var(ast_reserved, sem_type_reserved, &amp;result_var, &amp;result_var_is_null, &amp;result_var_value); \\ stack_level = stack_level_now; \\ Invariant(result_var.used &gt; 1); \\ bprintf(is_null, &quot;%s&quot;, result_var_is_null.ptr); \\ bprintf(value, &quot;%s&quot;, result_var_value.ptr)  Once the code generator decides that it will in fact be using a result variable to represent the answer, then the is_null and value buffers can be immediately populated to whatever the values were for the result variable. That text will be correct regardless of what codegen is used to populate the variable. The variable is the result. There is a simpler macro that reserves and uses the result variable in one step, it's used frequently. The &quot;reserve&quot; pattern is only necessary when there are some paths that need a result variable and some that don't. // This does reserve and use in one step #define CG_SETUP_RESULT_VAR(ast, sem_type) \\ CG_RESERVE_RESULT_VAR(ast, sem_type); \\ CG_USE_RESULT_VAR();  And now armed with this knowledge we can look at the rest of the scratch stack management. // Release the buffer holding the name of the variable. // If the result variable was used, we can re-use any temporaries // with a bigger number. They're no longer needed since they // are captured in this result. We know it was used if it // has .used &gt; 1 (there is always a trailing null so empty is 1). #define CG_CLEANUP_RESULT_VAR() \\ if (result_var.used &gt; 1) stack_level = stack_level_reserved + 1; \\ CHARBUF_CLOSE(result_var_value); \\ CHARBUF_CLOSE(result_var_is_null); \\ CHARBUF_CLOSE(result_var);  As it happens when you use CG_PUSH_EVAL it is entirely possible, even likely, that the result of cg_expr is in a result variable. The convention is that if the codegen requires a result variable it is allocated first, before any other temporaries. This is why there is a way to reserve a variable that you might need. Now if it turns out that you used the result variable at your level it means that any temporary result variables from deeper levels have been used and their values plus whatever math was needed is now in your result variable. This means that the stack_levelvariable can be decreased to one more than the level of the present result. This is in the fact the only time it is safe to start re-using result variables because you otherwise never know how many references to result variables that were &quot;deep in the tree&quot; are left in the contents of expr_value or expr_is_null. Now, armed with the knowledge that there are result variables and temporary variables and both come from the scratch variables we can resolve the last mystery we left hanging. Why does the scratch variable API accept an AST pointer? The only place that AST pointer can be not null is in the CG_USE_RESULT_VAR macro, it was this line: cg_scratch_var(ast_reserved, sem_type_reserved, &amp;result_var, &amp;result_var_is_null, &amp;result_var_value);  And ast_reserved refers to the AST that we are trying to evaluate. There's an important special case that we want to optimize that saves a lot of scratch variables. That case is handled by this code in cg_scratch_var:  // try to avoid creating a scratch variable if we can use the target of an assignment in flight. if (is_assignment_target_reusable(ast, sem_type)) { Invariant(ast &amp;&amp; ast-&gt;parent &amp;&amp; ast-&gt;parent-&gt;left); EXTRACT_ANY_NOTNULL(name_ast, ast-&gt;parent-&gt;left); EXTRACT_STRING(name, name_ast); if (is_out_parameter(name_ast-&gt;sem-&gt;sem_type)) { bprintf(var, &quot;*%s&quot;, name); } else { bprintf(var, &quot;%s&quot;, name); } }  The idea is that if the generator is doing an assignment like:  SET x := a + b;  Then the code generator doesn't need a scratch variable to hold the result of the expression a + b like it would in many other contexts. It can use x as the result variable! The SET codegen will discover that the value it's supposed to set is already in x so it does nothing and everything just works out. The price of this is a call to is_assignment_target_reusable and then some logic to handle the case where x is an out argument (hence call by reference, hence needs to be used as *x). "},{"title":"Basic Control Flow Patterns​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#basic-control-flow-patterns","content":"To get a sense of how the compiler generates code for statements, we can look at some of the easiest cases. // &quot;While&quot; suffers from the same problem as IF and as a consequence // generating while (expression) would not generalize. // The overall pattern for while has to look like this: // // for (;;) { // prep statements; // condition = final expression; // if (!condition) break; // // statements; // } // // Note that while can have leave and continue substatements which have to map // to break and continue. That means other top level statements that aren't loops // must not create a C loop construct or break/continue would have the wrong target. static void cg_while_stmt(ast_node *ast) { Contract(is_ast_while_stmt(ast)); EXTRACT_ANY_NOTNULL(expr, ast-&gt;left); EXTRACT(stmt_list, ast-&gt;right); sem_t sem_type = expr-&gt;sem-&gt;sem_type; // WHILE [expr] BEGIN [stmt_list] END bprintf(cg_main_output, &quot;for (;;) {\\n&quot;); CG_PUSH_EVAL(expr, C_EXPR_PRI_ROOT); if (is_nullable(sem_type)) { bprintf(cg_main_output, &quot;if (!cql_is_nullable_true(%s, %s)) break;\\n&quot;, expr_is_null.ptr, expr_value.ptr); } else { bprintf(cg_main_output, &quot;if (!(%s)) break;\\n&quot;, expr_value.ptr); } bool_t loop_saved = cg_in_loop; cg_in_loop = true; CG_POP_EVAL(expr); cg_stmt_list(stmt_list); bprintf(cg_main_output, &quot;}\\n&quot;); cg_in_loop = loop_saved; }  The comment before the cg_while_stmt actually describes the situation pretty clearly; the issue with this codegen is that the expression in the while statement might actually require many C statements to evaluate. There are many cases of this sort of thing, but the simplest is probably when any nullable types are in that expression. A particular example illustrates this pretty clearly: CREATE PROC p () BEGIN DECLARE x INTEGER NOT NULL; SET x := 1; WHILE x &lt; 5 BEGIN SET x := x + 1; END; END;  which generates: void p(void) { cql_int32 x = 0; x = 1; for (;;) { /* in trickier cases there would be code right here */ if (!(x &lt; 5)) break; x = x + 1; } }  In this case, a while statement could have been used because the condition is simply x &lt; 5so this more general pattern is overkill. But consider this program, just a tiny bit different: CREATE PROC p () BEGIN DECLARE x INTEGER; -- x is nullable SET x := 1; WHILE x &lt; 5 BEGIN SET x := x + 1; END; END;  which produces: void p(void) { cql_nullable_int32 x; cql_set_null(x); cql_nullable_bool _tmp_n_bool_0; cql_set_null(_tmp_n_bool_0); cql_set_notnull(x, 1); for (;;) { cql_set_nullable(_tmp_n_bool_0, x.is_null, x.value &lt; 5); if (!cql_is_nullable_true(_tmp_n_bool_0.is_null, _tmp_n_bool_0.value)) break; cql_set_nullable(x, x.is_null, x.value + 1); } }  Even for this small little case, the nullable arithmetic macros have to be used to keep x up to date. The result of x &lt; 5 is of type BOOL rather than BOOL NOT NULL so a temporary variable captures the result of the expression. This is an easy case, but similar things happen if the expression includes e.g. CASE...WHEN... or IN constructs. There are many other cases. So with this in mind, let's reconsider what cg_while_stmt is doing: we start the for statement in the output there's a bprintf for that we evaluate the while expression, the details will be in is_null and value we use CG_PUSH_EVAL for that if the result is nullable there is a helper macro cql_is_nullable_true that tells us if the value is not null and trueif the result is not nullable we can use expr_value.ptr directlywe make a note that we're in a loop (this matters for statement cleanup, more on that later)we recurse to do more statements with cg_stmt_listfinally we end the for that we began This kind of structure is common to all the control flow cases. Generally, we have to deal with the fact that CQL expressions often become C statements so we use a more general flow control strategy. But with this in mind, it's easy to imagine how the IF, LOOP, and SWITCH switch statements are handled. "},{"title":"Cleanup and Errors​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#cleanup-and-errors","content":"There are a number of places where things can go wrong when running a CQL procedure. The most common sources are: (1) SQLite APIs, almost all of which can fail, and, (2) calling other procedures which also might fail. Here's a very simple example: DECLARE PROC something_that_might_fail (arg TEXT) USING TRANSACTION; CREATE PROC p () BEGIN LET arg := &quot;test&quot;; CALL something_that_might_fail(arg); END;  Which generates: cql_string_literal(_literal_1_test_p, &quot;test&quot;); CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; cql_string_ref arg = NULL; cql_set_string_ref(&amp;arg, _literal_1_test_p); _rc_ = something_that_might_fail(_db_, arg); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_string_release(arg); return _rc_; }  Let's look at those fragments carefully: first, we had to declare something_that_might_fail the declaration included USING TRANSACTION indicating the procedure uses the databasewe didn't provide the procedure definition, this is like an extern ... foo(...); declaration there is a string literal named _literal_1_test_p that is auto-created cql_string_literal can expand into a variety of things, whatever you want &quot;make a string literal&quot; to meanit's defined in cqlrt.h and it's designed to be replaced cql_set_string_ref(&amp;arg, _literal_1_test_p); is expected to &quot;retain&quot; the string (+1 ref count)cql_cleanup is the exit label, this cleanup code will run on all exit paths cleanup statements are accumulated by writing to cg_cleanup_output which usually writes to the proc_cleanup bufferbecause cleanup is in its own buffer you can add to it freely whenever a new declaration that requires cleanup arisesin this case the declaration of the string variable caused the C variable arg to be created and also the cleanup code now we call something_that_might_fail passing it our database pointer and the argumentthe hidden _db_ pointer is passed to all procedures that use the databasethese procedures are also the ones that can failany failed return code (not SQLITE_OK) causes two things: the cql_error_trace() macro is invoked (this macro typically expands to nothing)the code is redirected to the cleanup block via goto cql_cleanup; The essential sequence is this one:  if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; }  The C code generator consistently uses this pattern to check if anything went wrong and to exit with an error code. Extensive logging can be very expensive, but in debug builds it's quite normal for cql_error_trace to expand into something like fprintf(stderr, &quot;error %d in %s %s:%d\\n&quot;, _rc_, _PROC_, __FILE__, __LINE_) which is probably a lot more logging than you want in a production build but great if you're debugging. Recall that CQL generates something like #define _PROC_ &quot;p&quot; before every procedure. This error pattern generalizes well and indeed if we use the exception handling pattern, we get a lot of control. Let's generalize this example a tiny bit: CREATE PROC p (OUT success BOOL NOT NULL) BEGIN LET arg := &quot;test&quot;; BEGIN TRY CALL something_that_might_fail(arg); SET success := 1; END TRY; BEGIN CATCH SET success := 0; END CATCH; END;  CQL doesn't have complicated exception objects or anything like that, exceptions are just simple control flow. Here's the code for the above: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, cql_bool *_Nonnull success) { cql_contract_argument_notnull((void *)success, 1); cql_code _rc_ = SQLITE_OK; cql_string_ref arg = NULL; *success = 0; // set out arg to non-garbage cql_set_string_ref(&amp;arg, _literal_1_test_p); // try { _rc_ = something_that_might_fail(_db_, arg); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto catch_start_1; } *success = 1; goto catch_end_1; } catch_start_1: { *success = 0; } catch_end_1:; _rc_ = SQLITE_OK; cql_string_release(arg); return _rc_; }  The code in this case is nearly the same as the previous example. Let's look at the essential differences: If there is an error, goto catch_start_1 will runIf the try block succeeds, goto catch_end_1 will runboth the TRY and CATCH branches set the success out parametersince an out argument was added, CQL generated an error check to ensure that success is not null cql_contract_argument_notnull((void *)success, 1), the 1 means &quot;argument 1&quot; and will appear in the error message if this test failsthe hidden _db_ argument doesn't count for error message purposes, so success is still the first argument How does this happen? Let's look at cg_trycatch_helper which does this work: // Very little magic is needed to do try/catch in our context. The error // handlers for all the sqlite calls check _rc_ and if it's an error they // &quot;goto&quot; the current error target. That target is usually CQL_CLEANUP_DEFAULT_LABEL. // Inside the try block, the cleanup handler is changed to the catch block. // The catch block puts it back. Otherwise, generate nested statements as usual. static void cg_trycatch_helper(ast_node *try_list, ast_node *try_extras, ast_node *catch_list) { CHARBUF_OPEN(catch_start); CHARBUF_OPEN(catch_end); // We need unique labels for this block ++catch_block_count; bprintf(&amp;catch_start, &quot;catch_start_%d&quot;, catch_block_count); bprintf(&amp;catch_end, &quot;catch_end_%d&quot;, catch_block_count); // Divert the error target. CSTR saved_error_target = error_target; bool_t saved_error_target_used = error_target_used; error_target = catch_start.ptr; error_target_used = 0; ...  The secret is the error_target global variable. All of the error handling will emit a goto error_target statement. The try/catch pattern simply changes the current error target. The rest of the code in the helper is just to save the current error target and to create unique labels for the try/catch block. The important notion is that, if anything goes wrong, whatever it is, the generator simply does a goto error_target and that will either hit the catch block or else go to cleanup. The THROW operation illustrates this well: // Convert _rc_ into an error code. If it already is one keep it. // Then go to the current error target. static void cg_throw_stmt(ast_node *ast) { Contract(is_ast_throw_stmt(ast)); bprintf(cg_main_output, &quot;_rc_ = cql_best_error(%s);\\n&quot;, rcthrown_current); bprintf(cg_main_output, &quot;goto %s;\\n&quot;, error_target); error_target_used = 1; rcthrown_used = 1; }  first we make sure _rc_ has some kind of error in it, either rcthrown_current or else SQLITE_ERRORthen we go to the current error targeterror_target_used tracks whether if the error label was used, this is just to avoid C compiler errors about unused labels. if the label is not used it won't be emittedthe code never jumps back to an error label, so we'll always know if the label was used before we need to emit it Note: every catch block captures the value of _rc_ in a local variable whose name is in rcthrown_current. This captured value is the current failing result code accessible by @RC in CQL. A catch block can therefore do stuff like: IF @RC = 1 THEN THROW; ELSE call attempt_retry(); END IF;  This entire mechanism is built with basically just a few state variables that nest. There is no complicated stack walking or anything like that. All the code has to do is chain the error labels together and let users create new catch blocks with new error labels. All that together gives you very flexible try/catch behaviour with very little overhead. "},{"title":"String Literals​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#string-literals","content":"Before we move on to more complex statements we have to discuss string literals a little bit. We've mentioned before that the compiler is going to generate something like this: cql_string_literal(_literal_1_test_p, &quot;test&quot;);  To create a reference counted object _literal_1_test_p that it can use. Now we're going to talk about how the text &quot;test&quot; was created and how that gets more complicated. The first thing to remember is that the generator creates C programs. That means no matter what kind of literal we might be processing it's ending up encoded as a C string for the C compiler. The C compiler will be the first thing the decodes the text the generator produces and puts the byte we need into the final programs data segment or wherever. That means if we have SQL format strings that need to go to SQLite they will be twice-encoded, the SQL string is escaped as needed for SQLite and that is escaped again for the C compiler. An example might make this clearer consider the following SQL:  SELECT '&quot;x''y&quot;' AS a, &quot;'y'\\n&quot; AS b;  The generated text for this statement will be:  &quot;SELECT '\\&quot;x''y\\&quot;', '''y''\\n'&quot;  Let's review that in some detail: the first string &quot;a&quot; is a standard SQL string it is represented unchanged in the AST, it is not unescapedeven the outer single quotes are preserved, CQL has no need to change it at allwhen we emit it into our output it will be read by the C compiler, soat that time it is escaped again into C format the double quotes which required no escaping in SQL become \\&quot; the single quote character requires no escape but there are still two of them because SQLite will also process this string the second string &quot;b&quot; is a C formatted string literal SQLite doesn't support this format or its escapes, thereforeas discussed in Part 1, it is decoded to plain text, then re-encoded as a SQL escaped stringinternal newlines do not require escaping in SQL, they are in the string as the newline character not '\\n' or anything like that to be completely precise the byte value 0x0a is in the string unescaped internal single quotes don't require escaping in C, these have to be doubled in a SQL stringthe outer double quotes are removed and replaced by single quotes during this processthe AST now has a valid SQL formatted string possibly with weird characters in itas before, this string has to be formatted for the C compiler so now it has to be escaped againthe single quotes require no further processing, though now there are quite a few of themthe embedded newline is converted to the escape sequence &quot;\\n&quot; so we're back to sort of where we started the C compiler will convert this back to the byte 0x0a which is what ends up in the data segment In the above example we were making one overall string for the SELECT statement so the outer double quotes are around the whole statement. That was just for the convenience of this example. If the literals had been in some other loose context then individual strings would be produced the same way. Except, not so fast, not every string literal is heading for SQLite. Some are just making regular strings. In that case even if they are destined for SQLite they will go as bound arguments to a statement not in the text of the SQL. That means those strings do not need SQL escaping. Consider:  LET a := '&quot;x''y&quot;'; LET b := &quot;'y'\\n&quot;;  To do those assignments we need: cql_string_literal(_literal_1_x_y_p, &quot;\\&quot;x'y\\&quot;&quot;); cql_string_literal(_literal_2_y_p, &quot;'y'\\n&quot;);  In both of these cases the steps are: unescape the escaped SQL string in the AST to plain text removing the outer single quotes of course re-escape the plain text (which might include newlines and such) as a C string emit that text, including its outer double quotes Trivia: the name of the string literal variables include a fragment of the string to make them a little easier to spot. encoders.h has the encoding functions cg_decode_string_literalcg_encode_string_literalcg_encode_c_string_literalcg_decode_c_string_literal As well as similar functions for single characters to make all this possible. Pretty much every combination of encoding and re-encoding happens in some path through the code generator. "},{"title":"Executing SQLite Statements​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#executing-sqlite-statements","content":"By way of example let's consider a pretty simple piece of SQL we might want to run. CREATE TABLE foo(id INTEGER, t TEXT); CREATE PROC p (id_ INTEGER, t_ TEXT) BEGIN UPDATE foo SET t = t_ WHERE id = id_; END;  To make this happen we're going to have to do the following things: create a string literal with the statement we needthe references to id_ and t_ have to be replaced with ?we prepare that statementwe bind the values of id_ and t_we step the statementwe finalize the statementsuitable error checks have to be done at each stage That's quite a bit of code and it's easy to forget a step, this is an area where CQL shines. The code we had to write in CQL was very clear and all the error checking is implicit. This is the generated code. We'll walk through it and discuss how it is created. CQL_WARN_UNUSED cql_code p( sqlite3 *_Nonnull _db_, cql_nullable_int32 id_, cql_string_ref _Nullable t_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *_temp_stmt = NULL; _rc_ = cql_prepare(_db_, &amp;_temp_stmt, &quot;UPDATE foo &quot; &quot;SET t = ? &quot; &quot;WHERE id = ?&quot;); cql_multibind(&amp;_rc_, _db_, &amp;_temp_stmt, 2, CQL_DATA_TYPE_INT32, &amp;id_, CQL_DATA_TYPE_STRING, t_); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(_temp_stmt); if (_rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } cql_finalize_stmt(&amp;_temp_stmt); _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;_temp_stmt); return _rc_; }  the functions signature includes the hidden _db_ parameter plus the two argumentswe need a hidden _rc_ variable to hold the result codes from SQLitewe need a scratch sqlite3_stmt * named _temp_stmt to talk to SQLite when this is created, the cleanup section gets cql_finalize_stmt(&amp;_temp_stmt);cql_finalize_stmt sets the statement to null and does nothing if it's already null the string &quot;INSERT INTO foo(id, t) VALUES(?, ?)&quot; is created from the AST recall that we have variables_callback as an option, it's used here to track the variables and replace them with ?more on this shortly cql_multibind is used to bind the values of id_ and t_ this is just a varargs version of the normal SQLite binding functions, it's only done this way to save spaceonly one error check is needed for any binding failurethe type of binding is encoded very economicallythe &quot;2&quot; here refers to two arguments the usual error processing happens with cql_error_trace and goto cql_cleanupthe statement is executed with sqlite3_steptemporary statements are finalized immediately with cql_finalize_stmt in this case its redundant because the code is going to fall through to cleanup anywayin general there could be many statements and we want to finalize immediatelythis is an optimization opportunity, procedures with just one statement are very common Most of these steps are actually hard coded. There is no variability in the sequence after the multibind call, so that's just boiler-plate the compiler can inject. We don't want to declare _temp_stmt over and over so there's a flag that records whether it has already been declared in the current procedure. // Emit a declaration for the temporary statement _temp_stmt_ if we haven't // already done so. Also emit the cleanup once. static void ensure_temp_statement() { if (!temp_statement_emitted) { bprintf(cg_declarations_output, &quot;sqlite3_stmt *_temp_stmt = NULL;\\n&quot;); bprintf(cg_cleanup_output, &quot; cql_finalize_stmt(&amp;_temp_stmt);\\n&quot;); temp_statement_emitted = 1; } }  This is a great example of how, no matter where the processing happens to be, the generator can emit things into the various sections. Here it adds a declaration and an cleanup with no concern about what else might be going on. So most of the above is just boiler-plate, the tricky part is: getting the text of the SQLbinding the variables All of this is the business of this function: // This is the most important function for sqlite access; it does the heavy // lifting of generating the C code to prepare and bind a SQL statement. // If cg_exec is true (CG_EXEC) then the statement is executed immediately // and finalized. No results are expected. To accomplish this we do the following: // * figure out the name of the statement, either it's given to us // or we're using the temp statement // * call get_statement_with_callback to get the text of the SQL from the AST // * the callback will give us all the variables to bind // * count the variables so we know what column numbers to use (the list is backwards!) // * if CG_EXEC and no variables we can use the simpler sqlite3_exec form // * bind any variables // * if there are variables CG_EXEC will step and finalize static void cg_bound_sql_statement(CSTR stmt_name, ast_node *stmt, int32_t cg_flags) { ... }  The core of this function looks like this:  gen_sql_callbacks callbacks; init_gen_sql_callbacks(&amp;callbacks); callbacks.variables_callback = cg_capture_variables; callbacks.variables_context = &amp;vars; // ... more flags CHARBUF_OPEN(temp); gen_set_output_buffer(&amp;temp); gen_statement_with_callbacks(stmt, &amp;callbacks);  It's set up the callbacks for variables and it calls the echoing function on the buffer. We've talked about gen_statement_with_callbacks in Part 1. Let's take a look at that callback function: // This is the callback method handed to the gen_ method that creates SQL for us // it will call us every time it finds a variable that needs to be bound. That // variable is replaced by ? in the SQL output. We end up with a list of variables // to bind on a silver platter (but in reverse order). static bool_t cg_capture_variables(ast_node *ast, void *context, charbuf *buffer) { list_item **head = (list_item**)context; add_item_to_list(head, ast); bprintf(buffer, &quot;?&quot;); return true; }  The context variable was set to be vars, we convert it back to the correct type and add the current ast to that list. add_item_to_list always puts things at the head so the list will be in reverse order. With this done, we're pretty much set. We'll produce the statement with a sequence like this one (there are a couple of variations, but this is the most general)  bprintf(cg_main_output, &quot;_rc_ = cql_prepare(_db_, %s%s_stmt,\\n &quot;, amp, stmt_name); cg_pretty_quote_plaintext(temp.ptr, cg_main_output, PRETTY_QUOTE_C | PRETTY_QUOTE_MULTI_LINE); bprintf(cg_main_output, &quot;);\\n&quot;);  cg_pretty_quote_plaintext is one of the C string encoding formats, it could have been just the regular C string encoding but that would have been a bit wasteful and it wouldn't have looked as nice. This function does a little transform. The normal echo of the update statement in question looks like this:  UPDATE foo SET t = ? WHERE id = ?;  Note that it has indenting and newlines embedded in it. The standard encoding of that would look like this: &quot; UPDATE foo\\n SET t = ?\\n WHERE id = ?;&quot;  That surely works, but it's wasteful and ugly. The pretty format instead produces:  &quot;UPDATE foo &quot; &quot;SET t = ? &quot; &quot;WHERE id = ?&quot;  So, the newlines are gone from the string (they aren't needed), instead the string literal was broken into lines for readability. The indenting is gone from the string, instead the string fragments are indented. So what you get is a string literal that reads nicely but doesn't have unnecessary whitespace for SQLite. Obviously you can't use pretty-quoted literals in all cases, it's exclusively for SQLite formatting. All that's left to do is bind the arguments. Remember that arg list is in reverse order:  uint32_t count = 0; for (list_item *item = vars; item; item = item-&gt;next, count++) ; // ... reverse_list(&amp;vars); if (count) { bprintf(cg_main_output, &quot;cql_multibind(&amp;_rc_, _db_, %s%s_stmt, %d&quot;, amp, stmt_name, count); // Now emit the binding args for each variable for (list_item *item = vars; item; item = item-&gt;next) { Contract(item-&gt;ast-&gt;sem-&gt;name); bprintf(cg_main_output, &quot;,\\n &quot;); cg_bind_column(item-&gt;ast-&gt;sem-&gt;sem_type, item-&gt;ast-&gt;sem-&gt;name); } bprintf(cg_main_output, &quot;);\\n&quot;); }  first compute the count, we don't need to bind if there are no variablesreverse_list does exactly what is sounds like (finally a real-world use-case for reverse-list-in-place)cg_bind_column creates one line of the var-args output: column type and variable name the type and name information is right there on the AST in the sem_node And that's it. With those few helpers we can bind any SQLite statement the same way. All of theDDL_STMT_INIT and DML_STMT_INIT statements are completely implemented by this path. "},{"title":"Reading Single Values​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#reading-single-values","content":"In many cases you need just one value CREATE PROC p (id_ INTEGER NOT NULL, OUT t_ TEXT) BEGIN SET t_ := ( SELECT t FROM foo WHERE id = id_ ); END;  This is going to be very similar to the examples we've seen so far: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, cql_int32 id_, cql_string_ref _Nullable *_Nonnull t_) { cql_contract_argument_notnull((void *)t_, 2); cql_code _rc_ = SQLITE_OK; cql_string_ref _tmp_text_0 = NULL; sqlite3_stmt *_temp_stmt = NULL; *(void **)t_ = NULL; // set out arg to non-garbage _rc_ = cql_prepare(_db_, &amp;_temp_stmt, &quot;SELECT t &quot; &quot;FROM foo &quot; &quot;WHERE id = ?&quot;); cql_multibind(&amp;_rc_, _db_, &amp;_temp_stmt, 1, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, id_); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(_temp_stmt); if (_rc_ != SQLITE_ROW) { cql_error_trace(); goto cql_cleanup; } cql_column_string_ref(_temp_stmt, 0, &amp;_tmp_text_0); cql_finalize_stmt(&amp;_temp_stmt); cql_set_string_ref(&amp;*t_, _tmp_text_0); _rc_ = SQLITE_OK; cql_cleanup: cql_string_release(_tmp_text_0); cql_finalize_stmt(&amp;_temp_stmt); return _rc_; }  _db_ : incoming arg for a procedure that uses the database same as always, check*(void **)t_ = NULL; : out args are always set to NULL on entry, note, there is no release here argument is assumed to be garbage, that's the ABIif argument is non-garbage caller must release it first, that's the ABI _rc_ : same as always, check_tmp_text_0 : new temporary text, including cleanup (this could have been avoided)_temp_stmt : as before, including cleanupcql_prepare : same as always, checkcql_multibind : just one integer bound this timesqlite3_step : as before, we're stepping once, this time we want the dataif (_rc_ != SQLITE_ROW) new error check and goto cleanup if no row this is the same as the IF NOTHING THROW variant of construct, that's the default cql_column_string_ref : reads one string from _temp_stmtcql_finalize_stmt : as beforecql_set_string_ref(&amp;*t_, _tmp_text_0) : copy the temporary string to the out arg includes retain, out arg is NULL so cql_set_string_ref will do no releaseif this were (e.g.) running in a loop, the out arg would not be null and there would be a release, as expectedif something else had previously set the out arg, again, there would be a release as expected There are variations of this form such as: CREATE PROC p (id_ INTEGER NOT NULL, OUT t_ TEXT) BEGIN SET t_ := ( SELECT t FROM foo WHERE id = id_ IF NOTHING ''); END;  This simply changes the handling of the case where there is no row. The that part of the code ends up looking like this:  if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (_rc_ == SQLITE_ROW) { cql_column_string_ref(_temp_stmt, 0, &amp;_tmp_text_1); cql_set_string_ref(&amp;_tmp_text_0, _tmp_text_1); } else { cql_set_string_ref(&amp;_tmp_text_0, _literal_1_p); }  any error code leads to cleanupSQLITE_ROW : leads to the same fetch as beforeSQLITE_DONE : leads to the no row case which sets _tmp_text_0 to the empty string cql_string_literal(_literal_1_p, &quot;&quot;); is included as a data declaration There is also the IF NOTHING OR NULL variant which is left as an exercise to the reader. You can find all the flavors in cg_c.c in the this function: // This is a nested select expression. To evaluate we will // * prepare a temporary to hold the result // * generate the bound SQL statement // * extract the exactly one argument into the result variable // which is of exactly the right type // * use that variable as the result. // The helper methods take care of sqlite error management. static void cg_expr_select(...  This handles all of the (select ...) expressions and it has the usual expression handler syntax. Another great example of a CQL expressions that might require many C statements to implement. "},{"title":"Reading Rows With Cursors​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#reading-rows-with-cursors","content":"This section is about the cases where we are expecting results back from SQLite. By results here I mean the results of some kind of query, not like a return code. SQLite does this by giving you a sqlite3_stmt * which you can then use like a cursor to read out a bunch of rows. So it should be no surprise that CQL cursors map directly to SQLite statements. Most of the code to get a statement we've already seen before, we only saw the _temp_stmtcase and we did very little with it. Let's look at the code for something a little bit more general and we'll see how little it takes to generalize. First, let's look at how a CQL cursor is initialized: CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; END;  Now in this case there can only be one row in the result, but it would be no different if there were more. Here's the C code: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; cql_bool _C_has_row_ = 0; _rc_ = cql_prepare(_db_, &amp;C_stmt, &quot;SELECT 1, 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); return _rc_; }  Let's look over that code very carefully and see what is necessary to make it happen. _db_ : incoming arg for a procedure that uses the database same as always, check_rc_ : same as always, checkC_stmt : we need to generate this instead of using _temp_stmt cql_finalize_stmt(&amp;C_stmt) in cleanup, just like _temp_stmt cql_prepare : same as always, checkcql_multibind : could have been binding, not none needed here, but same as always anyway, checkno step, no finalize (until cleanup) : that boiler-plate is removed And that's it, we now have a statement in C_stmt ready to go. We'll see later that _C_has_row_will track whether or not the cursor has any data in it. How do we make this happen? Well you could look at cg_declare_cursor and your eye might hurt at first. The truth is there are many kinds of cursors in CQL and this method handles all of them. We're going to go over the various flavors but for now we're only discussing the so-called &quot;statement cursor&quot;, so named because it simply holds a SQLite statement. This was the first, and for a while only, type of cursor added to the CQL language. OK so how do we make a statement cursor. It's once again cg_bound_sql_statement just like so: cg_bound_sql_statement(cursor_name, select_stmt, CG_PREPARE|CG_MINIFY_ALIASES);  The entire difference is that the first argument is the cursor name rather than NULL. If you pass NULL it means use the temporary statement. And you'll notice that even in this simple example the SQLite text was altered a bit: the text that went to SQLite was &quot;SELECT 1, 2&quot; -- that's CG_MINIFY_ALIASES at work. SQLite didn't need to see those column aliases, it makes no difference in the result. Column aliases are often long and numerous. Even in this simple example we saved 4 bytes. But the entire query was only 12 bytes long (including trailing null) so that's 25%. It's not a huge savings in general but it's something. The other flag CG_PREPARE tells the binder that it should not step or finalize the query. The alternative is CG_EXEC (which was used in the previous section for the UPDATE example). "},{"title":"Fetching Data From Cursors​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#fetching-data-from-cursors","content":"The first cursor reading primitive that was implemented as FETCH [cursor] INTO [variables] and it's the simplest to understand so let's start there. We change the example just a bit: CREATE PROC p () BEGIN DECLARE x INTEGER NOT NULL; DECLARE y INTEGER NOT NULL; DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; FETCH C INTO x, y; END;  For simplicity I will only include the code that is added. The rest is the same.  cql_int32 x = 0; cql_int32 y = 0; // same as before _rc_ = sqlite3_step(C_stmt); _C_has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; }  Do to the FETCH we do the following: step the cursorset the _C_has_row_ variable so to indicate if we got a row or notuse the varargs cql_multifetch to read 2 columns from the cursor this helper simply uses the usual sqlite3_*_column functions to read the data outagain, we do it this way so that there is less error checking needed in the generated codealso, there are fewer function calls so the code is overall smallertrivia: multibind and multifetch are totally references to The Fifth Element hence, they should be pronounced like Leeloo saying &quot;multipass&quot; multifetch uses the varargs to clobber the contents of the target variables if there is no row according to _rc_multifetch uses the CQL_DATA_TYPE_NOT_NULL to decide if it should ask SQLite first if the column is null So now this begs the question, in the CQL, how do you know if a row was fetched or not? The answer is, you can use the cursor name like a boolean. Let's complicate this up a little more. DECLARE PROCEDURE printf NO CHECK; CREATE PROC p () BEGIN DECLARE x INTEGER NOT NULL; DECLARE y INTEGER NOT NULL; DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; FETCH C INTO x, y; WHILE C BEGIN CALL printf(&quot;%d, %d\\n&quot;, x, y); FETCH C INTO x, y; END; END;  Again here is what is now added, we've seen the WHILE pattern before:  for (;;) { if (!(_C_has_row_)) break; printf(&quot;%d, %d\\n&quot;, x, y); _rc_ = sqlite3_step(C_stmt); _C_has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } }  So, if you use the cursors name in an ordinary expression that is converted to a reference to the boolean _C_has_row_. Within the loop we're going to print some data and then fetch the next row. The internal fetch is of course the same as the first. The next improvement that was added to the language was the LOOP statement. Let's take a look: BEGIN DECLARE x INTEGER NOT NULL; DECLARE y INTEGER NOT NULL; DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; LOOP FETCH C INTO x, y BEGIN CALL printf(&quot;%d, %d\\n&quot;, x, y); END; END;  The generated code is very similar:  for (;;) { _rc_ = sqlite3_step(C_stmt); _C_has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (!_C_has_row_) break; printf(&quot;%d, %d\\n&quot;, x, y); }  This is done by: emit the for (;;) { to start the loopgenerate the FETCH just as if it was standaloneemit if (!_C_has_row_) break; (with the correct cursor name)use cg_stmt_list to emit the internal statement list (CALL printf in this case)close the loop with } and we're done "},{"title":"Cursors With Storage​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#cursors-with-storage","content":"We now come to the big motivating reasons for having the notion of shapes in the CQL language. This particular case was the first such example in the language and it's very commonly used and saves you a lot of typing. Like the other examples it's only sugar in that it doesn't give you any new language powers you didn't have, but it does give clarity and maintenance advantages. And it's just a lot less to type. Let's go back to one of the earlier examples, but write it the modern way: CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; FETCH C; END;  And the generated C code: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 x; cql_int32 y; } p_C_row; CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; p_C_row C = { 0 }; _rc_ = cql_prepare(_db_, &amp;C_stmt, &quot;SELECT 1, 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); return _rc_; }  Let's look at what's different here: struct p_C_row has been created, it contains: _has_row_ for the cursorx and y the data fields_refs_count the number of reference fields in the cursor (0 in this case)_refs_offset the offset of the references fields (they always go at the end)because the references are together a cursor with lots of reference fields can be cleaned up easily in the generated code the variable C refers to the current data that has been fetched convenient for debugging p C in lldb shows you the row references to x and y became C.x and C.yreferences to _C_has_row_ became C._has_row_ That's pretty much it. The beauty of this is that you can't get the declarations of your locals wrong and you don't have to list them all no matter how big the data is. If the data shape changes the cursor change automatically changes to accommodate it. Everything is still statically typed. Now lets look at the loop pattern: CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; LOOP FETCH C BEGIN CALL printf(&quot;%d, %d\\n&quot;, C.x, C.y); END; END;  Note that the columns of the cursor were defined by the column aliases of the SELECT.  for (;;) { _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (!C._has_row_) break; printf(&quot;%d, %d\\n&quot;, C.x, C.y); }  The loop is basically the same except x and y have been replaced with C.x and C.yand again _C_has_row_ is now C._has_row_. The code generator knows that it should allocate storage for the C cursor if it has the flag SEM_TYPE_HAS_SHAPE_STORAGE on it. The semantic analyzer adds that flag if it ever finds FETCH C with no INTO part. Finally let's look at an example with cleanup required. We'll just change the test case a tiny bit. CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, &quot;2&quot; AS y; LOOP FETCH C BEGIN CALL printf(&quot;%d, %s\\n&quot;, C.x, C.y); END; END;  The x column is now text. We'll get this code which will be studied below: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_C_row; #define p_C_refs_offset cql_offsetof(p_C_row, x) // count = 1 CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset }; _rc_ = cql_prepare(_db_, &amp;C_stmt, &quot;SELECT '1', 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } for (;;) { _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_STRING, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (!C._has_row_) break; cql_alloc_cstr(_cstr_1, C.x); printf(&quot;%s, %d\\n&quot;, _cstr_1, C.y); cql_free_cstr(_cstr_1, C.x); } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); cql_teardown_row(C); return _rc_; }  It's very similar to what we had before, let's quickly review the differences. typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_C_row; #define p_C_refs_offset cql_offsetof(p_C_row, x) // count = 1  x is now cql_string_ref _Nonnull x; rather than cql_int32x has moved to the end (because it's a reference type)the offset of the first ref is computed in a constant Recall the reference types are always at the end and together.  p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset };  p_C_row is now initialized to to ref count 1 and refs offset p_C_refs_offset defined above  cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_STRING, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y);  C.x is now of type string  cql_alloc_cstr(_cstr_1, C.x); printf(&quot;%s, %d\\n&quot;, _cstr_1, C.y); cql_free_cstr(_cstr_1, C.x);  C.x has to be converted to a C style string before it can be used with printf as a %s argument  cql_teardown_row(C);  the cleanup section has to include code to teardown the cursor, this will release all of its reference variables in bulk remember we know the count, and the offset of the first one -- that's all we need to do them all With these primitives we can easily create cursors of any shape and load them up with data. We don't have to redundantly declare locals that match the shape of our select statements which is both error prone and a hassle. All of this is actually very easy for the code-generator. The semantic analysis phase knows if the cursor needs shape storage. And it also recognizes when a variable reference like C.x happens, the variable references are re-written in the AST so that the code-generator doesn't even have to know there was a cursor reference, from its perspective the variable IS C.x (which it sort of is). The code generator does have to create the storage for the cursor but it knows it should do so because the cursor variable is marked with SEM_TYPE_HAS_SHAPE_STORAGE. A cursor without this marking only gets its statement (but not always as we'll see later) and its _cursor_has_row_hidden variable. "},{"title":"Flowing SQLite Statements Between Procedures​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#flowing-sqlite-statements-between-procedures","content":"Earlier we saw that we can get a cursor from a SQLite SELECT statement. The cursor is used to iterate over the sqlite3_stmt * that SQLite provides to us. This process can be done between procedures. Here's a simple example: @ATTRIBUTE(cql:private) CREATE PROC q () BEGIN SELECT &quot;1&quot; AS x, 2 AS y; END;  This is the first example of a procedure that return a result set that we've seen. The wiring for this is very simple. static CQL_WARN_UNUSED cql_code q( sqlite3 *_Nonnull _db_, sqlite3_stmt *_Nullable *_Nonnull _result_stmt) { cql_code _rc_ = SQLITE_OK; *_result_stmt = NULL; _rc_ = cql_prepare(_db_, _result_stmt, &quot;SELECT '1', 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: if (_rc_ == SQLITE_OK &amp;&amp; !*_result_stmt) _rc_ = cql_no_rows_stmt(_db_, _result_stmt); return _rc_; }  First note that there are now two hidden parameters to q: _db_ : the database pointer as usual,_result_stmt : the statement produced by this procedure The rest of the code is just like any other bound SQL statement. Note that if_result_stmt isn't otherwise set by the code it will be initialized to a statement that will return zero rows. All of this is pretty much old news except for the new hidden variable. Note let's look how we might use this. We can write a procedure that calls q, like so: CREATE PROC p () BEGIN DECLARE C CURSOR FOR CALL q(); FETCH C; END;  This generates: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset }; _rc_ = q(_db_, &amp;C_stmt); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_STRING, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); cql_teardown_row(C); return _rc_; }  All of the above is exactly the same as the previous cases where we got data from the database except that instead of using cql_prepare the compiler produced _rc_ = q(_db_, &amp;C_stmt); That function call gives us, of course, a ready-to-use sqlite3_stmt * which we can then step, and use to fetch values. The shape of the cursor C is determined by the result type of procedure q -- hence they always match. If q was in some other module, it could be declared with: DECLARE PROC q () (x TEXT NOT NULL, y INTEGER NOT NULL);  This is a procedure that takes no arguments and returns a result with the indicated shape. CQL can generate this declaration for you if you add --generate_exports to the command line. Note that in this case q was marked with @attribute(cql:private) which caused q to be staticin the output. Hence it can't be called outside this translation unit and --generate_exportswon't provide the declaration. If the private annotation were removed, the full exports for this file would be: DECLARE PROC q () (x TEXT NOT NULL, y INTEGER NOT NULL); DECLARE PROC p () USING TRANSACTION;  And these would allow calling both procedures from elsewhere. Simply #include the exports file. There is a special function in the echoing code that can emit a procedure that was created in the form that is needed to declare it, this is gen_declare_proc_from_create_proc. "},{"title":"Value Cursors​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#value-cursors","content":"Once CQL had the ability to fetch rows into a cursor with no need to declare all the locals it was clear that it could benefit from the ability to save a copy of any given row. That is basic cursor operations seemed like they should be part of the calculus of CQL. Here's a simple sample program that illustrates this. CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT &quot;1&quot; AS x, 2 AS y; FETCH C; DECLARE D CURSOR LIKE C; FETCH D from C; END;  We already have a good idea what is going to happen with C in this program. Let's look at the generated code focusing just on the parts that involve D. First there is a row defintion for D. Unsurprisingly it is exactly the samea as the one for C. This must be the case since we specified D CURSOR LIKE C. typedef struct p_D_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_D_row; #define p_D_refs_offset cql_offsetof(p_D_row, x) // count = 1  Then the D cursor variables will be needed: p_D_row D = { ._refs_count_ = 1, ._refs_offset_ = p_D_refs_offset };  The above also implies the cleanup code:  cql_teardown_row(D);  finally, we fetch D from C. That's just some assignments:  D._has_row_ = 1; cql_set_string_ref(&amp;D.x, C.x); D.y = C.y;  Importantly, there is no D_stmt variable. D is not a statement cursor like C, it's a so-called &quot;value&quot; cursor. In that it can only hold values. A value cursor can actually be loaded from anywhere, it just holds data. You don't loop over it (attempts to do so will result in errors). The general syntax for loading such a cursor is something like this:  FETCH D(x, y) FROM VALUES(C.x, C.y);  And indeed the form FETCH D FROM C was rewritten automatically into the general form. The short form is just sugar. Once loaded, D.x and D.y can be used as always. The data type of D is similar to C. The AST would report: {declare_cursor_like_name}: D: select: { x: text notnull, y: integer notnull } variable shape_storage value_cursor  meaning D has the flags SEM_TYPE_STRUCT, SEM_TYPE_VARIABLE, SEM_TYPE_HAS_SHAPE_STORAGE, and SEM_TYPE_VALUE_CURSOR. That last flag indicates that there is no statement for this cursor, it's just values. And all such cursors must haveSEM_TYPE_HAS_SHAPE_STORAGE -- if they had no statement and no storage they would be -- nothing. Value cursors are enormously helpful and there is sugar for loading them from all kinds of sources with a shape. These forms are described more properly in Chapter 5 of the Guide but they all end up going through the general form, making the codegen considerably simpler. There are many examples where the semantic analyzer rewrites a sugar form to a canonical form to keep the codegen from forking into dozens of special cases and most of them have to do with shapes and cursors. "},{"title":"Returning Value Cursors​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#returning-value-cursors","content":"Let's look at an example that is similar to the previous one: @ATTRIBUTE(cql:private) CREATE PROC q () BEGIN DECLARE C CURSOR LIKE SELECT &quot;1&quot; AS x, 2 AS y; FETCH C USING &quot;foo&quot; x, 3 y; OUT C; END; CREATE PROC p () BEGIN DECLARE C CURSOR FETCH FROM CALL q(); -- do something with C END;  Let's discuss some of what is above, first looking at q: DECLARE C CURSOR LIKE SELECT &quot;1&quot; AS x, 2 AS y; : this makes an empty value cursor note the shape is LIKE the indicated SELECT, the SELECT does not actually run FETCH ... USING : this form is sugar, it lets you put the column names x and y adjacent to the values but is otherwise equivalent to the canonical form FETCH C(x, y) FROM VALUES(&quot;foo&quot;, 3); is the canonical formcodegen only ever sees the canonical form OUT C is new, we'll cover this shortly Now let's look at the C for q typedef struct q_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } q_row; #define q_refs_offset cql_offsetof(q_row, x) // count = 1 cql_string_literal(_literal_1_foo_q, &quot;foo&quot;); typedef struct q_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } q_C_row; #define q_C_refs_offset cql_offsetof(q_C_row, x) // count = 1 static void q(q_row *_Nonnull _result_) { memset(_result_, 0, sizeof(*_result_)); q_C_row C = { ._refs_count_ = 1, ._refs_offset_ = q_C_refs_offset }; C._has_row_ = 1; cql_set_string_ref(&amp;C.x, _literal_1_foo_q); C.y = 3; _result_-&gt;_has_row_ = C._has_row_; _result_-&gt;_refs_count_ = 1; _result_-&gt;_refs_offset_ = q_refs_offset; cql_set_string_ref(&amp;_result_-&gt;x, C.x); _result_-&gt;y = C.y; cql_teardown_row(C); }  the _result_ variable is clobbed with zeros, it is assumed to be junk coming in if it had valid data, the caller is expected to use cql_teardown_row to clean it up before calling q_row : this is new, this is the structure type for the result of q it's exactly the same shape as Cit has its own q_refs_offset like other shapes q_C_row : this is the same old same old row structure for cursor Cstatic void q(q_row *_Nonnull _result_) : q now accepts a q_row to fill in! note that q does not have the _db_ parameter, it doesn't use the database!it is entirely possible to fill value cursors from non-database sources, e.g. constants, math, whatever C : the value cursor is declared as usualC.x and C.y are loaded, this resolves the FETCH statementthe _result_ fields are copied from C, this resolves the OUT statementC can be torn downthere is no cleanup label, there are no error cases, nothing can go wrong! The net of all this is that we have loaded a value cursor that was passed in to the procedure via a hidden argument and it has retained references as appropriate. Now let's look at p: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_C_row; #define p_C_refs_offset cql_offsetof(p_C_row, x) // count = 1 void p(void) { p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset }; cql_teardown_row(C); q((q_row *)&amp;C); // q_row identical to cursor type // usually you do something with C at this point cql_teardown_row(C); }  p_C_row : the cursor type for C in the procedure p is definedp_C_refs_offset : the refs offset for C in p as usualC = {...} : the usual initialization for a cursor with shape note that C is a value cursor, so it has no C_stmt cql_teardown_row(C) : releases any references in C, there are none this pattern is general purpose, the call to q might be in a loop or somethingin this instance the teardown here is totally redundant, but harmless q((q_row *)&amp;C) : fetch C by calling q p_C_row has been constructed to be exactly the same as q_row so this cast is safethere are no error checks because q can't fail! some code that would use C is absent for this sample, it would go where the comment isthe cleanup label is missing because there are no error cases, emitting the label would just cause warnings such warnings are often escalated to errors in production builds... cql_teardown_row(C) is needed as always, even though there is no cleanup label the teardown is in the cleanup sectionthe teardown was added as usual when C was declared So with just normal value cursor codegen we can pretty easily create a situation where procedures can move structures from one to another. As we saw, the source of value cursors may or may not be the database. Value cursors are frequently invaluable in test code as they can easily hold mock rows based on any kind of computation. "},{"title":"Result Sets​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#result-sets","content":"In addition to returning a single row into a value cursor, or returning a statement to consume with a statement cursor, it's possible to generate a result set. So far the samples have included @attribute(cql:private) to suppress that code. This pattern is intended to let regular C code access the data so private suppresses it. Let's consider a simple example, this example returns only one row but the mechanism works for any number of rows, we're just using this form because it's what we've used so far and its simple. Let's begin: CREATE PROC p () BEGIN SELECT &quot;1&quot; AS x, 2 AS y; END;  The core generated function is this one: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, sqlite3_stmt *_Nullable *_Nonnull _result_stmt) { cql_code _rc_ = SQLITE_OK; *_result_stmt = NULL; _rc_ = cql_prepare(_db_, _result_stmt, &quot;SELECT '1', 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: if (_rc_ == SQLITE_OK &amp;&amp; !*_result_stmt) _rc_ = cql_no_rows_stmt(_db_, _result_stmt); return _rc_; }  We've seen this before, it creates the SQLite statement. But that isn't all the code that was generated, let's have a look at what else we got in our outputs: CQL_WARN_UNUSED cql_code p_fetch_results( sqlite3 *_Nonnull _db_, p_result_set_ref _Nullable *_Nonnull result_set) { sqlite3_stmt *stmt = NULL; cql_profile_start(CRC_p, &amp;p_perf_index); cql_code rc = p(_db_, &amp;stmt); cql_fetch_info info = { .rc = rc, .db = _db_, .stmt = stmt, .data_types = p_data_types, .col_offsets = p_col_offsets, .refs_count = 1, .refs_offset = p_refs_offset, .encode_context_index = -1, .rowsize = sizeof(p_row), .crc = CRC_p, .perf_index = &amp;p_perf_index, }; return cql_fetch_all_results(&amp;info, (cql_result_set_ref *)result_set); }  p_fetch_results does two main things: cql_code rc = p(_db_, &amp;stmt) : it calls the underlying function to get the statementcql_fetch_all_results : it calls a standard helper to read all the results from the statement and put them into result_set to do the fetch, it sets up a cql_fetch_info for this result set, this has all the information needed to do the fetchthe intent here is that even a complex fetch with lots of columns can be done economically, andthe code that does the fetching is shared Let's look at the things that are needed to load up that info structure. typedef struct p_row { cql_int32 y; cql_string_ref _Nonnull x; } p_row; uint8_t p_data_types[p_data_types_count] = { CQL_DATA_TYPE_STRING | CQL_DATA_TYPE_NOT_NULL, // x CQL_DATA_TYPE_INT32 | CQL_DATA_TYPE_NOT_NULL, // y }; #define p_refs_offset cql_offsetof(p_row, x) // count = 1 static cql_uint16 p_col_offsets[] = { 2, cql_offsetof(p_row, x), cql_offsetof(p_row, y) };  p_row : the row structure for this result set, same as always, reference types lastp_data_types : an array with the data types encoded as bytesp_refs_offset : the offset of the first reference typep_col_offsets : this is the offset of each column within the row structure these are in column order, not offset order Code generation creates a .c file and a .h file, we haven't talked much about the .hbecause it's mostly prototypes for the functions in the .c file. But in this case we have a few more interesting things. We need just two of them: #define CRC_p -6643602732498616851L #define p_data_types_count 2  Now we're armed to discuss loading the info structure: .rc : the fetcher needs to know if p was successful, it won't read from the statement if it wasn't.db : the database handle, the fetcher needs this to call SQLite APIs.stmt : the statement that came from p that is to be enumerated.data_types : types of the columns, this tells the fetcher what columns to read to the statement in what order.col_offsets : the column offsets, this tells the fetcher were to store the column data within each row.refs_count : the number of references in the row, this is needed to tear down the rows in the result set when it is released.refs_offset : the first reference offset, as usual this tells the fetcher where the references that need to be released are.encode_context_index : it's possible to have sensitive fields encoded, this identifies an optional column that will be combined with the sensitive data . .rowsize : the size of p_row, this is needed to allocate rows in a growable buffer.crc : this is a CRC of the code of p, it's used to uniquely identify p economically, performance logging APIs typically use this CRC in a begin/end logging pair.perf_index : performance data for stored procedures is typically stored in an array of stats, CQL provides storage for the index for each procedure With this data (which is in the end pretty small) the cql_fetch_all_results can do all the things it needs to do: cql_profile_start has already been called, it can call cql_profile_end once the data is fetched cql_profile_start and _end do nothing by default, but those macros can be defined to log performance data however you like it can allocate a bytebuf with cql_bytebuf_open and then grow it with cql_bytebuf_alloc in the end all the rows are in one contiguous block of storage cql_multifetch_meta is used to read each row from the result set, it's similar to cql_multifetch the meta version uses data_types and column_offsets instead of varargs but is otherwise the samethe first member of the col_offsets array is the count of columns With this background, cql_fetch_all_results should be very approachable. There's a good bit of work but it's all very simple. // By the time we get here, a CQL stored proc has completed execution and there is // now a statement (or an error result). This function iterates the rows that // come out of the statement using the fetch info to describe the shape of the // expected results. All of this code is shared so that the cost of any given // stored procedure is minimized. Even the error handling is consolidated. cql_code cql_fetch_all_results( cql_fetch_info *_Nonnull info, cql_result_set_ref _Nullable *_Nonnull result_set) {...}  The core of that function looks like this:  ... cql_bytebuf_open(&amp;b); ... for (;;) { rc = sqlite3_step(stmt); if (rc == SQLITE_DONE) break; if (rc != SQLITE_ROW) goto cql_error; count++; row = cql_bytebuf_alloc(&amp;b, rowsize); memset(row, 0, rowsize); cql_multifetch_meta((char *)row, info); } ... cql_profile_stop(info-&gt;crc, info-&gt;perf_index); ...  cql_bytebuf_open : open the buffer, get ready to start appending rowssqlite3_step : keep reading while we get SQLITE_ROW, stop on SQLITE_DONEcql_bytebuf_alloc : allocate a new row in the buffermemset : zero the rowcql_multifetch_meta : read the data from the the statement into the rowcql_profile_stop : signals that processing is done and profiling can stopif all goes well, SQLITE_OK is returned as usual The remaining logic is largely about checking for errors and tearing down the result set if anything goes wrong. There is not very much to it, and it's worth a read. Now recall that the way cql_fetch_all_results was used, was as follows:  return cql_fetch_all_results(&amp;info, (cql_result_set_ref *)result_set)  And result_set was the out-argument for the the p_fetch_results method. So p_fetch_results is used to get that result set. But what can you do with it? Well, the result set contains copy of all the selected data, ready to use in with a C-friendly API. The interface is in the generated .h file. Let's look at that now, it's the final piece of the puzzle. #ifndef result_set_type_decl_p_result_set #define result_set_type_decl_p_result_set 1 cql_result_set_type_decl(p_result_set, p_result_set_ref); #endif extern cql_string_ref _Nonnull p_get_x(p_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 p_get_y(p_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 p_result_count(p_result_set_ref _Nonnull result_set); extern CQL_WARN_UNUSED cql_code p_fetch_results(sqlite3 *_Nonnull _db_, p_result_set_ref _Nullable *_Nonnull result_set); #define p_row_hash(result_set, row) cql_result_set_get_meta( \\ (cql_result_set_ref)(result_set))-&gt;rowHash((cql_result_set_ref)(result_set), row) #define p_row_equal(rs1, row1, rs2, row2) \\ cql_result_set_get_meta((cql_result_set_ref)(rs1))-&gt;rowsEqual( \\ (cql_result_set_ref)(rs1), \\ row1, \\ (cql_result_set_ref)(rs2), \\  cql_result_set_type_decl : declares p_result_set_ref to avoid being defined more than once, the declaration is protected by #ifndef result_set_type_decl_p_result_set p_get_x, p_get_y : allow access to the named fields of the result set at any given rowp_result_count : provides the count of rows in the result setp_fetch_results : the declaration of the fetcher (previously discussed)p_row_hash : provides a hash of any given row, useful for detecting changes between result setsp_row_equal : tests two rows in two results sets of the same shape for equality The getters are defined very simply: cql_string_ref _Nonnull p_get_x(p_result_set_ref _Nonnull result_set, cql_int32 row) { p_row *data = (p_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data[row].x; } cql_int32 p_get_y(p_result_set_ref _Nonnull result_set, cql_int32 row) { p_row *data = (p_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data[row].y; }  The p_row is exactly the right size, and of course the right shape, the final access looks something like data[row].x. Result Sets from the OUT statement​ Recalling this earlier example: CREATE PROC q () BEGIN DECLARE C CURSOR LIKE SELECT &quot;1&quot; AS x, 2 AS y; FETCH C USING &quot;foo&quot; x, 3 y; OUT C; END;  The original example had @attribute(cql:private) to suppress the result set, but normally a one-row result is is generated from such a method. The C API is almost identical. However, there count is always 0 or 1. The getters do not have the row number: extern cql_string_ref _Nonnull q_get_x(q_result_set_ref _Nonnull result_set); extern cql_int32 q_get_y(q_result_set_ref _Nonnull result_set);  The actual getters are nearly the same as well cql_string_ref _Nonnull q_get_x(q_result_set_ref _Nonnull result_set) { q_row *data = (q_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data-&gt;x; } cql_int32 q_get_y(q_result_set_ref _Nonnull result_set) { q_row *data = (q_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data-&gt;y; }  Basically data[row].x just became data-&gt;x and the rest is nearly the same. Virtually all the code for this is shared. You can find all this and more in cg_c.c by looking here: // If a stored procedure generates a result set then we need to do some extra work // to create the C friendly rowset creating and accessing helpers. If stored // proc &quot;foo&quot; creates a row set then we need to: // * emit a struct &quot;foo_row&quot; that has the shape of each row // * this isn't used by the client code but we use it in our code-gen // * emit a function &quot;foo_fetch_results&quot; that will call &quot;foo&quot; and read the rows // from the statement created by &quot;foo&quot;. // * this method will construct a result set object via cql_result_create and store the data // * the remaining functions use cql_result_set_get_data and _get_count to get the data back out // * for each named column emit a function &quot;foo_get_[column-name]&quot; which // gets that column out of the rowset for the indicated row number. // * prototypes for the above go into the main output header file static void cg_proc_result_set(ast_node *ast)  There are many variations in that function to handle the cases mentioned so far, but they are substantially similar to each other with a lot of shared code. There is one last variation we should talk about and that is the OUT UNION form. It is the most flexible of them all. Result Sets from the OUT UNION statement​ The OUT statement, allows the programmer to produce a result set that has exactly one row,OUT UNION instead accumulates rows. This is very much like writing your own fetcher procedure with your own logic. The data could come from the database, by, for instance, enumerating a cursor. Or it can come from some computation or a mix of both. Here's a very simple example: CREATE PROC q () BEGIN DECLARE C CURSOR LIKE SELECT 1 AS x; LET i := 0; WHILE i &lt; 5 BEGIN FETCH C(x) FROM VALUES(i); OUT UNION C; SET i := i + 1; END; END;  Let's look at the code for the above, it will be very similar to other examples we've seen so far: typedef struct q_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 x; } q_C_row; void q_fetch_results(q_result_set_ref _Nullable *_Nonnull _result_set_) { cql_bytebuf _rows_; cql_bytebuf_open(&amp;_rows_); *_result_set_ = NULL; q_C_row C = { 0 }; cql_int32 i = 0; cql_profile_start(CRC_q, &amp;q_perf_index); i = 0; for (;;) { if (!(i &lt; 5)) break; C._has_row_ = 1; C.x = i; cql_retain_row(C); if (C._has_row_) cql_bytebuf_append(&amp;_rows_, (const void *)&amp;C, sizeof(C)); i = i + 1; } cql_results_from_data(SQLITE_OK, &amp;_rows_, &amp;q_info, (cql_result_set_ref *)_result_set_); }  q_C_row : the shape of the cursor, as always_rows_ : the bytebuf that will hold our datacql_bytebuf_open(&amp;_rows_); : initializes the buffercql_profile_start(CRC_q, &amp;q_perf_index); : start profiling as beforefor (;;) : the while pattern as beforeC.x = i; : loads the cursorcql_retain_row(C); : retains any references in the cursor (there are none) we're about to copy the cursor into the buffer so all refs need to be +1 cql_bytebuf_append : append the the cursor's bytes into the bufferthe loop does its repetitions until finallycql_results_from_data : used instead of cql_fetch_all_results because all the data is already prepared in this particular example there is nothing to go wrong so it always gets SQLITE_OKin a more complicated example, cql_results_from_data frees any partly created result set in case of errorcql_results_from_data also performs any encoding of sensitive data that might be needed q_info : created as before, but it can be static as it's always the same now Importantly, when using OUT UNION the codegen only produces q_fetch_results, there is no q. If you try to call q from CQL you will instead call q_fetch_results. But since many results as possible, a cursor is needed to make the call. Here's an example, here p calls the q method above: CREATE PROC p (OUT s INTEGER NOT NULL) BEGIN DECLARE C CURSOR FOR CALL q(); LOOP FETCH C BEGIN SET s := s + C.x; END; END;  And the relevant code for this is as follows: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 x; } p_C_row; CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, cql_int32 *_Nonnull s) { cql_contract_argument_notnull((void *)s, 1); cql_code _rc_ = SQLITE_OK; q_result_set_ref C_result_set_ = NULL; cql_int32 C_row_num_ = 0; cql_int32 C_row_count_ = 0; p_C_row C = { 0 }; *s = 0; // set out arg to non-garbage q_fetch_results(&amp;C_result_set_); C_row_num_ = C_row_count_ = -1; C_row_count_ = cql_result_set_get_count((cql_result_set_ref)C_result_set_); for (;;) { C_row_num_++; C._has_row_ = C_row_num_ &lt; C_row_count_; cql_copyoutrow(NULL, (cql_result_set_ref)C_result_set_, C_row_num_, 1, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.x); if (!C._has_row_) break; *s = (*s) + C.x; } _rc_ = SQLITE_OK; cql_object_release(C_result_set_); return _rc_; }  p_C_row : the cursor row as alwayscql_contract_argument_notnull((void *)s, 1) : verify that the out arg pointer is not nullC_result_set_ : this will hold the result set from q_fetch_resultsC_row_num_ : the current row number being processed in the result setC_row_count_ : the total number of rows in the result setother locals are intialized as usual*s = 0; : set the out arg to non-garbage as usualq_fetch_results : get the result set from q_fetch_results in this case no database access was required so this API can't failC_row_num : set to -1C_row_count : set to the row count cql_copyoutrow : copies one row from the result set into the cursor*s = (*s) + C.x; : computes the sumcql_object_release : the result set is torn downif there are any reference fields in the cursor there would have been a cql_teardown_row(C) In short, this is another form of cursor, it's a value cursor, so it has no statement but it also needs a result set, a count and an index to go with it so that it can enumerate the result set. In the AST it looks like this: {name C}: C: select: { x: integer notnull } variable shape_storage uses_out_union  This implies we have the semantic flags: SEM_TYPE_STRUCT, SEM_TYPE_VARIABLE, SEM_TYPE_HAS_SHAPE_STORAGE, and SEM_TYPE_USES_OUT_UNION. The difference is of course the presence of SEM_TYPE_USES_OUT_UNION. This is the last of the cursor forms and the final complication of cg_proc_result_set. "},{"title":"Recap​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#recap-1","content":"At present cg_c.c is a little over 7400 lines of code, maybe 1500 of those lines are comments. So cg_c.c is actually quite a bit smaller and simpler than sem.c (roughly 1/3 the size). It is, however, the most complex of the code generators by far. Part 3 of the internals guide has come out a lot larger than Part 2 but that's mainly because there are a few more cases worth discussing in detail and the code examples of Part 3 are bigger than the AST examples of Part 2. Topics covered included: compiling expressions into C, including nullable typestechniques used to generate control flowcreation of result sets, including: various helpers to do the reading economicallythe use of cql_bytebuf to manage the memory create the text for SQLite statements and binding variables to that texterror management, and how it relates to TRY and CATCH blocksuse of cleanup sections to ensure that references and SQLite statement lifetime is always correctthe contents of the .c and .h files and the key sections in themthe use of charbuf to create and assemble fragments As with the other parts, this is not a complete discussion of the code but a useful survey that should give readers enough context to understand cg_c.c and the runtime helpers in cqlrt.cand cqlrt_common.c. Good luck in your personal exploration! "},{"title":"Part 4: Testing​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-4-testing","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-3","content":"Part 4 continues with a discussion of the essentials testing frameworks for the CQL compiler. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how testing happens in general -- the core strategies and implementation choices -- so that when reading the tests you will have an idea how it all hangs together. To accomplish this, various key tools will be explained in detail as well as selected examples of their use. "},{"title":"Testing​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#testing","content":"There are several types of tests in the system, all of which are launched by the test.shscript which builds the compiler and does a full test pass, there are well over 3000 tests as of this writing. Broadly these are in these few categories: parse tests : these are in test.sql the test script verifies that the compiler can parse this file with no errorsthe parse pass echoes what it read in normalized form, this is compared against a reference copy and any differences are notedeach difference can be accepted or rejected; rejecting a difference stops the scriptverification here is very light and in fact much of parsing is actually tested in the next pass semantic tests : these are in sem_test.sql the file has no parse errors but it has MANY semantic errors, nearly every such error in factsemantic analysis is run with the --test flag which produces AST fragments and echoed CQLthe test file includes patterns which either must appear, or must not appear, in the output to pass the testthe AST includes full type information, so virtually anything about the semantic results can be, and is, verifiedmany tests are designed to exercise the parser as well, ensuring that the correct AST was built and then analyzed e.g. operator precedence can be verified herethe AST echoing logic can also be verified here, e.g. placement of parenthesis in the echoed output any semantic rewrites can be verified here because the rewritten form is emitted in the test output, not the original inputall other operations that happen during the semantic pass (e.g. constant evaluation) are also tested herethe full semantic output is also normalized (e.g. removing line numbers) and is compared against a reference copy, any differences are notedeach difference can be accepted or rejected; rejecting a difference stops the scriptthere are additional files to test different modes like &quot;previous schema&quot; validation (q.v.) as well as dev mode and the schema migrator, the files in this family are: sem_test.sql, sem_test_dev.sql, sem_test_migrate.sql, sem_test_prev.sql code gen tests : the basic test in this family is cg_test.sql which has the C codegen tests these test files do pattern matching just like the semantic case except the codegen output is checked rather than the ASTthe test output is normalized and checked against a reference, just like the semantic teststhere is generally no need to check for errors in test output because all errors are detected during semantic analysisthere are MANY tests in this family, at least one for each of the various generators: cg_test.sql, cg_test_assembly_query.sql, cg_test_base_fragment.sql, cg_test_c_type_getters.sql, cg_test_extension_fragment.sql, cg_test_generate_copy.sql, cg_test_generated_from.sql, cg_test_json_schema.sql, cg_test_no_result_set.sql, cg_test_out_object.sql, cg_test_out_union.sql, cg_test_prev_invalid.sql, cg_test_query_plan.sql, cg_test_schema_upgrade.sql, cg_test_single_proc_not_nullable.sql, cg_test_single_proc_nullable.sql, cg_test_suppressed.sql, cg_test_test_helpers.sql, cg_test_with_object.sql, run tests : the main run test creatively named run_test.sql this test code is compiled and excutedthe test contains expectations like any other unit testit has CQL parts and C parts, the C parts test the C API to the procedures, plus do initial setupthese test include uses of all CQL features and all of the CQL runtime featuresthe schema upgrader tests are arguably &quot;run tests&quot; as well in that they run the code but they have a much different verification strategy unit test : the compiler supports the --run_unit_tests flag this causes the compile to self-test certain of its helper functions that are otherwise difficult to testmostly this is buffers that need to be growable to but in practice only grow with huge input filesother exotic cases that would be hard to reliability hit in some other fashion are covered by this code Test coverage is maintained at 100% line coverage (sometimes there are a few hours when it drops to 99.9% or something like that but this never lasts). Branch coverage is not especially targetted but is nonethless quite high. To see the true branch coverage you have to build the compiler with the asserts (Contract and Invariant) off. Last time it was measured, it was well over 80%. To start the tests you should run test.sh, this launches common/test_common.sh to do the work. This structure allows anyone to make their own harness that launches the common test passes and adds their own extra tests, or passes in additional flags. test.sh itself uses make to build the compiler. As mentioned above, test.sh normally allows the user to accept or reject differences in output, but this is automatically disabled in non-terminal environments, and manually disabled if the script is run with --non_interactive. ok.sh can be run to copy all of the outputs from the most recent test run over the previous references. To get the coverage report, use cov.sh which in turn launches test.sh with suitable flags and then assembles the coverage report using gcovr. "},{"title":"Parse Tests​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#parse-tests","content":"Looking at test/test_common.sh we find the source for the most basic test. This is entirely unremarkable stuff. basic_test() { echo '--------------------------------- STAGE 2 -- BASIC PARSING TEST' echo running &quot;${TEST_DIR}/test.sql&quot; if ! ${CQL} --dev --in &quot;${TEST_DIR}/test.sql&quot; &gt;&quot;${OUT_DIR}/test.out&quot; then echo basic parsing test failed failed fi echo &quot; computing diffs (empty if none)&quot; on_diff_exit test.out }  it's &quot;STAGE 2&quot; because &quot;STAGE 1&quot; was the buildall it tries to do is run the compiler over test/test.sqlif there are errors the test failsif there are any differences between test.out and test.out.ref the test fails That's it. "},{"title":"Sematic Tests​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#sematic-tests","content":"The semantic tests are not much different but this is where the pattern matching comes in. First let's look at the shell script: semantic_test() { echo '--------------------------------- STAGE 4 -- SEMANTIC ANALYSIS TEST' echo running semantic analysis test if ! sem_check --sem --ast --dev --in &quot;${TEST_DIR}/sem_test.sql&quot; &gt;&quot;${OUT_DIR}/sem_test.out&quot; 2&gt;&quot;${OUT_DIR}/sem_test.err&quot; then echo &quot;CQL semantic analysis returned unexpected error code&quot; cat &quot;${OUT_DIR}/sem_test.err&quot; failed fi echo validating output trees if ! &quot;${OUT_DIR}/cql-verify&quot; &quot;${TEST_DIR}/sem_test.sql&quot; &quot;${OUT_DIR}/sem_test.out&quot; then echo failed verification failed fi echo running dev semantic analysis test ... same thing again for sem_test_dev.sql echo &quot; computing diffs (empty if none)&quot; on_diff_exit sem_test.out on_diff_exit sem_test.err ... same thing again for sem_test_dev.out and .err }  There are basically 3 steps: run the compiler over test/sem_test.sql fail if this generates no errors (yes you read that right, see below) do the pattern matching on the output using cql-verify to ensure the patterns match (discussed below) fail if the output is not consistent with the patterns compare the reference output for the AST and the errors fail if there are any differences In the first step the compiler MUST produce an error code, let's look at sem_check to see why: sem_check() { ${CQL} &quot;$@&quot; if [ &quot;$?&quot; -ne &quot;1&quot; ] then echo 'All semantic analysis checks have errors in the test' echo 'the normal return code is &quot;1&quot; -- any other return code is bad news' echo 'A return code of zero indicates we reported success in the face of errors' echo 'A return code other than 1 indicates an unexpected fatal error of some type' return 1 fi }  In short sem_test.sql is FULL of semantic errors, that's part of the test. If the compiler reports success something is seriously wrong. In the next phase we're going to do some pattern matching, let's look at a couple of examples to illustrate how this works. The program cql-verify actually does all this matching and that program is itself written in (mostly) CQL which is cute. It can be found in the tester directory. Here's a very simple example: -- TEST: we'll be using printf in lots of places in the tests as an external proc -- + {declare_proc_no_check_stmt}: ok -- - Error DECLARE PROCEDURE printf NO CHECK;  The code under test is of course DECLARE PROCEDURE printf NO CHECK. The patterns happen immediately before this code. Let's look at each line: -- TEST: etc. : this is just a comment, it means nothing and serves no purpose other than documentation-- + {declare_proc_no_check_stmt}: ok : the comment stats with &quot; + &quot;, this is a trigger the test output from the statement under test must include indicated textthis happens to be the text for the AST of declare_proc_no_check_stmt after semantic successthere is no type info hence the ok designation (recall SEM_TYPE_OK) -- Error : the comment starts with &quot; - &quot;, this is a trigger the test output from the statement under test must NOT include indicated textin this case that means no reported erros Easy enough. Now does this happen? The test output includes: text like &quot;The statement ending at line XXXX&quot; where XXXX is appropriate line numberan echo of the statement that was analyzed (after any rewrites)the AST of that statement including semantic type info that was computed Using the value of XXXX the tester searches the test file in this case sem_test.sql, it extracts the test patterns that happen AFTER the previous XXXX value for the previous statement and up to the indicated line number. This is The Price Is Right algorithm where you read up to the designated lines without going over. Each pattern is matched, or not matched, using the SQL LIKE or NOT LIKE operator. In case of errors the tester writes out the actual output and the expected patterns having all this information handy. The line numbers are all changed to literally &quot;XXXX&quot; after this pass so that the difference in later passes is not a cascade of of trivial line number changes in otherwise identical output. Let's look at another example: -- TEST: create a table using type discrimation: kinds -- + {create_table_stmt}: with_kind: { id: integer&lt;some_key&gt;, cost: real&lt;dollars&gt;, value: real&lt;dollars&gt; } -- + {col_def}: id: integer&lt;some_key&gt; -- + {col_def}: cost: real&lt;dollars&gt; -- + {col_def}: value: real&lt;dollars&gt; -- - Error create table with_kind( id integer&lt;some_key&gt;, cost real&lt;dollars&gt;, value real&lt;dollars&gt; );  This reads pretty easily now: {create_table_stmt} : the struct type of the table must be an exact match for what is expected{col_def} : there are 3 different {col_def} nodes, one for each column- Error : there are no reported errors So there are no errors reported nor are there any in the AST. At least the part of the AST that was checked. The AST actually had other stuff too but it's normal to just test the &quot;essential&quot; stuff. There are many tests that try many variations and we don't want to check every fact in every case of every test. If you want to see the whole AST output for this, it's easy enough. It's sitting in sem_test.out.ref The statement ending at line XXXX CREATE TABLE with_kind( id INTEGER&lt;some_key&gt;, cost REAL&lt;dollars&gt;, value REAL&lt;dollars&gt; ); {create_table_stmt}: with_kind: { id: integer&lt;some_key&gt;, cost: real&lt;dollars&gt;, value: real&lt;dollars&gt; } | {create_table_name_flags} | | {table_flags_attrs} | | | {int 0} | | {name with_kind} | {col_key_list} | {col_def}: id: integer&lt;some_key&gt; | | {col_def_type_attrs}: ok | | {col_def_name_type} | | {name id} | | {type_int}: integer&lt;some_key&gt; | | {name some_key} | {col_key_list} | {col_def}: cost: real&lt;dollars&gt; | | {col_def_type_attrs}: ok | | {col_def_name_type} | | {name cost} | | {type_real}: real&lt;dollars&gt; | | {name dollars} | {col_key_list} | {col_def}: value: real&lt;dollars&gt; | {col_def_type_attrs}: ok | {col_def_name_type} | {name value} | {type_real}: real&lt;dollars&gt; | {name dollars}  As you can see there was potentially a lot more than could have been verified but those view key lines were selected because their correctness really implies the rest. In fact just the {create_table_stmt} line really was enough to know that everthing was fine. Let's look at one more example, this time on that is checking for errors. Many tests check for errors because correctly reporting errors is the primary job of sem.c. It's fair to say that there are more tests for error cases than there are for correct cases because there are a lot more ways to write code incorrectly than correctly. Here's the test: -- TEST: join with bogus ON expression type -- + Error % expected numeric expression 'ON' -- +1 Error -- + {select_stmt}: err -- + {on}: err select * from foo inner join bar as T2 on 'v' where 'w' having 'x' limit 'y';  + Error % expected numeric expression 'ON' : there must be a reported Error message with the indicated error text+1 Error : this indicates that there must be exactly 1 match for the pattern &quot;Error&quot; (i.e. exactly one error) note that there are several problems with the test statement but error processing is supposed to stop after the first -- + {on}: err : verifies that the ON clause was marked as being in error-- + {select_stmt}: err : verifies that the error correctly propogated up to the top level statement Note that the patterns can be in any order and every pattern is matched against the whole input so for instance: -- + {on}: err -- + {on}: err  The above does not imply that there are two such {on} nodes. The second line will match the same text as the first. To to enforce that there were exactly two matches you use: -- +2 {on}: err  There is no syntax for &quot;at least two matches&quot; though one could easily be added. So far it hasn't been especially necessary. As we'll see this simple pattern is used in many other tests. All that is required for it work is output with lines of the form &quot;The statement ending at line XXXX&quot; The sem_test_dev.sql test file is a set of tests that are run with the --dev flag passed to CQL. This is the mode where certain statements that are prohibited in production code are verified. This file is very small indeed and the exact prohibitions are left as an exercise to the reader. "},{"title":"Code Generation Tests​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#code-generation-tests","content":"The test logic for the &quot;codegen&quot; family of tests (cg_test*.sql) is virtually identical to the semantic test family. The same testing utililty is used, and it works the same way, looking for the same marker. The only difference in this stage is that the test output is generated code, not an AST. The codegen tests are a great way to lock down important code fragments in the output. Note that the codegen tests do not actually execute any generated code. That's the next category. Here's an sample test: -- TEST: unused temp in unary not emitted -- - cql_int32 _tmp_int_0 = 0; -- - cql_int32 _tmp_int_1 = 0; -- + o = i.value; -- + o = - 1; create proc unused_temp(i integer, out o integer not null) begin set o := coalesce(i, -1); end;  This test is verifying one of the optimizations that we talked about inPart 3. In many cases temporary variables for results (such as function calls) can be elided. - cql_int32 _tmp_int_0 = 0; : verifies that this temporary is NOT created- cql_int32 _tmp_int_1 = 0; : likewise+ o = i.value; : the first alternative in coalesce directly assigns to o+ o = - 1; : as does the second It might be helpful to look at the full output, which as always is in a .ref file. In this case cg_test.c.ref. Here is the full output with the line number normalized: // The statement ending at line XXXX /* CREATE PROC unused_temp (i INTEGER, OUT o INTEGER NOT NULL) BEGIN SET o := coalesce(i, -1); END; */ #define _PROC_ &quot;unused_temp&quot; // export: DECLARE PROC unused_temp (i INTEGER, OUT o INTEGER NOT NULL); void unused_temp(cql_nullable_int32 i, cql_int32 *_Nonnull o) { cql_contract_argument_notnull((void *)o, 2); *o = 0; // set out arg to non-garbage do { if (!i.is_null) { *o = i.value; break; } *o = - 1; } while (0); } #undef _PROC_  As we can see, the test has picked out the bits that it wanted to verify. The coalescefunction is verified elsewhere -- in this test we're making sure that this pattern doesn't cause extra temporaries. Let's take a quick look at the part of test_common.sh that runs this: code_gen_c_test() { echo '--------------------------------- STAGE 5 -- C CODE GEN TEST' echo running codegen test if ! ${CQL} --test --cg &quot;${OUT_DIR}/cg_test_c.h&quot; &quot;${OUT_DIR}/cg_test_c.c&quot; \\ &quot;${OUT_DIR}/cg_test_exports.out&quot; --in &quot;${TEST_DIR}/cg_test.sql&quot; \\ --global_proc cql_startup --generate_exports 2&gt;&quot;${OUT_DIR}/cg_test_c.err&quot; then echo &quot;ERROR:&quot; cat &quot;${OUT_DIR}/cg_test_c.err&quot; failed fi echo validating codegen if ! &quot;${OUT_DIR}/cql-verify&quot; &quot;${TEST_DIR}/cg_test.sql&quot; &quot;${OUT_DIR}/cg_test_c.c&quot; then echo &quot;ERROR: failed verification&quot; failed fi echo testing for successful compilation of generated C rm -f out/cg_test_c.o if ! do_make out/cg_test_c.o then echo &quot;ERROR: failed to compile the C code from the code gen test&quot; failed fi ... echo &quot; computing diffs (empty if none)&quot; on_diff_exit cg_test_c.c on_diff_exit cg_test_c.h ... other tests }  Briefly reviewing this, we see the following important steps: {CQL} --test --cg etc. : run the compiler on the test input the test fails if there are any errors cql-verify : performs the pattern matching the output has the same statement markers as in the semantic case do_make : use make to build the generated code ensuring it compiles cleanly if the C compiler returns any failure, the test fails on_diff_exit : compares the test output to the reference output any difference fails the test This is all remarkably similar to the semantic tests. All the code generators are tested in the same way. "},{"title":"Run Tests​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#run-tests","content":"The last category of tests actually does execution. The main &quot;run test&quot; happens at &quot;stage 13&quot;, because there are many codegen tests for the various output formats and these all pass before before we try to execute anything. This is not so bad because the tests are quite quick with a full test pass taking less than 90s on my laptop. run_test() { echo '--------------------------------- STAGE 13 -- RUN CODE TEST' echo running codegen test with execution if ! cc -E -x c -w &quot;${TEST_DIR}/run_test.sql&quot; \\ &gt;&quot;${OUT_DIR}/run_test_cpp.out&quot; then echo preprocessing failed. failed elif ! ${CQL} --nolines \\ --cg &quot;${OUT_DIR}/run_test.h&quot; &quot;${OUT_DIR}/run_test.c&quot; \\ --in &quot;${OUT_DIR}/run_test_cpp.out&quot; \\ --global_proc cql_startup --rt c then echo codegen failed. failed elif ! (echo &quot; compiling code&quot;; do_make run_test ) then echo build failed failed elif ! (echo &quot; executing tests&quot;; &quot;./${OUT_DIR}/a.out&quot;) then echo tests failed failed fi ...  The main structure is mostly what one would expect: cc -E -x c : this is used to pre-process the run test file so that we can use C pre-processor features to define tests there are quite a few helpful macros as we'll seeif pre-processing fails, the test fails {CQL} --nolines --cg ... : this is used to create the .h and .c file for the compiland --nolines is used to suppress the # directives that would associate the generated code with the .sql filecompilation failures cause the test to fail do_make : as before this causes make to build the compiland (run_test) this build target includes the necessary bootstrap code to open a database and start the testsany failures cause the test to fail a.out : the tests execute the tests return a failure status code if anything goes wrongany failure causes the test to fail The test file run_test.sql includes test macros from cqltest.h -- all of these are very simple. The main ones are BEGIN_SUITE, END_SUITE, BEGIN_TEST and END_TEST for structure; and EXPECT to verify a boolean expression. Here's a simple test case with several expectations: BEGIN_TEST(arithmetic) EXPECT_SQL_TOO((1 + 2) * 3 == 9); EXPECT_SQL_TOO(1 + 2 * 3 == 7); EXPECT_SQL_TOO(6 / 3 == 2); EXPECT_SQL_TOO(7 - 5 == 2); EXPECT_SQL_TOO(6 % 5 == 1); EXPECT_SQL_TOO(5 / 2.5 == 2); EXPECT_SQL_TOO(-(1+3) == -4); EXPECT_SQL_TOO(-1+3 == 2); EXPECT_SQL_TOO(1+-3 == -2); EXPECT_SQL_TOO(longs.neg == -1); EXPECT_SQL_TOO(-longs.neg == 1); EXPECT_SQL_TOO(- -longs.neg == -1); END_TEST(arithmetic)  We should also reveal EXPECT_SQL_TOO, discussed below: -- use this for both normal eval and SQLite eval #define EXPECT_SQL_TOO(x) EXPECT(x); EXPECT((select x))  Now back to the test: EXPECT(x) : verifies that x is true (i.e. a non-zero numeric) not used directly in this example EXPECT_SQL_TOO : as the definition shows, x must be true (as above)(select x) must also be true, i.e. when SQLite is asked to evaluate the expression the result is also a &quot;pass&quot; this is used to verify consistency of order of operations and other evaluations that must be the same in both formsnote that when (select ...) is used, CQL plays no part in evaluating the expression, the text of the expression goes to SQLite and any variables are bound as described in Part 3. The run test exercises many features, but the testing strategy is always the same: exercise some code patternuse EXPECT to validate the results are correctthe expressions in the EXPECT are usually crafted carefully to show that a certain mistake is not being made e.g. expressions where the result would be different if there are bugs in order of operationse.g. expressions that would crash with divide by zero if code that isn't supposed to run actually ran "},{"title":"Schema Upgrade Testing​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#schema-upgrade-testing","content":"The schema upgrade tester is quite a bit different than the others and relies heavily on execution of the upgraders. Before we get into that there is a preliminary topic: &quot;Previous Schema&quot; Validation​ In order to ensure that it is possible to create an upgrader, CQL provides features to validate the current schema against the previous schema ensuring that nothing has been done that would make an upgrader impossible. This is more fully discussed inChapter 11 of the Guide. &quot;Previous Schema&quot; validation is a form of semantic check and so its testing happens as described above. Importantly, as with the other back-end passes the schema upgrader does not have to concern itself with error cases as they are already ruled out. The upgrader itself will be the subject of Part 5. Packing List​ The test assets for upgrade tests are found in the upgrade directory and consist of SchemaPersistentV0.sql : baseline version of the test schemaSchemaPersistentV1.sql : v1 of the test schemaSchemaPersistentV2.sql : v2 of the test schemaSchemaPersistentV3.sql : v3 of the test schemadowngrade_test.c : a test that simulates attemping to go backwards in schema versionsupgrade_test.c : the C harness that launches the upgraders and fires the testsupgrade_test.sh : the shell script that makes all this happenupgrade_validate.sql : some simple code that sanity checks the recorded schema version against tables in it used to ensure that the schema we are on is the schema we think we are on, not to validate all facets of italso renders the contents of sqlite_master in a canonical form We haven't yet discussed the internals of schema upgrade, so for purposes of this part we're only going to discuss how the testing proceeds. The upgrade will be considered &quot;magic&quot; for now. In addition to these assets, we also have reference files: upgrade_schema_v0.out.ref : expected content of v0upgrade_schema_v1.out.ref : expected content of v1upgrade_schema_v2.out.ref : expected content of v2upgrade_schema_v3.out.ref : expected content of v3 upgrade_validate.sql​ This file has a single procedure validate_transition which does the two jobs: emits the canonicalized version of sqlite_master to the output this is needed because sqlite_master text can vary between Sqlite versions checks for basic things that should be present in a given version The output of the validator looks like this: reference results for version 0 ----- g1 ----- type: table tbl_name: g1 CREATE TABLE g1( id INTEGER PRIMARY KEY, name TEXT) ----- sqlite_autoindex_test_cql_schema_facets_1 ----- type: index tbl_name: test_cql_schema_facets ----- test_cql_schema_facets ----- type: table tbl_name: test_cql_schema_facets CREATE TABLE test_cql_schema_facets( facet TEXT NOT NULL PRIMARY KEY, version LONG_INT NOT NULL)  The formatting rules are very simple and so the output is pretty readable. The verifications are very simple. First this happens: let version := cast(test_cql_get_facet_version(&quot;cql_schema_version&quot;) as integer);  The printing happens, then this simple validation:  let recreate_sql := ( select sql from sqlite_master where name = 'test_this_table_will_become_create' if nothing null); ... switch version when 0 then if recreate_sql is null or recreate_sql not like '%xyzzy INTEGER%' then call printf(&quot;ERROR! test_this_table_will_become_create should have a column named xyzzy in v%d\\n&quot;, version); throw; end if; ... else call printf(&quot;ERROR! expected schema version v%d\\n&quot;, version); throw; end;  In short, the version number must be one of the valid versions and each version is expecting that particular table to be in some condition it can recognize. The real validation is done by noting any changes in the reference output plus a series of invariants. Prosecution of the Upgrade Test​  Launch  We kick things off as follows: test.sh calls upgrade/upgrade_test.sh this test doesn't usually run standalone (but it can)  Build Stage  This creates the various binaries we will need: upgrade_validate.sql is compiled down to C this code works for all schema versions, it's generic SchemaPersistentV[0-3].sql are compiled into C (this takes two steps) first, the CQL upgrader is generated from the schemasecond, the CQL upgrader is compiled to C make is used to lower all of the C into executables upgrade[0-3] plus downgrade_test the shared validation code is linked into all 4 upgradersdowngrade_test.c is linked with the code for upgrade1  Basic Upgrades  Here we test going from scratch to each of the 4 target versions: upgrade[0-3] are each run in turn with no initial database i.e. their target database is deleted before each run the validation output is compared against the reference output any differences fail the test  Previous Schema Validation  This sanity checks that the chain of schema we have built should work when upgrading from one version to the next: try each schema with this predecessor: SchemaPersistentV1.sql with SchemaPersistentV0.sql as the previousSchemaPersistentV2.sql with SchemaPersistentV1.sql as the previousSchemaPersistentV3.sql with SchemaPersistentV2.sql as the previous if any of these produce errors something is structurally wrong with the test or else previous schema validation is broken  Two-Step Upgrades  Now we verify that we can go from any version to any other version with a stop in between to persist. An example should make this clearer: We start from scratch and go to v2 this should produce the v2 reference schema output as before We run the v4 upgrader on this v2 schema this should produce the v4 reference schema output as beforei.e. if we go from nothing to v2 to v4 we get the same as if we just go to v4 directly There are quite a few combinations like this, the test output lists them all: Upgrade from nothing to v0, then to v0 -- must match direct update to v0 Upgrade from nothing to v0, then to v1 -- must match direct update to v1 Upgrade from nothing to v1, then to v1 -- must match direct update to v1 Upgrade from nothing to v0, then to v2 -- must match direct update to v2 Upgrade from nothing to v1, then to v2 -- must match direct update to v2 Upgrade from nothing to v2, then to v2 -- must match direct update to v2 Upgrade from nothing to v0, then to v3 -- must match direct update to v3 Upgrade from nothing to v1, then to v3 -- must match direct update to v3 Upgrade from nothing to v2, then to v3 -- must match direct update to v3 Upgrade from nothing to v3, then to v3 -- must match direct update to v3  Note that one of the combinations tested is starting on Vn and &quot;upgrading&quot; from there to Vn. This should do nothing.  Testing downgrade  Here we make sure that any attempt to &quot;go backwards&quot; results in an error. the v3 schema created by the previous test is used as input to the downgrade testthe downgrade test was linked with the v2 upgraderwhen executed the v2 upgrader should report the error this test's verifier checks for a correct error report the test test fails if the error is no correctly reported The combination of testing reference outputs plus testing these many invariants at various stages results in a powerful integration test. The actual schema for the varios versions includes all the supported transitions such as creating and deleting tables and columns, and recreating views, indicies, and triggers. All of the possible transitions are more fully discussed inChapter 10 of the Guide which pairs nicely with the previous schema validions discussed inChapter 11. "},{"title":"Testing the #line directives produced by CQL​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#testing-the-line-directives-produced-by-cql","content":"[An additional section should be added for the code that verifies the source line number mappings even though this is a pretty exotic case.] "},{"title":"Summary​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#summary","content":"While there are a few more isolated verifications that happen in test.sh and of course there is the plumbing necessary to let cov.sh use the test script to create coverage reports, the above forms make up the vast majority of the test patterns. Generally, the test files are designed to hold as many tests as can reasonably fit with the gating factor being cases where different flags are necessary. There are two different stages were many different tiny input files are used to create trivial failures like missing command line arguments and such. But those cases are all just looking for simple error text and a failure code, so they should be self-evident. With so many options, many such baby tests are needed. "},{"title":"Part 5: CQL Runtime​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-5-cql-runtime","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-4","content":"Part 5 continues with a discussion of the essentials of the CQL Runtime. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how the runtime works in general -- the core strategies and implementation choices -- so that when reading the source you will have an idea how it all hangs together. To accomplish this, we'll illustrate the key pieces that can be customized and we'll discuss some interesting cases. "},{"title":"CQL Runtime​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#cql-runtime","content":"The parts of the runtime that you can change are in cqlrt.h, that file invariably ends by includingcqlrt_common.h which are the runtime parts that you shouldn't change. Of course this is open source so you can change anything, but the common things usually don't need to change -- cqlrt.h should provide you with everything you need to target new environments. The compiler itself can be customized see rt.c to emit different strings to work with your runtime. This is pretty easy to do without creating a merge hell for yourself. Meta Platforms, for instance, has its own CQL runtime customized for use on phones that is not open source (and really I don't think anyone would want it anyway). But the point is that you can make your own. In fact I know of two just within Meta Platforms. We'll go over cqlrt.h bit by bit. Keeping in mind it might change but this is essentially what's going on. And the essentials don't change very often. "},{"title":"Standard headers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#standard-headers","content":"The rest of the system will use these, cqlrt.h is responsible for bringing in what you need later, or what cqlrt_common.h needs on your system. #pragma once #include &lt;assert.h&gt; #include &lt;stddef.h&gt; #include &lt;stdint.h&gt; #include &lt;math.h&gt; #include &lt;sqlite3.h&gt; #ifndef __clang__ #ifndef _Nonnull /* Hide Clang-only nullability specifiers if not Clang */ #define _Nonnull #define _Nullable #endif #endif  "},{"title":"Contract and Error Macros​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#contract-and-error-macros","content":"CQL has a few different macros it uses for errors. contract, invariant, and tripwireusually all map to assert. Note that tripwire doesn't have to be fatal, it can log in production and continue. This is a &quot;softer&quot; assertion. Something that you're trying out that you'd like to be a contract but maybe there are lingering cases that have to be fixed first. #define cql_contract assert #define cql_invariant assert #define cql_tripwire assert #define cql_log_database_error(...) #define cql_error_trace()  "},{"title":"The Value Types​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-value-types","content":"You can define these types to be whatever is appropriate on your system. Usually the mapping is pretty obvious. // value types typedef unsigned char cql_bool; #define cql_true (cql_bool)1 #define cql_false (cql_bool)0 typedef unsigned long cql_hash_code; typedef int32_t cql_int32; typedef uint32_t cql_uint32; typedef uint16_t cql_uint16; typedef sqlite3_int64 cql_int64; typedef double cql_double; typedef int cql_code;  "},{"title":"The Reference Types​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-reference-types","content":"The default runtime first defines 4 types of reference objects. These are the only reference types that CQL creates itself. In fact CQL doesn't actually create CQL_C_TYPE_OBJECT but the tests do. CQL never creates raw object things, only external functions can do that. // metatypes for the straight C implementation #define CQL_C_TYPE_STRING 0 #define CQL_C_TYPE_BLOB 1 #define CQL_C_TYPE_RESULTS 2 #define CQL_C_TYPE_BOXED_STMT 3 #define CQL_C_TYPE_OBJECT 4  All the reference types are reference counted. So they need a simple shape that allows them to know their own type and have a count. They also have a finalize method to clean up their memory when the count goes to zero. You get to define cql_type_ref to be whatever you want. // base ref counting struct typedef struct cql_type *cql_type_ref; typedef struct cql_type { int type; int ref_count; void (*_Nullable finalize)(cql_type_ref _Nonnull ref); } cql_type;  Whatever you do with the types you'll need to define a retain and release method that uses them as the signature. Normal references should have a generic value comparison and a hash. void cql_retain(cql_type_ref _Nullable ref); void cql_release(cql_type_ref _Nullable ref); cql_hash_code cql_ref_hash(cql_type_ref _Nonnull typeref); cql_bool cql_ref_equal(cql_type_ref _Nullable typeref1, cql_type_ref _Nullable typeref2);  Now each of the various kinds of reference types needs an object which probably includes the base type above. It doesn't have to. You can arrange for some other universal way to do these. On iOS these can be easily mapped to CF types. The retain and release macros should all map to the same thing. The compiler emits different variations for readability only. It doesn't really work if they don't have common retain/release semantics. // builtin object typedef struct cql_object *cql_object_ref; typedef struct cql_object { cql_type base; const void *_Nonnull ptr; } cql_object; #define cql_object_retain(object) cql_retain((cql_type_ref)object); #define cql_object_release(object) cql_release((cql_type_ref)object);  Boxed statement gets its own implementation, same as object. // builtin statement box typedef struct cql_boxed_stmt *cql_boxed_stmt_ref; typedef struct cql_boxed_stmt { cql_type base; sqlite3_stmt *_Nullable stmt; } cql_boxed_stmt;  Same for blob, and blob has a couple of additional helper macros that are used to get information. Blobs also have hash and equality functions. // builtin blob typedef struct cql_blob *cql_blob_ref; typedef struct cql_blob { cql_type base; const void *_Nonnull ptr; cql_uint32 size; } cql_blob; #define cql_blob_retain(object) cql_retain((cql_type_ref)object); #define cql_blob_release(object) cql_release((cql_type_ref)object); cql_blob_ref _Nonnull cql_blob_ref_new(const void *_Nonnull data, cql_uint32 size); #define cql_get_blob_bytes(data) (data-&gt;ptr) #define cql_get_blob_size(data) (data-&gt;size) cql_hash_code cql_blob_hash(cql_blob_ref _Nullable str); cql_bool cql_blob_equal(cql_blob_ref _Nullable blob1, cql_blob_ref _Nullable blob2);  Strings are the same as the others but they have many more functions associated with them. // builtin string typedef struct cql_string *cql_string_ref; typedef struct cql_string { cql_type base; const char *_Nullable ptr; } cql_string; cql_string_ref _Nonnull cql_string_ref_new(const char *_Nonnull cstr); #define cql_string_retain(string) cql_retain((cql_type_ref)string); #define cql_string_release(string) cql_release((cql_type_ref)string);  The compiler uses this macro to create a named string literal. You decide how those will be implemented right here. #define cql_string_literal(name, text) \\ cql_string name##_ = { \\ .base = { \\ .type = CQL_C_TYPE_STRING, \\ .ref_count = 1, \\ .finalize = NULL, \\ }, \\ .ptr = text, \\ }; \\ cql_string_ref name = &amp;name##_  Strings get assorted comparison and hashing functions. Note blob also had a hash. int cql_string_compare(cql_string_ref _Nonnull s1, cql_string_ref _Nonnull s2); cql_hash_code cql_string_hash(cql_string_ref _Nullable str); cql_bool cql_string_equal(cql_string_ref _Nullable s1, cql_string_ref _Nullable s2); int cql_string_like(cql_string_ref _Nonnull s1, cql_string_ref _Nonnull s2);  Strings can be converted from their reference form to standard C form. These macros define how this is done. Note that temporary allocations are possible here but the standard implementation does not actually need to do an alloc. It stores UTF8 in the string pointer so it's ready to go. #define cql_alloc_cstr(cstr, str) const char *_Nonnull cstr = (str)-&gt;ptr #define cql_free_cstr(cstr, str) 0  The macros for result sets have somewhat less flexibility. The main thing that you can do here is add additional fields to the &quot;meta&quot; structure. It needs those key fields because it is created by the compiler. However the API is used to create a result set so that can be any object you like. It only has to respond to the get_meta, get_data, and get_count apis. Those can be mapped as you desire. In principle there could have been a macro to create the &quot;meta&quot; as well (a PR for this is welcome) but it's really a pain for not much benefit. The advantage of defining your own &quot;meta&quot; is that you can use it to add additional custom APIs to your result set that might need some storage. The additional API cql_result_set_note_ownership_transferred(result_set)is used in the event that you are moving ownership of the buffers from out of CQL's universe. So like maybe JNI is absorbing the result, or Objective C is absorbing the result. The default implementation is a no-op. // builtin result set typedef struct cql_result_set *cql_result_set_ref; typedef struct cql_result_set_meta { ... } typedef struct cql_result_set { cql_type base; cql_result_set_meta meta; cql_int32 count; void *_Nonnull data; } cql_result_set; #define cql_result_set_type_decl(result_set_type, result_set_ref) \\ typedef struct _##result_set_type *result_set_ref; cql_result_set_ref _Nonnull cql_result_set_create( void *_Nonnull data, cql_int32 count, cql_result_set_meta meta); #define cql_result_set_retain(result_set) cql_retain((cql_type_ref)result_set); #define cql_result_set_release(result_set) cql_release((cql_type_ref)result_set); #define cql_result_set_note_ownership_transferred(result_set) #define cql_result_set_get_meta(result_set) (&amp;((cql_result_set_ref)result_set)-&gt;meta) #define cql_result_set_get_data(result_set) ((cql_result_set_ref)result_set)-&gt;data #define cql_result_set_get_count(result_set) ((cql_result_set_ref)result_set)-&gt;count  "},{"title":"Mocking​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#mocking","content":"The CQL run test needs to do some mocking. This bit is here for that test. If you want to use the run test with your version of cqlrt you'll need to define a shim for sqlite3_step that can be intercepted. This probably isn't going to come up. #ifdef CQL_RUN_TEST #define sqlite3_step mockable_sqlite3_step SQLITE_API cql_code mockable_sqlite3_step(sqlite3_stmt *_Nonnull); #endif  "},{"title":"Profiling​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#profiling","content":"If you want to support profiling you can implement cql_profile_start and cql_profile_stopto do whatever you want. The CRC uniquely identifies a procedure (you can log that). Theindex provides you with a place to store something that you can use as a handle in your logging system. Typically an integer. This lets you assign indices to the procedures you actually saw in any given run and then log them or something like that. No data about parameters is provided, this is deliberate. // No-op implementation of profiling // * Note: we emit the crc as an expression just to be sure that there are no compiler // errors caused by names being incorrect. This improves the quality of the CQL // code gen tests significantly. If these were empty macros (as they once were) // you could emit any junk in the call and it would still compile. #define cql_profile_start(crc, index) (void)crc; (void)index; #define cql_profile_stop(crc, index) (void)crc; (void)index;  The definitions in cqlrt_common.c can provide codegen than either has generic &quot;getters&quot; for each column type (useful for JNI) or produces a unique getter that isn't shared. The rowset metadata will include the values for getBoolean, getDouble etc. if CQL_NO_GETTERS is 0. Getters are a little slower for C but give you a small number of functions that need to have JNI if you are targeting Java. // the basic version doesn't use column getters #define CQL_NO_GETTERS 1  "},{"title":"Encoding of Sensitive Columns​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#encoding-of-sensitive-columns","content":"By setting an attribute on any procedure that produces a result set you can have the selected sensitive values encoded. If this happens CQL first asks for the encoder and then calls the encode methods passing in the encoder. These aren't meant to be cryptograhically secure but rather to provide some ability to prevent mistakes. If you opt in, sensitive values have to be deliberately decoded and that provides an audit trail. The default implementation of all this is a no-op. // implementation of encoding values. All sensitive values read from sqlite db will // be encoded at the source. CQL never decode encoded sensitive string unless the // user call explicitly decode function from code. cql_object_ref _Nullable cql_copy_encoder(sqlite3 *_Nonnull db); cql_bool cql_encode_bool(...) cql_int32 cql_encode_int32(...) cql_int64 cql_encode_int64(...) cql_double cql_encode_double(...) cql_string_ref _Nonnull cql_encode_string_ref_new(...); cql_blob_ref _Nonnull cql_encode_blob_ref_new(..); cql_bool cql_decode_bool(...); cql_int32 cql_decode_int32(...); cql_int64 cql_decode_int64(...); cql_double cql_decode_double(...); cql_string_ref _Nonnull cql_decode_string_ref_new(...); cql_blob_ref _Nonnull cql_decode_blob_ref_new(...);  "},{"title":"The Common Headers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-common-headers","content":"The standard APIs all build on the above, so they should be included last. Now in some cases the signature of the things you provide in cqlrt.h is basically fixed, so it seems like it would be easier to move the prototpyes into cqlrt_common.h. However, in many cases additional things are needed like declspec or export or other system specific things. The result is that cqlrt.h is maybe a bit more verbose that it strictly needs to be. Also some versions of cqlrt.h choose to implement some of the APIs as macros... // NOTE: This must be included *after* all of the above symbols/macros. #include &quot;cqlrt_common.h&quot;  "},{"title":"The cqlrt_cf Runtime​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-cqlrt_cf-runtime","content":"In order to use the Objective-C code-gen (--rt objc) you need a runtime that has reference types that are friendly to Objective-C. For this purpose we created an open-source version of such a runtime: it can be found in the sources/cqlrt_cf directory. This runtime is also a decent example of how much customization you can do with just a little code. Some brief notes: This runtime really only makes sense on macOS, iOS, or maybe some other place that Core Foundation (CF) exists As such its build process is considerably less portable than other parts of the system The CQL reference types have been redefined so that they map to: CFStringRef (strings)CFTypeRef (objects)CFDataRef (blobs) The key worker functions use CF, e.g. cql_ref_hash maps to CFHashcql_ref_equal maps to CFEqualcql_retain uses CFRetain (with a null guard)cql_release uses CFRelease (with a null guard) Strings use CF idioms, e.g. string literals are created with CFSTRC strings are created by using CFStringGetCStringPtr or CFStringGetCString when needed Of course, since the meaning of some primitive types has changed, the contract to the CQL generated code has changed accordingly. For instance: procedures compiled against this runtime expect string arguments to be CFStringRefresult sets provide CFStringRef values for string columns The consequence of this is that the Objective-C code generation --rt objc finds friendly contracts that it can freely convert to types like NSString * which results in seamless integration with the rest of an Objective-C application. Of course the downside of all this is that the cqlrt_cf runtime is less portable. It can only go where CF exists. Still, it is an interesting demonstration of the flexablity of the system. The system could be further improved by creating a custom result type (e.g. --rt c_cf) and using some of the result type options for the C code generation. For instance, the compiler could do these things: generate CFStringRef foo; instead of cql_string_ref foo; for declarationsgenerate SInt32 an_integer instead of cql_int32 an_integer Even though cqlrt_cf is already mapping cql_int32 to something compatible with CF, making such changes would make the C output a little bit more CF idiomatic. This educational exercise could probably be completed in just a few minutes by interested readers. The make.sh file in the sources/cqlrt_cf directory illustrates how to get CQL to use this new runtime. The demo itself is a simple port of the code in Appendix 10. "},{"title":"Recap​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#recap-2","content":"The CQL runtime, cqlrt.c, is intended to be replaced. The version that ships with the distribution is a simple, portable implementation that is single threaded. Serious users of CQL will likely want to replace the default version of the runtime with something more tuned to their use case. Topics covered included: contract, error, and tracing macroshow value types are definedhow reference types are definedmocking (for use in a test suite)profilingencoding of sensitive columnsboxing statementsthe cqlrt_cf runtime As with the other parts, no attempt was made to cover every detail. That is best done by reading the source code. "},{"title":"Part 6: Schema Management​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-6-schema-management","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-5","content":"Part 6 continues with a discussion of the essentials of schema management in the CQL compiler. As in the previous parts, the goal here is not to go over every detail of the system but rather to give a sense of how schema management happens in general -- the core strategies and implementation choices -- so that when reading the management code you will have an idea how it all hangs together. To accomplish this, various key data structures will be explained in detail and accompanied by examples of their use. "},{"title":"Schema Management​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#schema-management","content":"The primary goal of the schema management features of the CQL compiler is to provide the ability to create a &quot;schema upgrader&quot; that can move a given user's database from a previous version of the schema to the current version. Because of the limitations of SQL in general, and SQLite in particular, not all transforms are possible; so additionally the system must correctly detect and prevent upgrades that cannot be safely performed. The full set of schema attributes and their meaning is described in Chapter 10and the full set of validations is described in Chapter 11. Briefly the directives are: @create(n): indicates a table/column is to be created at version n.@delete(n): indicates a table/column is to be deleted at version n.@recreate: indicates the table contents are not precious the table can be dropped and created when its schema changesthis does not combine with @createit applies only to tablesviews, triggers, and indices are always on the @recreate plan and do not have to be marked so Now the various annotations can occur substantially in any order as there are no rules that require that tables that are created later in time appear later in the input. This means the appearance order of tables is in general very inconvenient for any upgrading logic. However, the semantic validation pass gathers all the annotations into two large bytebuf objects which can be readily sorted -- one for things on the @create plan and one for the @recreate plan. These will be discussed below. At this point it's probably best to start looking at some of the code fragments. We're going to be looking at all the steps in the top level function: // Main entry point for schema upgrade code-gen. cql_noexport void cg_schema_upgrade_main(ast_node *head) { Contract(options.file_names_count == 1); ... }  Note that the schema upgrader code generator in CQL does not produce C but rather it produces more CQL which then has to be compiled down to C. This choice means that the codegen is a lot more readable and gets the benefit of the usual CQL error checking and exception management. "},{"title":"Check for errors, check for --global_proc​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#check-for-errors-check-for---global_proc","content":"We start with some simple error checks: Any semantic errors abort the code-generation. The --global_proc names the procedure that will do the upgrade. It is also used as a prefix on all of the tables that the upgrader requires. This makes it possible, if desired, to have separate upgraders for different parts of your schema, or to combine upgraders from two different unrelated subsystems in the same database.  cql_exit_on_semantic_errors(head); exit_on_no_global_proc();  "},{"title":"Preparing the Attributes​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preparing-the-attributes","content":"The two arrays schema_annotations and recreate_annotations are sorted. The item count can be easily computed using the allocated size of these items, both of which are of type bytebuf. The comparators provided to qsortput these arrays in exactly the order needed.  // first sort the schema annotations according to version, type etc. // we want to process these in an orderly fashion and the upgrade rules // are nothing like the declared order. void *base = schema_annotations-&gt;ptr; size_t schema_items_size = sizeof(schema_annotation); size_t schema_items_count = schema_annotations-&gt;used / schema_items_size; schema_annotation *notes = (schema_annotation*)base; int32_t max_schema_version = 0; if (schema_items_count) { qsort(base, schema_items_count, schema_items_size, annotation_comparator); max_schema_version = notes[schema_items_count - 1].version; } // likewise, @recreate annotations, in the correct upgrade order (see comparator) base = recreate_annotations-&gt;ptr; size_t recreate_items_size = sizeof(recreate_annotation); size_t recreate_items_count = recreate_annotations-&gt;used / recreate_items_size; if (recreate_items_count) { qsort(base, recreate_items_count, recreate_items_size, recreate_comparator); } recreate_annotation *recreates = (recreate_annotation *)base;  "},{"title":"Creating the Global CRC​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#creating-the-global-crc","content":"Schema upgrade is expensive, so we want to be able to quickly detect if the schema installed is already the latest version. To do this we compute a single global 64-bit CRC for the current version of the schema. This can be compared against a stored schema CRC from the last run. If the CRCs match, no work needs to be done.  CHARBUF_OPEN(all_schema); // emit canonicalized schema for everything we will upgrade // this will include the schema declarations for the ad hoc migrations, too; cg_generate_schema_by_mode(&amp;all_schema, SCHEMA_TO_UPGRADE); // compute the master CRC using schema and migration scripts llint_t schema_crc = (llint_t)crc_charbuf(&amp;all_schema); CHARBUF_CLOSE(all_schema);  The schema generator is used to emit the full schema, including annotations, into a buffer. A raw CRC of the buffer gives us the &quot;global&quot; or &quot;overall&quot; CRC for the whole schema. "},{"title":"Output Fragments​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#output-fragments","content":"A number of buffers will hold the various pieces of output.  CHARBUF_OPEN(preamble); CHARBUF_OPEN(main); CHARBUF_OPEN(decls); CHARBUF_OPEN(pending); CHARBUF_OPEN(upgrade); CHARBUF_OPEN(baseline);  These will be assembled as follows:  CHARBUF_OPEN(output_file); bprintf(&amp;output_file, &quot;%s\\n&quot;, decls.ptr); bprintf(&amp;output_file, &quot;%s&quot;, preamble.ptr); bprintf(&amp;output_file, &quot;%s&quot;, main.ptr); cql_write_file(options.file_names[0], output_file.ptr); CHARBUF_CLOSE(output_file);  In short: first decls, this declares the schema among other thingsthen, preamble, this contains helper proceduresthen, main, the primary upgrader steps go here We'll go over all of these in subsequent sections. "},{"title":"Declarations Section​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#declarations-section","content":"The result type includes a customizable prefix string. This is the first thing to go out. Typically this is the appropriate copyright notice. rt.c has this information and that file is replaceable.  bprintf(&amp;decls, &quot;%s&quot;, rt-&gt;source_prefix);  The schema upgrade script is in the business of creating tables from old versions and then altering them. The table declarations will be for the final shape. We need to emit @SCHEMA_UPGRADE_SCRIPT so that the CQL compiler knows that there will be multiple declarations of the same table and they might not be identical. The upgrade script is in the business of getting things to the end state. Likewise it is normal for the schema upgrade script to refer to columns that have been deleted, this is because a column might be created in say version 5 and then deleted in version 10. The upgrade code goes through the columns lifecycle, so even though the declarations already say the column is doomed to die in version 10, the creation code in version 5 is legal -- and necessary. Schema migration steps that run in version 6, 7, 8, or 9 might use the contents of the column as part of essential data migration. We can never know what version we might find in a database that is being upgraded, it could be very far in the past, at a time where a deleted column still existed.  bprintf(&amp;decls, &quot;-- no columns will be considered hidden in this script\\n&quot;); bprintf(&amp;decls, &quot;-- DDL in procs will not count as declarations\\n&quot;); bprintf(&amp;decls, &quot;@SCHEMA_UPGRADE_SCRIPT;\\n\\n&quot;);  A convenience comment goes in the decls section with the CRC.  bprintf(&amp;decls, &quot;-- schema crc %lld\\n\\n&quot;, schema_crc);  There are a set of functions that allow the creation of, and access to, an in-memory cache of the facet state. These functions are all defined in cqlrt_common.c. But they have to be declared to CQL to use them.  cg_schema_emit_facet_functions(&amp;decls);  The table sqlite_master is used to read schema state. That table has to be declared.  cg_schema_emit_sqlite_master(&amp;decls);  The full schema may be used by the upgraders, we need a declaration of all of that.  bprintf(&amp;decls, &quot;-- declare full schema of tables and views to be upgraded and their dependencies -- \\n&quot;); cg_generate_schema_by_mode(&amp;decls, SCHEMA_TO_DECLARE);  At this point a quick side-step to the output modes and region arguments is appropriate. Schema Region Arguments​ The upgrader honors the arguments --include_regions and --exclude_regions. If they are absent that is the same as &quot;include everything&quot; and &quot;exclude nothing&quot;. Recall that schema regions allow you to group schema as you wish. A typical use might be to define some &quot;core&quot; schema in a set of regions (maybe just one) and then a set of &quot;optional&quot; schema in some additional regions. An upgrader for just &quot;core&quot; could be created by adding --include_regions core. When creating upgraders for the optional parts, there are two choices: --include-regions optional1 : makes an upgrader for optional1 and core (the assumption being that optional1 was declared to depend on core)--include-regions optional1 --exclude-regions core : makes an upgrader for optional1 which should run after the standalone core upgrader has already run this allows you to share the &quot;core&quot; parts between any number of &quot;optional&quot; partsand of course this can nest; there can be several &quot;core&quot; parts; and so forth Schema Output Modes​ The flag bits are these: // We declare all schema we might depend on in this upgrade (this is the include list) // e.g. we need all our dependent tables so that we can legally use them in an FK #define SCHEMA_TO_DECLARE 1 // We only emit schema that we are actually updating (this is include - exclude) // e.g. a table on the exclude list is assumed to be upgraded by its own script // in a different run. #define SCHEMA_TO_UPGRADE 2 // We get TEMP items IF and ONLY IF this bit is set #define SCHEMA_TEMP_ITEMS 4  As we saw before, the schema we CRC is SCHEMA_TO_UPGRADE. This is all the regions that were selected but not their dependencies. The point of this is that you might make an upgrader for say a &quot;core&quot; part of your schema which can be shared and then make additional upgraders for various parts that use the &quot;core&quot; but are otherwise &quot;optional&quot;. Each of those &quot;optional&quot; upgraders needs its own CRC that includes its schema but not the &quot;core&quot; schema. However the &quot;optional&quot; schema can refer to &quot;core&quot; schema (e.g. in foreign keys) so all of the tables are declared. This is SCHEMA_TO_DECLARE mode. declare all schema you are allowed to refer toCRC, and upgrade, only the parts selected by the region arguments "},{"title":"The Schema Helpers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-schema-helpers","content":"This bit generates the facets table, the full name is your_global_proc_cql_schema_facets whereyour_global_proc is the --global_proc argument. This is referred to simply as the facets table. There is an identical temporary table that is used to store the contents of the facets table upon startup. This allows the upgrader to produce a complete difference. The facets table is nothing more than a mapping between the name of some facet of the schema (like a table, a view, a column) and its last known verison info -- usually its CRC. NOTE: this temp table predates the in-memory facets data structure so it could probably be removed the diff would have to work against the in-memory datastructure which is immutable hence just as good as a temp tablelook for a change like this soon The remaining procedures are for testing facet state or sqlite_master state. All of them get the usual global prefix. For ease of discussion I will elide the prefix for the rest of this document. check_column_exists : checks if the indicated column is present in sqlite_master necessary because there is no ALTER TABLE ADD COLUMN IF NOT EXISTS command create_cql_schema_facets_if_needed : actually creates the facets table if it does not existsave_cql_schema_facets : creates the cql_schema_facets_saved temp table and populates itcql_set_facet_version : sets one facet to the indicated value this writes to the database, not the in-memory version of the table cql_get_facet_version : reads a facet value from the facet table this is only used to check the master schema value, after that the in-memory version is used cql_get_version_crc : gets the CRC for a given schema version each schema version has its own CRC in addition to the global CRCthis information is stored in the facets table with a simple naming convention for the facet namethe in memory version of the table is always used here cql_set_version_crc : sets the CRC for a given schema version in the facet table this writes to the database, not the in-memory version of the table cql_drop_legacy_triggers : drops any triggers of the from tr__* for historical reasons the original triggers did not include tombstones when deletedthis kludge is here to clean up legacy triggers and its peculiar to Messenger onlythis should really be removed from the OSS version but it's never been a prioritysorry...  cg_schema_helpers(&amp;decls);  "},{"title":"Declared Upgrade Procedures​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#declared-upgrade-procedures","content":"The annotations can include an upgrade procedure. The term &quot;migration&quot; procedure is sometimes used as well and is synonymous. This is some code that should run after the schema alteration has been made to create/delete/move some data around in the new schema. Each of these must be declared before it is used and the declarations will be here, at the end of the decls section after this introductory comment.  bprintf(&amp;decls, &quot;-- declared upgrade procedures if any\\n&quot;);  "},{"title":"The Upgrading Workers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#the-upgrading-workers","content":"The main upgrader will invoke these key workers to do its job. This is where the preamblesection starts. It contains the meat of the upgrade steps wrapped in procedures that do the job.  cg_schema_emit_baseline_tables_proc(&amp;preamble, &amp;baseline); int32_t view_creates = 0, view_drops = 0; cg_schema_manage_views(&amp;preamble, &amp;view_drops, &amp;view_creates); int32_t index_creates = 0, index_drops = 0; cg_schema_manage_indices(&amp;preamble, &amp;index_drops, &amp;index_creates); int32_t trigger_creates = 0, trigger_drops = 0; cg_schema_manage_triggers(&amp;preamble, &amp;trigger_drops, &amp;trigger_creates); if (recreate_items_count) { cg_schema_manage_recreate_tables(&amp;preamble, recreates, recreate_items_count); } bool_t has_temp_schema = cg_schema_emit_temp_schema_proc(&amp;preamble); bool_t one_time_drop_needed = false;  These are the last of the worker methods: cg_schema_emit_baseline_tables_proc : emits a procedure that will create the schema at its baseline version this means whatever &quot;v0&quot; of the schema was, no creates or deletes have yet happened cg_schema_manage_views : creates the view management procedures cql_drop_all_views : drops all viewscql_create_all_views : creates all viewsboth of these run unless the global CRC matches cg_schema_manage_indices : creates the index management procedures cql_drop_all_indices : drops any index that exists and whose CRC changedcql_create_all_indices : creates any index whose CRC changedrecreating indices can be costly so it is only done if the index actually changed cg_schema_manage_triggers : creates the trigger management procedures cql_drop_all_triggers : drops all triggerscql_create_all_triggers : creates all triggersboth of these run unless the global CRC matchesadditionally any legacy triggers will be deleted (see cql_drop_legacy_triggers) cg_schema_manage_recreate_tables : creates the cql_recreate_tables worker the recreate_annotations array is used to find all the recreate tablesthe entries are sorted by group, then name, so that annotations within a group are togetherthe procedure contains code to delete the procedure or group and recreate it if the CRC does not matchthe CRC is computed using the code for create instructions and is stored in a facet with a suitable namethe easiest way to think of this code is that it always emits a chunk of recreates for a group ungrouped tables are a group of 1group delete/create instructions accumulate until the next entry is in a different group cg_schema_emit_temp_schema_proc : emits a procedure to create any temporary schema temp tables are always created in full at the latest versionthis code is run regardless of whether the global CRC matches or not All of these functions semantic outputs like all_indices_list, all_views_list, etc. to do their job (exceptcg_schema_manage_recreate_tables as noted). Generally they have all the data they need handed to them on a silver platter by the semantic pass. This is not an accident. Reading the Facets into Memory​ The setup_facets procedure simply selects out the entire facets table with a cursor and uses cql_facet_add to get them into a hash table. This is the primary source of facets information during the run. This is a good example of what the codegen looks like so we'll include this one in full.  // code to read the facets into the hash table bprintf(&amp;preamble, &quot;@attribute(cql:private)\\n&quot;); bprintf(&amp;preamble, &quot;CREATE PROCEDURE %s_setup_facets()\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot;BEGIN\\n&quot;); bprintf(&amp;preamble, &quot; BEGIN TRY\\n&quot;); bprintf(&amp;preamble, &quot; SET %s_facets := cql_facets_new();\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot; DECLARE C CURSOR FOR SELECT * from %s_cql_schema_facets;\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot; LOOP FETCH C\\n&quot;); bprintf(&amp;preamble, &quot; BEGIN\\n&quot;); bprintf(&amp;preamble, &quot; LET added := cql_facet_add(%s_facets, C.facet, C.version);\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot; END;\\n&quot;); bprintf(&amp;preamble, &quot; END TRY;\\n&quot;); bprintf(&amp;preamble, &quot; BEGIN CATCH\\n&quot;); bprintf(&amp;preamble, &quot; -- if table doesn't exist we just have empty facets, that's ok\\n&quot;); bprintf(&amp;preamble, &quot; END CATCH;\\n&quot;); bprintf(&amp;preamble, &quot;END;\\n\\n&quot;); ### The Main Upgrader And now we come to the main upgrading procedure `perform_upgrade_steps`. We'll go over this section by section. #### Standard Steps ```c // the main upgrade worker bprintf(&amp;main, &quot;\\n@attribute(cql:private)\\n&quot;); bprintf(&amp;main, &quot;CREATE PROCEDURE %s_perform_upgrade_steps()\\n&quot;, global_proc_name); bprintf(&amp;main, &quot;BEGIN\\n&quot;); bprintf(&amp;main, &quot; DECLARE schema_version LONG INTEGER NOT NULL;\\n&quot;); if (view_drops) { bprintf(&amp;main, &quot; -- dropping all views --\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_drop_all_views();\\n\\n&quot;, global_proc_name); } if (index_drops) { bprintf(&amp;main, &quot; -- dropping condemned or changing indices --\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_drop_all_indices();\\n\\n&quot;, global_proc_name); } if (trigger_drops) { bprintf(&amp;main, &quot; -- dropping condemned or changing triggers --\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_drop_all_triggers();\\n\\n&quot;, global_proc_name); } if (baseline.used &gt; 1) { llint_t baseline_crc = (llint_t)crc_charbuf(&amp;baseline); bprintf(&amp;main, &quot; ---- install baseline schema if needed ----\\n\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_get_version_crc(0, schema_version);\\n&quot;, global_proc_name); bprintf(&amp;main, &quot; IF schema_version != %lld THEN\\n&quot;, baseline_crc); bprintf(&amp;main, &quot; CALL %s_cql_install_baseline_schema();\\n&quot;, global_proc_name); bprintf(&amp;main, &quot; CALL %s_cql_set_version_crc(0, %lld);\\n&quot;, global_proc_name, baseline_crc); bprintf(&amp;main, &quot; END IF;\\n\\n&quot;); }  First we deal with the preliminaries: drop the views if there are anydrop the indices that need droppingdrop the triggers if there are anyinstall the baseline schema if there is any Process Standard Annotations​ In this phase we walk the annotations from schema_annotations which are now stored in notes. They have been sorted in exactly the right order to process them (by version, then type, then target). We'll create one set of instructions per version number as we simply accumulate instructions for any version while we're still on the same version then spit them all out. Adding target to the sort order ensures that the results have a total ordering (there are no ties that might yield an ambiguous order). We set up a loop to walk over the annotations and we flush if we ever encounter an annotation for a different version number. We'll have to force a flush at the end as well. cg_schema_end_versiondoes the flush.  int32_t prev_version = 0; for (int32_t i = 0; i &lt; schema_items_count; i++) { schema_annotation *note = &amp;notes[i]; ast_node *version_annotation = note-&gt;annotation_ast; uint32_t type = note-&gt;annotation_type; Contract(type &gt;= SCHEMA_ANNOTATION_FIRST &amp;&amp; type &lt;= SCHEMA_ANNOTATION_LAST); Contract(is_ast_version_annotation(version_annotation)); EXTRACT_OPTION(vers, version_annotation-&gt;left); Invariant(note-&gt;version == vers); Invariant(vers &gt; 0); if (prev_version != vers) { cg_schema_end_version(&amp;main, &amp;upgrade, &amp;pending, prev_version); prev_version = vers; }  If we find any item that is in a region we are not upgrading, we skip it.  CSTR target_name = note-&gt;target_name; Invariant(type &gt;= SCHEMA_ANNOTATION_FIRST &amp;&amp; type &lt;= SCHEMA_ANNOTATION_LAST); if (!include_from_region(note-&gt;target_ast-&gt;sem-&gt;region, SCHEMA_TO_UPGRADE)) { continue; }  There are several annotation types. Each one requires appropriate commands  switch (type) { case SCHEMA_ANNOTATION_CREATE_COLUMN: { ... emit ALTER TABLE ADD COLUMN if the column does not already exist break; } case SCHEMA_ANNOTATION_DELETE_COLUMN: { ... it's not possible to delete columns in SQLite (this is changing) ... we simply emit a comment and move on break; } case SCHEMA_ANNOTATION_CREATE_TABLE: { ... if the table is moving from @recreate to @create we have to drop any stale version ... of it one time. We emit a call to `cql_one_time_drop` and record that we need ... to generate that procedure in `one_time_drop_needed`. ...in all cases emit a CREATE TABLE IF NOT EXISTS break; } case SCHEMA_ANNOTATION_DELETE_TABLE: { ... emit DROP TABLE IF EXISTS for the target break; } case SCHEMA_ANNOTATION_DELETE_INDEX: case SCHEMA_ANNOTATION_DELETE_VIEW: case SCHEMA_ANNOTATION_DELETE_TRIGGER: ... this annotation indicates there is a tombstone on the item ... this was handled in the appropriate `manage` worker above, nothing needs ... to be done here except run any migration procs (see below) break; case SCHEMA_ANNOTATION_AD_HOC: ... ad hoc migration procs allow for code to be run one time when we hit ... a particular schema version, this just allows the migration proc to run // no annotation based actions other than migration proc (handled below) Contract(version_annotation-&gt;right); bprintf(&amp;upgrade, &quot; -- ad hoc migration proc %s will run\\n\\n&quot;, target_name); break; }  The above constitutes the bulk of the upgrading logic which, as you can see, isn't that complicated. Any of the above might have a migration proc. If there is one in the node, then generate: emit a call to cql_facet_find to see if the migration proc has already runemit a declaration for the migration proc into the decls sectionemit a call to the procedure (it accept no arguments)emit a call to cql_set_facet_version to record that the migrator ran When the loop is done, any pending migration code is flushed using cg_schema_end_version again. At this point we can move on to the finalization steps. Finalization Steps​ With the standard upgrade finished, there is just some house keeping left:  if (recreate_items_count) { bprintf(&amp;main, &quot; CALL %s_cql_recreate_tables();\\n&quot;, global_proc_name); } if (view_creates) { bprintf(&amp;main, &quot; CALL %s_cql_create_all_views();\\n&quot;, global_proc_name); } if (index_creates) { bprintf(&amp;main, &quot; CALL %s_cql_create_all_indices();\\n&quot;, global_proc_name); } if (trigger_creates) { bprintf(&amp;main, &quot; CALL %s_cql_create_all_triggers();\\n&quot;, global_proc_name); } bprintf(&amp;main, &quot; CALL %s_cql_set_facet_version('cql_schema_version', %d);\\n&quot;, global_proc_name, prev_version); bprintf(&amp;main, &quot; CALL %s_cql_set_facet_version('cql_schema_crc', %lld);\\n&quot;, global_proc_name, schema_crc); bprintf(&amp;main, &quot;END;\\n\\n&quot;);  cql_recreate_tables : must run if there are any tables marked recreate this procedure will have code to drop and recreate any changed tablesthis procedure was created by cg_schema_manage_recreate_tables and that process is described above basically, it uses recreate_annotations to do the job any that were condemned by marking with @delete will not be created again here cql_create_all_views : must run if there are any views, they need to be put back any that were condemned by marking with @delete are not created again here cql_create_all_indices : must run if there are any indices, this will create any that are missing any that were changing were previously deleted, this is where they come backany that were condemned by marking with @delete are not created again here cql_create_all_triggers : must run if there are any triggers, they need to be put back any that were condemned by marking with @delete are not created again heretriggers might cause weird side-effects during upgrade hence they are always droppedstale triggers especially could be problematicany triggers that refer to views couldn't possibly run as the views are gonehence, triggers are always dropped and recreated The &quot;Main&quot; Steps​ We're getting very close to the top level now perform_needed_upgrades : this orchestrates the upgrade, if it is called there is one cql_facet_find : is used to check for a schema &quot;downgrade&quot; abort with an error if that happens save_cql_schema_facets : saves the facets as they exist so we can diff themperform_upgrade_steps : does the upgradea LEFT OUTER JOIN between cql_schema_facets and cql_schema_facets_saved reports differencesany errors will cause the normal CQL error flow the main entry point is named by global_proc_name create_cql_schema_facets_if_needed is used to create the facets table if it doesn't already existthe special facet cql_schema_crc is read from the facets tableif the CRC stored there matches our target then we return &quot;no differences&quot;, otherwisesetup_facets : loads the in-memory version of the facets tableperform_needed_upgrades : does the work and creates the diffcql_facets_delete is used to free the in-memory storage, even if there were errors in perform_needed_upgrades cql_install_temp_schema : installs temporary schema if there is any, regardless of the CRC the one_time_drop code is emitted if it was needed Writing the Buffer​ At this point the main buffers decls, preamble, and main are ready to go. We're back to where we started but we can quickly recap the overall flow.  CHARBUF_OPEN(output_file); bprintf(&amp;output_file, &quot;%s\\n&quot;, decls.ptr); bprintf(&amp;output_file, &quot;%s&quot;, preamble.ptr); bprintf(&amp;output_file, &quot;%s&quot;, main.ptr); cql_write_file(options.file_names[0], output_file.ptr); CHARBUF_CLOSE(output_file);  There is nothing left but to CHARBUF_CLOSE the interim buffers we created. "},{"title":"Recap​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#recap-3","content":"At present cg_schema.c accomplishes a lot and is fairly light at only 1313 lines (at present). It is able to do so because it can leverage heavy lifting done in the semantic analysis phase and schema generation that can be done like all other SQL generation by the echoing code discussed in Part 1. Topics covered included: the essential sources of schema information from the semantic passthe state tables used in the database and helpers for read/write of the samethe interaction with schema regionsthe prosecution steps for tables, columns, views, triggers, indicesthe key annotation types and what code they createthe handling of recreate tables, temp tables, and the base schemahow all of these are wired together starting from the upgrader's &quot;main&quot; As with the other parts, no attempt was made to cover every function in detail. That is best done by reading the source code. But there is overall structure here and an understanding of the basic principles is helpful before diving into the source code. "},{"title":"Part 7: JSON Generation​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-7-json-generation","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-6","content":"Part 7 continues with a discussion of the JSON generation code. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how JSON creation works in general -- the core strategies and implementation choices -- so that when reading the source you will have an idea how it all hangs together. To accomplish this, we'll illustrate the key strategies used to extract the data and format the JSON. "},{"title":"JSON Schema​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#json-schema","content":"The JSON schema is described in Chapter 13 of the Guide and there is a nice diagram of its grammar for reference. So, we won't be discussing all the details of the output. Instead we're going to go over the theory of how the JSON generator works. It is structured very much like the other code generators but it happens to produce a JSON file. It's call the &quot;JSON Schema&quot; because most of the content is a description of the database schema in JSON form. As such it's almost entirely just a simple walk of the AST in the correct order. The only really tricky bit is the extra dependency analysis on the AST. This allows us to emit usage information in the output for downstream tools to use as needed. We'll cover these topics: walking the ASTformattingcomputing the dependencies This should be a short chapter compared to the others, this output really is much simpler to create than the C or the schema upgrader. "},{"title":"Walking the AST​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#walking-the-ast","content":"If you run this command: $ cql --in x --rt json_schema --cg x.json  Where x is an empty file, you'll get the following skeletal JSON, lightly reformatted for brevity: { &quot;tables&quot; : [ ], &quot;virtualTables&quot; : [ ], &quot;views&quot; : [ ], &quot;indices&quot; : [ ], &quot;triggers&quot; : [ ], &quot;attributes&quot; : [ ], &quot;queries&quot; : [ ], &quot;inserts&quot; : [ ], &quot;generalInserts&quot; : [ ], &quot;updates&quot; : [ ], &quot;deletes&quot; : [ ], &quot;general&quot; : [ ], &quot;regions&quot; : [ ], &quot;adHocMigrationProcs&quot; : [ ], &quot;enums&quot; : [ ] }  From this we can deduce a great deal of the structure of the code: // Main entry point for json schema format cql_noexport void cg_json_schema_main(ast_node *head) { Contract(options.file_names_count == 1); cql_exit_on_semantic_errors(head); tables_to_procs = symtab_new(); CHARBUF_OPEN(main); charbuf *output = &amp;main; bprintf(output, &quot;%s&quot;, rt-&gt;source_prefix); // master dictionary begins bprintf(output, &quot;\\n{\\n&quot;); BEGIN_INDENT(defs, 2); cg_json_tables(output); bprintf(output, &quot;,\\n&quot;); cg_json_virtual_tables(output); bprintf(output, &quot;,\\n&quot;); cg_json_views(output); bprintf(output, &quot;,\\n&quot;); cg_json_indices(output); bprintf(output, &quot;,\\n&quot;); cg_json_triggers(output); bprintf(output, &quot;,\\n&quot;); cg_json_stmt_list(output, head); bprintf(output, &quot;,\\n&quot;); cg_json_regions(output); bprintf(output, &quot;,\\n&quot;); cg_json_ad_hoc_migration_procs(output); bprintf(output, &quot;,\\n&quot;); cg_json_enums(output); if (options.test) { bprintf(output, &quot;,\\n&quot;); cg_json_table_users(output); } END_INDENT(defs); bprintf(output, &quot;\\n}\\n&quot;); cql_write_file(options.file_names[0], output-&gt;ptr); CHARBUF_CLOSE(main); SYMTAB_CLEANUP(tables_to_procs); }  cg_json_schema_main is the main function and you can see that it mirrors that skeletal JSON output nearly exactly with some additional test output options. We'll cover the test output in a later section when we've had a chance to discuss the dependency analysis. Example JSON Writer: Views​ These are sufficiently easy that we can just walk through one of the procedures front to back. Let's look at the &quot;views&quot; section. // The set of views look rather like the query section in as much as // they are in fact nothing more than named select statements. However // the output here is somewhat simplified. We only emit the whole select // statement and any binding args, we don't also emit all the pieces of the select. static void cg_json_views(charbuf *output) { bprintf(output, &quot;\\&quot;views\\&quot; : [\\n&quot;); BEGIN_INDENT(views, 2); int32_t i = 0; for (list_item *item = all_views_list; item; item = item-&gt;next) { ast_node *ast = item-&gt;ast; Invariant(is_ast_create_view_stmt(ast)); ast_node *misc_attrs = NULL; ast_node *attr_target = ast-&gt;parent; if (is_ast_stmt_and_attr(attr_target)) { EXTRACT_STMT_AND_MISC_ATTRS(stmt, misc, attr_target-&gt;parent); misc_attrs = misc; } cg_json_test_details(output, ast, misc_attrs); EXTRACT_OPTION(flags, ast-&gt;left); EXTRACT(view_and_attrs, ast-&gt;right); EXTRACT(name_and_select, view_and_attrs-&gt;left); EXTRACT_ANY_NOTNULL(select_stmt, name_and_select-&gt;right); EXTRACT_ANY_NOTNULL(name_ast, name_and_select-&gt;left); EXTRACT_STRING(name, name_ast); if (i &gt; 0) { bprintf(output, &quot;,\\n&quot;); } bprintf(output, &quot;{\\n&quot;); bool_t is_deleted = ast-&gt;sem-&gt;delete_version &gt; 0; BEGIN_INDENT(view, 2); bprintf(output, &quot;\\&quot;name\\&quot; : \\&quot;%s\\&quot;&quot;, name); bprintf(output, &quot;,\\n\\&quot;CRC\\&quot; : \\&quot;%lld\\&quot;&quot;, crc_stmt(ast)); bprintf(output, &quot;,\\n\\&quot;isTemp\\&quot; : %d&quot;, !!(flags &amp; VIEW_IS_TEMP)); bprintf(output, &quot;,\\n\\&quot;isDeleted\\&quot; : %d&quot;, is_deleted); if (is_deleted) { bprintf(output, &quot;,\\n\\&quot;deletedVersion\\&quot; : %d&quot;, ast-&gt;sem-&gt;delete_version); cg_json_deleted_migration_proc(output, view_and_attrs); } if (ast-&gt;sem-&gt;region) { cg_json_emit_region_info(output, ast); } if (misc_attrs) { bprintf(output, &quot;,\\n&quot;); cg_json_misc_attrs(output, misc_attrs); } cg_json_projection(output, select_stmt); cg_fragment_with_params(output, &quot;select&quot;, select_stmt, gen_one_stmt); cg_json_dependencies(output, ast); END_INDENT(view); bprintf(output, &quot;\\n}\\n&quot;); i++; } END_INDENT(views); bprintf(output, &quot;]&quot;); }  View Loop​ Already we can see the structure emerging, and of course its nothing more than a bunch of bprintf. Let's do it section by section: bprintf(output, &quot;\\&quot;views\\&quot; : [\\n&quot;); BEGIN_INDENT(views, 2); for (list_item *item = all_views_list; item; item = item-&gt;next) { .. } END_INDENT(views); bprintf(output, &quot;]&quot;);  Unsurprisingly, this code will iterate the all_views_list which was created precisely for this kind of output. The semantic pass populates this list for use downstream. We'll deal with BEGIN_INDENT a bit later, but it should be clear what it does by the name for now. So we've made the &quot;views&quot; section and we'll put 0 or more views in it. View Extraction​ The next section extracts the necessary information and emits the test output:  ast_node *ast = item-&gt;ast; Invariant(is_ast_create_view_stmt(ast)); ast_node *misc_attrs = NULL; ast_node *attr_target = ast-&gt;parent; if (is_ast_stmt_and_attr(attr_target)) { EXTRACT_STMT_AND_MISC_ATTRS(stmt, misc, attr_target-&gt;parent); misc_attrs = misc; } cg_json_test_details(output, ast, misc_attrs); EXTRACT_OPTION(flags, ast-&gt;left); EXTRACT(view_and_attrs, ast-&gt;right); EXTRACT(name_and_select, view_and_attrs-&gt;left); EXTRACT_ANY_NOTNULL(select_stmt, name_and_select-&gt;right); EXTRACT_ANY_NOTNULL(name_ast, name_and_select-&gt;left); EXTRACT_STRING(name, name_ast);  The is_ast_stmt_and_attr node tell us if there were any misc attributes on the statement. Those attributes can be extracted and printed. We have to look up the tree a little bit from where we are because this is the &quot;all views&quot; list, if there were attributes on this view they were attached two levels up. In any case misc_attrs ends with attributes if there are any. After the test output, the necessary view attributes are extracted the usual way with EXTRACT macros for the view shape. Test Output​ static void cg_json_test_details(charbuf *output, ast_node *ast, ast_node *misc_attrs) { if (options.test) { bprintf(output, &quot;\\nThe statement ending at line %d\\n&quot;, ast-&gt;lineno); bprintf(output, &quot;\\n&quot;); gen_set_output_buffer(output); if (misc_attrs) { gen_with_callbacks(misc_attrs, gen_misc_attrs, NULL); } gen_with_callbacks(ast, gen_one_stmt, NULL); bprintf(output, &quot;\\n\\n&quot;); } }  All of the JSON fragments have the usual test pattern &quot;The statement ending at line nnn&quot;. This means that the normal validator will be able to find comments in the test file and associate them with json parts. The testing strategies are discussed in[Part 4]((https://cgsql.dev/cql-guide/int04). In addition, while in test mode, we also emit the original statement that caused this JSON fragment to be created. This allows the test patterns to cross check the input and output and also makes the test output more readable for humans. Note that in test mode the JSON is effectively corrupted by the test output as it is not well-formed JSON in any way. So use of --test is strictly for validation only. View Basics​ All of the things that go into the JSON have some attributes that are universally present and generally come directly from the AST.  if (i &gt; 0) { bprintf(output, &quot;,\\n&quot;); } bprintf(output, &quot;{\\n&quot;); bool_t is_deleted = ast-&gt;sem-&gt;delete_version &gt; 0; BEGIN_INDENT(view, 2); bprintf(output, &quot;\\&quot;name\\&quot; : \\&quot;%s\\&quot;&quot;, name); bprintf(output, &quot;,\\n\\&quot;CRC\\&quot; : \\&quot;%lld\\&quot;&quot;, crc_stmt(ast)); bprintf(output, &quot;,\\n\\&quot;isTemp\\&quot; : %d&quot;, !!(flags &amp; VIEW_IS_TEMP)); bprintf(output, &quot;,\\n\\&quot;isDeleted\\&quot; : %d&quot;, is_deleted); if (is_deleted) { bprintf(output, &quot;,\\n\\&quot;deletedVersion\\&quot; : %d&quot;, ast-&gt;sem-&gt;delete_version); cg_json_deleted_migration_proc(output, view_and_attrs); } ... END_INDENT(view); bprintf(output, &quot;\\n}\\n&quot;); i++; }  This part of the output is the simplest we emit a comma if we need one (only the first entry doesn't)we start the view object '{'more indenting for the interior of the viewemit the view nameemit the CRC of the view (this makes it easy to see if the view changed) crc_stmt computes the CRC by echoing the statement into a scratch buffer and then running the CRC algorithm on that buffer note the &quot;,\\n&quot; pattern, this pattern is used because sometimes there are optional parts and using a leading &quot;,\\n&quot; makes it clear which part is supposed to emit the comma it turns out getting the commas right is one of the greater annoyances of JSON output emit &quot;isTemp&quot;emit &quot;isDeleted&quot;if the view is deleted, emit &quot;deletedVersion&quot;if there is a migration procedure on the @delete attribute emit that as well cg_json_deleted_migration_proc scans the attribute list for @delete attribute and emits the procedure name on that attribute if there is one Optional Info​ The next fragment emits two optional pieces that are present in many types of objects:  if (ast-&gt;sem-&gt;region) { cg_json_emit_region_info(output, ast); } if (misc_attrs) { bprintf(output, &quot;,\\n&quot;); cg_json_misc_attrs(output, misc_attrs); }  if there is a region assocatied with this view, we emit it here cg_json_emit_region_info emits two things: the view's regionthe &quot;deployment region&quot; of that region if any (regions are contained in deployable groups)see Chapter 10 for more info on regions and deployment regions if there are any miscellaneous attributes they are emitted we'll use cg_json_misc_attrs as our general formatting example when we get to that The View Details​ There is very little left in the view emitting code:  cg_json_projection(output, select_stmt); cg_fragment_with_params(output, &quot;select&quot;, select_stmt, gen_one_stmt); cg_json_dependencies(output, ast);  cg_json_projection emits the name and type of each column in the view select listcg_fragment_with_params emits the statement that creates the view in an attribute named &quot;select&quot; the normal echoing code emits the statementviews have no variables to bind but other statement forms inside of procedures can have variables in the statementthe variable names are replace with &quot;?&quot; in the text of the statementthe names of the variable appear in &quot;selectArgs&quot; (always empty for views) cg_json_dependencies emits the tables and views that were used by this view, it gets its own section Those few things produce all JSON for a view. All the other schema elements do basically the same things. Most of the helpers are shared so, for instance, regions, misc attributes, and dependencies appear in nearly every kind of object in the JSON. "},{"title":"Formatting the JSON​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#formatting-the-json","content":"To make the JSON pretty we want to indent it appropriately and put commas in the right places. There are some useful macros for this, and they all rely on the fact that the emitted text goes to a charbuf variable creatively called output. Here's a sample procedure that was mentioned earlier, it does the usual things: // Emit a list of attributes for the current entity, it could be any kind of entity. // Whatever it is we spit out the attributes here in array format. static void cg_json_misc_attrs(charbuf *output, ast_node *_Nonnull list) { Contract(is_ast_misc_attrs(list)); bprintf(output, &quot;\\&quot;attributes\\&quot; : [\\n&quot;); BEGIN_INDENT(attr, 2); BEGIN_LIST; for (ast_node *item = list; item; item = item-&gt;right) { COMMA; cg_json_misc_attr(output, item-&gt;left); } END_LIST; END_INDENT(attr); bprintf(output, &quot;]&quot;); }  The miscellaneous attributes are going to be emitted in a list, and since any one attribute can actually be a list of attributes, this ends up being recursive (cg_json_misc_attr can end up calling back to cg_json_misc_attrs). Attributes are actually quite flexible. Let's look at the helpers that will be used to do this formatting. From charbuf.h: // These helpers push a buffer and use it for the output temporarily. // When the buffer is finished (at END_INDENT) bindent is used to // indent it by the indicated amount. They assume the output buffer is called // &quot;output&quot;. #define BEGIN_INDENT(name, level) \\ charbuf *name##_saved = output; \\ int32_t name##_level = level; \\ CHARBUF_OPEN(name); \\ output = &amp;name; #define END_INDENT(name) \\ output = name##_saved; \\ bindent(output, &amp;name, name##_level); \\ CHARBUF_CLOSE(name); \\  BEGIN_INDENT : sets up the indenting save the current output bufferstash the desired indent level in a named localmake a new scratch buffer using the given nameset the output to be the scratch buffer END_INDENT : flushes the indented stuff restores the output buffer to what it waswrites the temporary buffer into the output buffer, indenting it by the desired abountclose the temporrary buffer bindent : a charbuf helper that reads the input line by line and writes it with indenting spaces to the output The rest of the helpers manage the commas in the (nested) lists: // These little helpers are for handling comma seperated lists where you may or may // not need a comma in various places. The local tracks if there is an item already // present and you either get &quot;,\\n&quot; or just &quot;\\n&quot; as needed. #define BEGIN_LIST bool_t list_start = 1 #define CONTINUE_LIST bool_t list_start = 0 #define COMMA if (!list_start) bprintf(output, &quot;,\\n&quot;); else list_start = 0 #define END_LIST if (!list_start) bprintf(output, &quot;\\n&quot;)  BEGIN_LIST : starts a list, records that we are at the beginning of the listCONTINUE_LIST : starts a list, but assumes things have already been put into itCOMMA : a new item is about to be emitted, add a comma if one is needed i.e. add a comma if we are not on the first item END_LIST : emits a blank line if anything went into the list this puts us in the write place to put an end marker such as ']' or '}' So reviewing this bit of code, emit the attribute name and start the array &quot;[&quot;we start indentingwe start a listwe emit a comma if neededwe emit the new misc attribute this will crack the AST, and get the attribute name and valuethis can recursecg_json_misc_attr is pretty simple and a good exercise for the reader repeat for all attributesend the listend the indentingemit the attribute end &quot;]&quot; Quoted Text​ Most quoted text in the JSON output is either hard-coded constants, or else is a CQL identifier and therefore has no special characters. Those two cases are very simple and no escaping or special formatting is needed. We just emit the text with quotes around it. However, there are cases where general text that might have special characters in it needs to be emitted. When that happens a call like this is used: cg_pretty_quote_plaintext( sql.ptr, output, PRETTY_QUOTE_JSON | PRETTY_QUOTE_SINGLE_LINE);  cg_pretty_quote_plaintext has been discussed before when it was used to create SQL strings for the C output. This usage is similar. Here we're using PRETTY_QUOTE_JSON to indicate that only escape sequences supported by JSON should appear in the output. The format for hexadecimal escape sequences for non-printable characters is different than C and some of the C short escapes are not supported (e.g. &quot;\\a&quot; is not legal JSON). We always use PRETTY_QUOTE_SINGLE_LINEin the JSON output so that multi-line SQL is rendered as one line. Remember here we are are JSON-escaping the SQL so the embedded newlines in the original SQL were already converted to '\\' 'n' (two characters) and therefore any newlines still in the string are those placed there by the line breaking of the SQL not by newlines in string literals. Hence those newlines are optional, any whitespace will do. In any case, cg_pretty_quote_plaintext is just the function to do what we need and this output is only slightly different than what would be emitted for the C codegen. "},{"title":"Dependency Analysis​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#dependency-analysis","content":"There are a number of places where dependencies have to be computed. To do this job, this function is used universally: // For procedures and triggers we want to walk the statement list and emit a set // of dependency entries that show what the code in question is using and how. // We track tables that are used and if they appear in say the FROM clause // (or some other read-context) or if they are the subject of an insert, update, // or delete. We also track the use of nested procedures and produce a list of // procs the subject might call. Of course no proc calls ever appear in triggers. static void cg_json_dependencies(charbuf *output, ast_node *ast) { ... }  In general this code walks any AST looking for a variety of patterns in the AST that correspond to use of tables, directly or indirectly. Actually more accurately,cg_json_dependencies uses find_table_refs to do the job, and it does so by: creating an output buffer for each kind of thing find_table_refs might findsetting up a simple callback to fill in the bufferinvoking find_table_refsformatting the buffers that have any resulting dependency data and emitting them as dependencies This works for any kind of AST really, though typically you do this for procedures or triggers because they have an interesting body. But the analysis also makes sense for views because views can refer to other views and to tables. The primary code looks like this:  table_callbacks callbacks = { .callback_any_table = cg_found_table, .callback_any_view = cg_found_view, .callback_inserts = cg_found_insert, .callback_updates = cg_found_update, .callback_deletes = cg_found_delete, .callback_from = cg_found_from, .callback_proc = cg_found_proc, .callback_context = &amp;context, }; find_table_refs(&amp;callbacks, ast);  And an example callback: // This is the callback function that tells us a view name was found in the body // of the stored proc we are currently examining. The void context information // is how we remember which proc we were processing. For each table we have // a character buffer. We look it up, create it if not present, and write into it. // We also write into the buffer for the current proc which came in with the context. static void cg_found_view( CSTR view_name, ast_node* table_ast, void* pvContext) { json_context *context = (json_context *)pvContext; Contract(context-&gt;cookie == cookie_str); // sanity check Contract(context-&gt;used_views); add_name_to_output(context-&gt;used_views, view_name); }  The callback gets the pvContext back, which is the context local variable from cg_json_dependencies. This has all the buffers in it. All we have to do is add the name to the buffer, which is done as follows: static void add_name_to_output(charbuf* output, CSTR table_name) { Contract(output); if (output-&gt;used &gt; 1) { bprintf(output, &quot;, &quot;); } bprintf(output, &quot;\\&quot;%s\\&quot;&quot;, table_name); }  add a comma if neededadd the namedone :D Note: The added name of course doesn't have to be a table name, but it usually is. So we can see that find_table_refs will tell us the kind of thing it found and the name of the thing. When all this is done each kind of dependency is emitted if it exists, like so:  if (used_views.used &gt; 1) { bprintf(output, &quot;,\\n\\&quot;usesViews\\&quot; : [ %s ]&quot;, used_views.ptr); }  This gives us a quoted list of the dependencies. Now, how do we find these? Walking the AST for Dependencies​ find_table_refs is a fairly simple tree walk that looks for certain key patterns actually the tree walk happens in find_table_node which looks for tables and procedure calls in the nested AST. find_table_refs records the callbacks that were specified, and it makes some symbol tables so that the same table/view/procedure is not reported twice. After that it starts walking the AST recursively looking for the patterns. Here's an example:  // Recursively finds table nodes, executing the callback for each that is found. The // callback will not be executed more than once for the same table name. static void find_table_node(table_callbacks *callbacks, ast_node *node) { // Check the type of node so that we can find the direct references to tables. We // can't know the difference between a table or view in the ast, so we will need to // later find the definition to see if it points to a create_table_stmt to distinguish // from views. find_ast_str_node_callback alt_callback = NULL; symtab *alt_visited = NULL; ast_node *table_or_view_name_ast = NULL; ... else if (is_ast_delete_stmt(node)) { EXTRACT_ANY_NOTNULL(name_ast, node-&gt;left); table_or_view_name_ast = name_ast; alt_callback = callbacks-&gt;callback_deletes; alt_visited = callbacks-&gt;visited_delete; } ... }  The code afterward will do these steps: notice that table_or_view_name_ast was set, hence something was founddetermine that it is in fact a tablecall the general callback for any table seen (but only once for this table)call the alternate callback that this is a table being deleted (but only once) Almost all the other operations work similarly: table_or_view_name_ast is setalt_callback is called but only ifalt_visited doesn't already have the symbol The exception to the above is the processing that's done for procedure calls. We've actually only talked about table dependencies so far but, additionally, any procedure includes dependencies on the procedures it calls. If a procedure call is found then callbacks-&gt;callback_proc is used andcallbacks-&gt;visited_proc verifies that there are no duplicates. So much the same except the names are procedure names. Note that the code does not do transitive closure of procedure calls because in general the called procedure is likely in a different translation unit. However with the direct calls in place it is easy enough to do transitive closure from the JSON if you do have all the procedures in one unit or if you have several JSON results from different compilations. However, when a view is encountered, the code does follow into the view body and recursively reports what the view uses. This means that the reported tables do include any tables that were used indirectly via views. Finally, any CTEs that are used will not be reported becausefind_table_or_view_even_deleted will fail for a CTE. However the body of the CTE is processed so while the CTE name does not appear, what the CTE uses does appear, just like any other table usage. "},{"title":"Additional Test Output​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#additional-test-output","content":"The extra test output is simply a reverse index: a mapping that goes from any table to the procedures that depend on that table. The mapping can easily be created by processing the JSON for procedures, each such procedure includes its dependency information. As a result it's only used for additional validation. "},{"title":"Recap​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#recap-4","content":"The JSON output produced by cg_json_schema.c is similar to other codegen output but lacks most of the complexities. It deals largely with the declared schema and the declared procedures and their parameters. Most of the output it needs to produce is well supported by the normal text emission features in the compiler and so we end up with a very straightforward walk of the AST, visiting each of the relevant kinds of nodes in order. Topics covered included: the types of output that will be producedthe general structure of the main JSON emitteran example emittertypical formatting features necessary to produce good quality JSONa tour of the dependency emitter As with the other parts, no attempt was made to cover every function in detail. That is best done by reading the source code. But there is overall structure here and an understanding of the basic principles is helpful before diving into the source code. "},{"title":"Part 8: Test Helpers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#part-8-test-helpers","content":""},{"title":"Preface​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#preface-7","content":"Part 8 continues with a discussion of the Test Helper generation code. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how helpers are created in general -- the core strategies and implementation choices -- so that when reading the source you will have an idea how it all hangs together. "},{"title":"Test Helpers​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#test-helpers","content":"The testability features are described in Chapter 12 of the Guide So, we won't be discussing all the details of what can be created. Instead we're going to go over the theory of how the generator works. This generator is somewhat different than others in that it only concerns itself with procedures and only those that have been suitably annotated -- there are large parts of the tree that are of no interest to the test helper logic, including, importantly the body of procedures. Only the signature matters. As we'll see there is a fairly large family of generators that are like this. We'll have one section for every kind of output, but really only the dummy_test helper is worthy of detailed discussion the others, as we'll see, are very simple. "},{"title":"Initialization​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#initialization","content":"The generator is wired like the others with a suitable main, this one is pretty simple: // Main entry point for test_helpers cql_noexport void cg_test_helpers_main(ast_node *head) { Contract(options.file_names_count == 1); cql_exit_on_semantic_errors(head); exit_on_validating_schema(); cg_test_helpers_reset_globals(); CHARBUF_OPEN(output_buf); cg_th_output = &amp;output_buf; bprintf(cg_th_output, &quot;%s&quot;, rt-&gt;source_prefix); cg_test_helpers_stmt_list(head); cql_write_file(options.file_names[0], cg_th_output-&gt;ptr); CHARBUF_CLOSE(output_buf); cg_test_helpers_reset_globals(); }  The text output will be ultimately put into output_buf defined here and helper_flags will track which kinds of helpers we saw. This helps us to emit the right sections of output as we'll see. The code iterates the AST looking at the top level statement list only and in particular looking for CREATE PROCstatements. // Iterate through statement list static void cg_test_helpers_stmt_list(ast_node *head) { Contract(is_ast_stmt_list(head)); init_all_trigger_per_table(); init_all_indexes_per_table(); CHARBUF_OPEN(procs_buf); CHARBUF_OPEN(decls_buf); cg_th_procs = &amp;procs_buf; cg_th_decls = &amp;decls_buf; for (ast_node *ast = head; ast; ast = ast-&gt;right) { EXTRACT_STMT_AND_MISC_ATTRS(stmt, misc_attrs, ast); if (is_ast_create_proc_stmt(stmt)) { EXTRACT_STRING(proc_name, stmt-&gt;left); cg_test_helpers_create_proc_stmt(stmt, misc_attrs); } } bprintf(cg_th_output, &quot;%s&quot;, decls_buf.ptr); bprintf(cg_th_output, &quot;\\n&quot;); bprintf(cg_th_output, &quot;%s&quot;, procs_buf.ptr); CHARBUF_CLOSE(decls_buf); CHARBUF_CLOSE(procs_buf); symtab_delete(all_tables_with_triggers); all_tables_with_triggers = NULL; symtab_delete(all_tables_with_indexes); all_tables_with_indexes = NULL; }  There are some preliminaries: we make a symbol table that maps from tables names to the list of triggers on that table by walking all the triggerswe make a symbol table that maps from tables names to the list of indices on that table by walking all the indiceswe'll need two buffers one for declarations (that must go first) and one for procedure bodieseach CREATE PROC statement potentially contributes to both sectionscg_test_helpers_create_proc_stmt checks for the helper attributes and sets up the dispatch to emit the test helpers To do this we have to walk any misc attributes on the procedure we're looking for things of the form @attribute(cql:autotest=xxx) static void cg_test_helpers_create_proc_stmt(ast_node *stmt, ast_node *misc_attrs) { Contract(is_ast_create_proc_stmt(stmt)); if (misc_attrs) { helper_flags = 0; dummy_test_infos = symtab_new(); find_misc_attrs(misc_attrs, test_helpers_find_ast_misc_attr_callback, stmt); symtab_delete(dummy_test_infos); dummy_test_infos = NULL; } }  find_misc_attrs calls test_helpers_find_ast_misc_attr_callback. We're going to keep track of which kinds of helpers we have found to help us with the output. This is where helper_flagscomes in. The flags are: #define DUMMY_TABLE 1 // dummy_table attribute flag #define DUMMY_INSERT 2 // dummy_insert attribute flag #define DUMMY_SELECT 4 // dummy_select attribute flag #define DUMMY_RESULT_SET 8 // dummy_result_set attribute flag #define DUMMY_TEST 0x10 // dummy_test attribute flag  And now we're ready for actual dispatch: // This is invoked for every misc attribute on every create proc statement // in this translation unit. We're looking for attributes of the form cql:autotest=(...) // and we ignore anything else. static void test_helpers_find_ast_misc_attr_callback( CSTR _Nullable misc_attr_prefix, CSTR _Nonnull misc_attr_name, ast_node *_Nullable ast_misc_attr_value_list, void *_Nullable context) { ast_node *stmt = (ast_node *)context; Contract(is_ast_create_proc_stmt(stmt)); if (misc_attr_prefix &amp;&amp; misc_attr_name &amp;&amp; !Strcasecmp(misc_attr_prefix, &quot;cql&quot;) &amp;&amp; !Strcasecmp(misc_attr_name, &quot;autotest&quot;)) { ... } }  The main dispatch looks like this: // In principle, any option can be combined with any other but some only make sense for procs with // a result. EXTRACT_STRING(autotest_attr_name, misc_attr_value); if (is_autotest_dummy_test(autotest_attr_name)) { cg_test_helpers_dummy_test(stmt); } // these options are only for procs that return a result set if (has_result_set(stmt) || has_out_stmt_result(stmt) || has_out_union_stmt_result(stmt)) { if (is_autotest_dummy_table(autotest_attr_name)) { helper_flags |= DUMMY_TABLE; cg_test_helpers_dummy_table(proc_name); } else if (is_autotest_dummy_insert(autotest_attr_name)) { helper_flags |= DUMMY_INSERT; cg_test_helpers_dummy_insert(proc_name); } else if (is_autotest_dummy_select(autotest_attr_name)) { helper_flags |= DUMMY_SELECT; cg_test_helpers_dummy_select(proc_name); } else if (is_autotest_dummy_result_set(autotest_attr_name)) { helper_flags |= DUMMY_RESULT_SET; cg_test_helpers_dummy_result_set(proc_name); } }  Most of these options are very simple indeed. cg_test_helpers_dummy_test is the trickiest by far and we'll save it for last, let's dispense with the easy stuff. "},{"title":"Dummy Table, Dummy Insert, Dummy Select, Dummy Result Set​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#dummy-table-dummy-insert-dummy-select-dummy-result-set","content":"All of these are a very simple template. The language includes just the right features to emit these procedures as nearly constant strings. The LIKE construct was literally designed to make these patterns super simple. You can see all the patterns in Chapter 12 but let's look at the code for the first one. This is &quot;dummy table&quot;. // Emit an open proc which creates a temp table in the form of the original proc // Emit a close proc which drops the temp table static void cg_test_helpers_dummy_table(CSTR name) { bprintf(cg_th_procs, &quot;\\n&quot;); bprintf(cg_th_procs, &quot;CREATE PROC open_%s()\\n&quot;, name); bprintf(cg_th_procs, &quot;BEGIN\\n&quot;); bprintf(cg_th_procs, &quot; CREATE TEMP TABLE test_%s(LIKE %s);\\n&quot;, name, name); bprintf(cg_th_procs, &quot;END;\\n&quot;); bprintf(cg_th_procs, &quot;\\n&quot;); bprintf(cg_th_procs, &quot;CREATE PROC close_%s()\\n&quot;, name); bprintf(cg_th_procs, &quot;BEGIN\\n&quot;); bprintf(cg_th_procs, &quot; DROP TABLE test_%s;\\n&quot;, name); bprintf(cg_th_procs, &quot;END;\\n&quot;); }  The purpose of this is to create helper functions that can create a temporary table with the same columns in it as the procedure you are trying to mock. You can then select rows out of that table (with dummy_select) or insert rows into the table (with dummy_insert). Or you can make a single row result set (often enough) with dummy_result_set. As we can see we simply prepend open_ to the procedure name and use that to create a test helper that make the temporary table. The table's columns are defined to be LIKE the result shape of the procedure under test. Recall this helper is only available to procedures that return a result set. The temporary table gets a test_ prefix. Assuming the procedure with the annotation is foo then this code is universal: CREATE TEMP TABLE test_foo(LIKE foo);  Is universal, no matter the result shape of foo you get a table with those columns. For this to work we need to emit a declaration of foo before this code. However, since we have the full definition of foo handy that is no problem. We remember that we'll need it by setting a flag in helper_flags. The code for close_foo is even simpler if that's possible. The great thing is all need to know the columns of foo has been removed from the test helper. The CQL compiler handles this as a matter of course and it is generally useful. See Chapter 5for more examples. All the others are equally simple and use similar tricks. These were the first test helpers. They're actually not that popular because they are so easy to create yourself anyway. "},{"title":"Dummy Test​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#dummy-test","content":"The dummy test code emitter is non-trivial. Let's quickly review the things it has to do and then we can go over how each of these is accomplished. Assuming we have an procedureyour_proc that has been annotated like this: @attribute(cql:autotest=(dummy_test)) create proc your_proc(..args...) begin -- assorted references to tables and views end;  Dummy test will produce the following: test_your_proc_create_tables a procedure that creates all the tables and views that your_proc needs test_your_proc_drop_tables a procedure that drops those same tables and views test_your_proc_create_indexes a procedure that creates your indices, in a test you may or may not want to create the indices test_your_proc_drop_indexes a procedure the drops those same indices test_your_proc_create_triggers a procedure that creates your trigger, in a test you may or may not want to create the triggers test_your_proc_drop_triggers a procedure the drops those same triggers test_your_proc_read_table1 for each table or view in the create_tables a procedure that selects all the data out of that object is created in case you need it test_your_proc_populate_tables a procedure that loads all the tables from create_tables with sample dataFK relationships are obeyeduser data may be specified in an attribute and that data will be used in preference to auto-generated data These are more fully discussed in Chapter 12. Building the Trigger and Index mappings​ In order to know which indices and triggers we might need we have to be able to map from the tables/views in your_proc to the indices. To set up for this a general purpose reverse mapping is created. We'll look at the triggers version. The indices version is nearly identical. // Walk through all triggers and create a dictionnary of triggers per tables. static void init_all_trigger_per_table() { Contract(all_tables_with_triggers == NULL); all_tables_with_triggers = symtab_new(); for (list_item *item = all_triggers_list; item; item = item-&gt;next) { EXTRACT_NOTNULL(create_trigger_stmt, item-&gt;ast); EXTRACT_NOTNULL(trigger_body_vers, create_trigger_stmt-&gt;right); EXTRACT_NOTNULL(trigger_def, trigger_body_vers-&gt;left); EXTRACT_NOTNULL(trigger_condition, trigger_def-&gt;right); EXTRACT_NOTNULL(trigger_op_target, trigger_condition-&gt;right); EXTRACT_NOTNULL(trigger_target_action, trigger_op_target-&gt;right); EXTRACT_ANY_NOTNULL(table_name_ast, trigger_target_action-&gt;left); EXTRACT_STRING(table_name, table_name_ast); if (create_trigger_stmt-&gt;sem-&gt;delete_version &gt; 0) { // dummy_test should not emit deleted trigger continue; } symtab_append_bytes(all_tables_with_triggers, table_name, &amp;create_trigger_stmt, sizeof(create_trigger_stmt)); } }  The steps are pretty simple: we make a symbol table that will map from the table name to an array of statementsthere is a convenient all_triggers list that has all the triggersfrom each trigger we EXTRACT the table or view name (named table_name even if it's a view)we append the trigger statement pointer to the end of such statements for the tableany triggers marked with @delete are not included for obvious reasons At the end of this looking up the table name will give you a list of trigger statement AST pointers. From there of course you can get everything you need. The index version is basically the same, the details of the EXTRACT ops to go from index to table name are different and of course we start from the all_indices_list Computing The Dependencies of a Procedure​ Sticking with our particular example, in order to determine that tables/views that your_proc might need, the generator has to walk its entire body looking for things that are tables. This is handled by thefind_all_table_nodes function. static void find_all_table_nodes(dummy_test_info *info, ast_node *node) { table_callbacks callbacks = { .callback_any_table = found_table_or_view, .callback_any_view = found_table_or_view, .callback_context = info, .notify_table_or_view_drops = true, .notify_fk = true, .notify_triggers = true, }; info-&gt;callbacks = &amp;callbacks; find_table_refs(&amp;callbacks, node); // stitch the views to the tables to make one list, views first for (list_item *item = info-&gt;found_views; item; item = item-&gt;next) { if (!item-&gt;next) { item-&gt;next = info-&gt;found_tables; info-&gt;found_tables = info-&gt;found_views; break; } } // this shouldn't be used after it's been linked in info-&gt;found_views = NULL; }  This code uses the general dependency walker in cg_common.c to visit all tables and views. It is a recursive walk and the general steps for prosecution go something like this: starting from your_proc the entire body of the procedure is visitedreferences to tables or views in update, delete, insert, select etc. statements are identifiedeach such table/view is added to the found tables list (at most once)for views, the recursion proceeds to the body of the view as though the body had been inline in the procedurefor tables, the recursion proceeds to the body of the table to discover any FK relationships that need to be followedif any found item has triggers, the trigger body is walked, any tables/views mentioned there become additional found itemsany given table/view and hence trigger is only visited once The net of all this, the &quot;found items&quot;, is a list of all the tables and views that the procedure uses, directly or indirectly. As discussed in Chapter 12this walk does not include tables and views used by procedures that your_proc calls. To get the dependencies in the correct order, the tables have been walked following the foreign key chain and all views go after all tables. The views are stitched together. The business of diving into views/tables/triggers and maintainence of the found items is done by the callback function found_table_or_view. The actual source is more descriptive comments than code but the code is included here as it is brief. // comments elided for brevity, the why of all this is described in the code static void found_table_or_view( CSTR _Nonnull table_or_view_name, ast_node *_Nonnull table_or_view, void *_Nullable context) { Contract(table_or_view); dummy_test_info *info = (dummy_test_info *)context; bool deleted = table_or_view-&gt;sem-&gt;delete_version &gt; 0; if (!deleted) { continue_find_table_node(info-&gt;callbacks, table_or_view); if (is_ast_create_view_stmt(table_or_view)) { add_item_to_list(&amp;info-&gt;found_views, table_or_view); } else { add_item_to_list(&amp;info-&gt;found_tables, table_or_view); } find_all_triggers_node(info, table_or_view_name); } }  The general purpose walker notifies exactly once on each visited table/view and continue_find_table_node is used to dive into the bodies of views/tables that would otherwise not be searched. Likewise find_all_triggers_nodedives into the body of any triggers that are on the found item. Emitting Indices and Triggers​ With the &quot;found tables&quot; computed (creatively stored in a field called found_tables) it's very easy to loop over these and generate the necessary indices for each found table (keeping in mind the &quot;found table&quot; can be a view). The create index statement is emitted by the usual gen_statement_with_callbacks form that echos the AST. The drop index can be trivially created by name. // Emit create and drop index statement for all indexes on a table. static void cg_emit_index_stmt( CSTR table_name, charbuf *gen_create_indexes, charbuf *gen_drop_indexes, gen_sql_callbacks *callback) { symtab_entry *indexes_entry = symtab_find(all_tables_with_indexes, table_name); bytebuf *buf = indexes_entry ? (bytebuf *)indexes_entry-&gt;val : NULL; ast_node **indexes_ast = buf ? (ast_node **)buf-&gt;ptr : NULL; int32_t count = buf ? buf-&gt;used / sizeof(*indexes_ast) : 0; gen_set_output_buffer(gen_create_indexes); for (int32_t i = 0; i &lt; count; i++) { ast_node *index_ast = indexes_ast[i]; EXTRACT_NOTNULL(create_index_stmt, index_ast); EXTRACT_NOTNULL(create_index_on_list, create_index_stmt-&gt;left); EXTRACT_ANY_NOTNULL(index_name_ast, create_index_on_list-&gt;left); EXTRACT_STRING(index_name, index_name_ast); gen_statement_with_callbacks(index_ast, callback); bprintf(gen_create_indexes, &quot;;\\n&quot;); bprintf(gen_drop_indexes, &quot;DROP INDEX IF EXISTS %s;\\n&quot;, index_name); } }  Triggers are done in exactly the same way except that instead of looping over found tables we can actually generate them as they are discovered inside of find_all_triggers_node. Recal that we had to visit the triggers when computing the found tables anyway. We did not have to visit the indices hence the difference. These walks allow us to produce: test_your_proc_create_indexes, test_your_proc_drop_indexes, test_your_proc_create_triggers, test_your_proc_drop_triggers Emitting Tables and Views​ Starting from the found tables, again it is very easy to generate the code to create and drop the tables and views. The only trick here is that the tables depend on one another so order is important. The tables are discovered with the deepest dependency first, new found items are added to the head of the found tables but it's a post-order walk so that means that the deepest tables/views are at the front of the list. This means the list is naturally in the order that it needs to be to delete the tables (parent tables at the end). So the algorithm goes like this: emit the drop tables/views in the found orderreverse the listemit the create tables/views in the reverse orderfor each table/view emit the reader `testyour_proc_read[item]for tables we emit an insertion fragment into test_your_proc_populate_tables using cg_dummy_test_populate population is discussed in the following sections As in the other cases gen_statement_with_callbacks is used to create the DDL statements: CREATE TABLECREATE VIEWCREATE VIRTUAL TABLE The delete side is easily created with ad hoc DROP TABLE or DROP VIEW statements. The reading procedure is always of the form SELECT * FROM foo so that too is trivial to generate with a fixed template. The &quot;echoing&quot; system once again is doing a lot of the heavy lifting. These walks give us test_your_proc_create_tables, test_your_proc_drop_tables, and test_your_proc_read_[item] and drive the population process Gathering Ad Hoc Data To Be Inserted​ Before we get into the mechanics of the population code, we have to visit one more area. It's possible to include data in the thedummy_test annotaiton itself. This is data that you want to have populated. This data will be included in the overall data populator. If there is enough of it (at least 2 rows per candidate table) then it might be all the data you get. Now the data format here is not designed to be fully general, after all it's not that hard to just write INSERT ... VALUES for all your tables anyway. The goal is to provide something that will help you not have to remember all the FK relationships and maybe let you economically specify some leaf data you need and get the rest for free. It's also possible to manually create dummy data that just won't work, again, scrubbing all this is way beyond the ability of a simple test helper. When the code runs you'll get SQLite errors which can be readily addressed. So keeping in mind this sort of &quot;entry level data support&quot; as the goal, we can take a look at how the system works -- it's all in the function collect_dummy_test_info which includes this helpful comment on structure. // the data attribute looks kind of like this: // @attribute(cql:autotest = ( // .. other auto test attributes // (dummy_test, // (table_name1, (col1, col2), (col1_val1, col2_val1), (col1_val2, col2_val2) ), // (table_name2, (col1, col2), (col1_val1, col2_val1), (col1_val2, col2_val2) ), // ... // ) // .. other auto test attributes // )) // // we're concerned with the dummy_test entries here, they have a very specific format // i.e. first the table then the column names, and then a list of matching columns and values  So we're going to walk a list of attributes each one begins with a table name, then a list of columns, and then a list of values. All of the data is in the symbol table dummy_test_infos which is indexed by table name. For each table name we find we ensure there is a symbol table at that slot. So dummy_test_infos is a symbol table of symbol tables. It's actually going to be something like value_list = dummy_test_infos['table']['column']  // collect table name from dummy_test info ast_node *table_list = dummy_attr-&gt;left; EXTRACT_STRING(table_name, table_list-&gt;left); symtab *col_syms = symtab_ensure_symtab(dummy_test_infos, table_name);  Next we're going to find the column names, they are the next entry in the list so we go right to get the column_name_list // collect column names from dummy_test info ast_node *column_name_list = table_list-&gt;right; for (ast_node *list = column_name_list-&gt;left; list; list = list-&gt;right) { EXTRACT_STRING(column_name, list-&gt;left); sem_t col_type = find_column_type(table_name, column_name); bytebuf *column_values = symtab_ensure_bytebuf(col_syms, column_name); // store the column meta data, create space to hold values in databuf bytebuf_append_var(&amp;col_data_buf, column_values); bytebuf_append_var(&amp;col_type_buf, col_type); bytebuf_append_var(&amp;col_name_buf, column_name); }  The primary purpose of this part of the loop is then to add the column names to col_syms so that they are linked to the dummy info for this table. The line bytebuf *column_values = symtab_ensure_bytebuf(col_syms, column_name); does this. And this also creates the byte buffer that will hold the eventual values. We also keep a side set of buffers that has the column name, type, and the values in the col_name, col_type, and col_data buffers respectively. These are used to handle the foreign key work shortly and they allow us to not have to look up all the names over and over. // collect column value from dummy_test info. We can have multiple rows of column value for (ast_node *values_ast = column_name_list-&gt;right; values_ast; values_ast = values_ast-&gt;right) { int32_t column_index = 0; // collect one row of column value for (ast_node *list = values_ast-&gt;left; list; list = list-&gt;right) { ast_node *misc_attr_value = list-&gt;left; Contract(col_data_buf.used); bytebuf *column_values = ((bytebuf **) col_data_buf.ptr)[column_index]; sem_t column_type = ((sem_t *) col_type_buf.ptr)[column_index]; CSTR column_name = ((CSTR *) col_name_buf.ptr)[column_index]; bytebuf_append_var(column_values, misc_attr_value); column_index++; ...foreign key stuff goes here... } .. some cleanup }  The most important part is bytebuf_append_var(column_values, misc_attr_value); this is where the attribute value is added to the list of values that are on the column. Finally, the &quot;foreign key stuff&quot;. What we need to do here is check the column name in the table to see if it's part of a foreign key and if it is we recursively add the current data value to the referenced column in the reference table. That way if you add an initalizer to a leaf table you don't also have to add it to all the parent tables. If it wasn't for this feature the manual data wouldn't be very useful at all, hand written INSERT statements would be just as good. // If a column value is added to dummy_test info for a foreign key column then // we need to make sure that same column value is also added as a value in the // the referenced table's dummy_test info. // e.g. // create table A(id integer primary key); // create table B(id integer primary key references A(id)); // // If there is sample data provided for B.id then we must also ensure that // the value provided for B.id is also add as a sample row in A with the same // value for id. if (is_foreign_key(column_type)) { add_value_to_referenced_table(table_name, column_name, column_type, misc_attr_value); }  When this is a done all of the initializers will have been added to the appropriate column of the appropriate table. Again the overall structure is something like: value_list = dummy_test_infos['table']['column'] Emitting the Table Population Fragments​ With any custom initalizers in the dummy_test_infos structure we can do the population fragment for any given table. The general algorithm here goes like this: the total number of rows we will generate will be the number of column values in the initializers or else DUMMY_TEST_INSERT_ROWS, whichever is largerthe insert statement generated will include dummy_seed([value_seed]) where value_seed starts at 123 and goes up 1 for every row generated dummy_seed will create values for any missing columns using the seed so any combination of included columns is ok, we'll always get a complete insert foreign key columns use a provided intializer from the parent table if there is one, or else they use 1, 2, 3 etc. likewise if a column is referenceable by some other table it uses the known sequence 1, 2, 3 etc. for its value rather than the varying seedin this way child tables can know that partent tables will have a value they can use since both tables will have at least DUMMY_TEST_INSERT_ROWS and any rows that were not manually initialized will matchnote that foreign key columns always get this treatment, whether they were mentioned or not to mix things up the dummy_nullables and dummy_defaults are added on every other row which makes missing values be NULL and/or the default value if one is present This is enough to generate a set of insert statements for the table in question and since the fragments are generated in the table creation order the resulting insert statements will have the parent tables first so the foreign keys of later tables will be correct. This can go wrong if the manual initializations use keys that conflict with the default generation or if the manual intializations have PK conflicts or other such things. No attempt is made to sort that out. The run time errors should be clear and these are, after all, only test helpers. It's very easy to avoid these hazards and you get a pretty clear error message if you don't so that seems good enough. These fragments are ultimately combined to make the body of the procedure test_your_proc_populate_tables. "},{"title":"Recap​","type":1,"pageTitle":"internal","url":"/cql-guide/generated/internal#recap-5","content":"The test helpers in cg_test_helpers.c are very simple nearly-constant templates with the exception of dummy_test which includes: table and view creationindex creationtrigger creationdata population Topics covered included: how the candidate procedures are discoveredhow the attributes are scanned for test directiveshow each dummy test type is dispatchedhow dummy_test handles data initializationhow dummy_test does its dependency analysis As with the other parts, no attempt was made to cover every function in detail. That is best done by reading the source code. But there is overall structure here and an understanding of the basic principles is helpful before diving into the source code. "},{"title":"Part 1: Lexing, Parsing, and the AST","type":0,"sectionRef":"#","url":"/cql-guide/int01","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#preface","content":"The following is a summary of the implementation theory of the CQL compiler. This is an adjunct to the Guide proper, which describes the language, and to a lesser extent the code that the compiler generates. The actual code is heavily commented, so it's better to read the code to see the details of how any particular operation happens rather than try to guess from the language specification or from this overview. However, some things, like general principles, really are nowhere (or everywhere) in the codebase and it's important to understand how things hang together. If you choose to go on adventures in the source code, especially if you aren't already familiar with compilers and how they are typically built, this document is a good place to start. "},{"title":"General Structure​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#general-structure","content":"The CQL compiler uses a very standard lex+yacc parser, though to be more precise it's flex+bison. The grammar is a large subset of the SQLite dialect of SQL augmented with control flow and compiler directives. As a consequence, it's a useful asset in-and-of-itself. If you're looking for an economical SQL grammar, you could do a lot worse than start with the one CQL uses. The grammar is of course in the usual .y format that bison consumes but it's also extracted into more readable versions for use in the railroad diagram and the Guide documentation. Any of those sources would be a good starting place for a modest SQL project in need of a grammar. "},{"title":"Lexical Analysis​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#lexical-analysis","content":"Inside of cql.l you'll find the formal definition of all the tokens. These of course correspond to the various tokens needed to parse the SQL language, plus a few more of the CQL control flow extensions. There's no need to discuss the approximately 150 such tokens, but the following points are of general interest: the lexer expects plain text files, and all the tokens are defined in plain ASCII only, however the presence of UTF8 characters in places where any text is legal (such as string literals) should just work all of the tokens are case-insensitive this means only vanilla ASCII insensitivity; no attempt is made to understand more complex UNICODE code-points multi-word tokens typically are defined with an expression like this: IS[ \\t]+NOT[ \\t]+FALSE/[^A-Z_] in most cases, to avoid ambiguity, and to get order of operations correct, the entire word sequence is one tokenonly spaces and tabs are allowed between the wordsthe token ends on non-identifier characters, so the text &quot;X IS NOT FALSEY&quot; must become the tokens { X, IS_NOT, FALSEY } and not { X, IS_NOT_FALSE, Y } the second option is actually the longest token, so without the trailing qualifier it would be preferredhence, where a continuation is possible, the trailing context must be specified in multi-word tokens there is special processing needed to lex /* ... */ comments correctlythere are token types for each of the sorts of literals that can be encountered special care is taken to keep the literals in string form so that no precision is lostinteger literals are compared against 0x7fffffff and if greater they automatically become long literals even if they are not marked with the trailing L as in 1Lstring literals include the quotation marks in the token text which distinguishes them from identifiers; they are otherwise encoded similarly the character class [-+&amp;~|^/%*(),.;!&lt;&gt;:=] produces single character tokens for operators; other non-matching single characters (e.g. '$') produce an errorline directives ^#line\\ [0-9]+\\ \\&quot;[^&quot;]*\\&quot;.* or ^#\\ [0-9]+\\ \\&quot;[^&quot;]*\\&quot;.* get special processing so that pre-processed input does not lose file and line number fidelity "},{"title":"Parsing and the Abstract Syntax Tree​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#parsing-and-the-abstract-syntax-tree","content":"Inside of cql.y you will find the token declarations, precedence rules, and all of the productions in the overall grammar. The grammar processing does as little as possible in that stage to create an abstract syntax tree (AST). The AST itself is a simple binary tree; where nodes might require more than just left and right children to specify the syntax fully, additional nodes are used in the tree shape rather than introduce n-ary nodes. This means the tree is sometimes bigger, but generally not very much bigger. The benefit of this choice is that the AST can always be walked generically as a binary tree, so if you need to find all the table_factor nodes it is easy to do so without having to worry about how every kind of node expands. If new node types come along the generic walkers can go through those new nodes as well. All of the grammar productions simply make one or more AST nodes and link them together so that in the end there is a single root for the entire program in a binary tree. There are 4 kinds of AST nodes, they all begin with the following five fields. These represent the AST &quot;base type&quot;, if you like.  const char *_Nonnull type; struct sem_node *_Nullable sem; struct ast_node *_Nullable parent; int32_t lineno; const char *_Nonnull filename;  type : a string literal that uniquely identifies the node type the string literal is compared for identity (it's an exact pointer match: you don't strcmp types) sem : begins as NULL this is where the semantic type goes once semantic processing happensparent : the parent node in the AST (not often used but sometimes indispensible)lineno : the line number of the file that had the text that led to this AST (useful for errors)filename : the name of the file that had the text that led to this AST (useful for errors) this string is durable, should not be mutated, and is shared between MANY nodes The Generic Binary AST node ast_node​ typedef struct ast_node { ... the common fields struct ast_node *_Nullable left; struct ast_node *_Nullable right; } ast_node;  This node gives the tree its shape and is how all the expression operators and statements get encoded. An example shows this more clearly: SET X := 1 + 3; {assign} | {name X} | {add} | {int 1} | {int 3}  In the above, &quot;assign&quot; and &quot;add&quot; are the generic nodes. Note that this node type can be a leaf but usually is not. The other types are always leaves. Note that in the above output, the node type was directly printed (because it's a meaningful name). Likewise, the type needs no decoding when viewing the AST in a debugger. Simply printing the node with something like p *ast in lldb will show you all the node fields and the type in human-readable form. The Grammar Code Node int_ast_node​ typedef struct int_ast_node { ... the common fields int64_t value; } int_ast_node;  This kind of node holds an integer that quantifies some kind of choice in the grammar. Note that this does NOT hold numeric literals (see below). The file ast.h includes many #define constants for this purpose such as: define JOIN_INNER 1 define JOIN_CROSS 2 define JOIN_LEFT_OUTER 3 define JOIN_RIGHT_OUTER 4 define JOIN_LEFT 5 define JOIN_RIGHT 6  The integer for this fragment will be one of those defined values. It can be a bitmask, or an enumeration. In this statement: SELECT x FROM a LEFT OUTER JOIN b;  a part of the AST will look like this: | {join_clause} | | {table_or_subquery} | | | {name a} | | {join_target_list} | | {join_target} | | {int 3} | | {table_join} | | {table_or_subquery} | | {name b}  The {int 3} above is an int_ast_node and it corresponds to JOIN_LEFT_OUTER. This node type is always a leaf. The String Node str_ast_node​ typedef struct str_ast_node { ... the common fields const char *_Nullable value; bool_t cstr_literal; } str_ast_node;  This node type holds: string literals blob literals identifiers value : the text of the string cstr_literal : true if the string was specified using &quot;C&quot; syntax (see below) CQL supports C style string literals with C style escapes such as &quot;foo\\n&quot;. These are normalized into the SQL version of the same literal so that SQLite will see a literal it understands. However, if the origin of the string was the C string form (i.e. like &quot;foo&quot; rather than 'bar') then the cstr_literal boolean flag will be set. When echoing the program back as plain text, the C string will be converted back to the C form for display to a user. But when providing the string to Sqlite, it's in SQL format. Identifiers can be distinguished from string literals because the quotation marks (always '') are still in the string. This node type is always a leaf. The Number Node num_ast_node​ typedef struct num_ast_node { ... the common fields int32_t num_type; const char *_Nullable value; } num_ast_node;  num_type : the kind of numericvalue : the text of the number All numerics are stored as strings so that there is no loss of precision. This is important because it is entirely possible that the CQL compiler is built with a different floating point library, than the target system, or different integer sizes. As a result CQL does not evaluate anything outside of an explicit const(...) expression. This policy avoids integer overflows at compile time or loss of floating point precision. Constants in the text of the output are emitted byte-for-byte as they appeared in the source code. This node type is always a leaf. "},{"title":"Examples​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#examples","content":"Example 1: A LET statement and expression​ LET x := 1 + (3 - 2); {let_stmt} | {name x} | {add} | {int 1} | {sub} | {int 3} | {int 2}  Note that there are no parentheses in the AST but it exactly and authoritatively captures the precedence with its shape. This means, among other things, that when CQL echos its input, any redundant parentheses will be gone. Example 2: An IF/ELSE construct​ IF x THEN LET x := 1.5e7; ELSE IF y THEN LET y := 'that'; ELSE LET z := &quot;this&quot;; END IF; {if_stmt} | {cond_action} | | {name x} | | {stmt_list} | | {let_stmt} | | {name x} | | {dbl 1.5e7} | {if_alt} | {elseif} | | {cond_action} | | {name y} | | {stmt_list} | | {let_stmt} | | {name y} | | {strlit 'that'} | {else} | {stmt_list} | {let_stmt} | {name z} | {strlit 'this'}  Note that the string &quot;this&quot; was normalized to 'this' (which was trivial in this case) but rest assured thatcstr_literal was set. This is shown because the text of the statement came out with double quotes. The text above was not the input to the compiler, the compiler was actually given this text: if x then let x := 1.5e7; else if y then let y := 'that'; else let z := &quot;this&quot;; end if;  And it was normalized into what you see as part of the output. We'll talk about this output echoing in coming sections. As you can see, the compiler can be used as a SQL normalizer/beautifier. Example 3: A SELECT statement​ SELECT * FROM foo INNER JOIN bar WHERE foo.x = 1 LIMIT 3; {select_stmt} | {select_core_list} | | {select_core} | | {select_expr_list_con} | | {select_expr_list} | | | {star} | | {select_from_etc} | | {join_clause} | | | {table_or_subquery} | | | | {name foo} | | | {join_target_list} | | | {join_target} | | | {int 1} | | | {table_join} | | | {table_or_subquery} | | | {name bar} | | {select_where} | | {opt_where} | | | {eq} | | | {dot} | | | | {name foo} | | | | {name x} | | | {int 1} | | {select_groupby} | | {select_having} | {select_orderby} | {select_limit} | {opt_limit} | | {int 3} | {select_offset}  As you can see the trees rapidly get more complex. The SELECT statement has many optional pieces and so the AST actually has places in its skeleton where these could go but are absent (e.g. GROUP BY,HAVING, ORDER BY, and OFFSET are all missing). The shape of the AST is largely self-evident from the above, but you can easily cross check it against what's in cql.y for details and then look at gen_sql.c for decoding tips (discussed below). The compiler can produce these diagrams in 'dot' format which makes pretty pictures, but the reality is that for non-trivial examples those pictures are so large as to be unreadable whereas the simple text format remains readable even up to several hundred lines of output. The text is also readily searchable, and diffable. The test suites for semantic analysis do pattern matching on the text of the AST to verify correctness. We'll discuss semantic analysis in Part 2. "},{"title":"AST definitions​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#ast-definitions","content":"ast.h defines all the tree types mentioned above. There are helper methods to create AST nodes with type safety. It includes helper functions for the various leaf types mentioned above but also for the various &quot;normal&quot; types. These are specified using the AST macros AST, AST1, and AST0. Examples: AST0(star) AST1(not) AST(or)  This says that: the star AST node (used in select *) is a leaf, it has 0 children this means the left and right nodes will always be NULL the not AST node (used in select NOT x) is unary this means only the left node is populated, the right is always NULLnode many unary nodes have optional children, so the left node might still be NULL the or AST node (used in select x OR y) is binary this means both its left and right children are populatednote that some binary nodes have optional children, so left or right still might be NULL At present there are about 300 unique AST node types. "},{"title":"Echoing the AST​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#echoing-the-ast","content":"The first set of features that were built (after parsing) provided the ability to echo back the parse tree as SQL again. This all happens in gen_sql.c. Since this code has to be able to echo back any tree, it often has the best and simplest examples of how to crack the AST for any particular type of node you might be interested in. There are several reasons why we might want to echo the SQL, but the inescapable one is this: any hunk of SQL that appears as part of a CQL program (i.e. DDL/DML rather than control flow like IF/WHILE) has to go to SQLite and SQLite expects that code to be plain text. So the AST must be reformatted as plain text that is exactly equivalent to the original input. The process of parsing removes extra white space and parentheses, so to get something that looks reasonable, some standard formatting (including indenting) is applied to the output text. This has the effect of normalizing the input and potentially beautifying it as well (especially if it was poorly formatted initially). To see these features, run cql with the --echo flag. For example: out/cql --echo &lt; your_file.sql  or out/cql --echo --in your_file.sql  By default, it reads stdin, makes the AST, and then emits the normalized, formatted text. If there are no syntax errors, the input and the output should be equivalent. Standard formatting is essential, but CQL also has a number of extra demands. CQL includes a lot of versioning directives like @create(...) @delete(...) and so forth. SQLite should never see these things when the DDL for SQLite is emitted. But when echoing the input they should be included. Additionally, any local or global variables in a SQL statement should be replaced with ? in the text that goes to SQLite and then followed up with binding instructions. We'll cover the binding more in the section code generation, but importantly this also has to significantly alter the output. As a result the standard formatter includes extensive configurability to get these various results. "},{"title":"Configuring the Output with Callbacks and Flags​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#configuring-the-output-with-callbacks-and-flags","content":"Some of these features, like variable binding, require a callback to the formatter's client. The client gets a notification, along with a few control variables, and it can then decide exactly what goes in the output. The control structure is struct gen_sql_callbacks, and it is described below. This structure includes the various callbacks (all of which are optional) and each callback gets a 'context' pointer of its choice. The context pointer is some arbitrary void * value that you provide, which will be given to your function along with the AST pointer relevant to the call. The callback also gets the current output buffer so it can choose to emit something (like '?') into the stream. // signature for a callback, you get your context plus the ast // if you return true then the normal output is suppressed // in any case the output you provide is emitted typedef bool_t (*_Nullable gen_sql_callback)( struct ast_node *_Nonnull ast, void *_Nullable context, charbuf *_Nonnull output );  The meaning of the bool_t return value varies depend on which callback it is. The coarsest control is provided by the generation mode. It is one of these values: // These modes control the overall style of the output enum gen_sql_mode { gen_mode_echo, // Prints everything in the original, with standard whitespace and parentheses gen_mode_sql, // Prints the AST formatted for SQLite consumption, omits anything CQL specific gen_mode_no_annotations // Equivalent to gen_mode_echo without versioning attributes or generic attributes // * @create, @delete, @recreate, and @attribute are removed // * statements like @echo are not affected, nor is the type specifier @sensitive };  The actual callbacks structure is optional, if it is NULL then a full echo of the AST with no changes will be produced. Otherwise the callbacks and flags alter the behavior of the echoer somewhat. // Callbacks allow you to significantly alter the generated sql, see the particular flags below. typedef struct gen_sql_callbacks { // Each time a local/global variable is encountered in the AST, this callback is invoked // this is to allow the variable reference to be noted and replaced with ? in the generated SQL gen_sql_callback _Nullable variables_callback; void *_Nullable variables_context; // Each time a column definition is emitted this callback is invoked, it may choose to // suppress that column. This is used to remove columns that were added in later schema // versions from the baseline schema. gen_sql_callback _Nullable col_def_callback; void *_Nullable col_def_context; // This callback is used to explain the * in select * or select T.* gen_sql_callback _Nullable star_callback; void *_Nullable star_context; // This callback is used to force the &quot;IF NOT EXISTS&quot; form of DDL statements when generating // schema upgrade steps. e.g. a &quot;CREATE TABLE Foo declarations get &quot;IF NOT EXISTS&quot; added // to them in upgrade steps. gen_sql_callback _Nullable if_not_exists_callback; void *_Nullable if_not_exists_context; // If true, hex literals are converted to decimal. This is for JSON which does not support hex literals. bool_t convert_hex; // If true casts like &quot;CAST(NULL as TEXT)&quot; are reduced to just NULL. The type information is not needed // by SQLite so it just wasts space. bool_t minify_casts; // If true then unused aliases in select statements are elided to save space. This is safe because // CQL always binds the top level select statement by ordinal anyway. bool_t minify_aliases; // mode to print cql statement: gen_mode_echo, gen_mode_sql, gen_mode_no_annotations. // gen_mode_sql mode causes the AS part of virtual table to be suppressed enum gen_sql_mode mode; // If CQL finds a column such as 'x' below' // // create table foo( // x long_int primary key autoincrement // ); // // that column must be converted to this form: // // create table foo( // x integer primary key autoincrement // ); // // This is because SQLite mandates that autoincrement must be exactly // in the second example above however, it is also the case that in SQLite // an integer can store a 64 bit value. So sending &quot;integer&quot; to SQLite while // keeping the sense that the column is to be treated as 64 bits in CQL works // just fine. // // However, when we are emitting CQL (rather than SQL) we want to keep // the original long_int type so as not to lose fidelity when processing // schema for other semantic checks (such as matching FK data types). // // This flag is for that purpose: It tells us that the target isn't SQLite // and we don't need to do the mapping (yet). Indeed, we shouldn't, or the // types will be messed up. // // In short, if CQL is going to process the output again, use this flag // to control the autoincrement transform. It might be possible to fold // this flag with the mode flag but it's sufficiently weird that this // extra documentation and special handling is probably worth the extra // boolean storage. bool_t long_to_int_conv; } gen_sql_callbacks;  Each callback can be best understood by reading the source, so we'll avoid trying to precisely define it here. But it is helpful to give the gist of these options. mode : one of the three enum modes that control overall behaviorvariables_callback : invoked when a variable appears in the SQL, the caller can record the specific variable and then use it for bindingcol_def_callback : when creating the &quot;baseline&quot; schema you don't want column definitions from later schema to be included, this gives you a chance to suppress themstar_callback : normally the * in select * or select T.* is expanded when emitting for SQLite, this callback does the expansion when appropriateif_not_exists_callback : when generating DDL for schema upgrade you typically want to force IF NOT EXISTS to be added to the schema even if it wasn't present in the declaration; this callback lets you do thatconvert_hex : if true, hex constants are converted to decimal; used when emitting JSON because JSON doesn't understand hex constantsminify_casts : minification converts casts like CAST(NULL AS TEXT) to just NULL -- the former is only useful for type information, SQLite does need to see itminify_aliases : unused column aliases as in select foo.x as some_really_long_alias can be removed from the output when targeting SQLite to save space "},{"title":"Invoking the Generator​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#invoking-the-generator","content":"There are several generation functions but they all follow a similar pattern, the differences are essentially what fragment of the AST they expect to begin on. We'll just cover one here. cql_noexport void gen_statement_with_callbacks(ast_node *_Nonnull ast, gen_sql_callbacks *_Nullable _callbacks);  This has the typical signature for all these generators: ast : the part of the tree to print_callbacks : the optional callbacks described above To use these you'll need to these functions as well: cql_noexport void gen_init(void); cql_noexport void gen_cleanup(void);  You'll want to call gen_init() one time before doing any generation. That sets up the necessary tables. When you're done use gen_cleanup() to release any memory that was allocated in setup. You don't have to do the cleanup step if the process is going to exit anyway, however, because of the amalgam options, cql_main() assumes it might be called again and so it tidies things up rather than risk leaking. With the one time initialization in place there are these preliminaries: cql_noexport void init_gen_sql_callbacks(gen_sql_callbacks *_Nullable callbacks);  Use init_gen_sql_callbacks to fill in your callback structure with the normal defaults. This give you normal echo for SQL by default. To get a full echo, a NULL callback may be used. And of course other options are possible. Finally, cql_noexport void gen_set_output_buffer(struct charbuf *_Nonnull buffer);  Use this before the call to gen_&lt;something&gt;_with_callbacks to redirect the output into a growable character buffer of your choice. The buffers can then be written where they are needed. Maybe further processed into a C string literal for compiler output, or into a C style comment, or just right back to stdout. There are a few simplified versions of this sequence like this one: cql_noexport void gen_stmt_list_to_stdout(ast_node *_Nullable ast);  This uses NULL for the callbacks and emits directly to stdout with no extra steps. The extra wiring is done for you. "},{"title":"Generator Internals​","type":1,"pageTitle":"Part 1: Lexing, Parsing, and the AST","url":"/cql-guide/int01#generator-internals","content":"The generator has to be able to walk the entire tree and emit plain text, and in many areas the tree is very flexible so we want a simple dynamic dispatch mechanism that can call the right formatting function from anywhere in the tree. It turns out two different signatures are needed to do this properly, one for formatting statements and the other for expressions -- the difference being that expressions have to concern themselves with the precedence of the various operators so that parentheses can be correctly (re)inserted into the output. To do this there are two symbol tables that map from an AST node type string to a formatting function. They are initialized with a series of statements similar to these: Generating Expressions​ cql_noexport void gen_init() { gen_stmts = symtab_new(); gen_exprs = symtab_new(); STMT_INIT(if_stmt); ... EXPR_INIT(mul, gen_binary, &quot;*&quot;, EXPR_PRI_MUL); EXPR_INIT(div, gen_binary, &quot;/&quot;, EXPR_PRI_MUL); EXPR_INIT(mod, gen_binary, &quot;%&quot;, EXPR_PRI_MUL); EXPR_INIT(add, gen_binary, &quot;+&quot;, EXPR_PRI_ADD); EXPR_INIT(sub, gen_binary, &quot;-&quot;, EXPR_PRI_ADD); EXPR_INIT(not, gen_unary, &quot;NOT &quot;, EXPR_PRI_NOT); EXPR_INIT(tilde, gen_unary, &quot;~&quot;, EXPR_PRI_TILDE); ... }  These statements populate the symbol tables. For statements, the entry maps if_stmt to the function gen_if_stmtFor expressions, the entry maps mul to gen_binary including the metadata &quot;*&quot; and EXPR_PRI_MUL As you can see, nearly all binary operators are handled identically as are all unary operators. Let's look at those two in detail. static void gen_binary(ast_node *ast, CSTR op, int32_t pri, int32_t pri_new) { // We add parens if our priority is less than the parent priority // meaning something like this: // * we're a + node, our parent is a * node // * we need parens because the tree specifies that the + happens before the * // // Also, grouping of equal operators is left to right // so for so if our right child is the same precedence as us // that means there were parens there in the original expression // e.g. 3+(4-7); // effectively it's like we're one binding strength higher for our right child // so we call it with pri_new + 1. If it's equal to us it must emit parens if (pri_new &lt; pri) gen_printf(&quot;(&quot;); gen_expr(ast-&gt;left, pri_new); gen_printf(&quot; %s &quot;, op); gen_expr(ast-&gt;right, pri_new + 1); if (pri_new &lt; pri) gen_printf(&quot;)&quot;); }  The convention gives us: ast : pointer to the current AST nodeop : the text of the operator (CSTR is simply const char *)pri : the binding strength of the node above uspri_new : the binding strength of this node (the new node) So generically, if the binding strength of the current operator pri_new is weaker than the context it is contained in pri, then parentheses are required to preserve order of operations. See the comment for more details. With parens taken care of, we emit the left expression, the operator, and the right expression. And as you can see below, unary operators are much the same. static void gen_unary(ast_node *ast, CSTR op, int32_t pri, int32_t pri_new) { if (pri_new &lt; pri) gen_printf(&quot;(&quot;); gen_printf(&quot;%s&quot;, op); gen_expr(ast-&gt;left, pri_new); if (pri_new &lt; pri) gen_printf(&quot;)&quot;); }  There are special case formatters for some of the postfix operators and other cases that are special like CASE... WHEN... THEN... ELSE... END but they operate on the same principles down to the leaf nodes. Generating Statements​ With no binding strength to worry about, statement processing is quite a bit simpler. Here's the code for the IF statement mentioned above. static void gen_if_stmt(ast_node *ast) { Contract(is_ast_if_stmt(ast)); EXTRACT_NOTNULL(cond_action, ast-&gt;left); EXTRACT_NOTNULL(if_alt, ast-&gt;right); EXTRACT(elseif, if_alt-&gt;left); EXTRACT_NAMED(elsenode, else, if_alt-&gt;right); gen_printf(&quot;IF &quot;); gen_cond_action(cond_action); if (elseif) { gen_elseif_list(elseif); } if (elsenode) { gen_printf(&quot;ELSE\\n&quot;); EXTRACT(stmt_list, elsenode-&gt;left); gen_stmt_list(stmt_list); } gen_printf(&quot;END IF&quot;); }  There is a general boilerplate sort of recursive form to all of these; they follow the same basic shape. These patterns are designed to make it impossible to walk the tree incorrectly. If the tree shape changes because of a grammar change, you get immediate concrete failures where the tree walk has to change. Since there are test cases to cover every tree shape you can always be sure you have it exactly right if the macros do not force assertion failures. The steps were: use Contract to assert that the node we are given is the type we expectuse EXTRACT macros (detailed below) to get the tree parts you want starting from your rootuse gen_printf to emit the constant pieces of the statementuse recursion to print sub-fragments (like the IF condition in this case)test the tree fragments where optional pieces are present, emit them as needed It might be instructive to include gen_cond_action; it is entirely unremarkable: static void gen_cond_action(ast_node *ast) { Contract(is_ast_cond_action(ast)); EXTRACT(stmt_list, ast-&gt;right); gen_root_expr(ast-&gt;left); gen_printf(&quot; THEN\\n&quot;); gen_stmt_list(stmt_list); }  A cond_action node has an expression on the left and a statement list on the right; it can appear in the base IF x THEN y part of the IF or as ELSE IF x THEN y. Either case is formatted the same. Extraction Macros​ These macros are used by all the parts of CQL that walk the AST. They are designed to make it impossible for you to get the tree shape wrong without immediately failing. We do not ever want to walk off the tree in some exotic way and then continue to several levels of recursion before things go wrong. CQL locks this down by checking the node type at every step -- any problems are found immediately, exactly at the extraction site, and can be quickly corrected. Again 100% coverage of all the tree shapes makes this rock solid, so CQL never compromises on 100% code coverage. The most common macros all appear in this example: EXTRACT_NOTNULL(cond_action, ast-&gt;left); read ast-&gt;left, assert that it is of type cond_action, it must not be NULLdeclare a local variable named cond_action to hold the result EXTRACT_NOTNULL(if_alt, ast-&gt;right); read ast-&gt;right, assert that it is of type if_alt, it must not be NULLdeclare a local variable named if_alt to hold the result EXTRACT(elseif, if_alt-&gt;left); read if_alt-&gt;left, assert that it is either NULL or else of type elseifdeclare a variable named elseif to hold the result EXTRACT_NAMED(elsenode, else, if_alt-&gt;right); read if_alt-&gt;right, assert that it is either NULL or else of type elsedeclare a variable named elsenode to hold the resultnote that we can't use a variable named else because else is a keyword in C Other options: EXTRACT_NAMED_NOTNULL : like the NAMED variantEXTRACT_ANY : if the tree type is not known (e.g. expr-&gt;left could be any expression type)EXTRACT_ANY_NOTNULL : as above but not optionalEXTRACT_NUM_TYPE : extracts the num_type field from a numeric AST node The ANY variants are usually re-dispatched with something like gen_expr that uses the name table again (and that will check the type) or else the extracted value is checked with ad hoc logic immediately after extraction if it's perhaps one of two or three variations. In all cases the idea is to force a failure very quickly. gen_root_expr() for instance in the if_cond example will fail immediately if the node it gets is not an expression type. Because of the clear use of EXTRACT, the gen_ family of functions are often the best/fastest way to understand the shape of the AST. You can dump a few samples and look at the gen_ function and quickly see exactly what the options are authoritatively. As a result it's very normal to paste the extraction code from a gen_ function into a new/needed semantic analysis or code-generation function. "},{"title":"Part 2: Semantic Analysis","type":0,"sectionRef":"#","url":"/cql-guide/int02","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#preface","content":"Part 2 continues with a discussion of the essentials of the semantic analysis pass of the CQL compiler. As in the previous sections, the goal here is not to go over every single rule but rather to give a sense of how semantic analysis happens in general -- the core strategies and implementation choices -- so that when reading the code you will have an idea of how smaller pieces fit into the whole. To accomplish this, various key data structures will be explained in detail and selected examples of their use are included. "},{"title":"Semantic Analysis​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#semantic-analysis","content":"The overall goal of the semantic analysis pass is to verify that a correct program has been submitted to the compiler. The compiler does this by &quot;decorating&quot; the AST with semantic information. This information is mainly concerned with the &quot;types&quot; of the various things in the program. A key function of the semantic analyzer, the primary &quot;weapon&quot; in computing these types, if you will, is name resolution. The semantic analyzer decides what any given name means in any context and then uses that meaning, which is itself based on the AST constructs that came before, to compute types and then check those types for errors. Broadly speaking, the errors that can be discovered are of these forms: mentioned names do not exist e.g. using a variable or table or column without declaring it mentioned names are not unique, or are ambiguous e.g. every view must have a unique namee.g. table names need to be unique, or aliased when joining tables operands are not compatible with each other or with the intended operation e.g. you can't add a string to a reale.g. you can't do the % operation on a reale.g. the expression in a WHERE clause must result in a numerice.g. the first argument to printf must be a string literale.g. you can't assign a long value to an integer variablee.g. you can't assign a possibly null result to a not-null variable there are too many or two few operands for an operation e.g. an INSERT statement must include sufficiently many columns and no extrase.g. a function or procedure call must have the correct number of operands an operation is happening in a context where it is not allowed e.g. use of aggregate functions in the WHERE clausee.g. use of unique SQLite functions outside of a SQL statement There are several hundred possible errors, and no attempt will be made to cover them all here but we will talk about how errors are created, recorded, and reported. "},{"title":"Decorated AST examples​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#decorated-ast-examples","content":"Recalling the AST output from Part 1, this is what that same tree looks like with semantic information attached: LET X := 1 + 3; {let_stmt}: X: integer notnull variable | {name X}: X: integer notnull variable | {add}: integer notnull | {int 1}: integer notnull | {int 3}: integer notnull  And here's an example with some structure types: SELECT 1 AS x, 3.2 AS y; {select_stmt}: select: { x: integer notnull, y: real notnull } | {select_core_list}: select: { x: integer notnull, y: real notnull } | | {select_core}: select: { x: integer notnull, y: real notnull } | | {select_expr_list_con}: select: { x: integer notnull, y: real notnull } | | {select_expr_list}: select: { x: integer notnull, y: real notnull } | | | {select_expr}: x: integer notnull | | | | {int 1}: integer notnull | | | | {opt_as_alias} | | | | {name x} | | | {select_expr_list} | | | {select_expr}: y: real notnull | | | {dbl 3.2}: real notnull | | | {opt_as_alias} | | | {name y} | | {select_from_etc}: ok | | {select_where} | | {select_groupby} | | {select_having} | {select_orderby} | {select_limit} | {select_offset}  These can be generated by adding --sem --ast to the CQL command line along with --in your_file.sql. Keep these shapes in mind as we discuss the various sources of type information. "},{"title":"The Base Data Structures​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#the-base-data-structures","content":"First recall that every AST node has this field in it: struct sem_node *_Nullable sem;  This is the pointer to the semantic information for that node. Semantic analysis happens immediately after parsing and before any of the code-generators run. Importantly, code generators never run if semantic analysis reported any errors. Before we get into the shape of the semantic node, we should start with the fundamental unit of type info sem_t which is usually stored in a variable called sem_type. typedef uint64_t sem_t;  The low order bits of a sem_t encode the core type and indeed there is a helper function to extract the core type from a sem_t. // Strips out all the flag bits and gives you the base/core type. cql_noexport sem_t core_type_of(sem_t sem_type) { return sem_type &amp; SEM_TYPE_CORE; }  The core bits are as follows: #define SEM_TYPE_NULL 0 // the subtree is a null literal (not just nullable) #define SEM_TYPE_BOOL 1 // the subtree is a bool #define SEM_TYPE_INTEGER 2 // the subtree is an integer #define SEM_TYPE_LONG_INTEGER 3 // the subtree is a long integer #define SEM_TYPE_REAL 4 // the subtree is a real #define SEM_TYPE_TEXT 5 // the subtree is a text type #define SEM_TYPE_BLOB 6 // the subtree is a blob type #define SEM_TYPE_OBJECT 7 // the subtree is any object type #define SEM_TYPE_STRUCT 8 // the subtree is a table/view #define SEM_TYPE_JOIN 9 // the subtree is a join #define SEM_TYPE_ERROR 10 // marks the subtree as having a problem #define SEM_TYPE_OK 11 // sentinel for ok but no type info #define SEM_TYPE_PENDING 12 // sentinel for type calculation in flight #define SEM_TYPE_REGION 13 // the ast is a schema region #define SEM_TYPE_CURSOR_FORMAL 14 // this is used for the cursor parameter type uniquely #define SEM_TYPE_CORE 0xff // bit mask for the core types #define SEM_TYPE_MAX_UNITARY (SEM_TYPE_OBJECT+1) // the last unitary type  These break into a few categories: NULL to OBJECT are the &quot;unitary&quot; types -- these are the types that a single simple variable can be a column can be any of these except OBJECT or NULLthe NULL type comes only from the NULL literal which has no typeinstances of, say, a TEXT column might have a NULL value but they are known to be TEXT STRUCT indicates that the object has many fields, like a table, or a cursorJOIN indicates that the object is the concatenation of many STRUCT types e.g. T1 inner join T2 is a JOIN type with T1 and T2 being the partsa JOIN could be flattened to STRUCT, but this is typically not donethe type of a SELECT statement will be a STRUCT representing the expressions that were selectedthose expressions in turn used columns from the JOIN that was the FROM clause ERROR indicates that the subtree had an error the error will have been already reportedthe error type generally cascades up the AST to the root OK indicates that there is no type information but there was no problem e.g. a correct IF statement will resolve to simply OK (no error) PENDING is used sometimes while a type computation is in progress this type doesn't appear in the AST, but has its own unique value so as to not conflict with any others REGION is used to identify AST fragments that correspond to schema regions see Chapter 10 of the Guide for more information on regions CORE is the mask for the core parts, 0xf would do the job but for easy reading in the debugger we use 0xff new core types are not added very often, adding a new one is usually a sign that you are doing something wrong The core type can be modified by various flags. The flags, in principle, can be combined in any way but in practice many combinations make no sense. For instance, HAS_DEFAULT is for table columns and CREATE_FUNC is for function declarations. There is no one object that could require both of these. The full list as of this writing is as follows: #define SEM_TYPE_NOTNULL _64(0x0100) // set if and only if null is not possible #define SEM_TYPE_HAS_DEFAULT _64(0x0200) // set for table columns with a default #define SEM_TYPE_AUTOINCREMENT _64(0x0400) // set for table columns with autoinc #define SEM_TYPE_VARIABLE _64(0x0800) // set for variables and parameters #define SEM_TYPE_IN_PARAMETER _64(0x1000) // set for in parameters (can mix with below) #define SEM_TYPE_OUT_PARAMETER _64(0x2000) // set for out parameters (can mix with above) #define SEM_TYPE_DML_PROC _64(0x4000) // set for stored procs that have DML/DDL #define SEM_TYPE_HAS_SHAPE_STORAGE _64(0x8000) // set for a cursor with simplified fetch syntax #define SEM_TYPE_CREATE_FUNC _64(0x10000) // set for a function that returns a created object +1 ref #define SEM_TYPE_SELECT_FUNC _64(0x20000) // set for a sqlite UDF function declaration #define SEM_TYPE_DELETED _64(0x40000) // set for columns that are not visible in the current schema version #define SEM_TYPE_VALIDATED _64(0x80000) // set if item has already been validated against previous schema #define SEM_TYPE_USES_OUT _64(0x100000) // set if proc has a one rowresult using the OUT statement #define SEM_TYPE_USES_OUT_UNION _64(0x200000) // set if proc uses the OUT UNION form for multi row result #define SEM_TYPE_PK _64(0x400000) // set if column is a primary key #define SEM_TYPE_FK _64(0x800000) // set if column is a foreign key #define SEM_TYPE_UK _64(0x1000000) // set if column is a unique key #define SEM_TYPE_VALUE_CURSOR _64(0x2000000) // set only if SEM_TYPE_HAS_SHAPE_STORAGE is set and the cursor has no statement #define SEM_TYPE_SENSITIVE _64(0x4000000) // set if the object is privacy sensitive #define SEM_TYPE_DEPLOYABLE _64(0x8000000) // set if the object is a deployable region #define SEM_TYPE_BOXED _64(0x10000000) // set if a cursor's lifetime is managed by a box object #define SEM_TYPE_HAS_CHECK _64(0x20000000) // set for table column with a &quot;check&quot; clause #define SEM_TYPE_HAS_COLLATE _64(0x40000000) // set for table column with a &quot;collate&quot; clause #define SEM_TYPE_INFERRED_NOTNULL _64(0x80000000) // set if inferred to not be nonnull (but was originally nullable) #define SEM_TYPE_VIRTUAL _64(0x100000000) // set if and only if this is a virtual table #define SEM_TYPE_HIDDEN_COL _64(0x200000000) // set if and only if hidden column on a virtual table #define SEM_TYPE_TVF _64(0x400000000) // set if and only table node is a table valued function #define SEM_TYPE_IMPLICIT _64(0x800000000) // set if and only the variable was declare implicitly (via declare out) #define SEM_TYPE_CALLS_OUT_UNION _64(0x1000000000) // set if proc calls an out union proc for  Note: _64(x) expands to either a trailing L or a trailing LL depending on the bitness of the compiler, whichever yields an int64_t. Going over the meaning of all of the above is again beyond the scope of this document; some of the flags are very specialized and essentially the validation just requires a bit of storage in the tree to do its job so that storage is provided with a flag. However two flag bits are especially important and are computed almost everywhere sem_t is used. These are SEM_TYPE_NOTNULL and SEM_TYPE_SENSITIVE. SEM_TYPE_NOTNULL indicates that the marked item is known to be NOT NULL, probably because it was declared as such, or directly derived from a not null item Typically when two operands are combined both must be marked NOT NULL for the result to still be NOT NULL (there are exceptions like COALESCE)Values that might be null cannot be assigned to targets that must not be null SEM_TYPE_SENSITIVE indicates that the marked item is some kind of PII or other sensitive data. Any time a sensitive operand is combined with another operand the resulting type is sensitiveThere are very few ways to &quot;get rid&quot; of the sensitive bit -- it corresponds to the presence of @sensitive in the data type declarationValues that are sensitive cannot be assigned to targets that are not marked sensitive The semantic node sem_node carries all the possible semantic info we might need, and the sem_type holds the flags above and tells us how to interpret the rest of the node. There are many fields -- we'll talk about some of the most important ones here to give you a sense of how things hang together. Note that CSTR is simply an alias for const char *. CSTR is used extensively in the codebase for brevity. typedef struct sem_node { sem_t sem_type; // core type plus flags CSTR name; // for named expressions in select columns, etc. CSTR kind; // the Foo in object&lt;Foo&gt;, not a variable or column name CSTR error; // error text for test output, not used otherwise struct sem_struct *sptr; // encoded struct if any struct sem_join *jptr; // encoded join if any int32_t create_version; // create version if any (really only for tables and columns) int32_t delete_version; // delete version if any (really only for tables and columns) bool_t recreate; // for tables only, true if marked @recreate CSTR recreate_group_name; // for tables only, the name of the recreate group if they are in one CSTR region; // the schema region, if applicable; null means unscoped (default) symtab *used_symbols; // for select statements, we need to know which of the ids in the select list was used, if any list_item *index_list; // for tables we need the list of indices that use this table (so we can recreate them together if needed) struct eval_node *value; // for enum values we have to store the evaluated constant value of each member of the enum } sem_node;  sem_type : already discussed above, this tells you how to interpret everything elsename : variables, columns, etc. have a canonical name -- when a name case-insensitivity resolves, the canonical name is stored here typically later passes emit the canonical variable name everywheree.g. because FoO and fOO might both resolve to an object declared as foo, we always emit foo in codegen kind : in CQL any type can be discriminated as in declare foo real&lt;meters&gt;, the kind here is meters two expressions of the same core type (e.g. real) are incompatible if they have a kind and the kind does not matche.g. if you have bar real&lt;liters&gt; then set foo := bar; this is an error even though both are real because foo above is real&lt;meters&gt; sptr : if the item's core type is SEM_TYPE_STRUCT then this is populated (see below)jptr : if the item's core type is SEM_TYPE_JOIN then this is populated (see below) If the object is a structure type then this is simply an array of names, kinds, and semantic types. In fact the semantic types will be all be unitary, possibly modified by NOT_NULL or SENSITIVE but none of the other flags apply. A single sptr directly corresponds to the notion of a &quot;shape&quot; in the analyzer. Shapes come from anything that looks like a table, such as a cursor, or the result of a SELECT statement. // for tables and views and the result of a select typedef struct sem_struct { CSTR struct_name; // struct name uint32_t count; // count of fields CSTR *names; // field names CSTR *kinds; // the &quot;kind&quot; text of each column, if any, e.g. integer&lt;foo&gt; foo is the kind sem_t *semtypes; // typecode for each field } sem_struct;  If the object is a join type (such as the parts of the FROM clause) then the jptr field will be populated. This is nothing more than a named list of struct types. // for the data type of (parts of) the FROM clause // sometimes I refer to as a &quot;joinscope&quot; typedef struct sem_join { uint32_t count; // count of table/views in the join CSTR *names; // names of the table/view struct sem_struct **tables; // struct type of each table/view } sem_join;  With these building blocks we can represent the type of anything in the CQL language. "},{"title":"Initiating Semantic Analysis​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#initiating-semantic-analysis","content":"The semantic analysis pass runs much the same way as the AST emitter. In sem.c there is the essential function sem_main. It suffices to call sem_main on the root of the AST. That root node is expected to be a stmt_list node. // This method loads up the global symbol tables in either empty state or // with the appropriate tokens ready to go. Using our own symbol tables for // dispatch saves us a lot of if/else string comparison verbosity. cql_noexport void sem_main(ast_node *ast) { // restore all globals and statics we own sem_cleanup(); eval_init(); ... }  As you can see, sem_main begins by resetting all the global state. You can of course do this yourself after calling sem_main (when you're done with the results). sem_main sets a variety of useful and public global variables that describe the results of the analysis. The ones in sem.h are part of the contract and you should feel free to use them in a downstream code-generator. Other items are internal and should be avoided. The internal items are typically defined statically in sem.c. The essential outputs will be described in the last section of this part. The cleanup has this structure: // This method frees all the global state of the semantic analyzer cql_noexport void sem_cleanup() { eval_cleanup(); BYTEBUF_CLEANUP(deployable_validations); BYTEBUF_CLEANUP(recreate_annotations); BYTEBUF_CLEANUP(schema_annotations); SYMTAB_CLEANUP(funcs); SYMTAB_CLEANUP(globals); SYMTAB_CLEANUP(indices); SYMTAB_CLEANUP(locals); ... // these are getting zeroed so that leaksanitizer will not count those objects as reachable from a global root. all_ad_hoc_list = NULL; all_functions_list = NULL; ...  This basically deallocates everything and resets all the globals to NULL. sem_main of course has to walk the AST and it does so in much the same way as we saw in gen_sql.c. There is a series of symbol tables whose key is an AST type and whose value is a function plus arguments to dispatch (effectively a lambda.) The semantic analyzer doesn't have to think about things like &quot;should I emit parentheses?&quot; so the signature of each type of lambda can be quite a bit simpler. We'll go over each kind with some examples. First we have the non-SQL statements, these are basic flow control or other things that SQLite will never see directly.  symtab *syms = non_sql_stmts; STMT_INIT(if_stmt); STMT_INIT(while_stmt); STMT_INIT(switch_stmt); STMT_INIT(leave_stmt); ...  Here STMT_INIT creates a binding between (e.g.) the AST type if_stmt and the function sem_if_stmt. This lets us dispatch any part of the AST to its handler directly. Next we have the SQL statements. These get analyzed in the same way as the others, and with functions that have the same signature, however, if you use one of these it means that procedure that contained this statement must get a database connection in order to run. Use of the database will require the procedure's signature to change; this is recorded by the setting the SEM_TYPE_DML_PROC flag bit to be set on the procedure's AST node.  syms = sql_stmts; STMT_INIT(create_table_stmt); STMT_INIT(drop_table_stmt); STMT_INIT(create_index_stmt); STMT_INIT(create_view_stmt); STMT_INIT(select_stmt); STMT_INIT(delete_stmt); STMT_INIT(update_stmt); STMT_INIT(insert_stmt); ...  Again STMT_INIT creates a binding between (e.g.) the AST type delete_stmt and the function sem_delete_stmt so we can dispatch to the handler. Next we have expression types. These are set up with EXPR_INIT. Many of the operators require exactly the same kinds of verification, so in order to be able to share the code, the expression analysis functions get an extra argument for the operator in question. Typically the string of the operator is only needed to make a good quality error message with validation being otherwise identical. Here are some samples...  EXPR_INIT(num, sem_expr_num, &quot;NUM&quot;); EXPR_INIT(str, sem_expr_str, &quot;STR&quot;); EXPR_INIT(blob, sem_expr_blob, &quot;BLB&quot;); EXPR_INIT(null, sem_expr_null, &quot;NULL&quot;); EXPR_INIT(dot, sem_expr_dot, &quot;DOT&quot;); EXPR_INIT(const, sem_expr_const, &quot;CONST&quot;); EXPR_INIT(mul, sem_binary_math, &quot;*&quot;); EXPR_INIT(mod, sem_binary_integer_math, &quot;%&quot;); EXPR_INIT(not, sem_unary_logical, &quot;NOT&quot;); EXPR_INIT(is_true, sem_unary_is_true_or_false, &quot;IS TRUE&quot;); EXPR_INIT(tilde, sem_unary_integer_math, &quot;~&quot;); EXPR_INIT(uminus, sem_unary_math, &quot;-&quot;);  Looking at the very first entry as an example, we see that EXPR_INIT creates a mapping between the AST type numand the analysis function sem_expr_num and that function will get the text &quot;NUM&quot; as an extra argument. As it happens sem_expr_num doesn't need the extra argument, but sem_binary_math certainly needs the &quot;*&quot;as that function handles a large number of binary operators. Let's quickly go over this list as these are the most important analyzers: sem_expr_num : analyzes any numeric constantsem_expr_str : analyzes any string literal or identifiersem_expr_blob : analyzes any blob literalsem_expr_null : analyzes the NULL literal (and nothing else)sem_expr_dot : analyzes a compound name like T1.idsem_expr_const : analyzes a const(...) expression, doing the constant evaluationsem_binary_math : analyzes any normal binary math operator like '+', '-', '/' etc.sem_binary_integer_math : analyzes any binary math operator where the operands must be integers like '%' or '|'sem_unary_logical : analyzes any unary logical operator (the result is a bool) -- this is really only NOTsem_unary_is_true_or_false : analyzes any of the IS TRUE, IS FALSE family of postfix unary operatorssem_unary_integer_math : analyzes any unary operator where the operand must be an integer -- this is really only ~sem_unary_math : analyzes any any math unary operator, presently only negation (but in the future unary + too) The last group of normal associations are for builtin functions, like these:  FUNC_INIT(changes); FUNC_INIT(printf); FUNC_INIT(strftime); FUNC_INIT(date); FUNC_INIT(time);  Each of these is dispatched when a function call is found in the tree. By way of example FUNC_INIT(changes)causes the changes function to map to sem_func_changes for validation. There are a few other similar macros for more exotic cases but the general pattern should be clear now. With these in place it's very easy to traverse arbitrary statement lists and arbitrary expressions with sub expressions and have the correct function invoked without having large switch blocks all over. "},{"title":"Semantic Errors​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#semantic-errors","content":"Some of the following examples will show the handling of semantic errors more precisely but the theory is pretty simple. Each of the analyzers that has been registered is responsible for putting an appropriate sem_node into the AST it is invoked on. The caller will look to see if that sem_nodeis of type SEM_TYPE_ERROR using is_error(ast). If it is, the caller will mark its own AST as errant using record_error(ast) and this continues all the way up the tree. The net of this is that wherever you begin semantic analysis, you can know if there were any problems by checking for an error at the top of the tree you provided. At the point of the initial error, the analyzer is expected to also call report_error providing a suitable message. This will be logged to stderr. In test mode it is also stored in the AST so that verification steps can confirm that errors were reported at exactly the right place. If there are no errors, then a suitable sem_node is created for the resulting type or else, at minimum, record_ok(ast) is used to place the shared &quot;OK&quot; type on the node. The &quot;OK&quot; type indicates no type information, but no errors either. &quot;OK&quot; is helpful for statements that don't involve expressions like DROP TABLE Foo. "},{"title":"The Primitive Types​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#the-primitive-types","content":"Perhaps the simplest analysis of all happens at the leaves of the AST. By way of example, here is the code for expression nodes of type num, the numeric literals. // Expression type for numeric primitives static void sem_expr_num(ast_node *ast, CSTR cstr) { Contract(is_ast_num(ast)); EXTRACT_NUM_TYPE(num_type, ast); switch (num_type) { case NUM_BOOL: ast-&gt;sem = new_sem(SEM_TYPE_BOOL | SEM_TYPE_NOTNULL); break; case NUM_INT: ast-&gt;sem = new_sem(SEM_TYPE_INTEGER | SEM_TYPE_NOTNULL); break; case NUM_LONG: ast-&gt;sem = new_sem(SEM_TYPE_LONG_INTEGER | SEM_TYPE_NOTNULL); break; default: // this is all that's left Contract(num_type == NUM_REAL); ast-&gt;sem = new_sem(SEM_TYPE_REAL | SEM_TYPE_NOTNULL); break; } }  As you can see the code simply looks at the AST node, confirming first that it is a num node. Then it extracts the num_type. Then ast-&gt;sem is set to a semantic node of the matching type adding in SEM_TYPE_NOTNULL because literals are never null. The new_sem function is used to make an empty sem_node with the sem_type filled in as specified. Nothing can go wrong creating a literal so there are no failure modes. It doesn't get much simpler unless maybe... // Expression type for constant NULL static void sem_expr_null(ast_node *ast, CSTR cstr) { Contract(is_ast_null(ast)); // null literal ast-&gt;sem = new_sem(SEM_TYPE_NULL); }  It's hard to get simpler than doing semantic analysis of the NULL literal. Its code should be clear with no further explanation needed. "},{"title":"Unary Operators​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#unary-operators","content":"Let's dive in to a simple case that does require some analysis -- the unary operators. There are comparatively few and there isn't much code required to handle them all. Here's the code for the unary math operators: // The only unary math operators are '-' and '~' // Reference types are not allowed static void sem_unary_math(ast_node *ast, CSTR op) { sem_t core_type, combined_flags; if (!sem_unary_prep(ast, &amp;core_type, &amp;combined_flags)) { return; } if (!sem_validate_numeric(ast, core_type, op)) { return; } // The result of unary math promotes to integer. Basically this converts // bool to integer. Long integer and Real stay as they are. Text is // already ruled out. sem_t sem_type_result = sem_combine_types( (SEM_TYPE_INTEGER | SEM_TYPE_NOTNULL), (core_type | combined_flags)); ast-&gt;sem = new_sem(sem_type_result); ast-&gt;sem-&gt;kind = ast-&gt;left-&gt;sem-&gt;kind; // note ast-&gt;sem-&gt;name is NOT propagated because SQLite doesn't let you refer to // the column 'x' in 'select -x' -- the column name is actually '-x' which is useless // so we have no name once you apply unary math (unless you use 'as') // hence ast-&gt;sem-&gt;name = ast-&gt;left-&gt;sem-&gt;name is WRONG here and it is not missing on accident }  Unary Prep OK already we need to pause because there is a &quot;prep&quot; pattern here common to most of the shared operators that we should discuss. The prep step takes care of most of the normal error handling which is the same for all the unary operators and the same pattern happens in binary operators. Let's take a look at sem_unary_prep. // The unary operators all have a similar prep to the binary. We need // to visit the left side (it's always the left node even if the operator goes on the right) // if that's ok then we need the combined_flags and core type. There is only // the one. Returns true if everything is ok. static bool_t sem_unary_prep(ast_node *ast, sem_t *core_type, sem_t *combined_flags) { // op left | left op sem_expr(ast-&gt;left); if (is_error(ast-&gt;left)) { *core_type = SEM_TYPE_ERROR; *combined_flags = 0; record_error(ast); return false; } sem_node *sem = ast-&gt;left-&gt;sem; sem_t sem_type = sem-&gt;sem_type; *core_type = core_type_of(sem_type); *combined_flags = not_nullable_flag(sem_type) | sensitive_flag(sem_type); Invariant(is_unitary(*core_type)); return true; }  Reviewing the steps: first we analyze the operand, it will be in ast-&gt;leftif that's an error, we just return the error code from the prep stepsnow that it's not an error, we pull the core type out of the operandthen we pull the not nullable and sensitive flag bits out of the operandfinally return a boolean indicating the presence of an error (or not) for convenience This is useful setup for all the unary operators, and as we'll see, the binary operators have a similar prep step. Back to Unary Processing Looking at the overall steps we see: sem_unary_prep : verifies that the operand is not an error, and gets its core type and flag bitssem_validate_numeric : verifies that the operand is a numeric type recall these are the math unary operators, so the operand must be numeric sem_combine_types : creates the smallest type that holds two compatible types by combining with &quot;integer not null&quot; we ensure that the resulting type is at least as big as an integerif the argument is of type long or real then it will be the bigger type and the resulting type will be long or real as appropriatein short, bool is promoted to int, everything else stays the samesem_combine_types also combines the nullability and sensitivity appropriately a new sem_node of the combined type is created the type &quot;kind&quot; of the operand is preserved (e.g. the meters in real&lt;meters&gt;)any column alias or variable name is not preserved, the value is now anonymous These primitives are designed to combine well, for instance, consider sem_unary_integer_math static void sem_unary_integer_math(ast_node *ast, CSTR op) { sem_unary_math(ast, op); sem_reject_real(ast, op); }  The steps are: sem_unary_math : do the sequence we just discussedsem_reject_real : report/record an error if the result type is real otherwise do nothing Note that in all cases the op string simply gets pushed down to the place where the errors happen. Let's take a quick look at one of the sources of errors in the above. Here's the numeric validator: static bool_t sem_validate_numeric(ast_node *ast, sem_t core_type, CSTR op) { if (is_blob(core_type)) { report_error(ast-&gt;left, &quot;CQL0045: blob operand not allowed in&quot;, op); record_error(ast); return false; } if (is_object(core_type)) { report_error(ast-&gt;left, &quot;CQL0046: object operand not allowed in&quot;, op); record_error(ast); return false; } if (is_text(core_type)) { report_error(ast-&gt;left, &quot;CQL0047: string operand not allowed in&quot;, op); record_error(ast); return false; } return true; }  That function is pretty much dumb as rocks. The non-numeric types are blob, object, and text. There is a custom error for each type (it could have been shared but specific error messages seem to help users.) This code doesn't know its context, but all it needs is op to tell it what the numeric-only operator was and it can produce a nice error message. It leaves an error in the AST using record_error. Its caller can then simply returnif anything goes wrong. It's not hard to guess how sem_reject_real works: // Some math operators like &lt;&lt; &gt;&gt; &amp; | % only make sense on integers // This function does the extra checking to ensure they do not get real values // as arguments. It's a post-pass after the normal math checks. static void sem_reject_real(ast_node *ast, CSTR op) { if (!is_error(ast)) { sem_t core_type = core_type_of(ast-&gt;sem-&gt;sem_type); if (core_type == SEM_TYPE_REAL) { report_error(ast, &quot;CQL0001: operands must be an integer type, not real&quot;, op); record_error(ast); } } }  if the AST node isn't already an error, and the node is of type &quot;real&quot;, report an errorit assumes the type is already known to be numericthe pre-check for errors is to avoid double reporting; if something has already gone wrong, the core type will be SEM_TYPE_ERROR no new error recording is needed in that case, as obviously an error was already recorded "},{"title":"Binary Operators​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#binary-operators","content":"Binary Prep​ With the knowledge we have so far, this code pretty much speaks for itself, but we'll walk through it. // All the binary ops do the same preparation -- they evaluate the left and the // right expression, then they check those for errors. Then they need // the types of those expressions and the combined_flags of the result. This // does exactly that for its various callers. Returns true if all is well. static bool_t sem_binary_prep(ast_node *ast, sem_t *core_type_left, sem_t *core_type_right, sem_t *combined_flags) { EXTRACT_ANY_NOTNULL(left, ast-&gt;left); EXTRACT_ANY_NOTNULL(right, ast-&gt;right); // left op right sem_expr(left); sem_expr(right); if (is_error(left) || is_error(right)) { record_error(ast); *core_type_left = SEM_TYPE_ERROR; *core_type_right = SEM_TYPE_ERROR; *combined_flags = 0; return false; } *core_type_left = core_type_of(left-&gt;sem-&gt;sem_type); *core_type_right = core_type_of(right-&gt;sem-&gt;sem_type); *combined_flags = combine_flags(left-&gt;sem-&gt;sem_type, right-&gt;sem-&gt;sem_type); Invariant(is_unitary(*core_type_left)); Invariant(is_unitary(*core_type_right)); return true; }  sem_expr : used to recursively walk the left and right nodesis_error : checks if either side had errors, and, if so, simply propagates the errorextract the left and right core typescombine nullability and sensitivity flags And that's it! These are the standard prep steps for all binary operators. With this done, the caller has the core types of the left and right operands plus combined flags on a silver platter and one check is needed to detect if anything went wrong. Example: Is or Is Not​ This analyzer is the simplest of all the binaries // IS and IS NOT are special in that they return a not null boolean. static void sem_binary_is_or_is_not(ast_node *ast, CSTR op) { sem_t core_type_left, core_type_right, combined_flags; if (!sem_binary_prep(ast, &amp;core_type_left, &amp;core_type_right, &amp;combined_flags)) { return; } if (!sem_verify_compat(ast, core_type_left, core_type_right, op)) { return; } // the result of is or is not is always a bool and never null ast-&gt;sem = new_sem(SEM_TYPE_BOOL | SEM_TYPE_NOTNULL | sensitive_flag(combined_flags)); }  sem_binary_prep : checks for errors in the left or rightsem_verify_compat : ensures that left and right operands are type compatible (discussed later)the result is always of type bool not null If either step goes wrong the error will naturally propagate. Example: Binary Math​ This is the general worker for binary math operations, the most common operations like '+', '-', '*' and so forth. // For all math operations, we combine the types and yield the type that // holds both using the helper. If any text, that's an error. static void sem_binary_math(ast_node *ast, CSTR op) { sem_t core_type_left, core_type_right, combined_flags; if (!sem_binary_prep(ast, &amp;core_type_left, &amp;core_type_right, &amp;combined_flags)) { return; } if (error_any_object(ast, core_type_left, core_type_right, op)) { return; } if (error_any_blob_types(ast, core_type_left, core_type_right, op)) { return; } if (error_any_text_types(ast, core_type_left, core_type_right, op)) { return; } sem_t core_type = sem_combine_types(core_type_left, core_type_right); CSTR kind = sem_combine_kinds(ast-&gt;right, ast-&gt;left-&gt;sem-&gt;kind); if (is_error(ast-&gt;right)) { record_error(ast); return; } ast-&gt;sem = new_sem(core_type | combined_flags); ast-&gt;sem-&gt;kind = kind; }  Let's have a look at those steps: sem_binary_prep : checks for errors on the left or righterror_any_object : reports an error if the left or right is of type objecterror_any_blob_types : reports an error if the left or right is of type bloberror_any_text_types : reports an error if the left or right is of type textsem_combine_type : computes the combined type, the smallest numeric type that holds both left and right note the operands are now known to be numericthe three type error checkers give nice tight errors about the left or right operand sem_combine_kinds : tries to create a single type kind for both operands if their kind is incompatible, records an error on the right new_sem : creates a sem_node with the combined type, flags, and then the kind is set At this point it might help to look at a few more of the base validators -- they are rather unremarkable. Example Validator: error_any_object​ // If either of the types is an object, then produce an error on the ast. static bool_t error_any_object(ast_node *ast, sem_t core_type_left, sem_t core_type_right, CSTR op) { if (is_object(core_type_left)) { report_error(ast-&gt;left, &quot;CQL0002: left operand cannot be an object in&quot;, op); record_error(ast); return true; } if (is_object(core_type_right)) { report_error(ast-&gt;right, &quot;CQL0003: right operand cannot be an object in&quot;, op); record_error(ast); return true; } return false; }  is_object : checks a sem_type against SEM_TYPE_OBJECT if the left or right child is an object, an appropriate error is generated there is no strong convention for returning true if ok, or true if error; it's pretty ad hoc this doesn't seem to cause a lot of problems Example Validator: sem_combine_kinds​ // Here we check that type&lt;Foo&gt; only combines with type&lt;Foo&gt; or type. // If there is a current object type, then the next item must match // If there is no such type, then an object type that arrives becomes the required type // if they ever don't match, record an error static CSTR sem_combine_kinds_general(ast_node *ast, CSTR kleft, CSTR kright) { if (kright) { if (kleft) { if (strcmp(kleft, kright)) { CSTR errmsg = dup_printf(&quot;CQL0070: expressions of different kinds can't be mixed: '%s' vs. '%s'&quot;, kright, kleft); report_error(ast, errmsg, NULL); record_error(ast); } } return kright; } return kleft; } // helper to crack the ast nodes first and then call the normal comparisons static CSTR sem_combine_kinds(ast_node *ast, CSTR kright) { CSTR kleft = ast-&gt;sem-&gt;kind; return sem_combine_kinds_general(ast, kleft, kright); }  sem_combine_kinds : uses the worker sem_combine_kinds_general after extracting the kind from the left node usually you already have one kind and you want to know if another kind is compatible, hence this helper sem_combine_kinds_general : applies the general rules for &quot;kind&quot; strings: NULL + NULL =&gt; NULLNULL + x =&gt; xx + NULL =&gt; xx + x =&gt; xx + y =&gt; error (if x != y) this is one of the rare functions that creates a dynamic error message Example Validator : is_numeric_compat​ This helper is frequently called several times in the course of other semantic checks. This one produces no errors, that's up to the caller. Often there is a numeric path and a non-numeric path so this helper can't create the errors as it doesn't yet know if anything bad has happened. Most of the is_something functions are the same way. cql_noexport bool_t is_numeric_compat(sem_t sem_type) { sem_type = core_type_of(sem_type); return sem_type &gt;= SEM_TYPE_NULL &amp;&amp; sem_type &lt;= SEM_TYPE_REAL; }  is_numeric_compat operates by checking the core type for the numeric range. Note that NULL is compatible with numerics because expressions like NULL + 2have meaning in SQL. The type of that expression is nullable integer and the result is NULL. Example Validator : sem_combine_types​ // The second workhorse of semantic analysis, given two types that // are previously known to be compatible, it returns the smallest type // that holds both. If either is nullable, the result is nullable. // Note: in the few cases where that isn't true, the normal algorithm for // nullability result must be overridden (see coalesce, for instance). static sem_t sem_combine_types(sem_t sem_type_1, sem_t sem_type_2) { ... too much code ... summary below }  This beast is rather lengthy but unremarkable. It follows these rules: text is only compatible with textobject is only compatible with objectblob is only compatible with blobnumerics are only compatible with other numerics and NULL NULL promotes the other operand, whatever it is (might still be NULL)bool promotes to integer if neededinteger promotes to long integer if neededlong integer promotes to real if neededthe combined type is the smallest numeric type that holds left and right according to the above rules Some examples might be helpful: 1 + 2L -&gt; longfalse + 3.1 -&gt; real2L + 3.1 -&gt; realtrue + 2 -&gt; integer'x' + 1 -&gt; not compatible Note that sem_combine_types assumes the types have already been checked for compatibility and will use Contract to enforce this. You should be using other helpers like is_numeric_compat and friends to ensure the types agree before computing the combined type. A list of values that must be compatible with each other (e.g. in needle IN (haystack)) can be checked using sem_verify_compat repeatedly. Example Validator : sem_verify_assignment​ The sem_verify_assignment function is used any time there is something like a logical assignment going on. There are two important cases: SET x := y : an actual assignmentcall foo(x) : the expression x must be &quot;assignable&quot; to the formal variable for the argument of foo This is a lot like normal binary operator compatibility with one extra rule: the source expression must not be a bigger type than the target. e.g. you cannot assign a long to an integer, nor pass a long expression to a function that has an integer parameter. // This verifies that the types are compatible and that it's ok to assign // the expression to the variable. In practice that means: // * the variable type core type and kind must be compatible with the expression core type and kind // * the variable must be nullable if the expression is nullable // * the variable must be sensitive if the assignment is sensitive // * the variable type must be bigger than the expression type // Here ast is used only to give a place to put any errors. static bool_t sem_verify_assignment(ast_node *ast, sem_t sem_type_needed, sem_t sem_type_found, CSTR var_name) { if (!sem_verify_compat(ast, sem_type_needed, sem_type_found, var_name)) { return false; } if (!sem_verify_safeassign(ast, sem_type_needed, sem_type_found, var_name)) { return false; } if (is_nullable(sem_type_found) &amp;&amp; is_not_nullable(sem_type_needed)) { report_error(ast, &quot;CQL0013: cannot assign/copy possibly null expression to not null target&quot;, var_name); return false; } if (sensitive_flag(sem_type_found) &amp;&amp; !sensitive_flag(sem_type_needed)) { report_error(ast, &quot;CQL0014: cannot assign/copy sensitive expression to non-sensitive target&quot;, var_name); return false; } return true; }  sem_verify_compat : checks for standard type compatibility between the left and the rightsem_verify_safeassign : checks that if the types are different the right operand is the smaller of the twonullability checks ensure you aren't trying to assign a nullable value to a not null variablesensitivity checks ensure you aren't trying to assign a sensitive value to a not sensitive variable "},{"title":"Simple Statement Validation​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#simple-statement-validation","content":"With the expression building blocks, most of the usual kind of language statements become quite simple to check for correctness. It's probably easiest to illustrate this with an example. Let's look at validation for the WHILE statement: // While semantic analysis is super simple. // * the condition must be numeric // * the statement list must be error-free // * loop_depth is increased allowing the use of interior leave/continue static void sem_while_stmt(ast_node *ast) { Contract(is_ast_while_stmt(ast)); EXTRACT_ANY_NOTNULL(expr, ast-&gt;left); EXTRACT(stmt_list, ast-&gt;right); // WHILE [expr] BEGIN [stmt_list] END sem_numeric_expr(expr, ast, &quot;WHILE&quot;, SEM_EXPR_CONTEXT_NONE); if (is_error(expr)) { record_error(ast); return; } if (stmt_list) { loop_depth++; sem_stmt_list(stmt_list); loop_depth--; if (is_error(stmt_list)) { record_error(ast); return; } } record_ok(ast); }  EXTRACT* : pulls out the tree parts we needsem_numeric_expr : verifies the loop expression is numericsem_stmt_list : recursively validates the body of the loop Note: the while expression is one of the loop constructs which means that LEAVE and CONTINUE are legal inside it. The loop_depth global tracks the fact that we are in a loop so that analysis for LEAVE and CONTINUE can report errors if we are not. It's not hard to imagine that sem_stmt_list will basically walk the AST, pulling out statements and dispatching them using the STMT_INIT tables previously discussed. You might land right back in sem_while_stmt for a nested WHILE -- it's turtles all the way down. If SEM_EXPR_CONTEXT_NONE is a mystery, don't worry, it's covered in the next section. "},{"title":"Expression Contexts​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#expression-contexts","content":"It turns out that in the SQL language some expression types are only valid in some parts of a SQL statement (e.g. aggregate functions can't appear in a LIMIT clause) and so there is always a context for any numeric expression. When a new root expression is being evaluated, it sets the expression context per the caller's specification. The expression contexts are as follows: #define SEM_EXPR_CONTEXT_NONE 0x0001 #define SEM_EXPR_CONTEXT_SELECT_LIST 0x0002 #define SEM_EXPR_CONTEXT_WHERE 0x0004 #define SEM_EXPR_CONTEXT_ON 0x0008 #define SEM_EXPR_CONTEXT_HAVING 0x0010 #define SEM_EXPR_CONTEXT_ORDER_BY 0x0020 #define SEM_EXPR_CONTEXT_GROUP_BY 0x0040 #define SEM_EXPR_CONTEXT_LIMIT 0x0080 #define SEM_EXPR_CONTEXT_OFFSET 0x0100 #define SEM_EXPR_CONTEXT_TABLE_FUNC 0x0200 #define SEM_EXPR_CONTEXT_WINDOW 0x0400 #define SEM_EXPR_CONTEXT_WINDOW_FILTER 0x0800 #define SEM_EXPR_CONTEXT_CONSTRAINT 0x1000  The idea here is simple: when calling a root expression, the analyzer provides the context value that has the bit that corresponds to the current context. For instance, the expression being validated in is the WHERE clause -- the code will provide SEM_EXPR_CONTEXT_WHERE. The inner validators check this context, in particular anything that is only available in some contexts has a bit-mask of that is the union of the context bits where it can be used. The validator can check those possibilities against the current context with one bitwise &quot;and&quot; operation. A zero result indicates that the operation is not valid in the current context. This bitwise &quot;and&quot; is performed by one of these two helper macros which makes the usage a little clearer: #define CURRENT_EXPR_CONTEXT_IS(x) (!!(current_expr_context &amp; (x))) #define CURRENT_EXPR_CONTEXT_IS_NOT(x) (!(current_expr_context &amp; (x)))  Expression Context Example : Concat​ The concatenation operator || is challenging to successfully emulate because it does many different kinds of numeric to string conversions automatically. Rather than perennially getting this wrong, we simply do not support this operator in a context where SQLite isn't going to be doing the concatenation. So typically users use &quot;printf&quot; instead to get formatting done outside of a SQL context. The check for invalid use of || is very simple and it happens, of course, in sem_concat.  if (CURRENT_EXPR_CONTEXT_IS(SEM_EXPR_CONTEXT_NONE)) { report_error(ast, &quot;CQL0241: CONCAT may only appear in the context of a SQL statement&quot;, NULL); record_error(ast); return; }  Expression Context Example : IN​ A slightly more complex example happens processing the IN operator. This operator has two forms: the form with an expression list, which can be used anywhere, and the form with a SELECT statement. The latter form can only appear in some sections of SQL, and not at all in loose expressions. For instance, that form may not appear in the LIMIT or OFFSET sections of a SQLite statement. We use this construct to do the validation:  uint32_t valid = SEM_EXPR_CONTEXT_SELECT_LIST |SEM_EXPR_CONTEXT_WHERE |SEM_EXPR_CONTEXT_ON |SEM_EXPR_CONTEXT_HAVING |SEM_EXPR_CONTEXT_TABLE_FUNC; if (CURRENT_EXPR_CONTEXT_IS_NOT(valid)) { report_error( ast, &quot;CQL0078: [not] in (select ...) is only allowed inside &quot; &quot;of select lists, where, on, and having clauses&quot;, NULL); record_error(ast); return; }  If the reader is interested in a simple learning exercise, run down the purpose of SEM_EXPR_CONTEXT_TABLE_FUNC -- it's simple, but important, and it only has one use case so it's easy to find. "},{"title":"Name Resolution​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#name-resolution","content":"We've gotten pretty far without talking about the elephant in the room: name resolution. Like SQL, many statements in CQL have names in positions where the type of the name is completely unambiguous. For instance nobody could be confused what sort of symbol Foo is in DROP INDEX Foo. This type, with a clear name category, is the easiest name resolutions, and there are a lot in this form. Let's do an example. Example: Index Name Resolution​ // This is the basic checking for the drop index statement // * the index must exist (have been declared) in some version // * it could be deleted now, that's ok, but the name has to be valid static void sem_drop_index_stmt(ast_node *ast) { Contract(is_ast_drop_index_stmt(ast)); EXTRACT_ANY_NOTNULL(name_ast, ast-&gt;right); EXTRACT_STRING(name, name_ast); ast_node *index_ast = find_usable_index(name, name_ast, &quot;CQL0112: index in drop statement was not declared&quot;); if (!index_ast) { record_error(ast); return; } record_ok(ast); }  Well, this is interesting. But what's going on with find_usable_index? What is usable? Why aren't we just looking up the index name in some name table? Let's have a look at the details of find_usable_index. // returns the node only if it exists and is not restricted by the schema region. static ast_node *find_usable_index(CSTR name, ast_node *err_target, CSTR msg) { ast_node *index_ast = find_index(name); if (!index_ast) { report_error(err_target, msg, name); return NULL; } if (!sem_validate_object_ast_in_current_region(name, index_ast, err_target, msg)) { return NULL; } return index_ast; }  We haven't discussed schema regions yet but what you need to know about them for now is this: any object can be in a region.a region may depend on other regions If an object is in a region, then it may only use schema parts that are in the same region, or the region's dependencies (transitively). The point of this is that you might have a rather large schema and you probably don't want any piece of code to use just any piece of schema. You can use regions to ensure that the code for feature &quot;X&quot; doesn't try to use schema designed exclusively for feature &quot;Y&quot;. That &quot;X&quot; code probably has no business even knowing of the existence of &quot;Y&quot; schema. So now usable simply means this: find_index can find the name in the symbol table for indicesthe found index is accessible in the current region If we had used an example that was looking up a table name, the same region considerations would apply, however, additionally tables can be deprecated with @delete so there would be additional checks to make sure we're talking about a live table and not a table's tombstone. In short, these simple cases just require looking up the entity and verifying that it's accessible in the current context. Flexible Name Resolution​ The &quot;hard case&quot; for name resolution is where the name is occurring in an expression. Such a name can refer to all manner of things. It could be a global variable, a local variable, an argument, a table column, a field in a cursor, and others. The general name resolver goes through several phases looking for the name. Each phase can either report an affirmative success or error (in which case the search stops), or it may simply report that the name was not found but the search should continue. We can demystify this a bit by looking at the most common way to get name resolution done. // Resolves a (potentially qualified) identifier, writing semantic information // into `ast` if successful, or reporting and recording an error for `ast` if // not. static void sem_resolve_id(ast_node *ast, CSTR name, CSTR scope) { Contract(is_id(ast) || is_ast_dot(ast)); Contract(name); // We have no use for `type` and simply throw it away. sem_t *type = NULL; sem_resolve_id_with_type(ast, name, scope, &amp;type); }  The name resolver works on either a vanilla name (e.g. x) or a scoped name (e.g. T1.x). The name and scope are provided. The ast parameter is used only as a place to report errors; there is no further cracking of the AST needed to resolve the name. As you can see sem_resolve_id just calls the more general function sem_resolve_id_with_type and is used in the most common case where you don't need to be able to mutate the semantic type info for the identifier. That's the 99% case. So let's move on to the &quot;real&quot; resolver. // This function is responsible for resolving both unqualified identifiers (ids) // and qualified identifiers (dots). It performs the following two roles: // // - If an optional `ast` is provided, it works the same way most semantic // analysis functions work: semantic information will be written into the // ast, errors will be reported to the user, and errors will be recorded in // the AST. // // - `*type_ptr` will be set to mutable type (`sem_t *`) in the current // environment if the identifier successfully resolves to a type. (There are, // unfortunately, a few exceptions in which a type will be successfully // resolved and yet `*type_ptr` will not be set. These include when a cursor is // in an expression position, when the expression is `rowid` (or similar), and // when the id resolves to an enum case. The reason no mutable type is // returned in these cases is that a new type is allocated as part of semantic // analysis, and there exists no single, stable type in the environment to // which a pointer could be returned. This is a limitation of this function, // albeit one that's currently not problematic.) // // Resolution is attempted in the order that the `sem_try_resolve_*` functions // appear in the `resolver` array. Each takes the same arguments: An (optional) // AST, a mandatory name, an optional scope, and mandatory type pointer. If the // identifier provided to one of these resolvers is resolved successfully, *or* // if the correct resolver was found but there was an error in the program, // `SEM_RESOLVE_STOP` is returned and resolution is complete, successful or not. // If a resolver is tried and it determines that it is not the correct resolver // for the identifier in question, `SEM_RESOLVE_CONTINUE` is returned and the // next resolver is tried. // // This function should not be called directly. If one is interested in // performing semantic analysis, call `sem_resolve_id` (or, if within an // expression, `sem_resolve_id_expr`.) Alternatively, if one wants to get a // mutable type from the environment, call `find_mutable_type`. static void sem_resolve_id_with_type(ast_node *ast, CSTR name, CSTR scope, sem_t **type_ptr) { Contract(name); Contract(type_ptr); *type_ptr = NULL; sem_resolve (*resolver[])(ast_node *ast, CSTR, CSTR, sem_t **) = { sem_try_resolve_arguments, sem_try_resolve_column, sem_try_resolve_rowid, sem_try_resolve_cursor_as_expression, sem_try_resolve_variable, sem_try_resolve_enum, sem_try_resolve_cursor_field, sem_try_resolve_arg_bundle, }; for (uint32_t i = 0; i &lt; sizeof(resolver) / sizeof(void *); i++) { if (resolver[i](ast, name, scope, type_ptr) == SEM_RESOLVE_STOP) { return; } } report_resolve_error(ast, &quot;CQL0069: name not found&quot;, name); record_resolve_error(ast); }  This function is well described in its own comments. We can easily see the &quot;mini-resolvers&quot; which attempt to find the name in order: sem_try_resolve_arguments : an argument in the argument listsem_try_resolve_column : a column name (possibly scoped)sem_try_resolve_rowid : the virtual rowid column (possibly scoped)sem_try_resolve_cursor_as_expression : use of a cursor as a boolean -- the bool is true if the cursor has datasem_try_resolve_variable : local or global variablessem_try_resolve_enum : the constant value of an enum (must be scoped)sem_try_resolve_cursor_field : a field in a cursor (must be scoped)sem_try_resolve_arg_bundle : a field in an argument bundle (must be scoped) These all use this enum to communicate progress: // All `sem_try_resolve_*` functions return either `SEM_RESOLVE_CONTINUE` to // indicate that another resolver should be tried, or `SEM_RESOLVE_STOP` to // indicate that the correct resolver was found. Continuing implies that no // failure has (yet) occurred, but stopping implies neither success nor failure. typedef enum { SEM_RESOLVE_CONTINUE = 0, SEM_RESOLVE_STOP = 1 } sem_resolve;  Each of these mini-resolvers will have a series of rules, for example sem_try_resolve_cursor_field is going to have to do something like this: if there is no scope, it can't be a cursor field, return CONTINUEif the scope is not the name of a cursor, return CONTINUEif the name is a field in the cursor, return STOP with successelse, report that the name is not a valid member of the cursor, and return STOP with an error All the mini-resolvers are similarly structured, generically: if it's not my case, return CONTINUEif it is my case return STOP (maybe with an error) Some of the mini-resolvers have quite a few steps, but any one mini-resolver is only about a screenful of code and it does one job. "},{"title":"Flow Analysis​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#flow-analysis","content":"CQL implements a basic form of control flow analysis in &quot;flow.c&quot;. The header &quot;flow.h&quot; exposes a small set of primitives used by &quot;sem.c&quot; during semantic analysis. Flow analysis in CQL involves two important concepts: flow contexts andimprovements. These are rather entangled concepts — one is useless without the other — and so the approach to describing them here will alternate between giving a bit of background on one and then the other, with a greater level of detail about the specific types of improvements being supplied later on. A flow context is used, in essence, to create a boundary around a portion of a user's program. At the moment, there are four types of contexts. The first type of context is called, rather boringly, a normal context. Normal contexts are used for portions of a user's code that may be entered conditionally. A good example of this is in SELECT expressions: When a WHEREclause is present, the expression list is only evaluated when the WHERE clause is true. If we look at sem_select_expr_list_con, we can get an idea of how this works in terms of flow contexts: static void sem_select_expr_list_con(ast_node *ast) { ... // Analyze the FROM portion (if it exists). sem_select_from(select_from_etc); error = is_error(select_from_etc); // Push a flow context to contain improvements made via the WHERE clause that // will be in effect for the SELECT expression list. FLOW_PUSH_CONTEXT_NORMAL(); if (!error) { ... sem_sensitive = sem_select_where_etc(select_from_etc); ... sem_set_improvements_for_true_condition(where_expr); ... } ... if (!error) { ... sem_select_expr_list(select_expr_list); ... } ... FLOW_POP_CONTEXT_NORMAL(); ... }  While very much simplified above, it can be seen that the steps are essentially as follows: Analyze the FROM clause.Push a new normal context.Analyze the WHERE clause.Set improvements given the WHERE clause (ultimately by callingflow_set_flag_for_type); we'll come back to this part shortly.Analyze the expression list with the improvements from the WHERE in effect.Pop the context, un-setting the improvements from the WHERE. This, of course, only begins to make sense once one understands what we mean by improvements. CQL, at the moment, supports two forms of improvements: nullability improvements and initialization improvements. Both of these will be discussed in more detail later, but the basic idea is that an improvement upgrades the type of some value within a particular flow context. For example, in the expression SELECT x + x FROM t WHERE x IS NOT NULL, we can reason that x + x can safely be given a nonnull type because of the WHERE clause. This is exactly what we do insem_select_expr_list_con: We make a context to hold the improvements that may come from the WHERE, analyze the WHERE, set the appropriate improvements given the WHERE, analyze the expression list, and then pop the context to unset the improvements (as they must not affect any enclosing expressions). In addition to normal contexts, there are also branch contexts and branch group contexts. These two context types are designed to work together for handling IF, CASE, IIF, SWITCH, et cetera. Like normal contexts, branch contexts assume that they are entered when some condition is true. The difference is that branch contexts lie within a branch group context, and branch groups know that at most one branch of a given set of branches will be entered. A great example of this can be found insem_if_stmt: static void sem_if_stmt(ast_node *ast) { ... // Each branch gets its own flow context in `sem_cond_action` where its // condition is known to be true. We also create one more context for the // entire set of branches. In addition to grouping the branches together, this // outer context holds all of the negative improvements that result from the // knowledge that, if a given branch's statements are being evaluated, all // previous branches' conditions must have been false. FLOW_PUSH_CONTEXT_BRANCH_GROUP(); // IF [cond_action] sem_cond_action(cond_action); ... if (elseif) { sem_elseif_list(elseif); ... } ... if (elsenode) { // ELSE [stmt_list] flow_set_context_branch_group_covers_all_cases(true); EXTRACT(stmt_list, elsenode-&gt;left); if (stmt_list) { FLOW_PUSH_CONTEXT_BRANCH(); sem_stmt_list_in_current_flow_context(stmt_list); FLOW_POP_CONTEXT_BRANCH(); ... } else { flow_context_branch_group_add_empty_branch(); } record_ok(elsenode); } ... FLOW_POP_CONTEXT_BRANCH_GROUP(); ... }  It's instructive to look at sem_cond_action as well: static void sem_cond_action(ast_node *ast) { ... // [expr] THEN stmt_list sem_expr(expr); ... if (stmt_list) { FLOW_PUSH_CONTEXT_BRANCH(); // Add improvements for `stmt_list` where `expr` must be true. sem_set_improvements_for_true_condition(expr); sem_stmt_list_in_current_flow_context(stmt_list); FLOW_POP_CONTEXT_BRANCH(); ... } else { flow_context_branch_group_add_empty_branch(); } // If a later branch will be taken, `expr` must be false. Add its negative // improvements to the context created in `sem_if_stmt` so that all later // branches will be improved by the OR-linked spine of IS NULL checks in // `expr`. sem_set_improvements_for_false_condition(expr); ... }  Putting all of this together, we can see that the basic steps for analyzing anIF statement are as follows: Push a new branch group context to hold all of the branch contexts that are to come.Analyze the condition in the IF condition THEN portion of the statement.Push a new branch context to hold the nullability improvements from the condition (e.g., in IF x IS NOT NULL THEN, we can improve x to have a nonnull type in the statement list after the THEN).Set the improvements.Anaylze the statement list after the THEN.Pop the branch context.Set the negative improvements resulting from the knowledge that conditionmust have been false if the previous branch wasn't entered (e.g., in IF y IS NULL THEN, we know that y must be nonnull from just after the end of the branch until the end of the current branch group).Repeat for the ELSE IF and ELSE branches (if any).Pop the branch group context. What makes all of this work are the following: When a branch context is popped, it resets all improvements such that they become exactly what they were before the branch was analyzed. This is done to reflect the fact that, because at most one branch will be entered, neither adding improvements (via flow_set_flag_for_type) nor removing existing improvements (via flow_unset_flag_for_type) in a branch should affect any of the other branches in the group. When a branch group context is popped, it merges the effects of all of its branches. This is a key step that allows CQL to retain an improvement after a branch group is popped whenever the same improvement is made within every one of its branches and when the branches given cover all possible cases (which is indicated by the call to flow_set_context_branch_group_covers_all_casesin the code above). The final type of context is called a jump context. Jump contexts are a maximally pessimistic form of context that assume every improvement that might be unset within them will be unset and that every improvement that might be set within them will not be set. Jump contexts are used to make semantic analysis safe in the possible presence of control flow statements like CONTINUE,LEAVE, and THROW, and so jump contexts are used for the analysis of statements like LOOP, WHILE, and TRY. Take the following line-numbered code as an example: 001 DECLARE x TEXT; 002 SET x := &quot;foo&quot;; 003 WHILE some_condition 004 BEGIN 005 IF another_condition THEN 006 SET x := NULL; 007 IF yet_another_condition THEN 008 LEAVE; 009 END IF; 010 SET x := &quot;bar&quot;; 011 ELSE 012 -- do nothing 013 END IF; 014 END; 015 CALL requires_text_notnull(x);  Here, even though the outer IF makes no change overall to the nullability improvement to x from line 2 -- it unsets it on line 6 and then re-sets it on line 10 and the ELSE does nothing—there is no guarantee that line 10 will ever be evaluated because we may jump straight from line 8 to line 15. As a result, it is necessary that x be un-improved after the WHILE loop; a normal context would not accomplish this, but a jump context does. See the comments within_flow_push_context_branch for additional discussion. While jump contexts are necessary for the safety of improvements in the presence of loops, they are not sufficient: It's actually necessary to analyze loopstwice. This is because execution of a loop might repeat, and so a statement that results in the unsetting of an improvement later in a loop must affect improvements earlier in that loop. For example: DECLARE x INT; SET x := 1; WHILE some_condition BEGIN -- okay on the first analysis pass, but not the second CALL requires_int_notnull(x); -- must negatively affect the call on the line above SET x := NULL; END;  Semantic analysis keeps track of whether or not it is currently reanalyzing the statement list of a loop via the current_loop_analysis_state variable: // The analysis of loops like LOOP and WHILE is done in two passes. First, we // analyze the loop to conservatively figure out every improvement that the loop // could possibly unset. After that, we reanalyze it with said improvements // unset to ensure that everything is safe. See `sem_stmt_list_within_loop` for // more information on why this is necessary. typedef enum { LOOP_ANALYSIS_STATE_NONE, LOOP_ANALYSIS_STATE_ANALYZE, LOOP_ANALYSIS_STATE_REANALYZE } loop_analysis_state; ... // Keeps tracks of the current loop analysis state. If this is equal to // `LOOP_ANALYSIS_STATE_ANALYZE`, we are analyzing with a non-final set of // improvements. This is useful for two reasons: // // 1. Procedures that perform rewrites based on improvements (e.g., // `sem_resolve_id_expr`) can use this to verify whether a rewrite is safe to // perform (`LOOP_ANALYSIS_STATE_NONE` or `LOOP_ANALYSIS_STATE_REANALYZE`) or // whether they should wait because they do not yet have definitive // information (`LOOP_ANALYSIS_STATE_ANALYZE`). // // 2. Analyses that would otherwise fail if called during reanalysis (e.g., // `sem_verify_legal_variable_name`) can use this to check whether the // current state is `LOOP_ANALYSIS_STATE_REANALYZE` and adjust their // behaviors accordingly. static loop_analysis_state current_loop_analysis_state = LOOP_ANALYSIS_STATE_NONE;  As indicated in the first comment above, the comments withinsem_stmt_list_within_loop go into further detail. At this point, we've only scratched the surface of control flow analysis in CQL. Fortunately, the files &quot;flow.h&quot; and &quot;flow.c&quot; are heavily commented and can be studied to deepen one's understanding. "},{"title":"Nullability Improvements​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#nullability-improvements","content":"Via a form of occurrence typing (also known as flow typing), CQL has the ability to determine that, due to a prior conditional check, a nullable variable or cursor field cannot be null within a particular context, and CQL will improve its type in that context. Unlike most forms of semantic analysis performed by CQL, the analysis for nullability improvements, as is the case for all types of improvements, makes heavy use of the find_mutable_type function: // Returns the *mutable* type (`sem_t *`) for a given (potentially qualified) // identifier if one exists in the environment. See the documentation for // `sem_resolve_id_with_type` for limitations. static sem_t *find_mutable_type(CSTR name, CSTR scope);  This function allows us to look up the type of the original binding referred to by a particular name/scope pair. In essence, it provides access to the current type environment for whichever part of the program we are analyzing. It also allows us to mutate that environment by virtue of the fact that it returns a pointer to the type of the binding, not merely the type itself. By using find_mutable_type to get a type pointer and toggling theSEM_TYPE_INFERRED_NOTNULL flag via flow_set_flag_for_type andflow_unset_flag_for_type, the procedures sem_set_notnull_improved andsem_unset_notnull_improved are able to record that a nullable identifier or cursor field is either temporarily nonnull or no longer nonnull respectively: // Enables a nonnull improvement, if possible. static void sem_set_notnull_improved(CSTR name, CSTR scope); // This needs to be called for everything that is no longer safe to consider NOT // NULL due to a mutation. It is fine to call this for something not currently // subject to improvement, but it must only be called with a name/scope pair // referring to something that has a mutable type (e.g., it must not be an unbound // variable, a cursor used as an expression, an enum case, et cetera). static void sem_unset_notnull_improved(CSTR name, CSTR scope);  Similarly, sem_is_notnull_improved uses find_mutable_type to check whether or not something is currently improved: // Returns true if currently improved to be nonnull, else false. static bool_t sem_is_notnull_improved(CSTR name, CSTR scope);  Why does nullability inference use this approach? The reason is that the alternative would be maintaining some sort of set of currently improved identifiers and cursor fields and checking it whenever resolving an identifier or cursor field. The problem would be that merely knowing that some identifier &quot;x&quot; is improved would not be sufficient, however: We'd have to know which &quot;x&quot;. Is it the local variable &quot;x&quot;? Is it the column &quot;x&quot; of the table from which we're currently selecting? In essence, correctly maintaining an independent set of all currently active improvements would involve re-implementing all of the scoping rules of the language. By using find_mutable_type, we can simply piggyback on the existing name resolution logic and avoid all of these issues. A nullability improvement is always created within a particular flow context. When an improvement is added via sem_set_notnull_improved, a record of that improvement is recorded in the current context. When that context ends, that same record is used to remove the improvement. It is also the case thatsem_unset_notnull_improved may be used to remove an improvement before a context has ended due to a SET, FETCH, or call to a procedure or function with an OUT argument resulting in the improvement no longer being safe. Improvements can be introduced into the current context viasem_set_notnull_improved directly (when a variable is SET to a value of a nonnull type), but more commonly they are introduced via one of the following two functions: // Given a conditional expression `ast` possibly containing AND-linked // subexpressions, set all of the applicable nullability and has-row // improvements within the current flow context. Generally speaking, calls to // this function should be bounded by a new flow context corresponding to the // portion of the program for which the condition `ast` must be be true. static void sem_set_improvements_for_true_condition(ast_node *expr); // Improvements for known-false conditions are dual to improvements for // known-true conditions. // // For nullability, known-false conditions improve ids and dots verified to be // NULL via `IS NULL` along the outermost spine of `OR` expressions, whereas // known-true conditions improve ids and dots verified to be nonnull via `IS NOT // NULL` along the outermost spine of `AND` expressions. For example, the // following two statements introduce the same improvements: // // IF a IS NOT NULL AND b IS NOT NULL THEN // -- `a` and `b` are improved here because we know the condition is true // END IF; // // IF a IS NULL OR b IS NULL RETURN; // -- `a` and `b` are improved here because we know the condition is false // -- since we must not have returned if we got this far // // ... static void sem_set_improvements_for_false_condition(ast_node *ast);  These functions introduce improvements by gathering up all of the IS NOT NULLchecks (in the true case) or IS NULL checks (in the false case) and introducing improvements appropriately. The true version is used when we enter a context that will only be evaluated at runtime when some particular condition is true; the false version, conversely, is used when we enter a context that will only be evaluated at runtime when some particular condition is false: IF some_condition THEN -- &quot;true&quot; improvements from `some_condition` are in -- effect here ELSE IF another_condition THEN -- &quot;false&quot; improvements from `some_condition` and true -- improvements from `another_condition` are in effect -- here ELSE -- &quot;false&quot; improvements from both `some_condition` and -- `another_condition` are in effect here END IF;  Global variables in CQL require special treatment when it comes to nullability improvements. This is because any procedure call could potentially mutate any number of global variables, and so all currently improved globals must be un-improved at every such call. The following list keeps track of which global variables are currently improved: typedef struct global_notnull_improvement_item { sem_t *type; struct global_notnull_improvement_item *next; } global_notnull_improvement_item; ... // Keeps track of all global variables that may currently be improved to be NOT // NULL. We need this because we must un-improve all such variables after every // procedure call (because we don't do interprocedural analysis and cannot know // which globals may have been set to NULL). static global_notnull_improvement_item *global_notnull_improvements;  The fact that we don't do interprocedural analysis (as the comment above indicates) is not a deficiency. Programmers should be able to reason locally about nullability improvements, and an analysis that depended upon the details of how other procedures were implemented would make that impossible. So far, we have talked a lot about how improvements are set and unset, but we haven't talked about how the improvement actually happens in terms of code generation. Since CQL represents values of nullable and nonnull types differently (at least in the case of non-reference types), we cannot simply treat a value of a nullable type as though it were of a nonnull type: We need to actually change its representation. The way this works is that, whenever we resolve a name/scope pair viasem_resolve_id_expr, we check whether the pair is currently improved viasem_is_notnull_improved. If it is, we call rewrite_nullable_to_notnull to wrap the id or dot we're resolving with a call to the functioncql_inferred_notnull (for which we generate code incg_func_cql_inferred_notnull): // Wraps an id or dot in a call to cql_inferred_notnull. cql_noexport void rewrite_nullable_to_unsafe_notnull(ast_node *_Nonnull ast); // The `cql_inferred_notnull` function is not used by the programmer directly, // but rather inserted via a rewrite during semantic analysis to coerce a value // of a nullable type to be nonnull. The reason for this approach, as opposed to // just changing the type directly, is that there are also representational // differences between values of nullable and nonnull types; some conversion is // required. static void cg_func_cql_inferred_notnull(ast_node *call_ast, charbuf *is_null, charbuf *value);  As the comment for cg_func_cql_inferred_notnull indicates, programmers do not use cql_inferred_notnull directly: It is only inserted as a product of the above-mentioned rewrite. In fact, we explicitly disallow its use by programmers in the parser: // We insert calls to `cql_inferred_notnull` as part of a rewrite so we expect // to see it during semantic analysis, but it cannot be allowed to appear in a // program. It would be unsafe if it could: It coerces a value from a nullable // type to a nonnull type without any runtime check. #define YY_ERROR_ON_CQL_INFERRED_NOTNULL(x) \\ EXTRACT_STRING(proc_name, x); \\ if (!strcmp(proc_name, &quot;cql_inferred_notnull&quot;)) { \\ yyerror(&quot;Call to internal function is not allowed 'cql_inferred_notnull'&quot;); \\ }  One subtle aspect of the rewrite is that the rewrite itself performs analysis to validate the product of the rewrite (as do other many other rewrites). To avoid going into a loop of rewriting, analyzing the result (which ultimately happens in sem_special_func_cql_inferred_notnull), rewriting again because the result contains a name that is improved, et cetera, we keep track of whether or not we're currently analyzing a subexpression under a call to cql_inferred_notnulland avoid re-rewriting appropriately: // This is true if we are analyzing a call to `cql_inferred_notnull`. This can // happen for three reasons: // // * We just did a rewrite that produced a `cql_inferred_notnull` call and now // we're computing its type. // * We're analyzing an expression that was already analyzed (e.g., in a CTE). // * We're analyzing the output of a previous CQL run within which calls to // `cql_inferrred_notnull` may occur. // // Regardless of the cause, if `is_analyzing_notnull_rewrite` is true, we do not // want to rewrite again. static bool_t is_analyzing_notnull_rewrite; static void sem_special_func_cql_inferred_notnull(ast_node *ast, uint32_t arg_count, bool_t *is_aggregate) { ... // Since we're checking a call to `cql_inferred_notnull`, its arguments have // already been rewritten and we don't want to do it again. Setting // `is_analyzing_notnull_rewrite` prevents that. is_analyzing_notnull_rewrite = true; sem_arg_list(arg_list, IS_NOT_COUNT); is_analyzing_notnull_rewrite = false; ... } // Like `sem_resolve_id`, but specific to expression contexts (where nullability // improvements are applicable). static void sem_resolve_id_expr(ast_node *ast, CSTR name, CSTR scope) { ... if (is_analyzing_notnull_rewrite) { // If we're analyzing the product of a rewrite and we're already inside of a // call to `cql_inferred_notnull`, do not expand again. // forever. return; } ... }  At this point, you should have a decent understanding of how nullability improvements function, both in terms of semantic analysis and in terms of code generation. The implementation is heavily commented, so reading the code and searching for calls to the core functions listed below should be sufficient to fill in any gaps: bool_t sem_is_notnull_improved(CSTR name, CSTR scope); void sem_set_notnull_improved(CSTR name, CSTR scope); void sem_unset_notnull_improved(CSTR name, CSTR scope); void sem_unset_global_notnull_improvements(); void sem_set_improvements_for_true_condition(ast_node *ast); void sem_set_improvements_for_false_condition(ast_node *ast); void sem_special_func_cql_inferred_notnull(ast_node *ast, uint32_t arg_count, bool_t *is_aggregate) void rewrite_nullable_to_unsafe_notnull(ast_node *_Nonnull ast);  "},{"title":"Initialization Improvements​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#initialization-improvements","content":"Compared to nullability improvements, initialization improvements are relatively simple. The idea behind initialization improvements is that, if one declares a variable of a reference type (BLOB, OBJECT, or TEXT) that is also NOT NULL, it is not safe to use the variable until it has been given a value. For example: DECLARE x TEXT NOT NULL; IF some_condition THEN SET x := some_text_notnull_value; -- `x` is safe to use here ELSE -- `x` is NOT safe to use here (it might be uninitialized) END IF; -- `x` is NOT safe to use here either (it might be uninitialized)  As with nullability improvements, initialization improvements rely heavily on flow contexts. The function sem_set_initialization_improved, similarly tosem_set_notnull_improved for nullability, is used to enable an initialization improvement. (There is nothing analogous to sem_unset_notnull_improved for initialization because nothing can ever be uninitialized once it has been given a value.) Unlike nullability improvements, initialization improvements use two flags:SEM_TYPE_INIT_REQUIRED and SEM_TYPE_INIT_COMPLETE. Rather than assuming everything is uninitalized by default and requiring the presence of someSEM_TYPE_INITIALIZED flag before anything can be used, we explicitly tag things that are not initialized but need to be with SEM_TYPE_INIT_REQUIRED and later tag them with SEM_TYPE_INIT_COMPLETE once they've been initialized. Doing it in this way has two benefits: It reduces the amount of noise in the AST output significantly: Code likeLET x := 10; can remain {let_stmt}: x: integer notnull variable in the AST without the need of the extra noise of some initialized flag. More importantly, it means we only have to deal with initialization in a tiny portion of &quot;sem.c&quot;. For example, we must handle it in sem_declare_vars_typeto add the SEM_TYPE_INIT_REQUIRED flag and in sem_assign to addSEM_TYPE_INIT_COMPLETE, but sem_let_stmt can remain blissfully ignorant of initialization altogether. There are only three places in which a variable may be initialized: sem_assign(as mentioned), sem_fetch_stmt (for the FETCH...INTO form), andsem_arg_for_out_param (as passing a variable to a procedure requiring an OUTargument of a NOT NULL type can initialize it). Regarding sem_arg_for_out_param, we can only set initialization improvements when a variable is passed as an OUT argument because we require that all procedures initialize all of their OUT parameters of a nonnull reference type. This is handled in two places: In sem_param, we set the SEM_TYPE_INIT_REQUIRED flag whenparam_should_require_initialization is true. In sem_validate_current_proc_params_are_initialized, which is called both after analyzing the statement list of a procedure and for each return statement within the procedure, we ensure that SEM_TYPE_INIT_COMPLETE is present on all parameters that have SEM_TYPE_INIT_REQUIRED. There is only one wrinkle in all of this: the cql:try_is_proc_body attribute. If cql:try_is_proc_body is present on a TRY statement, we callsem_validate_current_proc_params_are_initialized at the end of the TRY andnot at the end of the procedure. The rationale for this is explained thoroughly in the comments forsem_find_ast_misc_attr_trycatch_is_proc_body_callback. That's all there is to it: &quot;flow.c&quot; does most of the hard work for us. "},{"title":"Structure types and the notion of Shapes​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#structure-types-and-the-notion-of-shapes","content":"Earlier we discussed SEM_TYPE_STRUCT briefly. Recall the basic notion of the structure type: // for tables and views and the result of a select typedef struct sem_struct { CSTR struct_name; // struct name uint32_t count; // count of fields CSTR *names; // field names CSTR *kinds; // the &quot;kind&quot; text of each column, if any, e.g. integer&lt;foo&gt; foo is the kind sem_t *semtypes; // typecode for each field } sem_struct;  The structure is nothing more than an array of names, types and kinds with a count. But it creates the notion of what's usually called a &quot;shape&quot; in the codebase. Shapes can be used in a variety of ways as is described inChapter 5 of the CQL Guide. But before we get into shapes, let's look at an example of how a structure type is created. The code that follows is the back end of sem_create_table_stmt. At this point the bulk of the analysis is done and the columns all have their types. We're about to build the struct type for the table. Let's see how that goes.  // now create a struct type with the correct number of columns // the types have already been computed so all we have to do is // check for duplicates sem_struct *sptr = new_sem_struct(name, cols); symtab *columns = symtab_new(); int32_t col = 0; for (ast_node *item = col_key_list; item; item = item-&gt;right) { Contract(is_ast_col_key_list(item)); EXTRACT_ANY_NOTNULL(def, item-&gt;left); if (is_ast_col_def(def)) { Invariant(def-&gt;sem-&gt;name); Invariant(col &lt;= cols); // it's possible that the rest are deleted and we're at the end. // columns must be unique, including deleted columns if (!symtab_add(columns, def-&gt;sem-&gt;name, NULL)) { EXTRACT_NOTNULL(col_def_type_attrs, def-&gt;left); EXTRACT_NOTNULL(col_def_name_type, col_def_type_attrs-&gt;left); EXTRACT_ANY_NOTNULL(col_def_ast, col_def_name_type-&gt;left); report_error(col_def_ast, &quot;CQL0142: duplicate column name&quot;, def-&gt;sem-&gt;name); record_error(ast); symtab_delete(columns); goto cleanup;; } if (is_deleted(def-&gt;sem-&gt;sem_type)) { continue; } Invariant(col &lt; cols); sptr-&gt;names[col] = def-&gt;sem-&gt;name; sptr-&gt;semtypes[col] = def-&gt;sem-&gt;sem_type; sptr-&gt;kinds[col] = def-&gt;sem-&gt;kind; col++; } } symtab_delete(columns); Invariant(col == cols); ast-&gt;sem = new_sem(SEM_TYPE_STRUCT); ast-&gt;sem-&gt;sptr = sptr; ast-&gt;sem-&gt;jptr = sem_join_from_sem_struct(sptr); ast-&gt;sem-&gt;region = current_region;  new_sem_struct : makes a struct to hold the result, we already have the count of columns and the table namesymtab_new : is going to give us a scratch symbol table so we can check for duplicate column nameswe walk all the items in the table and use is_ast_col_def(def) to find the column definitionsInvariant(def-&gt;sem-&gt;name) : claims that we must have already computed the semantic info for the column and it has its name populated this was done earlier symtab_add(columns, def-&gt;sem-&gt;name, NULL) : adds a nil entry under the column name -- if this fails we have a duplicate column, in which case we report errors and stop is_deleted : tells us if the column was marked with @delete in which case it no longer counts as part of the tableif all this is good we set the names, kinds, and semtypes from the column definition's semantic infosymtab_delete : cleans up the temporary symbol tablenew_sem : creates a sem_node of type SEM_TYPE_STRUCT which is filled in sem_join_from_sem_struct will be discussed shortly, but it creates a jptr with one table in it Structure types often come from the shape of a table, but other things can create a structure type. For instance, the columns of a view, or any select statement, are also described by a structure type and are therefore valid &quot;shapes&quot;. The return type of a procedure usually comes from a SELECT statement, so the procedure too can be the source of a shape. The arguments of a procedure form a shape. The fields of a cursor form a shape. You can even have a named subset of the arguments of a procedure and use them like a shape. All of these things are described by structure types. Shapes and the LIKE construct​ There are many cases where you want to be able to capture or re-use something with a known shape and you don't want to have to fully re-declare the thing. CQL uses the LIKE construct to do these sorts of things. This is more fully explained in Chapter 5 of the Guide, but for now let's look at two different cases that are of interest. First, a cursor: DECLARE C CURSOR LIKE Foo; -- Foo something with a shape  So, in the above, Foo could be a table, a view, a procedure with a result, another cursor, and so forth. How might we do this? This is the business of sem_declare_cursor_like_name // Here we're going to make a new value cursor using the indicated name for the shape. // The name has to be &quot;likeable&quot; meaning it refers to some named thing with a shape // such as a table, a view, another cursor, or a procedure that returns a result set. // These are the so called &quot;value cursors&quot; in that they have no underlying statement // that they move through. You can just load them up with a row and pass them around. static void sem_declare_cursor_like_name(ast_node *ast) { Contract(is_ast_declare_cursor_like_name(ast)); EXTRACT_ANY_NOTNULL(new_cursor_ast, ast-&gt;left); EXTRACT_STRING(new_cursor_name, new_cursor_ast); EXTRACT_ANY_NOTNULL(shape_def, ast-&gt;right); // no duplicates allowed if (!sem_verify_legal_variable_name(ast, new_cursor_name)) { record_error(new_cursor_ast); record_error(ast); return; } // must be a valid shape ast_node *found_shape = sem_find_shape_def(shape_def, LIKEABLE_FOR_VALUES); if (!found_shape) { record_error(ast); return; } // good to go, make our cursor, with storage. shape_def-&gt;sem = found_shape-&gt;sem; new_cursor_ast-&gt;sem = new_sem(SEM_TYPE_STRUCT | SEM_TYPE_VARIABLE | SEM_TYPE_VALUE_CURSOR | SEM_TYPE_HAS_SHAPE_STORAGE); new_cursor_ast-&gt;sem-&gt;sptr = found_shape-&gt;sem-&gt;sptr; new_cursor_ast-&gt;sem-&gt;name = new_cursor_name; ast-&gt;sem = new_cursor_ast-&gt;sem; symtab_add(current_variables, new_cursor_name, new_cursor_ast); }  EXTRACT : gets the pieces we need from the ASTsem_verify_legal_variable_name : makes sure the cursor name is unique and doesn't hide a table namesem_find_shape_def : searches for something with a suitable name that has a shapewe populate the name node with the semantic type that we foundnew_sem : makes a new sem_node for the cursor variable with SEM_TYPE_STRUCT set the sptr field using the discovered shape Note: name_ast-&gt;sem isn't actually used for anything but it is helpful for debugging. If the AST is printed it shows the original unmodified semantic type which can be helpful. Briefly sem_find_shape_def does these steps: if the right of the LIKE refers to procedure arguments (e.g. C LIKE Foo ARGUMENTS), get the args of the named procedure and use them as a shapeif the right is a local or global, and its a cursor, use the shape of that cursor for the new cursorif the right is the name of an argument bundle, use the shape of the bundle e.g. in CREATE PROC Foo(p1 like Person, p2 like Person) p1 and p2 are the names of argument bundles shaped like Person if the right is the name of a table or view, use that shapeif the right is the name of a procedure with a structure result, use that shapeif it's none of these, produce an error This is the primary source of shape reuse. Let's look at how we might use that. Suppose we want to write a procedure that inserts a row into the table Foo, we could certainly list the columns of Foo as arguments like this: CREATE PROC InsertIntoFoo(id integer, t text, r real, b blob) BEGIN INSERT INTO Foo(id, t, r, b) VALUES(id, t, r, b); END;  But that approach is going to get a lot less exciting when there are lots of columns and it will be increasingly a maintenance headache. Compare that with the following: CREATE PROC InsertIntoFoo(row LIKE Foo) BEGIN INSERT INTO Foo FROM row; END;  Those two versions of InsertIntoFoo compile into the same code. The semantic analyzer expands the (row LIKE Foo) into(row_id integer, row_t text, row_r real, row_b blob) and then replaces FROM row with(row_id, row_t, row_r, row_b). In both case it simply looked up the shape using sem_find_shape_defand then altered the AST to the canonical pattern. This kind of &quot;shape sugar&quot; is all over CQL and greatly increases maintainability while eliminating common errors. The most common operation is simply to expland a &quot;shape&quot; into a list of arguments or columns (maybe with or without type names). SQLite doesn't know any of this shape magic so by the time SQLite sees the code it has to look &quot;normal&quot; -- the shapes are all resolved. "},{"title":"Join Types​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#join-types","content":"The last of the type building data structure is the join type. Recall that we have this shape: // for the data type of (parts of) the FROM clause // sometimes I refer to as a &quot;joinscope&quot; typedef struct sem_join { uint32_t count; // count of table/views in the join CSTR *names; // names of the table/view struct sem_struct **tables; // struct type of each table/view } sem_join;  This is an array of named structure types, which is exactly what you get when you do something like this: select * from T1 INNER JOIN T2;  The result has all of the columns of T1 and all of the columns of T2. They can be referred to with scoped names like T1.x which means &quot;find the sptr corresponding to the name T1 then within that structure find the column named x&quot;. In general, when we join, we take a jptr on the left and concatenate it with a jptr on the right. For all this to work we have to start somewhere, usually single tables. As we saw when we make a table we use sem_join_from_sem_struct to make its initial jptr. Let's have a look at that now: // Create a base join type from a single struct. static sem_join *sem_join_from_sem_struct(sem_struct *sptr) { sem_join *jptr = new_sem_join(1); jptr-&gt;names[0] = sptr-&gt;struct_name; jptr-&gt;tables[0] = new_sem_struct_strip_table_flags(sptr); return jptr; }  It doesn't get much simpler than the above, here are the steps briefly: new_sem_join : gives us an empty sem_join with room for 1 tablewe use the struct name for the name and the table's sptr for the shapenew_sem_struct_strip_table_flags : copies the table's sptr keeping only the essential flags SEM_TYPE_HIDDEN_COLSEM_FLAG_NOTNULLSEM_FLAG_SENSITIVE The other flags (e.g. SEM_TYPE_PK) have no value in doing type checking and were only needed to help validate the table itself. Those extra flags would be harmless but they would also contaminate all of the debug output, so they are stripped. As a result the type of columns as they appear in say SELECT statements is simpler than how they appear in a CREATE TABLE statement. When we need to create a new join type we simply (*) make a new sem_join that is the concatenation of the left and right sides of the join operation. some join types change the nullability of columns like LEFT JOIN, so we have to handle that toothe names of the tables in the new concatenated joinscope have to be unambiguous so there is also error checking to dobut basically it's just a concatenation Importantly, we call the thing a &quot;joinscope&quot; because it creates a namespace. When we are evaluating names inside of the FROM clause or even later in, say, a WHERE clause, the joinscope that we have created so far controls the table.column combinations that you can use in expressions. This changes again when there is a subquery, so the joinscopes can be pushed and popped as needed. By way of example, you'll see these two patterns in the code:  PUSH_JOIN(from_scope, select_from_etc-&gt;sem-&gt;jptr); error = sem_select_orderby(select_orderby); POP_JOIN();  PUSH_JOIN : use the jptr from the FROM clause to put things back in scope for the ORDER BY clause  PUSH_JOIN_BLOCK(); sem_numeric_expr(ast-&gt;left, ast, &quot;LIMIT&quot;, SEM_EXPR_CONTEXT_LIMIT); POP_JOIN();  PUSH_JOIN_BLOCK : causes the name search to stop -- nothing deeper in the stack is searchedin this case we do not allow LIMIT expressions to see any joinscopes, as they may not use any columns even if the LIMIT clause is appearing in a subquery it can't refer to columns in the parent query "},{"title":"Schema Regions​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#schema-regions","content":"We touched briefly on schema regions earlier in this section. The purpose and language for regions is described more fully in Chapter 10 of the Guide. In this section we'll deal with how they are implemented and what you should expect to see in the code. When a region declaration is found this method is used: // A schema region is an partitioning of the schema such that it // only uses objects in the same partition or one of its declared // dependencies. One schema region may be upgraded independently // from any others (assuming they happen such that dependents are done first). // Here we validate: // * the region name is unique // * the dependencies (if any) are unique and exist static void sem_declare_schema_region_stmt(ast_node *ast) { ... }  The general rules are described in the comment, but effectively it accumulates the list of the declared region's dependencies. Sometimes these are called the antecedent regions. Since a region can only depend on regions that have already been declared, it's not possible to make any cycles. Regions are declared before you put anything into them. Pieces of schema or procedures (or anything really) can go into a region by putting that code inside a begin/end pair for the named region. Like so: @begin_schema_region your_region; -- your stuff @end_schema_region;  Now whatever happens to be in &quot;your stuff&quot; is: limited to seeing only the things that your_region is allowed to see, andcontributes its contents to your_region thereby limiting how others will be able to use &quot;your stuff&quot; To see how this happens, let's have a look at sem_begin_schema_region_stmt. // Entering a schema region makes all the objects that follow part of that // region. It also means that all the contained objects must refer to // only pieces of schema that are in the same region or a dependent region. // Here we validate that region we are entering is in fact a valid region // and that there isn't already a schema region. static void sem_begin_schema_region_stmt(ast_node * ast) { Contract(is_ast_begin_schema_region_stmt(ast)); EXTRACT_STRING(name, ast-&gt;left); // @BEGIN_SCHEMA_REGION name if (!verify_schema_region_out_of_proc(ast)) { record_error(ast); return; } if (current_region) { report_error(ast, &quot;CQL0246: schema regions do not nest; end the current region before starting a new one&quot;, NULL); record_error(ast); return; } ast_node *region = find_region(name); if (!region) { report_error(ast-&gt;left, &quot;CQL0244: unknown schema region&quot;, name); record_error(ast); return; } // Get the canonical name of the region (case adjusted) Contract(is_region(region)); EXTRACT_STRING(region_name, region-&gt;left); // we already know we are not in a region Invariant(!current_region_image); current_region_image = symtab_new(); sem_accumulate_public_region_image(current_region_image, region_name); // this is the one and only text pointer value for this region current_region = region_name; record_ok(ast); }  We see these basic steps: EXTRACT : gets the region nameverify_schema_region_out_of_proc : makes sure we are out of any procedure (we have to be at the top level) errors if in a procedure current_region : is tested to make sure we are not already in a region (no nesting) errors if already in a region find_region : is used to find the region AST by name errors if the region name isn't valid EXTRACT : is used again to get the canonical name of the region you could write @begin_schema_region YoUr_ReGION; but we want the canonical name your_region, as it was declared symtab_new : creates a new symbol table current_region_imagesem_accumulate_public_region_image : populates current_region_image by recursively walking this region adding the names of all the regions we find along the way note the regions form a DAG so we might find the same name twice; we can stop if we find a region that is already in the image symbol table current_region : set it to the now new current region Now we're all set up. We can use current_region to set the region in the sem_node of anything we encounterWe can use current_region_image to quickly see if we are allowed to use any given region if it's in the symbol table we can use it Recall that at the end of sem_create_table_stmt we do this:  ast-&gt;sem = new_sem(SEM_TYPE_STRUCT); ast-&gt;sem-&gt;sptr = sptr; ast-&gt;sem-&gt;jptr = sem_join_from_sem_struct(sptr); ast-&gt;sem-&gt;region = current_region;  That should make a lot more sense now. When doing the symmetric check in sem_validate_object_ast_in_current_region we see this pattern: // Validate whether or not an object is usable with a schema region. The object // can only be a table, view, trigger or index. static bool_t sem_validate_object_ast_in_current_region( CSTR name, ast_node *table_ast, ast_node *err_target, CSTR msg) { // We're in a non-region therefore no validation needed because non-region stmt // can reference schema in any region. if (!current_region) { return true; } if (table_ast-&gt;sem &amp;&amp; table_ast-&gt;sem-&gt;region) { // if we have a current region then the image is always computed! Invariant(current_region_image); if (!symtab_find(current_region_image, table_ast-&gt;sem-&gt;region)) { // The target region is not accessible from this region ... return false; } } else { // while in schema region '%s', accessing an object that isn't in a region is invalid ... return false; } return true; }  I've elided some of the code here, but only the part that generates error messages. The essential logic is: if we are not in a region we can access anythingif we're in a region then... the thing we're trying to access must also be in a region, andthat region must be in current_region_imageotherwise, we can't access it This is enough to do all the region validation we need. "},{"title":"Results of Semantic Analysis​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#results-of-semantic-analysis","content":"Semantic Analysis leaves a lot of global state ready for the remaining stages to harvest. If the state is defined in sem.h then it's ok to harvest. Here we'll highlight some of the most important things you can use in later passes. These are heavily used in the code generators. cql_data_decl( struct list_item *all_tables_list ); cql_data_decl( struct list_item *all_functions_list ); cql_data_decl( struct list_item *all_views_list ); cql_data_decl( struct list_item *all_indices_list ); cql_data_decl( struct list_item *all_triggers_list ); cql_data_decl( struct list_item *all_regions_list ); cql_data_decl( struct list_item *all_ad_hoc_list ); cql_data_decl( struct list_item *all_select_functions_list ); cql_data_decl( struct list_item *all_enums_list );  These linked lists are authoritiative; they let you easily enumerate all the objects of the specified type. For instance, if you wanted to do some validation of all indices, you could simply walk all_indices_list. cql_noexport ast_node *find_proc(CSTR name); cql_noexport ast_node *find_region(CSTR name); cql_noexport ast_node *find_func(CSTR name); cql_noexport ast_node *find_table_or_view_even_deleted(CSTR name); cql_noexport ast_node *find_enum(CSTR name);  These functions give you access to the core name tables (which are still valid!) so that you can look up procedures, functions, tables, etc. by name. Finally, information about all the schema annotations is invaluable for building schema upgraders. These two buffers hold dense arrays of annotation records as shown below. cql_data_decl( bytebuf *schema_annotations ); cql_data_decl( bytebuf *recreate_annotations ); typedef struct recreate_annotation { CSTR target_name; // the name of the target CSTR group_name; // group name or &quot;&quot; if no group (not null, safe to sort) ast_node *target_ast; // top level target (table, view, or index) ast_node *annotation_ast; // the actual annotation int32_t ordinal; // when sorting we want to use the original order (reversed actually) within a group } recreate_annotation; typedef struct schema_annotation { int32_t version; // the version number (always &gt; 0) ast_node *target_ast; // top level target (table, view, or index) CSTR target_name; // the name of the target uint32_t annotation_type; // one of the codes below for the type of annotation ast_node *annotation_ast; // the actual annotation int32_t column_ordinal; // -1 if not a column ast_node *column_ast; // a particular column if column annotation } schema_annotation; // Note: schema annotations are processed in the indicated order: the numbers matter! #define SCHEMA_ANNOTATION_INVALID 0 #define SCHEMA_ANNOTATION_FIRST 1 #define SCHEMA_ANNOTATION_UNSUB 1 #define SCHEMA_ANNOTATION_CREATE_TABLE 2 #define SCHEMA_ANNOTATION_CREATE_COLUMN 3 #define SCHEMA_ANNOTATION_DELETE_TRIGGER 4 #define SCHEMA_ANNOTATION_DELETE_VIEW 5 #define SCHEMA_ANNOTATION_DELETE_INDEX 6 #define SCHEMA_ANNOTATION_DELETE_COLUMN 7 #define SCHEMA_ANNOTATION_DELETE_TABLE 8 #define SCHEMA_ANNOTATION_AD_HOC 9 #define SCHEMA_ANNOTATION_RESUB 10 #define SCHEMA_ANNOTATION_LAST 10  And of course, each &quot;back end&quot; is provided with the root of the AST so that it can also search and/or walk the AST in its own manner. We will see examples of this in later sections. "},{"title":"Recap​","type":1,"pageTitle":"Part 2: Semantic Analysis","url":"/cql-guide/int02#recap","content":"At present, there are nearly 20000 lines in sem.c and it would no doubt take more than 20000 lines of text to explain what they all do, and that would be more imprecise than the source code, and probably less readable. sem.c includes over 4000 lines of comments, and probably should have more. While there is a lot of code there, it's very readable and I encourage you to do just that -- read it -- to get your answers. The point of this part of the Internals Guide isn't to fully explain all 400+ error checks in about as many semantic error checking functions, it is to showcase the key concepts shared by all of them. Things like: errors are reported largely in the AST and percolate upexpressions and statements have general purpose dispatch logic for continuing a statement walkEXTRACT macros are used to keep the tree walk on track and correct in the face of changesregions are used for visibilityversioning contributes to visibilitynullability and sensitivity are tracked throughout using type bitstype &quot;kind&quot; is managed by a simple string in the sem_node payloadthe three main payloads are sem_node for basic info, andsem_struct or sem_join for the non-unitary types This isn't everything but it should leave you well armed to begin your own exploration of sem.c. Note: details on unsub/resub are forthcoming. This code is under development. "},{"title":"guide","type":0,"sectionRef":"#","url":"/cql-guide/generated/guide","content":"","keywords":""},{"title":"Chapter 1: Introduction​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-1-introduction","content":"CQL was designed as a precompiled addition to the SQLite runtime system. SQLite lacks stored procedures, but has a rich C runtime interface that allows you to create any kind of control flow mixed with any SQL operations that you might need. However, SQLite's programming interface is both verbose and error-prone in that small changes in SQL statements can require significant swizzling of the C code that calls them. Additionally, many of the SQLite runtime functions have error codes which must be strictly checked to ensure correct behavior. In practice, it's easy to get some or all of this wrong. CQL simplifies this situation by providing a high level SQL language not unlike the stored procedure forms that are available in client/server SQL solutions and lowering that language to &quot;The C you could have written to do that job using the normal SQLite interfaces.&quot; As a result, the C generated is generally very approachable but now the source language does not suffer from brittleness due to query or table changes and CQL always generates correct column indices, nullability checks, error checks, and the other miscellany needed to use SQLite correctly. CQL is also strongly typed, whereas SQLite is very forgiving with regard to what operations are allowed on which data. Strict type checking is much more reasonable given CQL's compiled programming model. note CQL was created to help solve problems in the building of Meta Platforms's Messenger application, but this content is free from references to Messenger. The CQL code generation here is done in the simplest mode with the fewest runtime dependencies allowed for illustration. "},{"title":"Getting Started​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#getting-started","content":"Before starting this tutorial, make sure you have built the cql executable first in Building CG/SQL. The &quot;Hello World&quot; program rendered in CQL looks like this: hello.sql -- needed to allow vararg calls to C functions declare procedure printf no check; create proc hello() begin call printf(&quot;Hello, world\\n&quot;); end;  This very nearly works exactly as written but we'll need a little bit of glue to wire it all up. First, assuming you have built cql, you should have the power to do this: $ cql --in hello.sql --cg hello.h hello.c  This will produce the C output files hello.c and hello.h which can be readily compiled. However, hello.c will not have a main -- rather it will have a function like this: hello.c ... void hello(void); ...  The declaration of this function can be found in hello.h. Note: hello.h tries to include cqlrt.h. To avoid configuring include paths for the compiler, you might keep cqlrt.h in the same directory as the examples and avoid that complication. Otherwise you must make arrangements for the compiler to be able to find cqlrt.h either by adding it to an INCLUDE path or by adding some -I options to help the compiler find the source. That hello function is not quite adequate to get a running program, which brings us to the next step in getting things running. Typically you have some kind of client program that will execute the procedures you create in CQL. Let's create a simple one in a file we'll creatively name main.c. A very simple CQL main might look like this: main.c #include &lt;stdlib.h&gt; #include &quot;hello.h&quot; int main(int argc, char **argv) { hello(); return 0; }  Now we should be able to do the following: $ cc -o hello main.c hello.c $ ./hello Hello, world  Congratulations, you've printed &quot;Hello, world&quot; with CG/SQL! "},{"title":"Why did this work?​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#why-did-this-work","content":"A number of things are going on even in this simple program that are worth discussing: the procedure hello had no arguments, and did not use the database therefore its type signature when compiled will be simply void hello(void); so we know how to call ityou can see the declaration for yourself by examining the hello.c or hello.h since nobody used a database we didn't need to initialize onesince there are no actual uses of SQLite we didn't need to provide that libraryfor the same reason we didn't need to include a reference to the CQL runtimethe function printf was declared &quot;no check&quot;, so calling it creates a regular C call using whatever arguments are provided, in this case a stringthe printf function is declared in stdio.h which is pulled in by cqlrt.h, which appears in hello.c, so it will be available to call in the generated C codeCQL allows string literals with double quotes, and those literals may have most C escape sequences in them, so the &quot;\\n&quot; bit works Normal SQL string literals (also supported) use single quotes and do not allow, or need escape characters other than '' to mean one single quote All of these facts put together mean that the normal, simple linkage rules result in an executable that prints the string &quot;Hello, world&quot; and then a newline. "},{"title":"Variables and Arithmetic​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#variables-and-arithmetic","content":"Borrowing once again from examples in &quot;The C Programming Language&quot;, it's possible to do significant control flow in CQL without reference to databases. The following program illustrates a variety of concepts: -- needed to allow vararg calls to C functions declare procedure printf no check; -- print a conversion table for temperatures from 0 to 300 create proc conversions() begin declare fahr, celsius integer not null; declare lower, upper, step integer not null; set lower := 0; /* lower limit of range */ set upper := 300; /* upper limit of range */ set step := 20; /* step size */ set fahr := lower; while fahr &lt;= upper begin set celsius := 5 * (fahr - 32) / 9; call printf(&quot;%d\\t%d\\n&quot;, fahr, celsius); set fahr := fahr + step; end; end;  You may notice that both the SQL style -- line prefix comments and the C style /* */ forms [note you haven't used the second for of comment style yet]are acceptable comment forms. Indeed, it's actually quite normal to pass CQL source through the C pre-processor before giving it to the CQL compiler, thereby gaining #define and #include as well as other pre-processing options like token pasting in addition to the aforementioned comment forms. More on this later. Like C, in CQL all variables must be declared before they are used. They remain in scope until the end of the procedure in which they are declared, or they are global scoped if they are declared outside of any procedure. The declarations announce the names and types of the local variables. Importantly, variables stay in scope for the whole procedure even if they are declared within a nested begin and end block. The most basic types are the scalar or &quot;unitary&quot; types (as they are referred to in the compiler) type\taliases\tnotesinteger\tint\ta 32 bit integer long\tlong integer\ta 64 bit integer bool\tboolean\tan 8 bit integer, normalized to 0/1 real\tn/a\ta C double text\tn/a\tan immutable string reference blob\tn/a\tan immutable blob reference object\tn/a\tan object reference Note: SQLite makes no distinction between integer storage and long integer storage, but the declarations tell CQL whether it should use the SQLite methods for binding and reading 64-bit or 32-bit quantities when using the variable or column so declared. There will be more notes on these types later, but importantly, all keywords and names in CQL are case insensitive just like in the underlying SQL language. Additionally all of the above may be combined with not null to indicate that a null value may not be stored in that variable (as in the example). When generating the C code, the case used in the declaration becomes the canonical case of the variable and all other cases are converted to that in the emitted code. As a result the C remains case sensitively correct. The size of the reference types is machine dependent, whatever the local pointer size is. The non-reference types use machine independent declarations like int32_t to get exactly the desired sizes in a portable fashion. All variables of a reference type are set to NULL when they are declared, including those that are declared NOT NULL. For this reason, all nonnull reference variables must be initialized (i.e., assigned a value) before anything is allowed to read from them. This is not the case for nonnull variables of a non-reference type, however: They are automatically assigned an initial value of 0, and thus may be read from at any point. The programs execution begins with three assignments: set lower := 0; set upper := 300; set step := 20;  This initializes the variables just like in the isomorphic C code. Statements are seperated by semicolons, just like in C. The table is then printed using a while loop while fahr &lt;= upper begin ... end;  This has the usual meaning, with the statements in the begin/end block being executed repeatedly until the condition becomes false. The body of a begin/end block such as the one in the while statement can contain one or more statements. The typical computation of Celsius temperature ensues with this code: set celsius := 5 * (fahr - 32) / 9; call printf(&quot;%d\\t%d\\n&quot;, fahr, celsius); set fahr := fahr + step;  This computes the celsuis and then prints it out, moving on to the next entry in the table. Importantly, the CQL compiler uses the normal SQLite order of operations, which is NOT the C order of operations. As a result, the compiler may need to add parentheses in the C output to get the correct order; or it may remove some parentheses because they are not needed in the C order even though they were in the SQL order. The printf call operates as before, with the fahr and celsius variables being passed on to the C runtime library for formatting, unchanged. NOTE: when calling unknown foreign functions like printf string literals are simply passed right through unchanged as C string literals. No CQL string object is created. "},{"title":"Basic Conversion Rules​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#basic-conversion-rules","content":"As a rule, CQL does not perform its own conversions, leaving that instead to the C compiler. An exception to this is that boolean expressions are normalized to a 0 or 1 result before they are stored. However, even with no explicit conversions, there are compatibility checks to ensure that letting the C compiler do the conversions will result in something sensible. The following list summarizes the essential facts/rules as they might be applied when performing a + operation. the numeric types are bool, int, long, realnon-numeric types cannot be combined with numerics, e.g. 1 + 'x' always yields an errorany numeric type combined with itself yields the same typebool combined with int yields intbool or int combined with long yields longbool, int, or long combined with real yields real "},{"title":"Preprocessing Features​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#preprocessing-features","content":"CQL does not include its own pre-processor but it is designed to consume the output of the C pre-processor. To do this, you can either write the output of the pre-processor to a temporary file and read it into CQL as usual or you can set up a pipeline something like this: $ cc -x c -E your_program.sql | cql --cg your_program.h your_program.c  The above causes the C compiler to invoke only the pre-processor -E and to treat the input as though it were C code -x c even though it is in a .sql file. Later examples will assume that you have configured CQL to be used with the C pre-processor as above. "},{"title":"Chapter 2: Using Data​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-2-using-data","content":"The point of using CQL is to facilitate access to a SQLite database so we'll switch gears to a slightly more complicated setup. We'll still keep things fairly simple but let's start to use some database features. Note: it is not the intent of this tutorial to also be a primer for the SQLite programming language which is so ably documented on https://sqlite.org/. Please refer to that site for details on the meaning of the SQL statements used here if you are new to SQL. "},{"title":"A Sample Program​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#a-sample-program","content":"Suppose we have the following program: hello.sql -- needed to allow vararg calls to C functions declare procedure printf no check; create table my_data(t text not null); create proc hello() begin insert into my_data(t) values(&quot;Hello, world\\n&quot;); declare t text not null; set t := (select * from my_data); call printf('%s', t); end;  That looks like an interesting little baby program and it appears as though it would once again print that most famous of salutations, &quot;Hello, world&quot;. Well, it doesn't. At least, not yet. Let's walk through the various things that are going to go wrong as this will teach us everything we need to know about activating CQL from some environment of your choice. "},{"title":"Providing a Suitable Database​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#providing-a-suitable-database","content":"CQL is just a compiler, it doesn't know how the code it creates will be provisioned any more than say clang does. It creates functions with predictable signatures so that they can be called from C just as easily as the SQLite API itself, and using the same currency. Our new version of hello now requires a database handle because it performs database operations. Also there are now opportunities for the database operations to fail, and so hello now provides a return code. A new minimal main program might look something like this: main.c #include &lt;stdlib.h&gt; #include &lt;sqlite3.h&gt; #include &quot;hello.h&quot; int main(int argc, char **argv) { sqlite3 *db; int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db); if (rc != SQLITE_OK) { exit(1); /* not exactly world class error handling but that isn't the point */ } rc = hello(db); if (rc != SQLITE_OK) { exit(2); } sqlite3_close(db); }  If we re-run CQL and look in the hello.h output file we'll see that the declaration of the hello function is now: hello.h ... extern CQL_WARN_UNUSED cql_code hello(sqlite3 *_Nonnull _db_); ...  This indicates that the database is used and a SQLite return code is provided. We're nearly there. If you attempt to build the program as before there will be several link-time errors due to missing functions. Typically these are resolved by providing the SQLite library to the command line and also adding the CQL runtime. The new command line looks something like this: $ cc -o hello main.c hello.c cqlrt.c -lsqlite3 $ ./hello Hello, world  The cql runtime can be anywhere you want it to be, and of course the usual C separate compilation methods can be applied. More on that later. But actually, that program doesn't quite work yet. If you run it, you'll get an error result code, not the message &quot;Hello, world&quot;. Let's talk about the final missing bit. "},{"title":"Declaring Schema​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#declaring-schema","content":"In CQL a loose piece of Data Definition Language (henceforth DDL) does not actually create or drop anything. In most CQL programs the normal situation is that &quot;something&quot; has already created the database and put some data in it. You need to tell the CQL compiler about the schema so that it knows what the tables are and what to expect to find in those tables. This is because typically you're reconnecting to some sort of existing database. So, in CQL, loose DDL simply declares schema, it does not create it. To create schema you have to put the DDL into a procedure you can run. If you do that, then the DDL still serves a declaration, but also the schema will be created when the procedure is executed. We need to change our program a tiny bit. hello.sql -- needed to allow vararg calls to C functions declare procedure printf no check; create proc hello() begin create table my_data(t text not null); insert into my_data(t) values(&quot;Hello, world\\n&quot;); declare t text not null; set t := (select * from my_data); call printf('%s', t); drop table my_data; end;  If we rebuild the program, it will now behave as expected. "},{"title":"Explaining The New Hello World​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#explaining-the-new-hello-world","content":"Let's go over every important line of the new program, starting from main. int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db);  This statement gives us an empty, private, in-memory only database to work with. This is the simplest case and it's still very useful. The sqlite_open and sqlite_open_v2 functions can be used to create a variety of databases per the SQLite documentation. We'll need such a database to use our procedure, and we use it in the call here: rc = hello(db);  This provides a valid db handle to our procedure. Note that the procedure doesn't know what database it is supposed to operate on, it expects to be handed a suitable database on a silver platter. In fact any given proc could be used with various databases at various times. Just like SQLite, CQL does not enforce any particular database setup; it does what you tell it to. When hello runs we begin with create table my_data(t text not null);  This will create the my_data table with a single column t, of type text not null. That will work because we know we're going to call this with a fresh/empty database. More typically you might do create table if not exists ... or otherwise have a general attach/create phase or something to that effect. We'll dispense with that here. Next we'll run the insert statement: insert into my_data(t) values(&quot;Hello, world\\n&quot;);  This will add a single row to the table. Note that we have again used double quotes, meaning that this is a C string literal. This is highly convenient given the escape sequences. Normally SQLite text has the newlines directly embedded in it; that practice isn't very compiler friendly, hence the alternative. Next we declare a local variable to hold our data: declare t text not null;  Then, we can read back our data: set t := (select * from my_data);  This form of database reading has very limited usability but it does work for this case and it is illustrative. The presence of (select ...) indicates to the CQL compiler that the parenthesized expression should be given to SQLite for evaluation according to the SQLite rules. The expression is statically checked at compile time to ensure that it has exactly one result column. In this case the * is just column t, and actually it would have been clearer to use t directly here but then there wouldn't be a reason to talk about * and multiple columns. At run time, the select query must return exactly one row or an error code will be returned. It's not uncommon to see (select ... limit 1) to force the issue. But that still leaves the possibility of zero rows, which would be an error. We'll talk about more flexible ways to read from the database later. You can declare a variable and assign it in one step with the LET keyword, e.g. let t := (select * from my_data); The code would normally be written in this way but for discussion purposes, these examples continue to avoid LET. At this point it seems wise to bring up the unusual expression evaluation properties of CQL. CQL is by necessity a two-headed beast. On the one side there is a rich expression evaluation language for working with local variables. [What about the other side?] Those expressions are compiled into C logic that emulates the behavior of SQLite on the data. It provides complex expression constructs such as IN and CASE but it is ultimately evaluated by C execution. Alternately, anything that is inside of a piece of SQL is necessarily evaluated by SQLite itself. To make this clearer let's change the example a little bit before we move on. set t := (select &quot;__&quot;||t||' '||1.234 from my_data);  This is a somewhat silly example but it illustrates some important things: even though SQLite doesn't support double quotes, that's no problem because CQL will convert the expression into single quotes with the correct escape values as a matter of course during compilationthe || concatenation operator is evaluated by SQLiteyou can mix and match both kinds of string literals, they will all be the single quote variety by the time SQLite sees themthe || operator has lots of complex formatting conversions (such as converting real values to strings)in fact the conversions are so subtle as to be impossible to emulate in loose C code with any economy, so, like a few other operators, || is only supported in the SQLite context Returning now to our code as written, we see something very familiar: call printf('%s', t);  Note that we've used the single quote syntax here for no good reason other than illustration. There are no escape sequences here so either form would do the job. Importantly, the string literal will not create a string object as before but the text variable t is of course a string reference. Before it can be used in a call to an un-declared function it must be converted into a temporary C string. This might require allocation in general, that allocation is automatically managed. Also, note that CQL assumes that calls to &quot;no check&quot; functions should be emitted as written. In this way you can useprintf even though CQL knows nothing about it. Lastly we have: drop table my_data;  This is not strictly necessary because the database is in memory anyway and the program is about to exit but there it is for illustration. Now the Data Manipulation Language (i.e. insert and select here; and henceforth DML) and the DDL might fail for various reasons. If that happens the proc will goto a cleanup handler and return the failed return code instead of running the rest of the code. Any temporary memory allocations will be freed and any pending SQLite statements will be finalized. More on that later when we discuss error handling. With that we have a much more complicated program that prints &quot;Hello, world&quot; "},{"title":"Introducing Cursors​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#introducing-cursors","content":"In order to read data with reasonable flexibility, we need a more powerful construction. Let's change our example again and start using some database features. declare procedure printf no check; create proc hello() begin create table my_data( pos integer not null primary key, txt text not null ); insert into my_data values(2, 'World'); insert into my_data values(0, 'Hello'); insert into my_data values(1, 'There'); declare C cursor for select * from my_data order by pos; loop fetch C begin call printf(&quot;%d: %s\\n&quot;, C.pos, C.txt); end; close C; drop table my_data; end;  Reviewing the essential parts of the above. create table my_data( pos integer not null primary key, txt text not null );  The table now includes a position column to give us some ordering. That is the primary key. insert into my_data values(2, 'World');  The insert statements provide both columns, not in the printed order. The insert form where the columns are not specified indicates that all the columns will be present, in order; this is more economical to type. CQL will generate errors at compile time if there are any missing columns or if any of the values are not type compatible with the indicated column. The most important change is here: declare C cursor for select * from my_data order by pos;  We've created a non-scalar variable C, a cursor over the indicated result set. The results will be ordered by pos. loop fetch C begin ... end;  This loop will run until there are no results left (it might not run at all if there are zero rows, that is not an error). The FETCH construct allows you to specify target variables, but if you do not do so, then a synthetic structure is automatically created to capture the projection of the select. In this case the columns are pos and txt. The automatically created storage exactly matches the type of the columns in the select list which could itself be tricky to calculate if the select is complex. In this case the select is quite simple and the columns of the result directly match the schema for my_data. An integer and a string reference. Both not null. call printf(&quot;%d: %s\\n&quot;, C.pos, C.txt);  The storage for the cursor is given the same names as the columns of the projection of the select, in this case the columns were not renamed so pos and txt are the fields in the cursor. Double quotes were used in the format string to get the newline in there easily. close C;  The cursor is automatically released at the end of the procedure but in this case we'd like to release it before thedrop table happens so there is an explicit close. This is frequently elided in favor of the automatic cleanup. There is an open cursor statement as well but it doesn't do anything. It's there because many systems have that construct and it does balance the close. If you compile and run this program, you'll get this output: $ cc -x c -E hello.sql | cql --cg hello.h hello.c $ cc -o hello main.c hello.c cqlrt.c -lsqlite3 $ ./hello 0: Hello 1: There 2: World  So the data was inserted and then sorted. "},{"title":"Going Crazy​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#going-crazy","content":"We've only scratched the surface of what SQLite can do and most DML constructs are supported by CQL. This includes common table expressions, and even recursive versions of the same. But remember, when it comes to DML, the CQL compiler only has to validate the types and figure out what the result shape will be -- SQLite always does all the heavy lifting of evaluation. All of this means with remarkably little additional code, the example below from the SQLite documentation can be turned into a CQL stored proc using the constructs we have defined above. -- needed to allow vararg calls to C functions declare procedure printf no check; create proc mandelbrot() begin -- this is basically one giant select statement declare C cursor for with recursive -- x from -2.0 to +1.2 xaxis(x) as (select -2.0 union all select x + 0.05 from xaxis where x &lt; 1.2), -- y from -1.0 to +1.0 yaxis(y) as (select -1.0 union all select y + 0.1 from yaxis where y &lt; 1.0), m(iter, cx, cy, x, y) as ( -- initial seed iteration count 0, at each of the points in the above grid select 0 iter, x cx, y cy, 0.0 x, 0.0 y from xaxis, yaxis union all -- the next point is always iter +1, same (x,y) and the next iteration of z^2 + c select iter+1 iter, cx, cy, x*x-y*y + cx x, 2.0*x*y + cy y from m -- stop condition, the point has escaped OR iteration count &gt; 28 where (m.x*m.x + m.y*m.y) &lt; 4.0 and m.iter &lt; 28 ), m2(iter, cx, cy) as ( -- find the last iteration for any given point to get that count select max(iter), cx, cy from m group by cx, cy ), a(t) as ( -- convert the iteration count to a printable character, grouping by line select group_concat(substr(&quot; .+*#&quot;, 1 + min(iter/7,4), 1), '') from m2 group by cy ) -- group all the lines together select rtrim(t) line from a; -- slurp out the data loop fetch C begin call printf(&quot;%s\\n&quot;, C.line); end; end;  This code uses all kinds of SQLite features to produce this text: $ ....# ..#*.. ..+####+. .......+####.... + ..##+*##########+.++++ .+.##################+. .............+###################+.+ ..++..#.....*#####################+. ...+#######++#######################. ....+*################################. #############################################... ....+*################################. ...+#######++#######################. ..++..#.....*#####################+. .............+###################+.+ .+.##################+. ..##+*##########+.++++ .......+####.... + ..+####+. ..#*.. ....# +.  Which probably doesn't come up very often but it does illustrate several things: WITH RECURSIVE actually provides a full lambda calculus so arbitrary computation is possibleYou can use WITH RECURSIVE to create table expressions that are sequences of numbers easily, with no reference to any real data Note: A working version of this code can be found in the sources/demo directory of CG/SQL project.Additional demo code is available in Appendix 10. "},{"title":"Chapter 3: Expressions, Literals, Nullability, Sensitivity​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-3-expressions-literals-nullability-sensitivity","content":"Until this point we've only discussed simple kinds of expressions as well as variables and table columns marked with NOT NULL. These are indeed the easiest types for CQL to work with as they tend to correspond most directly to the types known to C. However, SQL provides for many more types of expressions as well as nullable types and these require handling in any language that purports to be like SQL. "},{"title":"Expression Examples​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#expression-examples","content":"The usual arithmetic operators apply in CQL: Example expressions (these are all true) (1 + 2) * 3 == 9 1 + 2 * 3 == 7 6 / 3 == 2 7 - 5 == 2 6 % 5 == 1 5 / 2.5 == 2 7 &amp; 3 == 2 | 1 1 &lt;&lt; 2 == 4  However, before going any further it's important to note that CQL is inherently a two-headed beast. Expressions are either evaluated by transpiling to C (like the predicate of an IF statement, or a variable assignment) or by sending them to SQLIte for evaluation (like expressions inside a SELECT statement or the WHERE part of a DELETE). CQL evaluation rules are designed to be as similar as possible but some variance is inevitable because evaluation is done in two fundamentally different ways. "},{"title":"Operator Precedence​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#operator-precedence","content":"The operator precedence rules in CQL are as follows; the top-most rule binds the most loosely and the bottom-most rule binds the most tightly: ASSIGNMENT: := LOGICAL_OR: OR LOGICAL_AND: AND LOGICAL_NOT: NOT EQUALITY: = == != &lt;&gt; IS [NOT], [NOT] IN, [NOT] LIKE, [NOT] MATCH, [NOT] GLOB, [NOT] BETWEEN INEQUALITY: &lt; &lt;= &gt; &gt;= BINARY: &lt;&lt; &gt;&gt; &amp; | ADDITION: + - MULTIPLICATION: * / % CONCAT: || COLLATE: COLLATE UNARY: ~ -  The above rules are not the same as C's operator precedence rules! Instead, CQL follows SQLite's rules. Parentheses are emitted in the C output as needed to force that order. NOTE: CQL emits minimal parentheses in all outputs. Different parentheses are often needed for SQL output as opposed to C output. "},{"title":"Order of Evaluation​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#order-of-evaluation","content":"In contrast to C, CQL guarantees a left-to-right order of evaluation for arguments. This applies both to arguments provided to the operators mentioned in the previous section as well as arguments provided to procedures. "},{"title":"Variables, Columns, Basic Types and Nullability​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#variables-columns-basic-types-and-nullability","content":"CQL needs type information for both variables in the code and columns in the database. Like SQL, CQL allows variables to hold a NULL value and just as in SQL the absence of NOT NULL implies that NULL is a legal value. Consider these examples: -- real code should use better names than this :) create table all_the_nullables( i1 integer, b1 bool, l1 long, r1 real, t1 text, bl1 blob ); declare i2 integer; declare b2 bool; declare l2 long; declare r2 real; declare t2 text; declare bl2 blob;  ALL of i1, i2, b1, b2, l1, l2, r1, r2, t1, t2, and bl1, bl2 are nullable. In some sense variables and columns declared nullable (by virtue of the missing NOT NULL) are the root sources of nullability in the SQL language. That and the NULL literal. Though there are other sources as we will see. NOT NULL could be added to any of these, e.g. -- real code should use better names than this :) declare i_nn integer not null;  In the context of computing the types of expressions, CQL is statically typed and so it must make a decision about the type of any expression based on the type information at hand at compile time. As a result it handles the static type of an expression conservatively. If the result might be null then the expression is of a nullable type and the compiled code will include an affordance for the possibility of a null value at runtime. The generated code for nullable types is considerably less efficient and so it should be avoided if that is reasonably possible. LET Statement​ You can declare and initialize a variable in one step using the LET form, e.g. LET x := 1;  The named variable is declared to be the exact type of the expression on the right. More on expressions in the coming sections. The right side is often a constant in these cases but does not need to be. LET i := 1; -- integer not null LET l := 1L; -- long not null LET t := &quot;x&quot;; -- text not null LET b := x IS y; -- bool not null LET b := x = y; -- bool (maybe not null depending on x/y)  The pseudo function &quot;nullable&quot; removes not null from the type of its argument but otherwise does no computation. This can be useful to initialize nullable types. LET n_i := nullable(1); -- nullable integer variable initialized to 1 LET n_l := nullable(1L); -- nullable long variable initialized to 1  The pseudo function &quot;sensitive&quot; adds @sensitive to the type of its argument but otherwise does no computation. This also can be useful to initialize nullable types. LET s_i := sensitive(1); -- sensitive nullable integer variable initialized to 1 LET s_l := sensitive(1L); -- sensitive nullable long variable initialized to 1  The @RC special variable​ CQL also has the special built-in variable @RC which refers to the most recent error code returned by a SQLite operation, e.g. 0 == SQLITE_OK, 1 == SQLITE_ERROR. @RC is of type integer not null. Specifically: each catch block captures the error code when it is entered into its own local variablethis variable is created lazily, so it only exists if it is used the variable is called _rc_thrown_n where n is the catch block number in the procedure any reference to @RC refers to the above error variable of the innermost catch block the @RC reference is inif the @RC reference happens outside of any catch block its value is SQLITE_OK (i.e. zero). "},{"title":"Types of Literals​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#types-of-literals","content":"There are a number of literal objects that may be expressed in CQL. These are as follows: String Literals​ A double quoted string is a C style string literal the usual simple C escape sequences are supportedthe \\xNN form for embedded hex characters is supported, howeverthe \\0NNN octal form is not supported, andembedded nulls in string literals (\\0 or \\0x00) are not supported (you must use blobs in such cases) A single quoted string is a SQL style string literal No escape sequences are supported other than '' to indicate a single quote character (this is just like normal SQLite) A sequence of single or double quoted strings separated by whitespace such as &quot;xx&quot; 'yy' &quot;zz&quot; which are concatenated to make one literalThe sequence @FILE(&quot;some_string&quot;) is a special string literal the value of this literal is the path of the current compiland starting at the letters in some_string, orthe entire path of the current compiland if some_string does not occur in the paththe purpose of the @FILE construct is to provide a partial path to a file for diagnostics that is consistent even if the file is built in various different root paths on different build machines Blob Literals​ SQLite Blob literals are supported in SQL contexts (i.e. where they will be processed by SQLite), CQL produces an error if you attempt to use a blob literal in a loose expression Numeric Literals​ All numeric literals are considered to be positive; negative numbers are actually a positive literal combined with unary minus (the negation operator)Base 10 and hexadecimal literals are supportedLiterals with a decimal point are of type REAL and stored as the C type doubleLiterals that can fit in a signed integer without loss, and do not end in the letter L are integer literalsLarger literals, or those ending with the letter L are long integer literals.Literals that begin with 0x are interpreted as hex Examples:  1.3 -- real 2L -- long 123456789123 -- long 123 -- integer 0x10 -- hex integer 0x10L -- hex long integer  The NULL literal​ The use of NULL always gives a nullable result however this literal is special in that it has no storage class. NULL is neither numeric nor string itself but rather mutates into whatever it is first combined with. For instance NULL + 1 results in a nullable integer. Because NULL has no primitive type in some cases where type knowledge is required you might have to use the CAST() function to cast the NULL to a specific type such as CAST(NULL as TEXT). This construct guarantees type consistence in cases like SELECT from different sources combined with UNION ALL Note: constructs like CAST(NULL as TEXT) are always rewritten to just NULL before going to SQLite as the cast is uninteresting except for the type information which SQLite doesn't need/use anyway. Other Considerations​ There are no boolean literals other than the integers 0 and 1. The C pre-processor is often combined with CQL in which case the _FILE_ and _LINE_ directives may be used to create literals; they will be preprocessed into normal literals. The use of _FILE_ can give surprising results in the presence of build systems, hence the existence of @FILE(...). "},{"title":"Const and Enumerations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#const-and-enumerations","content":"It's possible to use named constants in CQL with nothing more than the C pre-processor features that have already appeared, however use of #define in such a way is not entirely satisfactory. For one thing, CQL will not know these constants exist in any way as they will be replaced before it ever sees them. This means CQL can't provide their values for you in the JSON output for instance. To help with this problem, CQL includes constants, note, this is not the same as enumerated types as we'll see later. You can now write something like this: declare enum business_type integer ( restaurant, laundromat, corner_store = 11+3 /* math added for demo purposes only */ );  After this enum is declared, this: select business_type.corner_store;  is the same as this: select 14;  And that is exactly what SQLite will see, the literal 14. You can also use the enum to define column types: CREATE TABLE businesses ( name TEXT, type business_type );  CQL will then enforce that you use the correct enum to access those columns. For example, this is valid: SELECT * FROM businesses WHERE type = business_type.laundromat;  While this does not type check: SELECT * FROM businesses WHERE type = business_corp_state.delaware;  Enumerations follow these rules: the enumeration can be any numeric type (bool, integer, long integer, real)the values of the enumeration start at 1 (i.e. if there is no = expression the first item will be 1, not 0)if you don't specify a value, the next value is the previous value plus oneif you do specify a value it can be any constant expression and it will be cast to the type of the enumeration (even if that is lossy)the enumeration can refer to previous values in itself with no qualification (big = 100.0, medium = big/2, small = medium/2)the enumeration can refer to previously defined enumerations as usual (code = business_type.restaurant)once the enumeration is defined you refer to its members in a fully qualified fashion enum_name.member_name elsewhere With these forms you get some additional useful output: the JSON includes the enumerations and their values in their own sectionyou can use the @emit_enums directive to put declarations like this into the .h file that corresponds to the current compiland enum business_type { business_type__restaurant = 1, business_type__laundromat = 2, business_type__corner_store = 14 };  Note that C does not allow for floating point enumerations, so in case of floating point values such as: declare enum floating real ( one = 1.0, two = 2.0, e = 2.71828, pi = 3.14159 );  you get: // enum floating (floating point values) #define floating__one 1.000000e+00 #define floating__two 2.000000e+00 #define floating__e 2.718280e+00 #define floating__pi 3.141590e+00  In order to get useful expressions in enumeration values, constant folding and general evaluation was added to the compiler; these expressions work on any numeric type and the literal null. The supported operations include: +, -, *, /, %, |, &amp;, &lt;&lt;, &gt;&gt;, ~, and, or, not, ==, &lt;=, &gt;=, !=, &lt;, &gt;, the cast operator and the case forms (including the iif function). These are enough to make a lot of very interesting expressions, all of which are evaluated at compile time. Constant folding was added to allow for rich enum expressions, but there is also the const() primitive in the language which can appear anywhere a literal could appear. This allows you do things like: create table something( x integer default const((1&lt;&lt;16)|0xf) /* again the math is just for illustration */ );  The const form is also very useful in macros: #define SOMETHING const(12+3)  This form ensures that the constant will be evaluated at compile time. The const pseudo-function can also nest so you can build these kinds of macros from other macros or you can build enum values this way. Anywhere you might need literals, you can use const. "},{"title":"Named Types​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#named-types","content":"A common source of errors in stored procedures is incorrect typing in arguments. For instance, a particular key for an entity might need to be LONG or even always LONG NOT NULL or LONG NOT NULL @SENSITIVE and the only way to do this in the past was maybe with some #define thing. Otherwise you have to diligently get the type right in all the places, and should it ever change, again you have to visit all the places. To help with this situation, and to make the code a little more self-describing we added named types to the language. This is a lot like typedef in the C language. They do not create different incompatible types but they do let you name things well. You can now write these sorts of forms: declare foo_id type long not null; create table foo( id foo_id primary key autoincrement, name text ); create proc inserter(name_ text, out id foo_id) begin insert into foo(id, name) values(NULL, name_); set id := last_insert_rowid(); end; declare function func_return_foo_id() foo_id; declare var foo_id;  Additionally any enumerated type can be used as a type name. e.g. declare enum thing integer ( thing1, thing2 ); declare thing_type type thing;  Enumerations always get &quot;not null&quot; in addition to their base type. Enumerations also have a unique &quot;kind&quot; associated, specifically the above enum has type integer&lt;thing&gt; not null. The rules for type kinds are described below. "},{"title":"Type Kinds​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#type-kinds","content":"Any CQL type can be tagged with a &quot;kind&quot; for instance real can become real&lt;meters&gt;, integer can become integer&lt;job_id&gt;. The idea here is that the additional tag, the &quot;kind&quot; can help prevent type mistakes in arguments, in columns and in procedure calls. For instance: create table things( size real&lt;meters&gt;, duration real&lt;seconds&gt; ); create proc do_something(size_ real&lt;meters&gt;, duration_ real&lt;seconds&gt;) begin insert into things(size, duration) values(size_, duration_); end;  In this situation you couldn't accidentally switch the columns in do_something even though both are real, and indeed SQLite will only see the type real for both. If you have your own variables typed real&lt;size&gt; and real&lt;duration&gt; you can't accidentally do:  call do_something(duration, size);  even though both are real. The type kind won't match. Importantly, an expression with no type kind is compatible with any type kind (or none). Hence all of the below are legal. declare generic real; set generic := size; -- no kind may accept &lt;meters&gt; set generic := duration; -- no kind may accept &lt;seconds&gt; set duration := generic; -- no kind may be stored in &lt;seconds&gt;  Only mixing types where both have a kind, and the kind is different generates errors. This choice allows you to write procedures that (for instance) log any integer or any real, or that return an integer out of a collection. These rules are applied to comparisons, assignments, column updates, anywhere and everywhere types are checked for compatibility. To get the most value out of these constructs, the authors recommend that type kinds be used universally except when the extra compatibility described above is needed (like low level helper functions.) Importantly, type kind can be applied to object types as well, allowing object&lt;dict&gt; to be distinct from object&lt;list&gt;. At run time the kind information is lost. But it does find it's way into the JSON output so external tools also get to see the kinds. "},{"title":"Nullability​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#nullability","content":"Nullability Rules​ Nullability is tracked via CQL's type system. To understand whether or not an expression will be assigned a nullable type, you can follow these rules; they will hopefully be intuitive if you are familiar with SQL: The literal NULL is, of course, always assigned a nullable type. All other literals are nonnull. In general, the type of an expression involving an operator (e.g., +, ==,!=, ~, LIKE, et cetera) is nullable if any of its arguments are nullable. For example, 1 + NULL is assigned the type INTEGER, implying nullability. 1 + 2, however, is assigned the type INTEGER NOT NULL. IN and NOT IN expressions are assigned a nullable type if and only if their left argument is nullable: The nullability of the right side is irrelevant. For example, &quot;foo&quot; IN (a, b) will always have the type BOOL NOT NULL, whereas some_nullable IN (a, b) will have the type BOOL. NOTE: In CQL, the IN operator behaves like a series of equality tests (i.e., == tests, not IS tests), and NOT IN behaves symmetrically. SQLite has slightly different nullability rules for IN and NOT IN. This is the one place where CQL has different evaluation rules from SQLite, by design. The result of IS and IS NOT is always of type BOOL NOT NULL, regardless of the nullability of either argument. For CASE expressions, the result is always of a nullable type if no ELSEclause is given. If an ELSE is given, the result is nullable if any of theTHEN or ELSE expressions are nullable. NOTE: The SQL CASE construct is quite powerful: Unlike the C switchstatement, it is actually an expression. In this sense, it is rather more like a highly generalized ternary a ? b : c operator than a C switch statement. There can be arbitrarily many conditions specified, each with their own result, and the conditions need not be constants; typically, they are not. IFNULL and COALESCE are assigned a NOT NULL type if one or more of their arguments are of a NOT NULL type. In most join operations, the nullability of each column participating in the join is preserved. However, in a LEFT OUTER join, the columns on the right side of the join are always considered nullable; in a RIGHT OUTER join, the columns on the left side of the join are considered nullable. As in most other languages, CQL does not perform evaluation of value-level expressions during type checking. There is one exception to this rule: An expression within a const is evaluated at compilation time, and if its result is then known to be nonnull, it will be given a NOT NULL type. For example, const(NULL or 1) is given the type BOOL NOT NULL, whereas merelyNULL or 1 has the type BOOL. Nullability Improvements​ CQL is able to &quot;improve&quot; the type of some expressions from a nullable type to aNOT NULL type via occurrence typing, also known as flow typing. There are three kinds of improvements that are possible: Positive improvements, i.e., improvements resulting from the knowledge that some condition containing one or more AND-linked IS NOT NULL checks must have been true: IF statements: IF a IS NOT NULL AND c.x IS NOT NULL THEN -- `a` and `c.x` are not null here ELSE IF b IS NOT NULL THEN -- `b` is not null here END IF; CASE expressions: CASE WHEN a IS NOT NULL AND c.x IS NOT NULL THEN -- `a` and `c.x` are not null here WHEN b IS NOT NULL THEN -- `b` is not null here ELSE ... END; IIF expressions: IIF(a IS NOT NULL AND c.x IS NOT NULL, ..., -- `a` and `c.x` are not null here ... ) SELECT expressions: SELECT -- `t.x` and `t.y` are not null here FROM t WHERE x IS NOT NULL AND y IS NOT NULL Negative improvements, i.e., improvements resulting from the knowledge that some condition containing one or more OR-linked IS NULL checks must have been false: IF statements: IF a IS NULL THEN ... ELSE IF c.x IS NULL THEN -- `a` is not null here ELSE -- `a` and `c.x` are not null here END IF; IF statements, guard pattern: IF a IS NULL RETURN; -- `a` is not null here IF c.x IS NULL THEN ... THROW; END IF; -- `a` and `c.x` are not null here CASE expressions: CASE WHEN a IS NULL THEN ... WHEN c.x IS NULL THEN -- `a` is not null here ELSE -- `a` and `c.x` are not null here END; IIF expressions: IIF(a IS NULL OR c.x IS NULL, ..., ... -- `a` and `c.x` are not null here ) Assignment improvements, i.e., improvements resulting from the knowledge that the right side of a statement (or a portion therein) cannot be NULL: SET statements: SET a := 42; -- `a` is not null here NOTE: Assignment improvements from FETCH statements are not currently supported. This may change in a future version of CQL. There are several ways in which improvements can cease to be in effect: The scope of the improved variable or cursor field has ended: IF a IS NOT NULL AND c.x IS NOT NULL THEN -- `a` and `c.x` are not null here END IF; -- `a` and `c.x` are nullable here An improved variable was SET to a nullable value: IF a IS NOT NULL THEN -- `a` is not null here SET a := some_nullable; -- `a` is nullable here END IF; An improved variable was used as an OUT (or INOUT) argument: IF a IS NOT NULL THEN -- `a` is not null here CALL some_procedure_that_requires_an_out_argument(a); -- `a` is nullable here END IF; An improved variable was used as a target for a FETCH statement: IF a IS NOT NULL THEN -- `a` is not null here FETCH c INTO a; -- `a` is nullable here END IF; An improved cursor field was re-fetched: IF c.x IS NOT NULL THEN -- `c.x` is not null here FETCH c; -- `c.x` is nullable here END IF; A procedure call was made (which removes improvements from all globalsbecause the procedure may have mutated any of them; locals are unaffected): IF a IS NOT NULL AND some_global IS NOT NULL THEN -- `a` and `some_global` are not null here CALL some_procedure(); -- `a` is still not null here -- `some_global` is nullable here END IF;  CQL is generally smart enough to understand the control flow of your program and infer nullability appropriately; here are a handful of examples: IF some_condition THEN SET a := 42; ELSE THROW; END IF; -- `a` is not null here because it must have been set to 42 -- if we've made it this far  IF some_condition THEN SET a := 42; ELSE SET a := 100; END IF; -- `a` is not null here because it was set to a value of a -- `NOT NULL` type in all branches and the branches cover -- all of the possible cases  IF a IS NOT NULL THEN IF some_condition THEN SET a := NULL; ELSE -- `a` is not null here despite the above `SET` because -- CQL understands that, if we're here, the previous -- branch must not have been taken END IF; END IF;  IF a IS NOT NULL THEN WHILE some_condition BEGIN -- `x` is nullable here despite `a IS NOT NULL` because -- `a` was set to `NULL` later in the loop and thus `x` -- will be `NULL` when the loop repeats LET x := a; SET a := NULL; ... END; END IF;  Here are some additional details to note regarding conditions: For positive improvements, the check must be exactly of the form IS NOT NULL; other checks that imply a variable or cursor field must not be null when true have no effect: IF a &gt; 42 THEN -- `a` is nullable here END IF; NOTE: This may change in a future version of CQL. For multiple positive improvements to be applied from a single condition, they must be linked by AND expressions along the outer spine of the condition; uses of IS NOT NULL checks that occur as subexpressions within anything other than AND have no effect: IF (a IS NOT NULL AND b IS NOT NULL) OR c IS NOT NULL THEN -- `a`, `b`, and `c` are all nullable here END IF; For negative improvements, the check must be exactly of the form IS NULL; other checks that imply a variable or cursor field must not be null when false have no effect: DECLARE equal_to_null INT; IF a IS equal_to_null THEN ... ELSE -- `a` is nullable here END IF; For multiple negative improvements to be applied from a single condition, they must be linked by OR expressions along the outer spine of the condition; uses of IS NULL checks that occur as subexpressions within anything other than OR have no effect: IF (a IS NULL OR b IS NULL) AND c IS NULL THEN ... ELSE -- `a`, `b`, and `c` are all nullable here END IF;  Forcing Nonnull Types​ If possible, it is best to use the techniques described in &quot;Nullability Improvements&quot; to verify that the value of a nullable type is nonnull before using it as such. Sometimes, however, you may know that a value with a nullable type cannot be null and simply wish to use it as though it were nonnull. The ifnull_crashand ifnull_throw &quot;attesting&quot; functions convert the type of an expression to be nonnull and ensure that the value is nonnull with a runtime check. They cannot be used in SQLite contexts because the functions are not known to SQLite, but they can be used in loose expressions. For example: CREATE PROC square_if_odd(a INT NOT NULL, OUT result INT) BEGIN IF a % 2 = 0 THEN SET result := NULL; ELSE SET result := a * a; END IF; END; -- `x` has type `INT`, but we know it can't be `NULL` let x := call square_if_odd(3); -- `y` has type `INT NOT NULL` let y := ifnull_crash(x);  Above, the ifnull_crash attesting function is used to coerce the expressionx to be of type INT NOT NULL. If our assumptions were somehow wrong, however—and x were, in fact, NULL—our program would crash. As an alternative to crashing, you can use ifnull_throw. The following two pieces of code are equivalent: CREATE PROC y_is_not_null(x INT) BEGIN let y := ifnull_throw(x); END;  CREATE PROC y_is_not_null(x INT) BEGIN DECLARE y INT NOT NULL; IF x IS NOT NULL THEN SET y := x; ELSE THROW; END IF; END;  "},{"title":"Expression Types​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#expression-types","content":"CQL supports a variety of expressions, nearly everything from the SQLite world. The following are the various supported operators; they are presented in order from the weakest binding strength to the strongest. Note that the binding order is NOT the same as C, and in some cases it is radically different (e.g. boolean math) UNION and UNION ALL​ These appear only in the context of SELECT statements. The arms of a compound select may include FROM, WHERE, GROUP BY, HAVING, and WINDOW. If ORDER BY or LIMIT ... OFFSET are present, these apply to the entire UNION. example: select A.x x from A inner join B using(z) union all select C.x x from C where x = 1;  The WHERE applies only to the second select in the union. And each SELECT is evaluated before the the UNION ALL select A.x x from A inner join B using(z) where x = 3 union all select C.x x from C where x = 1 order by x;  The ORDER BY applies to the result of the union, so any results from the 2nd branch will sort before any results from the first branch (because x is constrained in both). Assignment​ Assignment only occurs in the UPDATE statement or in the SET statement. In both cases the left side is a simple target and the right side is a general expression. The expression is evaluated before the assignment. example: SET x := 1 + 3 AND 4; -- + before AND then :=  Logical OR​ The logical OR operator does shortcut evaluation, much like the C || operator (not to be confused with SQL's concatenation operator with the same lexeme). The truth table for logical OR is as follows: A\tB\tA OR B0\t0\t0 0\t1\t1 0\tNULL\tNULL 1\t0\t1 1\t1\t1 1\tNULL\t1 NULL\t0\tNULL NULL\t1\t1 NULL\tNULL\tNULL Logical AND​ The logical AND operator does shortcut evaluation, much like the C &amp;&amp; operator, so if the left side is zero the result is 0 and the right side is not evaluated. The truth table for logical AND is as follows: A\tB\tA AND B0\t0\t0 0\t1\t0 0\tNULL\t0 1\t0\t0 1\t1\t1 1\tNULL\tNULL NULL\t0\t0 NULL\t1\tNULL NULL\tNULL\tNULL BETWEEN and NOT BETWEEN​ These are ternary operators. The general forms are:  expr1 BETWEEN expr2 AND expr3 expr1 NOT BETWEEN expr2 AND expr3  Note that there is an inherent ambiguity in the language because expr2 or expr3 could be logical expressions that include AND. CQL resolves this ambiguity by insisting that expr2 and expr3 be &quot;math expressions&quot; in the grammar. These expressions may not have ungrouped AND or OR operators. Examples:: -- oh hell no (syntax error) a between 1 and 2 and 3; -- all ok a between (1 and 2) and 3; a between 1 and (2 and 3); a between 1 and b between c and d; -- binds left to right a between 1 + 2 and 12 / 2;  Logical NOT​ The one operand of logical NOT must be a numeric. NOT 'x' is illegal. Non-ordering tests !=, &lt;&gt;, =, ==, LIKE, GLOB, MATCH, REGEXP, IN, IS, IS NOT​ These operations do some non-ordered comparison of their two operands. IS and IS NOT never return NULL, So for instance X IS NOT NULL gives the natural answer. x IS y is true if and only if: 1. both x and y are NULL or 2. if they are equal.The other operators return NULL if either operand is NULL and otherwise perform their usual test to produce a boolean!= and &lt;&gt; are equivalent as are = and ==strings and blobs compare equal based on their value, not their identity (i.e. not the string/blob pointer)objects compare equal based on their address, not their content (i.e. reference equality)MATCH, GLOB, and REGEXP are only valid in SQL contexts, LIKE can be used in any context (a helper method to do LIKE in C is provided by SQLite, but not the others)MATCH, GLOB, REGEXP, LIKE, and IN may be prefixed with NOT which reverses their value  NULL IS NULL -- this is true (NULL == NULL) IS NULL -- this is also true because NULL == NULL is not 1, it's NULL. (NULL != NULL) IS NULL -- this is also true because NULL != NULL is not 0, it's also NULL. 'xy' NOT LIKE 'z%'` -- this is true  Ordering comparisons &lt;, &gt;, &lt;=, &gt;=​ These operators do the usual order comparison of their two operands. If either operand is NULL the result is NULLObjects and Blobs may not be compared with these operandsStrings are compared based on their value (as with other comparisons) not their addressNumerics are compared as usual with the usual promotion rules NOTE: CQL uses strcmp for string comparison. In SQL expressions the comparison happens in whatever way SQLite has been configured. Typically general purpose string comparison should be done with helper functions that deal with collation and other considerations. This is a very complex topic and CQL is largely silent on it. Bitwise operators &lt;&lt;, &gt;&gt;, &amp;, |​ These are the bit-manipulation operations. Their binding strength is VERY different than C so beware. And notably the &amp; operator has the same binding strength as the | operator so they bind left to right, which is utterly unlike most systems. Many parentheses are likely to be needed to get the usual &quot;or of ands&quot; patterns codified correctly. Likewise, the shift operators &lt;&lt; and &gt;&gt; are the same strength as &amp; and | which is very atypical. Consider: x &amp; 1 &lt;&lt; 7; -- probably doesn't mean what you think (this is not ambiguous, it's well defined, but unlike C) (x &amp; 1) &lt;&lt; 7; -- means the same as the above x &amp; (1 &lt;&lt; 7) -- probably what you intended  Note that these operators only work on integer and long integer data. If any operand is NULL the result is `NULL. Addition and Subtraction +, -​ These operators do the typical math. Note that there are no unsigned numerics so it's always signed math that is happening here. operands are promoted to the &quot;biggest&quot; type involved as previously described (bool -&gt; int -&gt; long -&gt; real)only numeric operands are legal (no adding strings)if any operand is NULL the result is NULL Multiplication, Division, Modulus *, /, %​ These operators do the typical math. Note that there are no unsigned numerics so it's always signed math that is happening here. operands are promoted to the &quot;biggest&quot; type as previously described (bool -&gt; int -&gt; long -&gt; real)only numeric operands are legal (no multiplying strings)if any operand is NULL the result is NULL EXCEPTION: the % operator doesn't make sense on real values, so real values produce an error. Unary operators -, ~​ Unary negation (-) and bitwise invert (~) are the strongest binding operators. The ~ operator only works on integer types (not text, not real)the usual promotion rules otherwise applyif the operand is NULL the result is NULL "},{"title":"CASE Expressions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#case-expressions","content":"The case expression has two major forms and provides a great deal of flexibility in an expression. You can kind of think of it as the C ?: operator on steroids. set x := 'y'; select case x when 'y' then 1 when 'z' then 2 else 3 end;  In this form the case expression (x here) is evaluated exactly once and then compared against each when clause. Every when clause must be type compatible with the case expression. The then expression that corresponds to the matching when is evaluated and becomes the result. If no when matches then the else expression is used. If there is no else and no matching when then the result is null. If that's not general enough, there is an alternate form: set y := 'yy'; set z := 'z'; select case when y = 'y' then 1 when z = 'z' then 2 else 3 end;  The second form, where there is no value before the first when keyword, each when expression is a separate independent boolean expression. The first one that evaluates to true causes the corresponding then to be evaluated and that becomes the result. As before, if there is no matching when clause then the result is the else expression if present, or null if there is no else. The result types must be compatible and the best type to hold the answer is selected with the usual promotion rules. SELECT expressions​ Single values can be extracted from SQLite using an inline select expression. For instance: set x_ := (select x from somewhere where id = 1);  The select statement in question must extract exactly one column and the type of the expression becomes the type of the column. This form can appear anywhere an expression can appear, though it is most commonly used in assignments. Something like this would also be valid: if (select x from somewhere where id = 1) == 3 then ... end if;  The select statement can of course be arbitrarily complex. Note, if the select statement returns no rows this will result in the normal error flow. In that case, the error code will be SQLITE_DONE, which is treated like an error because in this context SQLITE_ROW is expected as a result of the select. This is not a typical error code and can be quite surprising to callers. If you're seeing this failure mode it usually means the code had no affordance for the case where there were no rows and probably that situation should have been handled. This is an easy mistake to make, so to avoid it, CQL also supports these more tolerant forms: set x_ := (select x from somewhere where id = 1 if nothing -1);  And even more generally if the schema allows for null values and those are not desired: set x_ := (select x from somewhere where id = 1 if nothing or null -1);  Both of these are much safer to use as only genuine errors (e.g. the table was dropped and no longer exists) will result in the error control flow. Again note that: set x_ := (select ifnull(x,-1) from somewhere where id = 1);  Would not avoid the SQLITE_DONE error code, because no rows returned is not at all the same as a null value returned. The if nothing or null form above is equivalent to the following, but it is more economical, and probably clearer: set x_ := (select ifnull(x,-1) from somewhere where id = 1 if nothing -1);  To compute the type of the overall expression, the rules are almost the same as normal binary operators. In particular: if the default expression is present it must be type compatible with the select result the result type is the smallest type that holds both the select value and the default expression (see normal promotion rules above) object types are not allowed (SQLite cannot return an object)in (select ...) the result type is not null if and only if the select result type is not null (see select statement, many cases)in (select ... if nothing) the result type is not null if and only if both the select result and the default expression types are not null (normal binary rules)in (select ... if nothing or null) the result type is not null if and only if the default expression type is not null Finally, the form (select ... if nothing throw) is allowed; this form is exactly the same as normal(select ...) but makes the explicit that the error control flow will happen if there is no row. Consequently this form is allowed even if @enforce_strict select if nothing is in force. "},{"title":"Marking Data as Sensitive​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#marking-data-as-sensitive","content":"CQL supports the notion of 'sensitive' data in a first class way. You can think of it as very much like nullability; It largely begins by tagging data columns with @sensitive Rather than go through the whole calculus, it's easier to understand by a series of examples. So let's start with a table with some sensitive data. create table with_sensitive( id integer, name text @sensitive, sens integer @sensitive );  The most obvious thing you might do at this point is create a stored proc that would read data out of that table. Maybe something like this: create proc get_sensitive() begin select id as not_sensitive_1, sens + 1 sensitive_1, name as sensitive_2, 'x' as not_sensitive_2, -sens as sensitive_3, sens between 1 and 3 as sensitive_4 from with_sensitive; end;  So looking at that procedure we can see that it's reading sensitive data, so the result will have some sensitive columns in it. the &quot;id&quot; is not sensitive (at least not in this example)sens + 1 is sensitive, math on a sensitive field leaves it sensitivename is sensitive, it began that way and is unchanged'x' is just a string literal, it's not sensitive-sens is sensitive, that's more mathand the between expression is also sensitive Generally sensitivity is &quot;radioactive&quot; - anything it touches becomes sensitive. This is very important because even a simple looking expression like sens IS NOT NULL must lead to a sensitive result or the whole process would be largely useless. It has to be basically impossible to wash away sensitivity. These rules apply to normal expressions as well as expressions in the context of SQL. Accordingly: Sensitive variables can be declared: declare sens integer @sensitive;  Simple operations on the variables are sensitive -- this is sensitive (and the same would be true for any other math) sens + 1;  The IN expression gives you sensitive results if anything about it is sensitive -- all of these are sensitive sens in (1, 2); 1 in (1, sens); (select id in (select sens from with_sensitive));  Similarly sensitive constructs in CASE expressions result in a sensitive output -- not sensitive case 0 when 1 then 2 else 3 end; -- all of these are sensitive case sens when 1 then 2 else 3 end; case 0 when sens then 2 else 3 end; case 0 when 1 then sens else 3 end; case 0 when 1 then 2 else sens end;  Cast operations preserve sensitivity -- sensitive result select cast(sens as INT);  Aggregate functions likewise preserve sensitivity -- all of these are sensitive select AVG(T1.sens) from with_sensitive T1; select MIN(T1.sens) from with_sensitive T1; select MAX(T1.sens) from with_sensitive T1; select SUM(T1.sens) from with_sensitive T1; select COUNT(T1.sens) from with_sensitive T1;  There are many operators that get similar treatment such as COALESCE, IFNULL, IS and IS NOT. Things get more interesting when we come to the EXISTS operator: -- sensitive if and only if any selected column is sensitive exists(select * from with_sensitive) -- sensitive because &quot;info&quot; is sensitive exists(select info from with_sensitive) -- not sensitive because &quot;id&quot; is not sensitive exists(select id from with_sensitive)  If this is making you nervous, it probably should, we need a little more protection because of the way EXISTS is typically used. The predicates matter, consider the following: -- id is now sensitive because the predicate of the where clause was sensitive select id from with_sensitive where sens = 1; -- this expression is now sensitive because id is sensitive in this context exists(select id from with_sensitive where sens = 1)  In general: if the predicate of a WHERE or HAVING clause is sensitive then all columns in the result become sensitive. Similarly when performing joins, if the column specified in the USING clause is sensitive or the predicate of the ON clause is sensitive then the result of the join is considered to be all sensitive columns (even if the columns were not sensitive in the schema). Likewise a sensitive expression in LIMIT or OFFSET will result in 100% sensitive columns as these can be used in a WHERE-ish way. There is no reasonable defense against using LIMIT and testing for the presence or absence of a row as a way to wash away sensitivity so that is a weakness, but the rules that are present are likely to be very helpful. -- join with ON select T1.id from with_sensitive T1 inner join with_sensitive T2 on T1.sens = T2.sens -- join with USING select T1.id from with_sensitive T1 inner join with_sensitive T2 using(sens);  All of these expression and join propagations are designed to make it impossible to simply wash-away sensitivity with a little bit of math. Now we come to enforcement, which boils down to what assignments or &quot;assignment-like&quot; operations we allow. If we have these: declare sens integer @sensitive; declare not_sens integer;  We can use those as stand-ins for lots of expressions, but the essential calculus goes like this: -- assigning a sensitive to a sensitive is ok set sens := sens + 1; -- assigning not sensitive data to a sensitive is ok -- this is needed so you can (e.g.) initialize to zero set sens := not_sens; -- not ok set not_sens := sens;  Now these &quot;assignments&quot; can happen in a variety of ways: you can set an out parameter of your procedurewhen calling a function or procedure, we require: any IN parameters of the target be &quot;assignable&quot; from the value of the argument expressionany OUT parameters of the target be &quot;assignable&quot; from the procedures type to the argument variableany IN/OUT parameters require both the above Now it's possible to write a procedure that accepts sensitive things and returns non-sensitive things. This is fundamentally necessary because the proc must be able return (e.g.) a success code, or encrypted data, that is not sensitive. However, if you write the procedure in CQL it, too, will have to follow the assignment rules and so cheating will be quite hard. The idea here is to make it easy to handle sensitive data well and make typical mistakes trigger errors. With these rules it's possible to compute the the type of procedure result sets and also to enforce IN/OUT parameters. Since the signature of procedures is conveniently generated with --generate_exports good practices are fairly easy to follow and sensitivity checks flow well into your programs. This is a brief summary of CQL semantics for reference types -- those types that are ref counted by the runtime. The three reference types are: TEXTOBJECTBLOB Each of these has their own macro for retain and release though all three actually turn into the exact same code in all the current CQL runtime implementations. In all cases the object is expected to be promptly freed when the reference count falls to zero. "},{"title":"Reference Semantics​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#reference-semantics","content":"Stored Procedure Arguments​ in and inout arguments are not retained on entry to a stored procout arguments are assumed to contain garbage and are nulled without retaining on entryif your out argument doesn't have garbage in it, then it is up to you do release it before you make a callWhen calling a proc with an out argument CQL will release the argument variable before the call site, obeying its own contract Local Variables​ assigning to a local variable retains the object, and then does a release on the previous objectthis order is important; all assignments are done in this way in case of aliasing (release first might accidentally free too soon)CQL calls release on all local variable when the method exits Assigning to an out parameter or a global variable​ out, inout parameters, and global variables work just like local variables except that CQL does not call release at the end of the procedure "},{"title":"Function Return Values​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#function-return-values","content":"Stored procedures do not return values, they only have out arguments and those are well defined as above. Functions however are also supported and they can have either get or create semantics Get Semantics​ If you declare a function like so: declare function Getter() object;  Then CQL assumes that the returned object should follow the normal rules above, retain/release will balance by the end of the procedure for locals and globals or out arguments could retain the object Create Semantics​ If you declare a function like so: declare function Getter() create text;  then CQL assumes that the function created a new result which it is now responsible for releasing. In short, the returned object is assumed to arrive with a retain count of 1 already on it. When CQL stores this return value it will: release the object that was present at the storage location (if any)copy the returned pointer without further retaining it this one time As a result if you store the returned value in a local variable it will be released when the procedure exits (as usual) or if you instead store the result in a global or an out parameter the result will survive to be used later. "},{"title":"Comparison​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#comparison","content":"CQL tries to adhere to normal SQL comparison rules but with a C twist. "},{"title":"OBJECT​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#object","content":"The object type has no value based comparison, so there is no &lt;, &gt; and so forth. The following table is useful. Let's suppose there are exactly two distinct objects 'X' and 'Y' true expressions: X = X X &lt;&gt; Y Y = Y Y &lt;&gt; X X IN (X, Y) X NOT IN (Y) false expressions: X = Y X &lt;&gt; X Y = X Y &lt;&gt; Y X NOT IN (X, Y) null expressions: null = null X &lt;&gt; null x = null null &lt;&gt; null Y &lt;&gt; null y = null null = null resulting in null is particular surprising but consistent with the usual SQL rules. And again, as in SQL, the IS operator returns true for X IS Y even if both are null. "},{"title":"TEXT​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#text","content":"Text has value comparison semantics but normal string comparison is done only with strcmp which is of limited value. Typically you'll want to either delegate the comparison to Sqlite (with (select x &lt; y)) or else use a helper function with a suitable comparison mechanism. For text comparisons including equality: true: if and only if both operands are not null and the comparison matches (using strcmp) false: if and only if both operands are not null and the comparison does not match (using strcmp) null: if and only if at least one operand is null EXAMPLE: 'x' &lt; 'y' is true because strcmp(&quot;x&quot;, &quot;y&quot;) &lt; 0 The IS and IS NOT operators behave similarly to equality and inequality, but never return null. If X is some value that doesn't happen to be null then we have the following: true: null is null X is X X is not null null is not Xfalse: null is not null X is not X X is null null is X The IN and NOT IN operators also work for text using the same value comparisons as above. Additionally there are special text comparison operators such as LIKE, MATCH and GLOB. These comparisons are defined by SQLite. "},{"title":"BLOB​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#blob","content":"Blobs are compared by value (equivalent to memcmp) but have no well-defined ordering. The memcmp order is deemed not helpful as blobs usually have internal structure hence the valid comparisons are only equality and inequality. You can use user defined functions to do better comparisons of your particular blobs if needed. The net comparison behavior is otherwise just like strings. "},{"title":"Sample Code​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#sample-code","content":"Out Argument Semantics​ DECLARE FUNCTION foo() OBJECT; CREATE PROC foo_user (OUT baz OBJECT) BEGIN SET baz := foo(); END;  void foo_user(cql_object_ref _Nullable *_Nonnull baz) { *(void **)baz = NULL; // set out arg to non-garbage cql_set_object_ref(baz, foo()); }  Function with Create Semantics​ DECLARE FUNCTION foo() CREATE OBJECT; CREATE PROCEDURE foo_user (INOUT baz OBJECT) BEGIN DECLARE x OBJECT; SET x := foo(); SET baz := foo(); END;  void foo_user(cql_object_ref _Nullable *_Nonnull baz) { cql_object_ref x = NULL; cql_object_release(x); x = foo(); cql_object_release(*baz); *baz = foo(); cql_cleanup: cql_object_release(x); }  Function with Get Semantics​ DECLARE FUNCTION foo() OBJECT; CREATE PROCEDURE foo_user (INOUT baz OBJECT) BEGIN DECLARE x OBJECT; SET x := foo(); SET baz := foo(); END;  void foo_user(cql_object_ref _Nullable *_Nonnull baz) { cql_object_ref x = NULL; cql_set_object_ref(&amp;x, foo()); cql_set_object_ref(baz, foo()); cql_cleanup: cql_object_release(x); }  "},{"title":"Chapter 4: Procedures, Functions, and Control Flow​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-4-procedures-functions-and-control-flow","content":"All kinds of control flow happens in the context of some procedure. Though we've already introduced examples of procedures let's now go over some of the additional aspects we have not yet illustrated. "},{"title":"Out Parameters​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#out-parameters","content":"Consider this procedure: create procedure echo_integer(in arg1 integer not null, out arg2 integer not null) begin set arg2 := arg1; end;  arg1 has been declared in. This is the default: in arg1 integer not nulland arg1 integer not null mean the exact same thing. arg2, however, has been declared out. When a parameter is declared usingout, arguments for it are passed by reference. This is similar to by-reference arguments in other languages; indeed, they compile into a simple pointer reference in the generated C code. Given that arg2 is passed by reference, the statement set arg2 := arg1;actually updates a variable in the caller. For example: declare x int not null; call echo_integer(42, x); -- `x` is now 42  It is important to note that values cannot be passed into a procedure via anout parameter. In fact, out parameters are immediately assigned a new value as soon as the procedure is called: All nullable out parameters are set to null. Nonnull out parameters of a non-reference type (e.g., integer, long,bool, et cetera) are set to their default values (0, 0.0, false, et cetera). Nonnull out parameters of a reference type (e.g., blob, object, andtext) are set to null as there are no default values for reference types. They must, therefore, be assigned a value within the procedure so that they will not be null when the procedure returns. CQL enforces this. In addition to in and out parameters, there are also inout parameters.inout parameters are, as one might expect, a combination of in and outparameters: The caller passes in a value as with in parameters, but the value is passed by reference as with out parameters. inout parameters allow for code such as the following: create procedure times_two(inout arg integer not null) begin -- note that a variable in the caller is both -- read from and written to set arg := arg + arg; end; let x := 2; call times_two(x); -- `x` is now 4  "},{"title":"Procedure Calls​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedure-calls","content":"The usual call syntax is used to invoke a procedure. It returns no value but it can have any number of out arguments.  declare scratch integer not null; call echo_integer(12, scratch); scratch == 12; -- true  Let's go over the most essential bits of control flow. "},{"title":"The IF statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-if-statement","content":"The CQL IF statement has no syntatic ambiguities at the expense of being somewhat more verbose than many other languages. In CQL the ELSE IF portion is baked into the IF statement, so what you see below is logically a single statement. create proc checker(foo integer, out result integer not null) begin if foo = 1 then set result := 1; else if foo = 2 then set result := 3; else set result := 5; end if; end;  "},{"title":"The WHILE statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-while-statement","content":"What follows is a simple procedure that counts down its input argument. declare procedure printf no check; create proc looper(x integer not null) begin while x &gt; 0 begin call printf('%d\\n', x); set x := x - 1; end; end;  The WHILE loop has additional keywords that can be used within it to better control the loop. A more general loop might look like this: declare procedure printf no check; create proc looper(x integer not null) begin while 1 begin set x := x - 1; if x &lt; 0 then leave; else if x % 100 = 0 then continue; else if x % 10 = 0 then call printf('%d\\n', x); end if; end; end;  Let's go over this peculiar loop:  while 1 begin ... end;  This is an immediate sign that there will be an unusual exit condition. The loop will never end without one because 1 will never be false.  if x &lt; 0 then leave;  Now here we've encoded our exit condition a bit strangely: we might have done the equivalent job with a normal condition in the predicate part of the while statement but for illustration anyway, when x becomes negative leave will cause us to exit the loop. This is likebreak in C.  else if x % 100 = 0 then continue;  This bit says that on every 100th iteration we go back to the start of the loop. So the next bit will not run, which is the printing.  else if x % 10 = 0 then call printf('%d\\n', x); end if;  Finishing up the control flow, on every 10th iteration we print the value of the loop variable. "},{"title":"The SWITCH Statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-switch-statement","content":"The CQL SWITCH is designed to map to the C switch statement for better codegen and also to give us the opportunity to do better error checking.SWITCH is a statement like IF not an expression like CASE..WHEN..END so it combines with other statements. The general form looks like this: SWITCH switch-expression [optional ALL VALUES] WHEN expr1, expr2, ... THEN [statement_list] WHEN expr3, ... THEN [statement_list] WHEN expr4 THEN NOTHING ELSE [statement_list] END;  the switch-expression must be a not-null integral type (integer not null or long integer not null)the WHEN expressions [expr1, expr2, etc.] are made from constant integer expressions (e.g. 5, 1+7, 1&lt;&lt;2, or my_enum.thing)the WHEN expressions must be compatible with the switch expression (long constants cannot be used if the switch expression is an integer)the values in the WHEN clauses must be unique (after evaluation)within one of the interior statement lists the LEAVE keyword exits the SWITCH prematurely, just like break in C a LEAVE is not required before the next WHENthere are no fall-through semantics as you can find in C, if fall-through ever comes to SWITCH it will be explicit if the keyword NOTHING is used after THEN it means there is no code for that case, which is useful with ALL VALUES (see below)the ELSE clause is optional and works just like default in C, covering any cases not otherwise explicitly listedif you add ALL VALUES then: the expression must be an from an enum typethe WHEN values must cover every value of the enum enum members that start with a leading _ are by convention considered pseudo values and do not need to be covered there can be no extra WHEN values not in the enumthere can be no ELSE clause (it would defeat the point of listing ALL VALUES which is to get an error if new values come along) Some more complete examples: let x := get_something(); switch x when 1,1+1 then -- constant expressions ok set y := 'small'; -- other stuff when 3,4,5 then set y := 'medium'; -- other stuff when 6,7,8 then set y := 'large'; -- other stuff else set y := 'zomg enormous'; end; declare enum item integer ( pen = 0, pencil, brush, paper, canvas, _count ); let x := get_item(); -- returns one of the above switch x all values when item.pen, item.pencil then call write_something(); when item.brush then nothing -- itemize brush but it needs no code when item.paper, item.canvas then call setup_writing(); end;  Using THEN NOTHING allows the compiler to avoid emitting a useless break in the C code. Hence that choice is better/clearer than when brush then leave; Note that the presence of _count in the enum will not cause an error in the above because it starts with _. The C output for this statement will be a direct mapping to a C switch statement. "},{"title":"The TRY, CATCH, and THROW Statements​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-try-catch-and-throw-statements","content":"This example illustrates catching an error from some DML, and recovering rather than letting the error cascade up. This is the common &quot;upsert&quot; pattern (insert or update) declare procedure printf no check; create procedure upsert_foo(id_ integer, t_ text) begin begin try insert into foo(id, t) values(id_, t_) end try; begin catch begin try update foo set t = t_ where id = id_; end try; begin catch call printf(&quot;Error code %d!\\n&quot;, @rc); throw; end catch; end catch; end;  Once again, let's go over this section by section:  begin try insert into foo(id, t) values(id_, t_) end try;  Normally if the insert statement fails, the procedure will exit with a failure result code. Here, instead, we prepare to catch that error.  begin catch begin try update foo set t = t_ where id = id_; end try;  Now, having failed to insert, presumably because a row with the provided id already exists, we try to update that row instead. However that might also fail, so we wrap it in another try. If the update fails, then there is a final catch block:  begin catch call printf(&quot;Error code %d!\\n&quot;, @rc); throw; end catch;  Here we see a usage of the @rc variable to observe the failed error code. In this case we simply print a diagnostic message and then use the throw keyword to rethrow the previous failure (exactly what is stored in @rc). In general, throw will create a failure in the current block using the most recent failed result code from SQLite (@rc) if it is an error, or else the generalSQLITE_ERROR result code if there is no such error. In this case the failure code for the update statement will become the result code of the current procedure. This leaves only the closing markers:  end catch; end;  If control flow reaches the normal end of the procedure it will return SQLITE_OK. "},{"title":"Procedures as Functions: Motivation and Example​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedures-as-functions-motivation-and-example","content":"The calling convention for CQL stored procedures often (usually) requires that the procedure returns a result code from SQLite. This makes it impossible to write a procedure that returns a result like a function, as the result position is already used for the error code. You can get around this problem by using out arguments as your return codes. So for instance, this version of the Fibonacci function is possible. -- this works, but it is awkward create procedure fib (in arg integer not null, out result integer not null) begin if (arg &lt;= 2) then set result := 1; else declare t integer not null; call fib(arg - 1, result); call fib(arg - 2, t); set result := t + result; end if; end;  The above works, but the notation is very awkward. CQL has a &quot;procedures as functions&quot; feature that tries to make this more pleasant by making it possible to use function call notation on a procedure whose last argument is an out variable. You simply call the procedure like it was a function and omit the last argument in the call. A temporary variable is automatically created to hold the result and that temporary becomes the logical return of the function. For semantic analysis, the result type of the function becomes the type of the out argument. -- rewritten with function call syntax create procedure fib (in arg integer not null, out result integer not null) begin if (arg &lt;= 2) then set result := 1; else set result := fib(arg - 1) + fib(arg - 2); end if; end;  This form is allowed when: all but the last argument of the procedure was specifiedthe formal parameter for that last argument was marked with out (neither in nor inout are acceptable)the procedure does not return a result set using a select statement or out statement (more on these later) If the procedure in question uses SQLite, or calls something that uses SQLite, then it might fail. If that happens the result code will propagate just like it would have with the usual call form. Any failures can be caught with try/catch as usual. This feature is really only syntatic sugar for the &quot;awkward&quot; form above, but it does allow for slightly better generated C code. "},{"title":"Chapter 5: Types of Cursors, Shapes, OUT and OUT UNION, and FETCH​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-5-types-of-cursors-shapes-out-and-out-union-and-fetch","content":"In the previous chapters we have used cursor variables without fully discussing them. Most of the uses are fairly self-evident but a more exhaustive discussion is also useful. First there are three types of cursors, as we will see below. "},{"title":"Statement Cursors​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#statement-cursors","content":"A statement cursor is based on a SQL SELECT statement. A full example might look like this: -- elsewhere create table xy_table(x integer, y integer); declare C cursor for select x, y from xy_table;  When compiled, this will result in creating a SQLite statement object (type sqlite_stmt *) and storing it in a variable called C_stmt. This statement can then be used later in various ways. Here's perhaps the simplest way to use the cursor above: declare x, y integer; fetch C into x, y;  This will have the effect of reading one row from the results of the query into the local variables x and y. These variables might then be used to create some output such as: /* note use of double quotes so that \\n is legal */ call printf(&quot;x:%d y:%d\\n&quot;, ifnull(x, 0), ifnull(y,0));  More generally, there the cursor may or may not be holding fetched values. The cursor variable C can be used by itself as a boolean indicating the presence of a row. So a more complete example might be if C then call printf(&quot;x:%d y:%d\\n&quot;, ifnull(x, 0), ifnull(y,0)); else call printf(&quot;nada\\n&quot;); end if  And even more generally loop fetch C into x, y begin call printf(&quot;x:%d y:%d\\n&quot;, ifnull(x, 0), ifnull(y,0)); end;  The last example above reads all the rows and prints them. Now if the table xy_table had instead had dozens of columns, those declarations would be very verbose and error prone, and frankly annoying, especially if the table definition was changing over time. To make this a little easier, there are so-called 'automatic' cursors. These happen implicitly and include all the necessary storage to exactly match the rows in their statement. Using the automatic syntax for the above looks like so: declare C cursor for select * from xy_table; fetch C; if C then call printf(&quot;x:%d y:%d\\n&quot;, ifnull(C.x, 0), ifnull(C.y,0)); end if;  or the equivalent loop form: declare C cursor for select * from xy_table; loop fetch C begin call printf(&quot;x:%d y:%d\\n&quot;, ifnull(C.x, 0), ifnull(C.y,0)); end;  All the necessary local state is automatically created, hence &quot;automatic&quot; cursor. This pattern is generally preferred, but the loose variables pattern is in some sense more general. In all the cases if the number or type of variables do not match the select statement, semantic errors are produced. "},{"title":"Value Cursors​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#value-cursors","content":"The purpose of value cursors is to make it possible for a stored procedure to work with structures as a unit rather than only field by field. SQL doesn't have the notion of structure types, but structures actually appear pretty directly in many places. Generally we call these things &quot;Shapes&quot; and there are a variety of source for shapes including: the columns of a tablethe projection of a SELECT statementthe columns of a cursorthe result type of a procedure that returns a selectthe arguments of a procedureother things derived from the above Let's first start with how you declare a value cursor. It is providing one of the shape sources above. So: declare C cursor like xy_table; declare C cursor like select 1 a, 'x' b; declare C cursor like (a integer not null, b text not null); declare C cursor like my_view; declare C cursor like my_other_cursor; declare C cursor like my_previously_declared_stored_proc; declare C cursor like my_previously_declared_stored_proc arguments;  Any of those forms define a valid set of columns -- a shape. Note that theselect example in no way causes the query provided to run. Instead, the select statement is analyzed and the column names and types are computed. The cursor gets the same field names and types. Nothing happens at run time. The last two examples assume that there is a stored procedure defined somewhere earlier in the same translation unit and that the procedure returns a result set or has arguments, respectively. In all cases the cursor declaration makes a cursor that could hold the indicated result. That result can then be loaded with FETCH or emitted with OUT or OUT UNION which will be discussed below. Once we have declared a value cursor we can load it with values using FETCH in its value form. Here are some examples: Fetch from compatible values: fetch C from values(1,2);  Fetch from a call to a procedure that returns a single row: fetch C from call my_previously_declared_stored_proc();  Fetch from another cursor: fetch C from D;  In this last case if D is a statement cursor it must also be &quot;automatic&quot; (i.e. it has the storage). This form lets you copy a row and save it for later. For instance, in a loop you could copy the current max-value row into a value cursor and use it after the loop, like so: declare C cursor for select * from somewhere; declare D cursor like C; loop fetch C begin if (not D or D.something &lt; C.something) then fetch D from C; end if; end;  After the loop, D either empty because there were no rows (thus if D would fail) or else it has the row with the maximum value of something, whatever that is. Value cursors are always have their own storage, so you could say all value cursors are &quot;automatic&quot;. And as we saw above, value cursors may or may not be holding a row. declare C cursor like xy_table; if not C then call printf(&quot;this will always print because C starts empty\\n&quot;); end if;  When you call a procedure you may or may not get a row as we'll see below. The third type of cursor is a &quot;result set&quot; cursor but that won't make any sense until we've discussed result sets a little which requires OUT and/or OUT UNIONand so we'll go on to those statements next. As it happens, we are recapitulating the history of cursor features in the CQL language by exploring the system in this way. Benefits of using named typed to declare a cursor​ This form allows any kind of declaration, for instance: declare C cursor like ( id integer not null, val real, flag boolean );  This wouldn't really give us much more than the other forms, however typed name lists can include LIKE in them again, as part of the list. Which means you can do this kind of thing: declare C cursor like (like D, extra1 real, extra2 bool)  You could then load that cursor like so: fetch C from values (from D, 2.5, false);  and now you have D plus 2 more fields which maybe you want to output. Importantly this way of doing it means that C always includes D, even if D changes over time. As long as the extra1 and extra2 fields don't conflict names it will always work. "},{"title":"OUT Statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#out-statement","content":"Value cursors were initially designed to create a convenient way for a procedure to return a single row from a complex query without having a crazy number of OUT parameters. It's easiest to illustrate this with an example. Suppose you want to return several variables, the &quot;classic&quot; way to do so would be a procedure like this: create proc get_a_row( id_ integer not null, out got_row bool not null, out w integer not null, out x integer, out y text not null, out z real) begin declare C cursor for select w, x, y, z from somewhere where id = id_; fetch C into w, x, y, z; set got_row := C; end;  This is already verbose, but you can imagine the situation gets very annoying if get_a_row has to produce a couple dozen column values. And of course you have to get the types exactly right. And they might evolve over time. Joy. On the receiving side you get to do something just as annoying: declare w integer not null declare x integer; declare y text; declare z real; declare got_row bool not null; call get_a_row(id, got_row, w, x, y, z);  Using the out statement we get the equivalent functionality with a much simplified pattern. It looks like this: create proc get_a_row(id_ integer not null) begin declare C cursor for select w, x, y, z from somewhere where id = id_; fetch C; out C; end;  To use the new procedure you simply do this: declare C cursor like get_a_row; fetch C from call get_a_row(id);  In fact, originally you did the two steps above in one statement and that was the only way to load a value cursor. Later, the calculus was generalized. The original form still works: declare C cursor fetch from call get_a_row(id);  The OUT statement lets you return a single row economically and lets you then test if there actually was a row and if so, read the columns. It infers all the various column names and types so it is resilient to schema change and generally a lot less error prone than having a large number of out arguments to your procedure. Once you have the result in a value cursor you can do the usual cursor operations to move it around or otherwise work with it. The use of the LIKE keyword to refer to groups of columns spread to other places in CQL as a very useful construct, but it began here with the need to describe a cursor shape economically, by reference. "},{"title":"OUT UNION Statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#out-union-statement","content":"The semantics of the OUT statement are that it always produces one row of output (a procedure can produce no row if an out never actually rans but the procedure does use OUT). If an OUT statement runs more than once, the most recent row becomes the result. So the OUT statement really does mirror having one out variable for each output column. This was its intent and procedures that return at most, or exactly, one row are very common so it works well enough. However, in general, one row results do not suffice; you might want to produce a result set from various sources, possibly with some computation as part of the row creation process. To make general results, you need to be able to emit multiple rows from a computed source. This is exactly what OUT UNION provides. Here's a (somewhat contrived) example of the kind of thing you can do with this form: create proc foo(n integer not null) begin declare C cursor like select 1 value; let i := 0; while i &lt; n begin -- emit one row for every integer fetch C from values(i); out union C; set i := i + 1; end; end;  In foo above, we make an entire result set out of thin air. It isn't a very interesting result, but of course any computation would have been possible. This pattern is very flexible as we see below in bar where we merge two different data streams. create table t1(id integer, stuff text, [other things too]); create table t2(id integer, stuff text, [other things too]); create proc bar() begin declare C cursor for select * from t1 order by id; declare D cursor for select * from t2 order by id; fetch C; fetch D; -- we're going to merge these two queries while C or D begin -- if both have a row pick the smaller id if C and D then if C.id &lt; D.id then out union C; fetch C; else out union D; fetch D; end if; else if C then -- only C has a row, emit that out union C; fetch C; else -- only D has a row, emit that out union D; fetch D; end if; end; end;  Just like foo, in bar, each time OUT UNION runs a new row is accumulated. Now, if you build a procedure that ends with a SELECT statement CQL automatically creates a fetcher function that does something like an OUT UNION loop -- it loops over the SQLite statement for the SELECT and fetches each row, materializing a result. With OUT UNION you take manual control of this process, allowing you to build arbitrary result sets. Note that either of C or D above could have been modified, replaced, skipped, normalized, etc. with any kind of computation. Even entirely synthetic rows can be computed and inserted into the output as we saw in foo. "},{"title":"Result Set Cursors​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#result-set-cursors","content":"Now that we have OUT UNION it makes sense to talk about the final type of cursor. OUT UNION makes it possible to create arbitrary result sets using a mix of sources and filtering. Unfortunately this result type is not a simple row, nor is it a SQLite statement. This meant that neither of the existing types of cursors could hold the result of a procedure that used OUT UNION. -- CQL could not itself consume its own results. To address this hole, we need an additional cursor type. The syntax is exactly the same as the statement cursor cases described above but, instead of holding a SQLite statement, the cursor holds a result set pointer and the current and maximum row numbers. Stepping through the cursor simply increments the row number and fetches the next row out of the rowset instead of from SQLite. Example: -- reading the above create proc reader() begin declare C cursor for call bar(); loop fetch C begin call printf(&quot;%d %s\\n&quot;, C.id, C.stuff); -- or whatever fields you need end; end;  If bar had been created with a SELECT, UNION ALL, and ORDER BY to merge the results, the above would have worked with C being a standard statement cursor, iterating over the union. Since foo produces a result set, CQL transparently produces a suitable cursor implementation behind the scenes, but otherwise the usage is the same. Note this is a lousy way to simply iterate over rows; you have to materialize the entire result set so that you can just step over it. Re-consuming like this is not recommended at all for production code, but it is ideal for testing result sets that were made withOUT UNION which otherwise would require C/C++ to test. Testing CQL with CQL is generally a lot easier. "},{"title":"Reshaping Data, Cursor LIKE forms​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#reshaping-data-cursor-like-forms","content":"There are lots of cases where you have big rows with many columns, and there are various manipulations you might need to do. What follows is a set of useful syntactic sugar constructs that simplify handling complex rows. The idea is that pretty much anywhere you can specify a list of columns you can instead use the LIKE x construct to get the columns as they appear in the shape x -- which is usually a table or a cursor. It’s a lot easier to illustrate with examples, even though these are, again, a bit contrived. First we need some table with lots of columns -- usually the column names are much bigger which makes it all the more important to not have to type them over and over, but in the interest of some brevity, here's a big table: create table big ( id integer primary key, id2 integer unique, a integer, b integer, c integer, d integer, e integer, f integer);  This example showcases several of the cursor and shape slicing features by emitting two related rows: create proc foo(id_ integer not null) begin -- this is the shape of the result we want -- it's some of the columns of &quot;big&quot; -- note this query doesn't run, we just use its shape to create a cursor -- with those columns. declare result cursor like select id, b, c, d from big; -- fetch the main row, specified by id_ -- main row has all the fields, including id2 declare main_row cursor for select * from big where id = id_; fetch main_row; -- now fetch the result columns out of the main row -- 'like result' means &quot;the column names found in 'result'&quot; fetch result from cursor main_row(like result); -- this is our first result row out union result; -- now we want the related row, but we only need two columns -- from the related row, 'b' and 'c' declare alt_row cursor for select b, c from big where big.id2 = main_row.id2; fetch alt_row; -- update some of the fields 'result' from the the new cursor update cursor result(like alt_row) from cursor alt_row; -- and emit the 2nd row out union result; end;  Now let's briefly discuss what is above. The two essential parts are: fetch result from cursor main_row(like result); and update cursor result(like alt_row) from cursor alt_row; In the first case what we're saying is that we want to load the columns of result from main_row but we only want to take the columns that are actually present in result. So this is a narrowing of a wide row into a smaller row. In this case, the smaller row, result, is what we want to emit. We needed the other columns to compute alt_row. The second case, what we're saying is that we want to update result by replacing the columns found in alt_row with the values in alt_row. So in this case we're writing a smaller cursor into part of a wider cursor. Note that we used the update cursor form here because it preserves all other columns. If we used fetch we would be rewriting the entire row contents, using NULL if necessary, and that is not desired here. Here is the rewritten version of the above procedure; this is what ultimately gets compiled into C. CREATE PROC foo (id_ INTEGER NOT NULL) BEGIN DECLARE result CURSOR LIKE SELECT id, b, c, d FROM big; DECLARE main_row CURSOR FOR SELECT * FROM big WHERE id = id_; FETCH main_row; FETCH result(id, b, c, d) FROM VALUES(main_row.id, main_row.b, main_row.c, main_row.d); OUT UNION result; DECLARE alt_row CURSOR FOR SELECT b, c FROM big WHERE big.id2 = main_row.id2; FETCH alt_row; UPDATE CURSOR result(b, c) FROM VALUES(alt_row.b, alt_row.c); OUT UNION result; END;  Of course you could have typed the above directly but if there are 50 odd columns it gets old fast and is very error prone. The sugar form is going to be 100% correct and will require much less typing and maintenance. Finally, while I've shown both LIKE forms separately, they can also be used together. For instance:  update cursor C(like X) from cursor D(like X);  The above would mean, &quot;move the columns that are found in X from cursor D to cursor C&quot;, presuming X has columns common to both. "},{"title":"Fetch Statement Specifics​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#fetch-statement-specifics","content":"Many of the examples used the FETCH statement in a sort of demonstrative way that is hopefully self-evident but the statement has many forms and so it's worth going over them specifically. Below we'll use the letters C and D for the names of cursors. Usually C; Fetch with Statement or Result Set Cursors​ A cursor declared in one of these forms: declare C cursor for select * from foo;declare C cursor for call foo(); (foo might end with a select or use out union) is either a statement cursor or a result set cursor. In either case the cursor moves through the results. You load the next row with: FETCH C, orFETCH C into x, y, z; In the first form C is said to be automatic in that it automatically declares the storage needed to hold all its columns. As mentioned above, automatic cursors have storage for their row. Having done this fetch you can use C as a scalar variable to see if it holds a row, e.g. declare C cursor for select * from foo limit 1; fetch C; if C then -- bingo we have a row call printf(&quot;%s\\n&quot;, C.whatever); end if  You can easily iterate, e.g. declare C cursor for select * from foo; loop fetch C begin -- one time for every row call printf(&quot;%s\\n&quot;, C.whatever); end;  Automatic cursors are so much easier to use than explicit storage that explicit storage is rarely seen. Storing to out parameters is one case where explicit storage actually is the right choice, as the out parameters have to be declared anyway. Fetch with Value Cursors​ A value cursor is declared in one of these ways: declare C cursor fetch from call foo(args) foo must be a procedure that returns one row with OUT declare C cursor like select 1 id, &quot;x&quot; name; declare C cursor like X; where X is the name of a table, a view, another cursor, or a procedure that returns a structured result A value cursor is always automatic; it's purpose is to hold a row. It doesn't iterate over anything but it can be re-loaded in a loop. fetch C or fetch C into ... is not valid on such a cursor, because it doesn't have a source to step through. The canonical way to load such a cursor is: fetch C from call foo(args); foo must be a procedure that returns one row with OUT fetch C(a,b,c...) from values(x, y, z); The first form is in some sense the origin of the value cursor. Value cursors were added to the language initially to provide a way to capture the single row OUT statement results, much like result set cursors were added to capture procedure results from OUT UNION. In the first form, the cursor storage (a C struct) is provided by reference as a hidden out parameter to the procedure and the procedure fills it in. The procedure may or may not use the OUT statement in its control flow, as the cursor might not hold a row. You can use if C then ... as before to test for a row. The second form is more interesting as it allows the cursor to be loaded from arbitrary expressions subject to some rules: you should think of the cursor as a logical row: it's either fully loaded or it's not, therefore you must specify enough columns in the column list to ensure that all NOT NULL columns will get a valueif not mentioned in the list, NULL will be loaded where possibleif insufficient columns are named, an error is generatedif the value types specified are not compatible with the column types mentioned, an error is generatedlater in this chapter, we'll show that columns can also be filled with dummy data using a seed value With this form, any possible valid cursor values could be set, but many forms of updates that are common would be awkward. So there are various forms of syntactic sugar that are automatically rewritten into the canonical form. See the examples below: fetch C from values(x, y, z) if no columns are specified this is the same as naming all the columns, in declared order fetch C from arguments the arguments to the procedure in which this statement appears are used as the values, in orderin this case C was also rewritten into C(a,b,c,..) etc. fetch C from arguments like C the arguments to the procedure in which this statement appears are used, by name, as the values, using the names of of the indicated shapethe order in which the arguments appeared no longer matters, the names that match the columns of C are used if presentthe formal parameter name may have a single trailing underscore (this is what like C would generate)e.g. if C has columns a and b then there must exist formals named a or a_ and b or b_, in any position fetch C(a,b) from cursor D(a,b) the named columns of D are used as the valuesin this case the statement becomes: fetch C(a,b) from values(D.a, D.b); That most recent form doesn't seem like it saves much, but recall the first rewrite: fetch C from cursor D both cursors are expanded into all their columns, creating a copy from one to the otherfetch C from D can be used if the cursors have the exact same column names and types; it also generates slightly better code and is a common case It is very normal to want to use only some of the columns of a cursor; these like forms do that job. We saw some of these forms in an earlier example. fetch C from cursor D(like C) here D is presumed to be &quot;bigger&quot; than C, in that it has all of the C columns and maybe more. The like C expands into the names of the C columns so C is loaded from the C part of Dthe expansion might be fetch C(a, b, g) from values (D.a, D.b, D.g)D might have had fields c, d, e, f which were not used because they are not in C. The symmetric operation, loading some of the columns of a wider cursor can be expressed neatly: fetch C(like D) from cursor D the like D expands into the columns of D causing the cursor to be loaded with what's in D and NULL (if needed)when expanded, this might look like fetch C(x, y) from values(D.x, D.y) LIKE can be used in both places, for instance suppose E is a shape that has a subset of the rows of both C and D. You can write a form like this: fetch C(like E) from cursor D(like E) this means take the column names found in E and copy them from D to C.the usual type checking is done As is mentioned above, the fetch form means &quot;load an entire row into the cursor&quot;. This is important because &quot;half loaded&quot; cursors would be semantically problematic. However there are many cases where you might like to amend the values of an already loaded cursor. You can do this with the update form. update cursor C(a,b,..) from values(1,2,..); the update form is a no-op if the cursor is not already loaded with values (!!)the columns and values are type checked so a valid row is ensured (or no row)all the re-writes above are legal so update cursor C(like D) from D is possible; it is in fact the use-case for which this was designed. "},{"title":"Calling Procedures with Argument Bundles​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#calling-procedures-with-argument-bundles","content":"It's often desirable to treat bundles of arguments as a unit, or cursors as a unit, especially when calling other procedures. The shape patterns above are very helpful for moving data between cursors, and the database. These can be rounded out with similar constructs for procedure definitions and procedure calls as follows. First we'll define some shapes to use in the examples. Note that we made U using T. create table T(x integer not null, y integer not null, z integer not null); create table U(like T, a integer not null, b integer not null);  We haven't mentioned this before but the implication of the above is that you can use the LIKE construct inside a table definition to add columns from a shape. We can also use the LIKE construct to create procedure arguments. To avoid conflicts with column names, when used this way the procedure arguments all get a trailing underscore appended to them. The arguments will be x_, y_, and z_ as we can see if the following: create proc p1(like T) begin call printf(&quot;%d %d %d\\n&quot;, x_, y_, z_); end;  Shapes can also be used in a procedure call, as showed below. This next example is obviously contrived, but of course it generalizes. It is exactly equivalent to the above. create proc p2(like T) begin call printf(&quot;%d %d %d\\n&quot;, from arguments); end;  Now we might want to chain these things together. This next example uses a cursor to call p1. create proc q1() begin declare C cursor for select * from T; loop fetch C begin /* this is the same as call p(C.x, C.y, C.z) */ call p1(from C); end; end;  The like construct allows you to select some of the arguments, or some of a cursor to use as arguments. This next procedure has more arguments than just T. The arguments will be x_, y_, z_, a_, b_. But the call will still have the T arguments x_, y_, and z_. create proc q2(like U) begin /* just the args that match T: so this is still call p(x_, y_, z_) */ call p1(from arguments like T); end;  Or similarly, using a cursor. create proc q3(like U) begin declare C cursor for select * from U; loop fetch C begin /* just the columns that match T so this is still call p(C.x, C.y, C.z) */ call p1(from C like T); end; end;  Note that the from argument forms do not have to be all the arguments. For instance you can get columns from two cursors like so:  call something(from C, from D)  All the varieties can be combined but of course the procedure signature must match. And all these forms work in function expressions as well as procedure calls. e.g.  set x := a_function(from C);  Since these forms are simply syntatic sugar, they can also appear inside of function calls that are in SQL statements. The variables mentioned will be expanded and become bound variables just like any other variable that appears in a SQL statement. Note the form x IN (from arguments) is not supported at this time, though this would be a relatively easy addition. "},{"title":"Using Named Argument Bundles​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#using-named-argument-bundles","content":"There are many cases where stored procedures require complex arguments using data shapes that come from the schema, or from other procedures. As we have seen the LIKE construct for arguments can help with this, but it has some limitations. Let's consider a specific example to study: create table Person ( id text primary key, name text not null, address text not null, birthday real );  To manage this table we might need something like this: create proc insert_person(like Person) begin insert into Person from arguments; end;  As we have seen, the above expands into: create proc insert_person( id_ text not null, name_ text not null, address_ text not null, birthday_ real) begin insert into Person(id, name, address, birthday) values(id_, name_, address_, birthday_); end;  It's clear that the sugared version is a lot easier to reason about than the fully expanded version, and much less prone to errors as well. This much is already helpful, but just those forms aren't general enough to handle the usual mix of situations. For instance, what if we need a procedure that works with two people? A hypothetical insert_two_people procedure cannot be written with the forms we have so far. To generalize this the language adds the notion of named argument bundles. The idea here is to name the bundles which provides a useful scoping. Example: create proc insert_two_people(p1 like Person, p2 like Person) begin -- using a procedure that takes a Person args call insert_person(from p1); call insert_person(from p2); end;  or alternatively create proc insert_two_people(p1 like Person, p2 like Person) begin -- inserting a Person directly insert into Person from p1; insert into Person from p2; end;  The above expands into: create proc insert_two_people( p1_id text not null, p1_name text not null, p1_address text not null, p1_birthday real, p2_id text not null, p2_name text not null, p2_address text not null, p2_birthday real) begin insert into Person(id, name, address, birthday) values(p1_id, p1_name, p1_address, p1_birthday); insert into Person(id, name, address, birthday) values(p2_id, p2_name, p2_address, p2_birthday); end;  Or course different named bundles can have different types -- you can create and name shapes of your choice. The language allows you to use an argument bundle name in all the places that a cursor was previously a valid source. That includes insert,fetch, update cursor, and procedure calls. You can refer to the arguments by their expanded name such as p1_address or alternatively p1.address -- they mean the same thing. Here's another example showing a silly but illustrative thing you could do: create proc insert_lotsa_people(P like Person) begin -- make a cursor to hold the arguments declare C cursor like P; -- convert arguments to a cursor fetch C from P; -- set up to patch the cursor and use it 20 times let i := 0; while i &lt; 20 begin update cursor C(id) from values(printf(&quot;id_%d&quot;, i)); insert into Person from C; set i := i + 1; end; end;  The above shows that you can use a bundle as the source of a shape, and you can use a bundle as a source of data to load a cursor. After which you can do all the usual value cursor things. Of course in this case the value cursor was redundant, we could just as easily have done something like this:  set P_id := printf(&quot;id_%d&quot;, i); insert into Person from P; set i := i + 1;  Note: the CQL JSON output includes extra information about procedure arguments if they originated as part of a shape bundle do identify the shape source for tools that might need that information. "},{"title":"The COLUMNS/LIKE construct in the SELECT statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-columnslike-construct-in-the-select-statement","content":"The select list of a SELECT statement already has complex syntax and functionality, but it is a very interesting place to use shapes. To make it possible to use shape notations and not confuse them with standard SQL the COLUMNS construct was added to the language. This allows for a sugared syntax for extracting columns in bulk. The COLUMNS clause is like of a generalization of the select T.* with shape slicing and type-checking. The forms are discussed below: Columns from a join table or tables​ This is the simplest form, it's just like T.*: -- same as A.* select columns(A) from ...; -- same as A.*, B.* select columns(A, B) from ...;  Columns from a particular joined table that match a shape​ This allows you to choose some of the columns of one table of the FROM clause. -- the columns of A that match the shape Foo select columns(A like Foo) from ...; -- get the Foo shape from A and the Bar shape from B select columns(A like Foo, B like Bar) from ...;  Columns from any that match a shape, from anywhere in the FROM​ Here we do not specify a particular table that contains the columns, the could come from any of the tables in the FROM clause. --- get the Foo shape from anywhere in the join select columns(like Foo) from ...; -- get the Foo and Bar shapes, from anywhere in the join select columns(like Foo, like Bar) from ...;  Specific columns​ This form allows you to slice out a few columns without defining a shape, you simply list the exact columns you want. -- T1.x and T2.y plus the Foo shape select columns(T1.x, T2.y, like Foo) from ...;  Distinct columns​ Its often the case that there are duplicate column names in the FROM clause. For instance, you could join A to B with both having a column pk. The final result set can only have one column named pk, the distinct clause helps you to get distinct column names. In this context distinct is about column names, not values. -- removes duplicate column names -- e.g. there will be one copy of 'pk' select columns(distinct A, B) from A join B using(pk); -- if both Foo and Bar have an (e.g.) 'id' field you only get one copy select columns(distinct like Foo, like Bar) from ...;  If a specific column is mentioned it is always included, but later expressions that are not a specific column will avoid that column name. -- if F or B has an x it won't appear again, just T.x select columns(distinct T.x, F like Foo, B like Bar) from F, B ..;  Of course this is all just sugar, so it all compiles to a column list with table qualifications -- but the syntax is very powerful. You can easily narrowin a wide table, or fusing joins that share common keys. -- just the Foo columns select columns(like Foo) from Superset_Of_Foo_From_Many_Joins_Even; -- only one copy of 'pk' select columns(distinct A,B,C) from A join B using (pk) join C using (pk);  And of course you can define shapes however you like and then use them to slice off column chucks of your choice. There are many ways to build up shapes from other shapes. For instance, you can declare procedures that return the shape you want and never actually create the procedure -- a pattern is very much like a shape &quot;typedef&quot;. E.g. declare proc shape1() (x integer, y real, z text); declare proc shape2() (like shape1, u bool, v bool);  With this combination you can easily define common column shapes and slice them out of complex queries without having to type the columns names over and over. "},{"title":"Missing Data Columns, Nulls and Dummy Data​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#missing-data-columns-nulls-and-dummy-data","content":"What follows are the rules for columns that are missing in an INSERT, or FETCH statement. As with many of the other things discussed here, the forms result in automatic rewriting of the code to include the specified dummy data. So SQLite will never see these forms. Two things to note: First, the dummy data options described below are really only interesting in test code, it's hard to imagine them being useful in production code. Second, none of what follows applies to the update cursor statement because its purpose is to do partial updates on exactly the specified columns and we're about to talk about what happens with the columns that were not specified. When fetching a row all the columns must come from somewhere; if the column is mentioned or mentioned by rewrite then it must have a value mentioned, or a value mentioned by rewrite. For columns that are not mentioned, a NULL value is used if it is legal to do so. For example, fetch C(a) from values(1) might turn into fetch C(a,b,c,d) from values (1, NULL, NULL, NULL) In addition to the automatic NULL you may add the annotation @dummy_seed([long integer expression]). If this annotion is present then: the expression is evaluated and stored in the hidden variable seedall integers, and long integers get seed as their value (possibly truncated)booleans get 1 if and only if seed is non-zerostrings get the name of the string column an underscore and the value as text (e.g. &quot;myText7&quot; if _seed is 7)blobs are not currently supported for dummy data (CQL is missing blob conversions which are needed first) This construct is hugely powerful in a loop to create many complete rows with very little effort, even if the schema change over time. declare i integer not null; declare C like my_table; set i := 0; while (i &lt; 20) begin fetch C(id) from values(i+10000) @dummy_seed(i); insert into my_table from cursor C; end;  Now in this example we don't need to know anything about my_table other than that it has a column named id. This example shows several things: we got the shape of the cursor from the table we were inserting intoyou can do your own computation for some of the columns (those named) and leave the unnamed values to be defaultedthe rewrites mentioned above work for the insert statement as well as fetchin fact insert into my_table(id) values(i+10000) @dummy_seed(i) would have worked too with no cursor at all bonus, dummy blob data does work in insert statements because SQLite can do the string conversion easilythe dummy value for a blob is a blob that holds the text of the column name and the text of the seed just like a string column The @dummy_seed form can be modified with @dummy_nullables, this indicates that rather than using NULL for any nullable value that is missing, CQL should use the seed value. This overrides the default behavior of using NULL where columns are needed. Note the NULL filling works a little differently on insert statements. Since SQLite will provide a NULL if one is legal the column doesn't have to be added to the list with a NULL value during rewriting, it can simply be omitted, making the statement smaller. Finally for insert statement only, SQLite will normally use the default value of a column if it has one, so there is no need to add missing columns with default values to the insert statement. However if you specify @dummy_defaults then columns with a default value will instead be rewritten and they will get _seed_ as their value. Some examples. Suppose columns a, b, c are not null; m, n are nullable; and x, y have defaults. -- as written insert into my_table(a) values(7) @dummy_seed(1000) -- rewrites to insert into my_table(a, b, c) values(7, 1000, 1000);  -- as written insert into my_table(a) values(7) @dummy_seed(1000) @dummy_nullables -- rewrites to insert into my_table(a, b, c, m, n) values(7, 1000, 1000, 1000, 1000);  -- as written insert into my_table(a) values(7) @dummy_seed(1000) @dummy_nullables @dummy_defaults -- rewrites to insert into my_table(a, b, c, m, n, x, y) values(7, 1000, 1000, 1000, 1000, 1000, 1000);  The sugar features on fetch, insert, and update cursor are as symmetric as possible, but again, dummy data is generally only interesting in test code. Dummy data will continue to give you valid test rows even if columns are added or removed from the tables in question. "},{"title":"Generalized Cursor Lifetimes aka Cursor \"Boxing\"​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#generalized-cursor-lifetimes-aka-cursor-boxing","content":"Generalized Cursor Lifetime refers to capturing a Statement Cursor in an object so that it can used more flexibly. Wrapping something in an object is often called &quot;boxing&quot;. Since Generalized Cursor Lifetime is a mouthful we'll refer to it as &quot;boxing&quot; from here forward. The symmetric operation &quot;unboxing&quot; refers to converting the boxed object back into a cursor. The normal cursor usage pattern is by far the most common, a cursor is created directly with something like these forms: declare C cursor for select * from shape_source; declare D cursor for call proc_that_returns_a_shape();  At this point the cursor can be used normally as follows: loop fetch C begin -- do stuff with C end;  Those are the usual patterns and they allow statement cursors to be consumed sort of &quot;up&quot; the call chain from where the cursor was created. But what if you want some worker procedures that consume a cursor? There is no way to pass your cursor down again with these normal patterns alone. To generalize the patterns, allowing, for instance, a cursor to be returned as an out parameter or accepted as an in parameter you first need to declare an object variable that can hold the cursor and has a type indicating the shape of the cursor. To make an object that can hold a cursor: declare obj object&lt;T cursor&gt;;  Where T is the name of a shape. It can be a table name, or a view name, or it can be the name of the canonical procedure that returns the result. T should be some kind of global name, something that could be accessed with #include in various places. Referring to the examples above, choices for T might be shape_source the table or proc_that_returns_a_shape the procedure. Note: it's always possible make a fake procedure that returns a result to sort of &quot;typedef&quot; a shape name. e.g. declare proc my_shape() (id integer not null, name text);  The procedure here my_shape doesn’t have to actually ever be created, in fact it’s better if it isn't. It won’t ever be called; its hypothetical result is just being as a shape. This can be useful if you have several procedures like proc_that_returns_a_shapethat all return results with the columns of my_shape. To create the boxed cursor, first declare the object variable that will hold it and then set object from the cursor. Note that in the following example the cursor C must have the shape defined by my_shape or an error is produced. The type of the object is crucial because, as we'll see, during unboxing that type defines the shape of the unboxed cursor. -- recap: declare the box that holds the cursor (T changed to my_shape for this example) declare box_obj object&lt;my_shape cursor&gt;; -- box the cursor into the object (the cursor shape must match the box shape) set box_obj from cursor C;  The variable box_obj can now be passed around as usual. It could be stored in a suitable out variable or it could be passed to a procedure as an in parameter. Then, later, you can &quot;unbox&quot; box_obj to get a cursor back. Like so -- unboxing a cursor from an object, the type of box_obj defines the type of the created cursor declare D cursor for box_obj;  These primitives will allow cursors to be passed around with general purpose lifetime. Example: -- consumes a cursor create proc cursor_user(box_obj object&lt;my_shape cursor&gt;) begin declare C cursor for box_obj; -- the cursors shape will be my_shape matching box loop fetch C begin -- do something with C end; end; -- captures a cursor and passes it on create proc cursor_boxer() begin declare C cursor for select * from something_like_my_shape; declare box_obj object&lt;my_shape cursor&gt; set box from cursor C; -- produces error if shape doesn't match call cursor_user(box_obj); end;  Importantly, once you box a cursor the underlying SQLite statement’s lifetime is managed by the box object with normal retain/release semantics. The box and underlying statement can be released simply by setting all references to it to null as usual. With this pattern it's possible to, for instance, create a cursor, box it, consume some of the rows in one procedure, do some other stuff, and then consume the rest of the rows in another different procedure. Important Notes: the underlying SQLite statement is shared by all references to it. Unboxing does not reset the cursor's position. It is possible, even desirable, to have different procedures advancing the same cursorthere is no operation for &quot;peeking&quot; at a cursor without advancing it; if your code requires that you inspect the row and then delegate it, you can do this simply by passing the cursor data as a value rather than the cursor statement. Boxing and unboxing are for cases where you need to stream data out of the cursor in helper proceduresdurably storing a boxed cursor (e.g. in a global) could lead to all manner of problems -- it is exactly like holding on to a sqlite3_stmt * for a long time with all the same problems because that is exactly is happening Summarizing, the main reason for using the boxing patterns is to allow for standard helper procedures that can get a cursor from a variety of places and process it. Boxing isn’t the usual pattern at all and returning cursors in a box, while possible, should be avoided in favor of the simpler patterns, if only because then then lifetime management is very simple in all those cases. "},{"title":"Chapter 6: Calling Procedures Defined Elsewhere​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-6-calling-procedures-defined-elsewhere","content":"CQL generally doesn't see the whole world in one compilation. In this way it's a lot more like, say, the C compiler than it is like, say, Java or C# or something like that. This means several things: You don't have to tell CQL about all your schema in all your files, so particular stored procs can be more encapsulatedYou can have different databases mounted in different places and CQL won't care; you provide the database connection to the stored procedures when you call them, and that database is assumed to have the tables declared in this translation unitSeveral different schema can be maintained by CQL, even in the same database, and they won't know about each other To make this possible there are a few interesting features "},{"title":"Declaring Procedures Defined Elsewhere​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#declaring-procedures-defined-elsewhere","content":"Stored procedures defined in another file can be declared to CQL in various ways for each major type of stored procedure. These are covered in the sections below. "},{"title":"Simple Procedures (database free):​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#simple-procedures-database-free","content":"DECLARE PROCEDURE foo(id integer, out name text not null);  This introduces the symbol name without providing the body. This has important variations. "},{"title":"Procedures that use the database​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedures-that-use-the-database","content":"DECLARE PROCEDURE foo(id integer, out name text not null) USING TRANSACTION;  Most procedures you write will use SQLite in some fashion, maybe a select or something. The USING TRANSACTION annotation indicates that the proc in question uses the database and therefore the generated code will need a database connection in-argument and it will return a SQLite error code. "},{"title":"Procedures that create a result set​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedures-that-create-a-result-set","content":"If the procedure in question is going to use select or call to create a result set, the type of that result set has to be declared. An example might look like this: DECLARE PROC with_result_set () (id INTEGER NOT NULL, name TEXT, rate LONG INTEGER, type INTEGER, size REAL);  This says that the procedure takes no arguments (other than the implicit database connection) and it has an implicit out-argument that can be read to get a result set with the indicated columns: id, name, rate, type, and size. This form implies USING TRANSACTION. "},{"title":"Procedures that return a single row with a value cursor​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedures-that-return-a-single-row-with-a-value-cursor","content":"If the procedure emits a cursor with the OUT statement to produce a single row then it can be declared as follows: DECLARE PROC with_result_set () OUT (id INTEGER NOT NULL, name TEXT, rate LONG INTEGER, type INTEGER, size REAL);  This form can have USING TRANSACTION or not, since it is possible to emit a row with a value cursor and never use the database. See the previous chapter for details on the OUT statement. "},{"title":"Procedures that return a full result set​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedures-that-return-a-full-result-set","content":"If the procedure emits many rows with the OUT UNION statement to produce a full result set then it can be declared as follows: DECLARE PROC with_result_set () OUT UNION (id INTEGER NOT NULL, name TEXT, rate LONG INTEGER, type INTEGER, size REAL);  This form can have USING TRANSACTION or not, since it is possible to emit a rows with a value cursor and never use the database. See the previous chapter for details on the OUT UNION statement. "},{"title":"Exporting Declared Symbols Automatically​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#exporting-declared-symbols-automatically","content":"To avoid errors, the declarations for any given file can be automatically created by adding something like --generate_exports to the command line. This will require an additonal file name to be passed in the --cg portion to capture the exports. That file can then be used with #include when you combine the C pre-processor with CQL as is normally done. Nomenclature is perhaps a bit weird here. You use --generate_exports to export the stored procedure declarations from the translation units. Of course those exported symbols are what you then import in some other module. Sometimes this output file is called foo_imports.sql because those exports are of course exactly what you need to import foo. You can use whatever convention you like of course, CQL doesn't care. The full command line might look something like this: cql --in foo.sql --cg foo.h foo.c foo_imports.sql --generate_exports  Using the pre-processor you can get declarations from elsewhere with a directive like this: #include &quot;foo_imports.sql&quot;  "},{"title":"Declaration Examples​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#declaration-examples","content":"Here are some more examples directly from the CQL test cases; these are all auto-generated with --generate_exports. DECLARE PROC test (i INTEGER NOT NULL); DECLARE PROC out_test (OUT i INTEGER NOT NULL, OUT ii INTEGER); DECLARE PROC outparm_test (OUT foo INTEGER NOT NULL) USING TRANSACTION; DECLARE PROC select_from_view () (id INTEGER NOT NULL, type INTEGER); DECLARE PROC make_view () USING TRANSACTION; DECLARE PROC copy_int (a INTEGER, OUT b INTEGER); DECLARE PROC complex_return () (_bool BOOL NOT NULL, _integer INTEGER NOT NULL, _longint LONG INTEGER NOT NULL, _real REAL NOT NULL, _text TEXT NOT NULL, _nullable_bool BOOL); DECLARE PROC outint_nullable ( OUT output INTEGER, OUT result BOOL NOT NULL) USING TRANSACTION; DECLARE PROC outint_notnull ( OUT output INTEGER NOT NULL, OUT result BOOL NOT NULL) USING TRANSACTION; DECLARE PROC obj_proc (OUT an_object OBJECT); DECLARE PROC insert_values ( id_ INTEGER NOT NULL, type_ INTEGER) USING TRANSACTION;  So far we've avoided discussing the generated C code in any details but here it seems helpful to show exactly what these declarations correspond to in the generated C to demystify all this. There is a very straightforward conversion. void test(cql_int32 i); void out_test( cql_int32 *_Nonnull i, cql_nullable_int32 *_Nonnull ii); cql_code outparm_test( sqlite3 *_Nonnull _db_, cql_int32 *_Nonnull foo); cql_code select_from_view_fetch_results( sqlite3 *_Nonnull _db_, select_from_view_result_set_ref _Nullable *_Nonnull result_set); cql_code make_view(sqlite3 *_Nonnull _db_); void copy_int(cql_nullable_int32 a, cql_nullable_int32 *_Nonnull b); cql_code complex_return_fetch_results( sqlite3 *_Nonnull _db_, complex_return_result_set_ref _Nullable *_Nonnull result_set); cql_code outint_nullable( sqlite3 *_Nonnull _db_, cql_nullable_int32 *_Nonnull output, cql_bool *_Nonnull result); cql_code outint_notnull( sqlite3 *_Nonnull _db_, cql_int32 *_Nonnull output, cql_bool *_Nonnull result); void obj_proc( cql_object_ref _Nullable *_Nonnull an_object); cql_code insert_values( sqlite3 *_Nonnull _db_, cql_int32 id_, cql_nullable_int32 type_);  As you can see, these declarations use exactly the normal SQLite types and so it is very easy to declare a procedure in CQL and then implement it yourself in straight C, simply by conforming to the contract. Importantly, SQLite does not know anything about CQL stored procedures, or anything at all about CQL really so CQL stored procedure names cannot be used in any way in SQL statements. CQL control flow like the call statement can be used to invoke other procedures and results can be captured by combing the OUT statement and a DECLARE CURSOR construct but SQLite is not involved in those things. This is another place where the inherent two-headed nature of CQL leaks out. Finally, this is a good place to reinforce that procedures with any of the structured result types (select, out, out union) can be used with a suitable cursor. create procedure get_stuff() begin select * from stuff; end;  Can be used in two interesting ways: create procedure meta_stuff(meta bool) begin if meta then call get_stuff(); else call get_other_stuff(); end if; end;  Assuming that get_stuff and get_other_stuff have the same shape, then this procedure simply passes on one or the other's result set unmodified as its own return value. But you could do more than simply pass on the result. create procedure meta_stuff(meta bool) begin declare C cursor for call get_stuff(); -- or get_meta_stuff(...) loop fetch C begin -- do stuff with C -- may be out union some of the rows after adjustment even end; end;  Here we can see that we used the procedure to get the results and then process them directly somehow. And of course the result of an OUT can similarly be processed using a value cursor, as previously seen. These combinations allow for pretty general composition of stored procedures so long as they are not recombined with SQLite statements. "},{"title":"Chapter 7: CQL Result Sets​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-7-cql-result-sets","content":"Most of this tutorial is about the CQL language itself but here we must diverge a bit. The purpose of the result set feature of CQL is to create a C interface to SQLite data. Because of this there are a lot of essential details that require looking carefully at the generated C code. Appendix 2 covers this code in even more detail but here it makes sense to at least talk about the interface. Let's say we have this simple stored procedure: create table foo(id integer not null, b bool, t text); create proc read_foo(id_ integer not null) begin select * from foo where id = id_; end;  We've created a simple data reader: this CQL code will cause the compiler to generate helper functions to read the data and materialize a result set. Let's look at the public interface of that result set now considering the most essential pieces. /* this is almost everything in the generated header file */ #define read_foo_data_types_count 3 cql_result_set_type_decl( read_foo_result_set, \\ read_foo_result_set_ref); extern cql_int32 read_foo_get_id(read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_bool read_foo_get_b_is_null(read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_bool read_foo_get_b_value(read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nullable read_foo_get_t( read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 read_foo_result_count(read_foo_result_set_ref _Nonnull result_set); extern cql_code read_foo_fetch_results(sqlite3 *_Nonnull _db_, read_foo_result_set_ref _Nullable *_Nonnull result_set, cql_int32 id_); #define read_foo_row_hash(result_set, row) \\ cql_result_set_get_meta((cql_result_set_ref)(result_set))-&gt;\\ rowHash((cql_result_set_ref)(result_set), row) #define read_foo_row_equal(rs1, row1, rs2, row2) \\ cql_result_set_get_meta((cql_result_set_ref)(rs1)) \\ -&gt;rowsEqual( \\ (cql_result_set_ref)(rs1), row1, \\ (cql_result_set_ref)(rs2), row2)  Let's consider some of these individually now cql_result_set_type_decl( read_foo_result_set, read_foo_result_set_ref);  This declares the data type for read_foo_result_set and the associated object reference read_foo_result_set_ref. As it turns out, the underlying data type for all result sets is the same, and only the shape of the data varies. extern cql_code read_foo_fetch_results(sqlite3 *_Nonnull _db_, read_foo_result_set_ref _Nullable *_Nonnull result_set, cql_int32 id_);  The result set fetcher method gives you a read_foo_result_set_ref if it succeeds. It accepts the id_ argument which it will internally pass along to read_foo(...). The latter function provides a sqlite3_stmt* which can then be iterated in the fetcher. This method is the main public entry point for result sets. Once you have a result set, you can read values out of it. extern cql_int32 read_foo_result_count(read_foo_result_set_ref _Nonnull result_set);  That function tells you how many rows are in the result set. For each row you can use any of the row readers: extern cql_int32 read_foo_get_id(read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_bool read_foo_get_b_is_null(read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_bool read_foo_get_b_value(read_foo_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nullable read_foo_get_t( read_foo_result_set_ref _Nonnull result_set, cql_int32 row);  These let you read the id of a particular row, and get a cql_int32 or you can read the nullable boolean, using the read_foo_get_b_is_null function first to see if the boolean is null and then read_foo_get_b_valueto get the value. Finally the string can be accessed with read_foo_get_t. As you can see, there is a simple naming convention for each of the field readers. Note: The compiler has runtime arrays that control naming conventions as well as using CamelCasing. Additional customizations may be created by adding new runtime arrays into the CQL compiler. Finally, also part of the public interface, are these macros: #define read_foo_row_hash(result_set, row) #define read_foo_row_equal(rs1, row1, rs2, row2)  These use the CQL runtime to hash a row or compare two rows from identical result set types. Metadata included in the result set allows general purpose code to work for every result set. Based on configuration, result set copying methods can also be generated. When you're done with a result set you can use the cql_release(...)method to free the memory. Importantly, all of the rows from the query in the stored procedure are materialized immediately and become part of the result set. Potentially large amounts of memory can be used if a lot of rows are generated. The code that actually creates the result set starting from the prepared statement is always the same. The essential parts are: First, a constant array that holds the data types for each column. uint8_t read_foo_data_types[read_foo_data_types_count] = { CQL_DATA_TYPE_INT32 | CQL_DATA_TYPE_NOT_NULL, // id CQL_DATA_TYPE_BOOL, // b CQL_DATA_TYPE_STRING, // t };  All references are stored together at the end of the row, so we only need the count of references and the offset of the first one to do operations like cql_retain or cql_releaseon the row. #define read_foo_refs_offset cql_offsetof(read_foo_row, t) // count = 1  Lastly we need metadata to tell us count of columns and the offset of each column within the row. static cql_uint16 read_foo_col_offsets[] = { 3, cql_offsetof(read_foo_row, id), cql_offsetof(read_foo_row, b), cql_offsetof(read_foo_row, t) };  Using the above we can now write this fetcher CQL_WARN_UNUSED cql_code read_foo_fetch_results( sqlite3 *_Nonnull _db_, read_foo_result_set_ref _Nullable *_Nonnull result_set, cql_int32 id_) { sqlite3_stmt *stmt = NULL; cql_profile_start(CRC_read_foo, &amp;read_foo_perf_index); // we call the original procedure, it gives us a prepared statement cql_code rc = read_foo(_db_, &amp;stmt, id_); // this is everything you need to know to fetch the result cql_fetch_info info = { .rc = rc, .db = _db_, .stmt = stmt, .data_types = read_foo_data_types, .col_offsets = read_foo_col_offsets, .refs_count = 1, .refs_offset = read_foo_refs_offset, .rowsize = sizeof(read_foo_row), .crc = CRC_read_foo, .perf_index = &amp;read_foo_perf_index, }; // this function does all the work, it cleans up if .rc is an error code. return cql_fetch_all_results(&amp;info, (cql_result_set_ref *)result_set); }  "},{"title":"Results Sets From OUT UNION​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#results-sets-from-out-union","content":"The out keyword was added for writing procedures that produce a single row result set. With that, it became possible to make any single row result you wanted, assembling it from whatever sources you needed. That is an important case as single row results happen frequently and they are comparatively easy to create and pass around using C structures for the backing store. However, it's not everything; there are also cases where full flexibility is needed while producing a standard many-row result set. For this we have out union which was discussed fully in Chapter 5. Here we'll discuss the code generation behind that. Here’s an example from the CQL tests: create proc some_integers(start integer not null, stop integer not null) begin declare C cursor like select 1 v, 2 v_squared, &quot;xx&quot; some_text; declare i integer not null; set i := start; while (i &lt; stop) begin fetch C(v, v_squared, junk) from values (i, i*i, printf(&quot;%d&quot;, i)); out union C; set i := i + 1; end; end;  In this example the entire result set is made up out of thin air. Of course any combination of this computation or data-access is possible, so you can ultimately make any rows you want in any order using SQLite to help you as much or as little as you need. Virtually all the code pieces to do this already exist for normal result sets. The important parts of the output code look like this in your generated C. We need a buffer to hold the rows we are going to accumulate; We use cql_bytebuf just like the normal fetcher above. // This bit creates a growable buffer to hold the rows // This is how we do all the other result sets, too cql_bytebuf _rows_; cql_bytebuf_open(&amp;_rows_);  We need to be able to copy the cursor into the buffer and retain any internal references // This bit is what you get when you &quot;out union&quot; a cursor &quot;C&quot; // first we +1 any references in the cursor then we copy its bits cql_retain_row(C_); // a no-op if there is no row in the cursor if (C_._has_row_) cql_bytebuf_append(&amp;_rows_, (const void *)&amp;C_, sizeof(C_));  Finally, we make the rowset when the procedure exits. If the procedure is returning with no errors the result set is created, otherwise the buffer is released. The global some_integers_info has constants that describe the shape produced by this procedure just like the other cases that produce a result set. cql_results_from_data(_rc_, &amp;_rows_, &amp;some_integers_info, (cql_result_set_ref *)_result_set_);  The operations here are basically the same ones that will happen inside of the standard helpercql_fetch_all_results, the difference, of course, is that you write the loop manually and therefore have full control of the rows as they go in to the result set. In short, the overhead is pretty low. What you’re left with is pretty much the base cost of your algorithm. The cost here is very similar to what it would be for any other thing that make rows. Of course, if you make a million rows, well, that would burn a lot of memory. "},{"title":"A Working Example​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#a-working-example","content":"Here's a fairly simple example illustrating some of these concepts including the reading of rowsets. -- hello.sql: create proc hello() begin create table my_data( pos integer not null primary key, txt text not null ); insert into my_data values(2, 'World'); insert into my_data values(0, 'Hello'); insert into my_data values(1, 'There'); select * from my_data order by pos; end;  And this main code to open the database and access the procedure: // main.c #include &lt;stdlib.h&gt; #include &lt;sqlite3.h&gt; #include &quot;hello.h&quot; int main(int argc, char **argv) { sqlite3 *db; int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db); if (rc != SQLITE_OK) { exit(1); /* not exactly world class error handling but that isn't the point */ } hello_result_set_ref result_set; rc = hello_fetch_results(db, &amp;result_set); if (rc != SQLITE_OK) { printf(&quot;error: %d\\n&quot;, rc); exit(2); } cql_int32 result_count = hello_result_count(result_set); for(cql_int32 row = 0; row &lt; result_count; row++) { cql_string_ref text = hello_get_txt(result_set, row); cql_alloc_cstr(ctext, text); printf(&quot;%d: %s\\n&quot;, row, ctext); cql_free_cstr(ctext, text); } cql_result_set_release(result_set); sqlite3_close(db); }  From these pieces you can make a working example like so: # ${cgsql} refers to the root directory of the CG-SQL sources # cql --in hello.sql --cg hello.h hello.c cc -o hello -I ${cgsql}/sources main.c hello.c ${cgsql}/sources/cqlrt.c -lsqlite3 ./hello  Additional demo code is available in Appendix 10. "},{"title":"Nested Result Sets (Parent/Child)​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#nested-result-sets-parentchild","content":"There are many cases where you might want to nest one result set inside of another one. In order to do this ecomomically you must be able to run a parent query and a child query and then link the child rows to the parent rows. One way to do this is of course to run one query for each &quot;child&quot; but then you end up with O(n) child queries and if there are sub-children it would beO(n*m) and so forth. What you really want to do here is something more like a join, only without the cross-product part of the join. Many systems have such features, sometimes they are called &quot;chaptered rowsets&quot; but in any case there is a general need for such a thing. To reasonably support nested results sets the CQL language has to be extended a variety of ways, as discussed below. Here are some things that happened along the way that are interesting. Cursor Types and Result Types​ One of the first problems we run into thinking about how a CQL program might express pieces of a rowset and turn them into child results is that a program must be able to hash a row, append row data, and extract a result set from a key. These are the essential operations required. In order to do anything at all with a child rowset, a program must be able to describe its type. Result sets must appear in the type system as well as in the runtime. To address this we use an object type with a special &quot;kind&quot;, similar to how boxed statements are handled. A result set has a type that looks like this: object &lt;proc_name set&gt;. Here proc_name must the the name of a procedure that returns a result set and the object will represent a result set with the corresponding columns in it. Creating New Cursor Types From Existing Cursor Types​ In addition to creating result set types, the language must be able to express cursors that capture the necessary parent/child column. These are rows with all of the parent columns plus additional columns for the child rows (note that you can have more than one child result set per parent). So for instance you might have a list of people, and one child result might be the details of the schools they attended and another could be the details of the jobs they worked. To accomplish this kind of shape, the language must be able to describe a new output row is that is the same as the parent but includes columns for the the child results, too. This is done using a cursor declaration that comes from a typed name list. An example might be: declare C cursor like (id integer, name text);  Importantly, such constructs include the ability to reference existing shapes by name. So we might create a cursor we need like so: declare result cursor like (like parent_proc, child_result object&lt;child_proc set&gt;);  Where the above indicates all the parent columns plus a child result set. Or more than one child result set if needed. In addition, the language needs a way to conveniently declare a cursor that is only some of the columns of an existing cursor. In particular, nested result sets require us to extract the columns that link the parent and child result sets. The columns we will &quot;join&quot; on. To accomplish this the language extends the familiar notion: declare D cursor like C;  To the more general form: declare pks cursor like C(pk1, pk2);  Which chooses just the named fields from C and makes a cursor with only those. In this case this primary key fields, pk1 and pk2. Additionally, for completeness, we add this form: declare vals cursor like C(-pk1, -pk2);  To mean the cursor vals should have all the columns of C except pk1 and pk2 i.e. all the &quot;values&quot;. Using any number of intermediate construction steps, and maybe some declare X type ... statements, any type can be formed from existing shapes by adding and removing columns. Having done the above we can load a cursor that has just the primary keys with the usual form fetch pks from C(like pks);  Which says we want to load pks from the fields of C, but using only the columns of pks. That operation is of course going to be an exact type match by construction. Cursor Arguments​ In order to express the requisite parent/child join, the language must be able to express operations like &quot;hash a cursor&quot; (any cursor) or &quot;store this row into the appropriate partition&quot;. The language provides no way to write functions that can take any cursor and dynamically do things to it based on type information, but: we don't need very many of them,it's pretty easy to do that job in C (or lua if lua codegen is being used) The minimum requirement is that the language must be able to declare a functions that takes a generic cursor argument and to call such functions a generic cursor construct that has the necessary shape info. This form does the job: declare function cursor_hash(C cursor) long not null;  And it can be used like so: let hash := cursor_hash(C); -- C is any cursor  When such a call is made the C function cursor_hash is passed a so-called &quot;dynamic cursor&quot; pointer which includes: a pointer to the data for the cursorthe count of fieldsthe names of the fieldsthe type/offset of every field in the cursor With this information you can (e.g.) generically do the hash by applying a hash to each field and then combining all of those hashes. This kind of function works on any cursor and all the extra data about the shape that's needed to make the call is static, so really the cost of the call stays modest. Details of the dynamic cursor type are incqlrt_common.h and there are many example functions now in the cqlrt_common.c file. The Specific Parent/Child Functions​ Three helper functions are used to do the parent/child join, they are: DECLARE FUNC cql_partition_create () CREATE OBJECT&lt;partitioning&gt; NOT NULL; DECLARE FUNC cql_partition_cursor ( part OBJECT&lt;partitioning&gt; NOT NULL, key CURSOR, value CURSOR) BOOL NOT NULL; DECLARE FUNC cql_extract_partition ( part OBJECT&lt;partitioning&gt; NOT NULL, key CURSOR) CREATE OBJECT NOT NULL;  The first function makes a new partitioning. The second function hashes the key columns of a cursor (specified by the key argument) and appends the values provided in the second argument into a bucket for that key. By making a pass over the child rows a procedure can easily create a partitioning with each unique key combo having a buffer of all the matching rows. The third function is used once the partitioning is done. Given a key again, this time from the parent rows, a procedure can get the buffer it had accumulated and then make a result set out of it and return that. Note that the third function returns a vanilla object type because it could be returning a result set of any shape so a cast is required for correctness. Result Set Sugar​ Using the features mentioned above a developer could now join together any kind of complex parent and child combo as needed, but the result would be a lot of error-prone code, To avoid this CQL adds language sugar to do such partitionings automatically and type-safely, like so: -- parent and child defined elsewhere declare proc parent(x integer not null) (id integer not null, a integer, b integer); declare proc child(y integer not null) (id integer not null, u text, v text); -- join together parent and child using 'id' -- example x_, y_ arguments for illustration only create proc parent_child(x_ integer not null, y_ integer not null) begin out union call parent(x_) join call child(y_) using (id); end;  The generated code is simple enough, even though there's a good bit of it. But it's a useful exercise to look at it once. Comments added for clarity. CREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL) BEGIN DECLARE __result__0 BOOL NOT NULL; -- we need a cursor to hold just the key of the child row DECLARE __key__0 CURSOR LIKE child(id); -- we need our partitioning object (there could be more than one per function -- so it gets a number, likewise everything else gets a number LET __partition__0 := cql_partition_create(); -- we invoke the child and then iterate its rows DECLARE __child_cursor__0 CURSOR FOR CALL child(y_); LOOP FETCH __child_cursor__0 BEGIN -- we extract just the key fields (id in this case) FETCH __key__0(id) FROM VALUES(__child_cursor__0.id); -- we add this child to the partition using its key SET __result__0 := cql_partition_cursor(__partition__0, __key__0, __child_cursor__0); END; -- we need a shape for our result, it is the columns of the parent plus the child rowset DECLARE __out_cursor__0 CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER, child1 OBJECT&lt;child SET&gt; NOT NULL); -- now we call the parent and iterate it DECLARE __parent__0 CURSOR FOR CALL parent(x_); LOOP FETCH __parent__0 BEGIN -- we load the key values out of the parent this time, same key fields FETCH __key__0(id) FROM VALUES(__parent__0.id); -- now we create a result row using the parent columns and the child result set FETCH __out_cursor__0(id, a, b, child1) FROM VALUES(__parent__0.id, __parent__0.a, __parent__0.b, cql_extract_partition(__partition__0, __key__0)); -- and then we emit that row OUT UNION __out_cursor__0; END; END;  This code iterates the child once and the parent once and only has two database calls, one for the child and one for the parent. And this is enough to create parent/child result sets for the most common examples. Result Set Values​ While the above is probably the most common case, a developer might also want to make a procedure call for each parent row to compute the child. And, more generally, to work with result sets from procedure calls other than iterating them with a cursor. The iteration pattern: declare C cursor for call foo(args);  is very good if the data is coming from (e.g.) a select statement and we don't want to materialize all of the results if we can stream instead. However, when working with result sets the whole point is to create materialized results for use elsewhere. Since we can express a result set type with object&lt;proc_name set&gt; the language also includes the ability to call a procedure that returns a result set and capture that result. This yields these forms: declare child_result object&lt;child set&gt;; set child_result := child(args);  or better still: let child_result := child(args);  And more generally, this examples shows a manual iteration: declare proc parent(x integer not null) (id integer not null, a integer, b integer); declare proc child(id integer not null) (id integer not null, u text, v text); create proc parent_child(x_ integer not null, y_ integer not null) begin -- the result is like the parent with an extra column for the child declare result cursor like (like parent, child object&lt;child set&gt;); -- call the parent and loop over the results declare P cursor for call parent(x_); loop fetch P begin -- compute the child for each P and then emit it fetch result from values(from P, child(P.id)); out union result; end; end;  After the sugar is applied to expand the types out, the net program is the following: DECLARE PROC parent (x INTEGER NOT NULL) (id INTEGER NOT NULL, a INTEGER, b INTEGER); DECLARE PROC child (id INTEGER NOT NULL) (id INTEGER NOT NULL, u TEXT, v TEXT); CREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL) BEGIN DECLARE result CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER, child OBJECT&lt;child SET&gt;); DECLARE P CURSOR FOR CALL parent(x_); LOOP FETCH P BEGIN FETCH result(id, a, b, child) FROM VALUES(P.id, P.a, P.b, child(P.id)); OUT UNION result; END; END;  Note the LIKE and FROM forms are make it a lot easier to express this notion of just adding one more column to the result. The code for emitting the parent_childresult doesn't need to specify the columns of the parent or the columns of the child, only that the parent has at least the id column. Even that could have been removed. This call could have been used instead: fetch result from values(from P, child(from P like child arguments));  That syntax would result in using the columns of P that match the arguments of child -- justP.id in this case. But if there were many such columns the sugar would be easier to understand and much less error prone. Generated Code Details​ Normally all result sets that have an object type in them use a generic object cql_object_refas their C data type. This isn't wrong exactly but it would mean that a cast would be required in every use case on the native side, and it's easy to get the cast wrong. So the result type of column getters is adjusted to be a child_result_set_ref instead of just cql_object_refwhere child is the name of the child procedure. "},{"title":"Chapter 8: Functions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-8-functions","content":"CQL stored procs have a very simple contract so it is easy to declare procedures and then implement them in regular C; the C functions just have to conform to the contract. However, CQL procedures have their own calling conventions and this makes it very inconvenient to use external code that is not doing database things and wants to return values. Even a random number generator or something would be difficult to use because it could not be called in the context of an expression. To allow for this CQL adds declared functions In another example of the two-headed nature of CQL, there are two ways to declare functions. As we have already seen you can make function-like procedures and call them like functions simply by making a procedure with an out parameter. However, there are also cases where it is reasonable to make function calls to external functions of other kinds. There are three major types of functions you might wish to call. "},{"title":"Function Types​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#function-types","content":"Ordinary Scalar Functions​ These functions are written in regular C and provide for the ability to do operations on in-memory objects. For instance, you could create functions that allow you to read and write from a dictionary. You can declare these functions like so: declare function dict_get_value(dict object, key_ text not null) text;  Such a function is not known to SQLite and therefore cannot appear in SQL statements. CQL will enforce this. The above function returns a text reference, and, importantly, this is a borrowed reference. The dictionary is presumably holding on to the reference and as long as it is not mutated the reference is valid. CQL will retain this reference as soon as it is stored and release it automatically when it is out of scope. So, in this case, the dictionary continues to own the object. It is also possible to declare functions that create objects. Such as this example: declare function dict_create() create object;  This declaration tells CQL that the function will create a new object for our use. CQL does not retain the provided object, rather assuming ownership of the presumably one reference count the object already has. When the object goes out of scope it is released as usual. If we also declare this procedure: declare procedure dict_add( dict object not null, key_ text not null, value text not null);  then with this family of declarations we could write something like this: create proc create_and_init(out dict object not null) begin set dict := dict_create(); call dict_add(dict, &quot;k1&quot;, &quot;v1&quot;); call dict_add(dict, &quot;k2&quot;, &quot;v2&quot;); if (dict_get_value(dict, &quot;k1&quot;) == dict__get_value(dict, &quot;k2&quot;)) then call printf(&quot;insanity has ensued\\n&quot;); end if; end;  Note: Ordinary scalar functions may not use the database in any way. When they are invoked they will not be provided with the database pointer and so they will be unable to do any database operations. To do database operations, use regular procedures. You can create a function-like-procedure using the out convention discussed previously. SQL Scalar Functions​ SQLite includes the ability to add new functions to its expressions using sqlite3_create_function. In order to use this function in CQL, you must also provide its prototype definition to the compiler. You can do so following this example: declare select function strencode(t text not null) text not null;  This introduces the function strencode to the compiler for use in SQL constructs. With this done you could write a procedure something like this: create table foo(id integer, t text); create procedure bar(id_ integer) begin select strencode(T1.t) from foo T1 where T1.id = id_; end;  This presumably returns the &quot;encoded&quot; text, whatever that might be. Note that if sqlite3_create_functionis not called before this code runs, a run-time error will ensue. Just as CQL must assume that declared tables really are created, it also assumes that declared function really are created. This is another case of telling the compiler in advance what the situation will be at runtime. SQLite allows for many flexible kinds of user defined functions. CQL doesn't concern itself with the details of the implementation of the function, it only needs the signature so that it can validate calls. Note that SQL Scalar Functions cannot contain object parameters. To pass an object, you should instead pass the memory address of this object using a LONG INT parameter. To access the address of an object at runtime, you should use the ptr() function. See the notes section below for more information. See also: Create Or Redefine SQL Functions. SQL Table Valued Functions​ More recent versions of SQLite also include the ability to add table-valued functions to statements in place of actual tables. These functions can use their arguments to create a &quot;virtual table&quot; value for use in place of a table. For this to work, again SQLite must be told of the existence of the table. There are a series of steps to make this happen beginning with sqlite3_create_module which are described in the SQLite documents under &quot;The Virtual Table Mechanism Of SQLite.&quot; Once that has been done, a table-valued function can be defined for most object types. For instance it is possible to create a table-valued function like so: declare select function dict_contents(dict object not null) (k text not null, v text not null);  This is just like the previous type of select function but the return type is a table shape. Once the above has been done you can legally write something like this: create proc read_dict(dict object not null, pattern text) begin if pattern is not null then select k, v from dict_contents(dict) T1 where T1.k LIKE pattern; else select k, v from dict_contents(dict); end if; end;  This construct is very general indeed but the runtime set up for it is much more complicated than scalar functions and only more modern versions of SQLite even support it. "},{"title":"SQL Functions with Unchecked Parameter Types​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#sql-functions-with-unchecked-parameter-types","content":"Certain SQL functions like json_extract are variadic (they accept variable number of arguments). To use such functions within CQL, you can declare a SQL function to have untyped parameters by including the NO CHECK clause instead of parameter types. For example: declare select function json_extract no check text;  This is also supported for SQL table-valued functions: declare select function table_valued_function no check (t text, i int);  Note: currently the NO CHECK clause is not supported for non SQL Ordinary Scalar Functions. "},{"title":"Notes on Builtin Functions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#notes-on-builtin-functions","content":"Some of the SQLite builtin functions are hard-coded; these are the functions that have semantics that are not readily captured with a simple prototype. Other SQLite functions can be declared with declare select function ... and then used. CQL's hard-coded builtin list includes: Aggregate Functions countmaxminsumtotalavggroup_concat Scalar Functions ifnullnullifuppercharabsinstrcoalescelast_insert_rowidprintfstrftimedatetimedatetimejuliandaysubstrreplaceroundtrimltrimrtrim Window Functions row_numberrankdense_rankpercent_rankcume_distntilelagleadfirst_valuelast_valuenth_value Special Functions nullablesensitiveptr Nullable casts an operand to the nullable version of its type and otherwise does nothing. This cast might be useful if you need an exact type match in a situation. It is stripped from any generated SQL and generated C so it has no runtime effect at all other than the indirect consequences of changing the storage class of its operand. Sensitive casts an operand to the sensitive version of its type and otherwise does nothing. This cast might be useful if you need an exact type match in a situation. It is stripped from any generated SQL and generated C so it has no runtime effect at all other than the indirect consequences of changing the storage class of its operand. Ptr is used to cause a reference type variable to be bound as a long integer to SQLite. This is a way of giving object pointers to SQLite UDFs. Not all versions of Sqlite support binding object variables, so passing memory addresses is the best we can do on all versions. "},{"title":"Chapter 9: Statements Summary and Error Checking​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-9-statements-summary-and-error-checking","content":"The following is a brief discussion of the major statement types and the semantic rules that CQL enforces for each of the statements. A detailed discussion of SQL statements (the bulk of these) is beyond the scope of this document and you should refer to the SQLite documentation for most details. However, in many cases CQL does provide additional enforcement and it is helpful to describe the basic checking that happens for each fragment of CQL. A much more authoritative list of the things CQL checks for can be inferred from the error documentation. &quot;Tricky&quot; errors have examples and suggested remediation. "},{"title":"The Primary SQL Statements​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-primary-sql-statements","content":"These are, roughly, the statements that involve the database. The SELECT Statement​ Top level statement list processing for select. This is easily the hardest statement to process. Each clause has its own set of complex rules and the result of previous clauses constrains the next in a complex fashion. Among the things that are verified: the mentioned tables exist and have the mentioned columnsthe columns are type compatible in their contextany variables in the expressions are compatibleaggregate functions are used only in places where aggregation makes sensecolumn and table names are unambiguous, especially when self-joins are involvedcompound selects (e.g. with UNION) are type-consistent in all the fragmentsthe projection of a select has unique column labels if they are used The SELECT * Statement​ SELECT * is special in that it creates its own struct type by assembling all the columns of all the tables in the select's join result. CQL rewrites these column names into a new SELECT with the specific columns explicitly listed. While this makes the program slightly bigger it means that logically deleted columns are never present in results because SELECT * won't select them and attempting to use a logically deleted column results in an error. The CREATE TABLE Statement​ Unlike the other parts of DDL we actually deeply care about the tables. We have to grab all the columns and column types out of it and create the appropriate structure type for the table. Along the way we validate a bunch of stuff like: verify unique table nameno duplicate column namesrecursive correctness of constraints (see constraints discussion below) The UNIQUE KEY Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. Again, we use the usual helpers for name lookup within the context of the table that contains the constraint. The FOREIGN KEY Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. Again, we use the usual helpers for name lookup within the context of the table with the foreign key. Note that the foreign key has to be validated against two tables to fully validate it. The PRIMARY KEY Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. Again, we use the usual helpers for name lookup within the context of the table with the primary key. The CHECK Clause​ Similar to other constraints, we don't actually do anything with this other than offer some validation. The CHECK clause is validated after the entire table has been processed so that even if it appears early in the table, the clause can use any columns defined later in the table. The CREATE INDEX Statement​ CQL doesn't really do anything with indices but we do validate that they make sense (so we lookup all the names of all the columns and so forth.) The CREATE VIEW Statement​ Create view analysis is very simple because the select analysis does the heavy lifting. All we have to do is validate that the view is unique, then validate the select statement. Additionally, views must not be allowed to have any NULL type columns; all nulls must be converted to some type with a CAST. e.g. create view foo as select NULL n is not valid. NULL is not a real storage type. The CREATE TRIGGER Statement​ The create trigger statement is quite a beast, and validations include: The trigger name must be uniqueFor insert the &quot;new.*&quot; table is available in expressions/statementFor delete the &quot;old.*&quot; table is available in expressions/statementsFor update both are available If optional columns present in the update, they must be unique/valid The when expression must evaluate to a numericThe statement list must be error free with the usual rules plus new/oldThe raise function may be used inside a trigger (NYI)The table name must be a table (not a view) UNLESS the trigger type is INSTEAD OFSelect statements inside the statement block do not count as returns for the procedure and that includes the create trigger The DROP TABLE Statement​ This is the basic checking for the drop table statement: the table must exist in some versionit has to be a table and not a view The DROP VIEW Statement​ This is the basic checking for the drop view statement: the view must exist in some versionit has to be a view and not a table The DROP INDEX Statement​ This is the basic checking for the drop index statement: the index must exist in some versionit could be deleted now, that's ok, but the name has to be valid The DROP TRIGGER Statement​ This is the basic checking for the drop trigger statement the trigger must exist in some versionit could be deleted now, that's ok, but the name has to be valid The RAISE Statement​ CQL validates that RAISE is being used in the context of a trigger and that it has the correct arguments The ALTER TABLE ADD COLUMN Statement​ To validate alter table add column we check the following: the table must exist and not be a view (in any version)the column definition of the new column must be self-consistentno auto-increment columns may be addedadded columns must be either nullable or have a default value Note: Alter statements are typically used in the context of migration, so it's possible the table that is mentioned is condemned in a future version. We still have to run the intervening upgrade steps so basically DDL gets to ignore the current deadness of the table as in context it might be &quot;not dead yet&quot;. This will be more obvious in the context of the schema maintenance features. (q.v.) The DELETE Statement​ The delete analyzer sets up a scope for the table being deleted and then validates the WHERE clause, if present, against that scope. Additionally, we verify that the table actually was defined and is not a view. The UPDATE Statement​ The update analyzer sets up the scope for the table(s) being updated. If there are optional clauses (e.g. LIMIT), they are evaluated just like in a select statement with those same helper methods. Expression fragments are evaluated similarly as in a select statement. The INSERT Statement​ We check that the table exists and then we walk the columns and the value list to make sure they are valid for the table. Also, we cannot insert into a view. Details: The column list specifies the columns we will provide; they must exist and be unique.The columns specified must suffice to insert a row (all not nulls and not default present.)The insert list specifies the values that are to be inserted.The type of each value must match the type of the column.Auto-increment columns may be specified as NULL.If there are too many or too few columns, that is considered an error.If no columns are specified, that is the same as if all columns had been specified, in table order. The THROW Statement​ Throw can literally go anywhere, so it's always ok. The BEGIN TRANSACTION Statement​ Begin transaction can go anywhere, so it's always ok. The sqlite documentation can be helpful here (CQL syntax is a subset). See: https://www.sqlite.org/lang_transaction.html The COMMIT TRANSACTION Statement​ Commit transaction can go anywhere, so it's always ok. The sqlite documentation can be helpful here (CQL syntax is a subset). See: https://www.sqlite.org/lang_transaction.html The ROLLBACK TRANSACTION Statement​ Rollback transaction can go anywhere but if you're using the format where you rollback to a particular save point, then we must have seen that name in a savepoint statement previously. Otherwise, it's an error. The sqlite documentation can be helpful here again (CQL syntax is a subset). See: https://www.sqlite.org/lang_transaction.html The SAVEPOINT Statement​ The savepoint statement can go anywhere but we do record this savepoint name as having been seen, so that we can verify it in rollback. So this is sort of a weak declaration of the savepoint name. The sqlite documentation can be helpful here (CQL syntax is a subset). https://www.sqlite.org/lang_savepoint.html The RELEASE SAVEPOINT Statement​ Release savepoint can go anywhere but we must have seen that name in a previous savepoint statement, otherwise it's an error. The sqlite documentation can be helpful here (CQL syntax is a subset). https://www.sqlite.org/lang_savepoint.html The PROCEDURE SAVEPOINT Statement​ A common pattern is to have a savepoint associated with a particular procedure. The savepoint's scope is the same as the procedure's scope. More precisely create procedure foo() begin proc savepoint begin -- your code end; end;  becomes: create procedure foo() begin savepoint @proc; -- @proc is always the name of the current procedure begin try -- your code release savepoint @proc; end try; begin catch rollback transaction to savepoint @proc; release savepoint @proc; throw; end catch; end;  This form is not quite syntactic sugar because there are some interesting rules: the proc savepoint form must be used at the top level of the procedure, hence no leave or continue may escape itwithin begin/end the return form may not be used; you must use rollback return or commit return (see below)throw may be used to return an error as usualproc savepoint may be used again, at the top level, in the same procedure, if there are, for instance, several sequential stagesa procedure using proc savepoint could call another such procedure, or a procedure that manipulates savepoints in some other way The ROLLBACK RETURN Statement​ This form may be used only inside of a proc savepoint block. It indicates that the savepoint should be rolled back and then the procedure should return. It is exactly equivalent to:  rollback transaction to savepoint @proc; release savepoint @proc; return; -- wouldn't actually be allowed inside of proc savepoint; see note below  Note: to avoid errors, the loose return above is not actually allowed inside of proc savepoint -- you must use rollback return or commit return. The COMMIT RETURN Statement​ This form may be used only inside of a proc savepoint block. It indicates that the savepoint should be released and then the procedure should return. It is exactly equivalent to:  release savepoint @proc; return; -- wouldn't actually be allowed inside of proc savepoint; see note below  Of course this isn't exactly a commit, in that there might be an outer savepoint or outer transaction that might still be rolled back, but it is commited at its level of nesting, if you will. Or, equivalently, you can think of it as merging the savepoint into the transaction in flight. Note: to avoid errors, the loose return above is not actually allowed inside of proc savepoint and you must use rollback return or commit return. The CREATE VIRTUAL TABLE Statement​ The SQLite CREATE VIRTUAL TABLE form (https://sqlite.org/lang_createvtab.html) is problematic from CQL because: it is not parseable, because the module arguments can be literally anything (or nothing), even a letter to your grandmathe arguments do not necessarily say anything about the table's schema at all So the CQL form departs from the standard syntax to this form: create virtual table virt_table using my_module [(module arguments)] as ( id integer not null, name text );  The part after the AS is used by CQL as a table declaration for the virtual table. The grammar for that is exactly the same as a normal CREATE TABLE statement. However, that part is not transmitted to SQLite; when the table is created, SQLite sees only the part it cares about, which is the part before the AS. In order to have strict parsing rules, the module arguments follow one of these forms: no arguments at alla list of identifiers, constants, and parenthesized sublists, just like in the @attribute formthe words arguments following Case 1 Example​ create virtual table virt_table using my_module as ( id integer not null, name text );  becomes (to SQLite) CREATE VIRTUAL TABLE virt_table USING my_module;  Note: empty arguments USING my_module() are not allowed in the SQLite docs but do seem to work in SQLite. We take the position that no args should be formatted with no parentheses, at least for now. Case 2 Example​ create virtual table virt_table using my_module(foo, 'goo', (1.5, (bar, baz))) as ( id integer not null, name text );  CREATE VIRTUAL TABLE virt_table USING my_module(foo, &quot;goo&quot;, (1.5, (bar, baz)));  This form allows for very flexible arguments but not totally arbitrary arguments, so it can still be parsed and validated. Case 3 Example​ This case recognizes the popular choice that the arguments are often the actual schema declaration for the table in question. So: create virtual table virt_table using my_module(arguments following) as ( id integer not null, name text );  becomes CREATE VIRTUAL TABLE virt_table USING my_module( id INTEGER NOT NULL, name TEXT );  The normalized text (keywords capitalized, whitespace normalized) of the table declaration in the as clause is used as the arguments. Other details​ Virtual tables go into their own section in the JSON and they include the module and moduleArgs entries; they are additionally marked isVirtual in case you want to use the same processing code for virtual tables as normal tables. The JSON format is otherwise the same, although some things can't happen in virtual tables (e.g. there is no TEMP option so &quot;isTemp&quot; must be false in the JSON.) For purposes of schema processing, virtual tables are on the @recreate plan, just like indices, triggers, etc. This is the only option since the alter table form is not allowed on a virtual table. Semantic validation enforces &quot;no alter statements on virtual tables&quot; as well as other things like no indices, and no triggers, since SQLite does not support any of those things. CQL supports the notion of eponymous virtual tables. If you intend to register the virtual table's module in this fashion, you can use create virtual table @eponymous ... to declare this to CQL. The only effect this has is to ensure that CQL will not try to drop this table during schema maintenance as dropping such a table is an invalid operation. In all other ways, the fact that the table is eponymous makes no difference. Finally, because virtual tables are on the @recreate plan, you may not have foreign keys that reference virtual tables. Such keys seem like a bad idea in any case. "},{"title":"The Primary Procedure Statements​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-primary-procedure-statements","content":"These are the statements which form the language of procedures, and do not involve the database. The CREATE PROCEDURE Statement​ Semantic analysis of stored procedures is fairly easy at the core: check for duplicate namesvalidate the parameters are well formedset the current proc in flight (this not allowed to nest)recurse on the statement list and prop errorsrecord the name of the procedure for callers In addition, while processing the statement:we determine if it uses the database; this will change the emitted signature of the proc to include a sqlite3 *dbinput argument and it will return a sqlite error code (e.g. SQLITE_OK)select statements that are loose in the proc represent the &quot;return&quot; of that select; this changes the signature to include a sqlite3_stmt **pstmt parameter corresponding to the returned statement The IF Statement​ The top level if node links the initial condition with a possible series of else_if nodes and then the else node. Each condition is checked for validity. The conditions must be valid expressions that can each be converted to a boolean. The SET Statement​ The set statement is for variable assignment. We just validate that the target exists and is compatible with the source. Cursor variables cannot be set with simple assignment and CQL generates errors if you attempt to do so. The LET Statement​ Let combines a DECLARE and a SET. The variable is declared to be the exact type of the right hand side. All the validations for DECLAREand SET are applicable, but there is no chance that the variable will not be compatible with the expression. The expression could still be erroneous in the first place. The variable could be a duplicate. The SWITCH Statement​ The SWITCH form requires a number of conditions to successfully map down to a C switch statement. These are: the switch-expression must be a not-null integral type (integer not null or long integer not null) the WHEN expressions must be losslessly promotable to the type of the switch-expression the values in the WHEN clauses must be uniqueIf ALL VALUES is present then: the switch-expression must be of an enum typethe WHEN values must cover every value of the enum except those beginning with '_'there can be no extra WHEN values not in the enumthere can be no ELSE clause The DECLARE PROCEDURE Statement​ There are three forms of this declaration: a regular procedure with no DML e.g. declare proc X(id integer); a regular procedure that uses DML (it will need a db parameter and returns a result code) e.g. declare proc X(id integer) using transaction; a procedure that returns a result set, and you provide the result columns e.g. declare proc X(id integer) : (A bool not null, B text);The main validations here are that there are no duplicate parameter names, or return value columns. The DECLARE FUNCTION Statement​ Function declarations are similar to procedures; there must be a return type (use proc if there is none). The DECLARE SELECT FUNCTION form indicates a function visible to SQLite; other functions are usable in the call statement. The DECLARE Variable Statement​ This declares a new local or global variable that is not a cursor. The type is computed with the same helper that is used for analyzing column definitions. Once we have the type we walk the list of variable names, check them for duplicates and such (see above) and assign their type. The canonical name of the variable is defined here. If it is later used with a different casing the output will always be as declared. e.g. declare Foo integer; set foo = 1; is legal but the output will always contain the variable written as Foo. The DECLARE Cursor Statement​ There are two forms of the declare cursor, both of which allow CQL to infer the exact type of the cursor. declare foo cursor for select etc. the type of the cursor is the net struct type of the select list declare foo cursor for call proc(); proc must be statement that produces a result set via select (see above)the type of the cursor is the struct of the select returned by the procnote if there is more than one loose select in the proc they must match exactly cursor names have the same rules regarding duplicates as other variables With this in mind, both cases simply recurse on either the select or the call and then pull out the structure type of that thing and use it for the cursor's shape. If thecall is not semantically valid according to the rules for calls or the select is not semantically valid, then of course this declaration will generate errors. The DECLARE Value Cursor Statement​ This statement declares a cursor that will be based on the return type of a procedure. When using this form the cursor is also fetched, hence the name. The fetch result of the stored proc will be used for the value. At this point, we use its type only. the call must be semantically validthe procedure must return an OUT parameter (not a result set)the cursor name must be unique The WHILE Statement​ While semantic analysis is super simple. the condition must be numericthe statement list must be error-freeloop_depth is increased allowing the use of interior leave/continue The LOOP Statement​ Loop analysis is just as simple as &quot;while&quot; -- because the loop_stmt literally has an embedded fetch, you simply use the fetch helper to validate that the fetch is good and then visit the statement list. Loop depth is increased as it is with while. The CALL Statement​ There are three ways that a call can happen: signatures of procedures that we know in full: call foo();declare cursor for call foo(); some external call to some outside function we don't know e.g. call printf('hello, world\\n'); The cursor form can be used if and only if the procedure has a loose select or a call to a procedure with a loose select. In that case, the procedure will have a structure type, rather than just &quot;ok&quot; (the normal signature for a proc). If the user is attempting to do the second case, cursor_name will be set and the appropriate verification happens here. Note: Recursively calling fetch cursor is not really doable in general because at the point in the call we might not yet know that the method does in fact return a select. You could make it work if you put the select before the recursive call. Semantic rules: for all cases each argument must be error-free (no internal type conflicts)for known procs the call has to have the correct number of argumentsif the formal is an out parameter the argument must be a variable the type of the variable must be an exact type match for the formal non-out parameters must be type-compatible, but exact match is not required The DECLARE OUT CALL Statement​ This form is syntactic sugar and corresponds to declaring any OUT parameters of the CALL portion that are not already declared as the exact type of theOUT parameter. This is intended to save you from declaring a lot of variables just so that you can use them as OUT arguments. Since any variables that already exist are not re-declared, there are no additional semantic rules beyond the normal call except that it is an error to use this form if no OUT variables needed to be declared. The FETCH Statement​ The fetch statement has two forms: fetch C into var1, var2, var3 etc.fetch C; The second form is called the auto_cursor. In the first form the variables of the cursor must be assignment compatible with declared structure type of the cursor and the count must be correct. In the second form, the codegen will implicitly create local variables that are exactly the correct type, but we'll cover that later. Since no semantic error is possible in that case, we simply record that this is an auto_cursor and then later we will allow the use of C.field during analysis. Of course &quot;C&quot; must be a valid cursor. The CONTINUE Statement​ We just need to ensure that continue is inside a loop or while. The LEAVE Statement​ We only need to ensure that leave is inside a loop, while or switch. The TRY/CATCH Statements​ No analysis needed here other than that the two statement lists are ok. The CLOSE CURSOR Statement​ For close [cursor], we just validate that the name is in fact a cursor and it is not a boxed cursor. Boxed cursor lifetime is managed by the box object so manually closing it is not allowed. Instead, the usual reference-counting semantics apply; the boxed cursor variable typically falls out of scope and is released, or is perhaps set to NULL to release its reference early. The OUT CURSOR Statement​ For out [cursor], we first validate that the name is a cursor then we set the output type of the procedure we're in accordingly. "},{"title":"The \"Meta\" Statements​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#the-meta-statements","content":"The program's control/ the overall meaning of the program / or may give the compiler specific directives as to how the program should be compiled. The @ECHO Statement​ Echo is valid in any top level contexts. The @PREVIOUS SCHEMA Statement​ Begins the region where previous schema will be compared against what has been declared before this directive for alterations that could not be upgraded. The @SCHEMA_UPGRADE_SCRIPT Statement​ When upgrading the DDL, it's necessary to emit create table statements for the original version of the schema. These create statements may conflict with the current version of the schema. This attribute tells CQL to 1) ignore DDL in stored procedures for declaration purposes; only DDL outside of a proc counts 2) do not make any columns &quot;hidden&quot; thereby allowing all annotations to be present so they can be used to validate other aspects of the migration script. The @SCHEMA_UPGRADE_VERSION Statement​ For sql stored procedures that are supposed to update previous schema versions you can use this attribute to put CQL into that mindset. This will make the columns hidden for the version in question rather than the current version. This is important because older schema migration procedures might still refer to old columns. Those columns truly exist at that schema version. The @ENFORCE_STRICT Statement​ Switch to strict mode for the indicated item. The choices and their meanings are: &quot;FOREIGN KEY ON DELETE&quot; indicates there must be some ON DELETE action in every FK&quot;FOREIGN KEY ON UPDATE&quot; indicates there must be some ON UPDATE action in every FK&quot;INSERT SELECT&quot; indicates that insert with SELECT for values may not include top level joins (avoiding a SQLite bug)&quot;IS TRUE&quot; indicates that IS TRUE IS FALSE IS NOT TRUE IS NOT FALSE may not be used (*)&quot;JOIN&quot; indicates only ANSI style joins may be used, and &quot;from A,B&quot; is rejected&quot;PROCEDURE&quot; indicates no calls to undeclared procedures (like loose printf calls)&quot;SELECT IF NOTHING&quot; indicates (select ...) expressions must include an IF NOTHING clause if they have a FROM part&quot;TABLE FUNCTIONS&quot; indicates table valued functions cannot be used on left/right joins (avoiding a SQLite bug)&quot;TRANSACTION&quot; indicates no transactions may be started, committed, or aborted&quot;UPSERT&quot; indicates no upsert statement may be used (*)&quot;WINDOW FUNCTION&quot; indicates no window functions may be used (*)&quot;WITHOUT ROWID&quot; indicates WITHOUT ROWID may not be used The items marked with * are present so that features can be disabled to target downlevel versions of SQLite that may not have those features. See the grammar details for exact syntax. The @ENFORCE_NORMAL Statement​ Turn off strict enforcement for the indicated item. The @ENFORCE_PUSH Statement​ Push the current strict settings onto the enforcement stack. This does not change the current settings. The @ENFORCE_POP Statement​ Pop the previous current strict settings from the enforcement stack. The @ENFORCE_RESET Statement​ Turns off all the strict modes. Best used immediately after @ENFORCE_PUSH. The @DECLARE_SCHEMA_REGION Statement​ A schema region is a partitioning of the schema such that it only uses objects in the same partition or one of its declared dependencies. One schema region may be upgraded independently from any others (assuming they happen such that dependents are done first.) Here we validate: the region name is uniquethe dependencies (if any) are unique and existthe directive is not inside a procedure The @BEGIN_SCHEMA_REGION Statement​ Entering a schema region makes all the objects that follow part of that region. It also means that all the contained objects must refer to only pieces of schema that are in the same region or a dependent region. Here we validate that region we are entering is in fact a valid region and that there isn't already a schema region. The @END_SCHEMA_REGION Statement​ Leaving a schema region puts you back in the default region. Here we check that we are in a schema region. The @EMIT_ENUMS Statement​ Declared enumarations can be voluminous and it is undesirable for every emitted .h file to contain every enumeration. To avoid this problem you can emit enumeration values of your choice using @emit_enums x, y, zwhich places the named enumerations into the .h file associated with the current translation unit. If no enumerations are listed, all enums are emitted. Note: generated enum definitions are protected by #ifndef X ... #endif so multiple definitions are harmless and hence you can afford to use @emit_enumsfor the same enum in several translations units, if desired. Note: Enumeration values also appear in the JSON output in their own section. The @EMIT_CONSTANTS Statement​ This statement is entirely analogous to the the @EMIT_ENUMS except that the parameters are one or more constant groups. In fact constants are put into groups precisely so that they can be emitted in logical bundles (and to encourage keeping related constants together). Placing @EMIT_CONSTANTScauses the C version of the named groups to go into the current .h file. Note: Global constants also appear in the JSON output in their own section. "},{"title":"Important Program Fragments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#important-program-fragments","content":"These items appear in a variety of places and are worthy of discussion. They are generally handled uniformly. Argument Lists​ In each case we walk the entire list and do the type inference on each argument. Note that this happens in the context of a function call, and depending on what the function is, there may be additional rules for compatibility of the arguments with the function. The generic code doesn't do those checks, there is per-function code that handles that sort of thing. At this stage the compiler computes the type of each argument and makes sure that, independently, they are not bogus. Procedures that return a Result Set​ If a procedure is returning a select statement then we need to attach a result type to the procedure's semantic info. We have to do some extra validation at this point, especially if the procedure already has some other select that might be returned. The compiler ensures that all the possible select results are are 100% compatible. General Name Lookups​ Every name is checked in a series of locations. If the name is known to be a table, view, cursor, or some other specific type of object then only those name are considered. If the name is more general a wider search is used. Among the places that are considered: columns in the current join if any (this must not conflict with #2)local or global variablesfields in an open cursorfields in enumerations and global constants Data Types with a Discriminator​ Discriminators can appear on any type, int, real, object, etc. Where there is a discriminator the compiler checks that (e.g.) object&lt;Foo&gt; only combines with object&lt;Foo&gt; or object. real&lt;meters&gt; only combines with real&lt;meters&gt; or real. In this way its not possible to accidentally add meters to kilograms or to store an int&lt;task_id&gt; where an int&lt;person_id&gt; is required. The CASE Expression​ There are two parts to this: the &quot;when&quot; expression and the &quot;then&quot; expression. We compute the aggregate type of the &quot;when&quot; expressions as we go, promoting it up to a larger type if needed (e.g. if one &quot;when&quot; is an int and the other is a real, then the result is a real). Likewise, nullability is computed as the aggregate. Note that if nothing matches, the result is null, so we always get a nullable resultm unless there is an &quot;else&quot; expression. If we started with case expression, then each &quot;when&quot; expression must be comparable to the case expression. If we started with case when xx then yy; then each case expression must be numeric (typically boolean). The BETWEEN EXPRESSIONS​ Between requires type compatibility between all three of its arguments. Nullability follows the usual rules: if any might be null then the result type might be null. In any case, the result's core type is BOOL. The CAST Expression​ For cast expressions we use the provided semantic type; the only trick is that we preserve the extra properties of the input argument. e.g. CAST does not remove NOT NULL. The COALESCE Function​ Coalesce requires type compatibility between all of its arguments. The result is a not null type if we find a not null item in the list. There should be nothing after that item. Note that ifnull and coalesce are really the same thing except ifnull must have exactly two arguments. The IN AND NOT IN Expressions​ The in predicate is like many of the other multi-argument operators. All the items must be type compatible. Note that in this case the nullablity of the items does not matter, only the nullability of the item being tested. Note that null in (null) is null, not true. Aggregate Functions​ Aggregate functions can only be used in certain places. For instance they may not appear in a WHERE clause. User Defined Functions​ User defined function - this is an external function. There are a few things to check: If this is declared without the select keyword then we can't use these in SQL, so this has to be a loose expression If this is declared with the select keyword then we can ONLY use these in SQL, not in a loose expression args have to be compatible with formals Calling a procedure as a function​ There are a few things to check: we can't use these in SQL, so this has to be a loose expressionargs have to be compatible with formals, exceptthe last formal must be an OUT arg and it must be a scalar typethat out arg will be treated as the return value of the &quot;function&quot;in code-gen we will create a temporary for it; semantic analysis doesn't care Root Expressions​ A top level expression defines the context for that evaluation. Different expressions can have constraints. e.g. aggregate functions may not appear in the WHERE clause of a statement. There are cases where expression nesting can happen. This nesting changes the evaluation context accordingly, e.g. you can put a nested select in a where clause and that nested select could legally have aggregates. Root expressions keep a stack of nested contexts to facilitate the changes. Table Factors​ A table factor is one of three things: a table name (a string) select * from Xa select subquery (select X,Y from..) as T2a list of table references select * from (X, Y, Z) Here we dispatch to the appropriate helper for each case. Joining with the USING Clause​ When specifying joins, one of the alternatives is to give the shared columns in the join e.g. select * from X inner join Y using (a,b). This method validates that all the columns are present on both sides of the join, that they are unique, and they are comparable. The return code tells us if any columns had SENSITIVE data. See Special Note on JOIN...USING below JOIN WITH THE ON Clause​ The most explicit join condition is a full expression in an ON clause this is like select a,b from X inner join Y on X.id = Y.id;The on expression should be something that can be used as a bool, so any numeric will do. The return code tells us if the ON condition used SENSITIVE data. TABLE VALUED FUNCTIONS​ Table valued functions can appear anywhere a table is allowed. The validation rules are: must be a valid function must return a struct type (i.e. a table-valued-function) must have valid arg expressions arg expressions must match formal parameters The name of the resulting table is the name of the function but it can be aliased later with &quot;AS&quot; Special Note on the select * and select T.* forms​ The select * construct is very popular in many codebases but it can be unsafe to use in production code because, if the schema changes, the code might get columns it does not expect. Note the extra columns could have appeared anywhere in the result set because the * applies to the entire result of the FROM clause, joins and all, so extra columns are not necessarily at the end and column ordinals are not preserved. CQL mitigates this situation somewhat with some useful constraints/features: in a select *, and indeed in any query, the column names of the select must be unique, this is because: they could form the field names of an automatically generated cursor (see the section on cursors)they could form the field names in a CQL result set (see section on result sets)it's weird/confusing to not have unique names generally when issuing a select * or a select T.* CQL will automatically expand the * into the actual logical columns that exist in the schema at the time the code was compiled this is important because if a column had been logically deleted from a table it would be unexpected in the result set even though it is still present in the database and would throw everything offlikewise if the schema were to change without updating the code, the code will still get the columns it was compiled with, not new columns Expanding the * at compile time means Sqlite cannot see anything that might tempt it to include different columns in the result. With this done we just have to look at the places a select * might appear so we can see if it is safe (or at least reasonably safe) to use * and, by extension of the same argument, T.*. In an EXISTS or NOT EXISTS clause like `where not exists (select from x)`* this is perfectly safe; the particular columns do not matter; select * is not even expanded in this case. In a statement that produces a result set like `select from table_or_view`* binding to a CQL result set is done by column name and we know those names are uniquewe won't include any columns that are logically deleted, so if you try to use a deleted column you'll get a compile time error In a cursor statement like declare C cursor for select * from table_or_view there are two cases here: Automatic Fetch fetch C; in this case you don't specify the column names yourself;2 they are inferredyou are therefore binding to the columns by name, so new columns in the cursor would be unused (until you choose to start using them)if you try to access a deleted column you get a compile-time error Manual Fetch: fetch C into a, b, c; In this case the number and type of the columns must match exactly with the specified variablesIf new columns are added, deleted, or changed, the above code will not compile So considering the cases above we can conclude that auto expanding the * into the exact columns present in the compile-time schema version ensures that any incompatible changes result in compile time errors. Adding columns to tables does not cause problems even if the code is not recompiled. This makes the * construct much safer, if not perfect, but no semantic would be safe from arbitrary schema changes without recompilation. At the very least here we can expect a meaningful runtime error rather than silently fetching the wrong columns. "},{"title":"Special Note on the JOIN...USING form​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#special-note-on-the-joinusing-form","content":"CQL varies slightly from SQLite in terms of the expected results for joins if the USING syntax is employed. This is not the most common syntax (typically an ON clause is used) but Sqlite has special rules for this kind of join. Let's take a quick look. First some sample data: create table A( id integer, a text, b text); create table B( id integer, c text, d text); insert into A values(1, 'a1', 'b1'); insert into B values(1, 'c1', 'd1'); insert into A values(2, 'a2', 'b2'); insert into B values(2, 'c2', 'd2');  Now let's look at the normal join; this is our reference: select * from A T1 inner join B T2 on T1.id = T2.id; result: 1|a1|b1|1|c1|d1 2|a2|b2|2|c2|d2  As expected, you get all the columns of A, and all the columns of B. The 'id' column appears twice. However, with the USING syntax: select * T1 inner join B T2 using (id); result: 1|a1|b1|c1|d1 2|a2|b2|c2|d2  The id column is now appearing exactly once. However, the situation is not so simple as that. It seems that what hapened was that the * expansion has not included two copies of the id. The following cases show that both copies of id are still logically in the join. select T1.*, 'xxx', T2.* from A T1 inner join B T2 using (id); result: 1|a1|b1|xxx|1|c1|d1 2|a2|b2|xxx|2|c2|d2  The T2.id column is part of the join, it just wasn't part of the * In fact, looking further: select T1.id, T1.a, T1.b, 'xxx', T2.id, T2.c, T2.d from A T1 inner join B T2 using (id); result: 1|a1|b1|xxx|1|c1|d1 2|a2|b2|xxx|2|c2|d2  There is no doubt, T2.id is a valid column and can be used in expressions freely. That means the column cannot be removed from the type calculus. Now in CQL, the * and T.* forms are automatically expanded; SQLite doesn't see the *. This is done so that if any columns have been logically deleted they can be elided from the result set. Given that this happens, the * operator will expand to ALL the columns. Just the same as if you did T1.* and T2.*. As a result, in CQL, there is no difference between the USING form of a join and the ON form of a join. In fact, only the select * form could possibly be different, so in most cases this ends up being moot anyway. Typically, you don't need to use * in the presence of joins because of name duplication and ambiguity of the column names of the result set. CQL's automatic expansion means you have a much better idea exactly what columns you will get - those that were present in the schema you declared. "},{"title":"Chapter 10: Schema Management Features​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-10-schema-management-features","content":"CQL has a lot of schema knowledge already and so it's well positioned to think about schema upgrades and versioning. It seemed essential to be able to record changes to the schema over time so CQL got an understanding of versioning. This lets you do things like: ensure columns are only added where they should begenerate compiler errors if you try to access columns that are deprecatedmove from one version to another tracking schema facets that have to be added To use cql in this fashion, the sequence will be something like the below. See Appendix 1 for command line details. cql --in input.sql --rt schema_upgrade --cg schema_upgrader.sql \\ --global_proc desired_upgrade_proc_name  "},{"title":"Annotations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#annotations","content":"There are three basic flavors of annotation @create(version [, migration proc])@delete(version [, migration proc])@recreate They have various constraints: @create and @delete can only be applied to tables and columns@recreate can only be applied to tables (nothing else needs it anyway)@recreate cannot mix with @create or @delete@recreate can include a group name as in @recreate(musketeers); if a group name is specified then all the tables in that group are recreated if any of them change Indices, Views, and Triggers are always &quot;recreated&quot; (just like tables can be) and so neither the @recreate nor the @create annotations are needed (or allowed). However when an Index, View, or Trigger is retired it must be marked with @delete so that it isn't totally forgotten but can be deleted anywhere it might still exist. Note that when one of these items is deleted, the definition is not used as it will only be dropped anyway. The simplest creation of the object with the correct name will do the job as a tombstone. e.g. create view used_to_be_fabulous as select 1 x @delete(12); suffices to drop the used_to_be_fabulous view in version 12 no matter how complicated it used to be. Its CREATE VIEW will not be emitted into the upgrade procedure in any case. Similarly, trivial indices and triggers of the correct name can be used for the tombstone. In addition, if there is some data migration that needs to happen at a particular schema version that isn't associated with any particular change in schema, you can run an ad hoc migrator at any time. The syntax for that is @schema_ad_hoc_migration(version, migration proc);. Ad hoc migrations are the last to run in any given schema version; they happen after table drop migrations. "},{"title":"Semantics​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#semantics","content":"@create declares that the annotated object first appeared in the indicated version, and at that time the migration proc needs to be executed to fill in default values, denormalize values, or whatever the case may be. @delete declares that the annotated object disappeared in the indicated version, and at that time the migration proc needs to be executed to clean up the contents, or potentially move them elsewhere. @recreate declares that the annotated object can be dropped and recreated when it changes because there is no need to preserve its contents during an upgrade. Such objects may be changed arbitrarily from version to version. no columns in a @recreate table may have @create or @delete (these aren't needed anyway) therefore tables with @recreate never have deprecated columns (since @delete isn't allowed on their columns) NOTE: all annotations are suppressed from generated SQL. SQLite never sees them. NOTE: looking at the annotations it is possible to compute the logical schema at any version, especially the original schema -- it's what you get if you disregard all @delete entirely (don't delete) and then remove anything marked with @create directives. "},{"title":"Allowable changes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#allowable-changes","content":"Not all migrations are possible in a sensible fashion, therefore CQL enforces certain limitations: the &quot;original&quot; schema has no annotations or just delete annotationsnew tables may be added (with @create)tables may be deleted (with @delete)columns may be added to a table, but only at the end of the tableadded columns must be nullable or have a default value (otherwise all existing insert statements would break for sure)columns may not be renamedcolumns may be deleted but this is only a logical delete, SQLite has no primitive to remove columns; once deleted you may no longer refer to that column in queriesdeleted columns must be nullable or have a default value (otherwise all existing and future insert statements would break for sure, the column isn't really gone)views, indices, and triggers may be added (no annotation required) and removed (with @delete) like tablesviews, indices, and triggers may be altered completely from version to versionno normal code is allowed to refer to deleted columns, tables, etc. This includes views, indices, and triggersschema migration stored procs see the schema as it existed in their annotation (so an older version). They are also forbidden from using views (see below)recreated objects (tables marked with @recreate, views, tables, and indices) have no change restrictions "},{"title":"Prosecution​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#prosecution","content":"Moving from one schema version to another is done in an orderly fashion with the migration proc taking these essential steps in this order: the cql_schema_facets table is created if needed -- this records the current state of the schema the last known schema hash is read from the cql_schema_facets tables (it is zero by default) if the overall schema hash code matches what is stored, processing stops; otherwise an upgrade ensues all known views are dropped (hence migration procs won't see them!) any index that needs to change is dropped (this includes items marked @delete or indices that are different than before) change is detected by hash (crc64) of the previous index definition vs. the current all known triggers are dropped (hence they will not fire during migration!) the current schema version is extracted from cql_schema_facets (it is zero by default) if the current schema version is zero, then the original versions of all the tables are created if the current schema version is &lt;= 1 then any tables that need to be created at schema version 1 are created as they exist at schema version 1any columns that need to be created at schema version 1 are created as they exist at schema version 1migration procedures schema version 1 are run in this order: create table migrationcreate column migrationdelete trigger migration (these are super rare and supported for uniformity)delete index migration (these are super rare and supported for uniformity)delete view migration (these are super rare and supported for uniformity)delete column migrationdelete table migrationad hoc migrationeach proc is run exactly one time any tables that need to be dropped at schema version 1 are droppedthe schema version is marked as 1 in cql_schema_facetseach sub-step in the above is recorded in cql_schema_facets as it happens so it is not repeated all that checking not shown for brevity the above process is repeated for all schema versions up to the current version all tables that are marked with @recreate are re-created if necessary i.e. if the checksum of the table definition has changed for any table (or group) then drop it and create the new version. all indices that changed and were not marked with @delete are re-created all views not marked with @delete are re-created all triggers not marked with @delete are re-installed the current schema hash is written to the cql_schema_facets table "},{"title":"Example Migration​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#example-migration","content":"Here's an example of a schema directly from the test cases: -- crazy amount of versioning here create table foo( id integer not null, rate long integer @delete(5), rate_2 long integer @delete(4, DeleteRate2Proc), id2 integer default 12345 @create(4, CreateId2Proc), name text @create(5), name_2 text @create(6) ); -- much simpler table, lots of stuff added in v2. -- note v1 is the first new version and v0 is base version create table table2( id integer not null, name1 text @create(2, CreateName1Proc), name2 text @create(2, CreateName2Proc), name3 text @create(2), -- no proc name4 text @create(2) -- no proc ); create table added_table( id integer not null, name1 text, name2 text @create(4) ) @create(3) @delete(5); -- this view is present in the output create view live_view as select * from foo; -- this view is also present in the output create view another_live_view as select * from foo; -- this view is not present in the output create view dead_view as select * from foo @delete(2); -- this index is present create index index_still_present on table2(name1, name2); -- this index is going away create index index_going_away on table2(name3) @delete(3); -- this is a simple trigger, and it's a bit silly but that doesn't matter create trigger trigger_one after insert on foo begin delete from table2 where table2.id = new.id; end;  This schema has a LOT of versioning... you can see tables and columns appearing in versions 2 through 6. There is a lot of error checking happening. things with no create annotation were present in the base schemaonly things with no delete annotation are visible to normal codecreated columns have to be at the end of their table (required by SQLite)they have to be in ascending schema version order (but you can add several columns in one version)there may or may not be a proc to run to populate data in that column when it's added or to remove data when it's deleted proc names must be unique you can't delete a table or column in a version before it was createdyou can't delete a column in a table in a version before the table was createdyou can't create a column in a table in a version after the table was deletedthere may be additional checks not listed here "},{"title":"Sample Upgrade Script​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#sample-upgrade-script","content":"With just those annotations you can automatically create the following upgrade script which is itself CQL (and hence has to be compiled). Notice that this code is totally readable! The script has been split into logical pieces to make it easier to explain what's going on. Preamble​ -- ...copyright notice... possibly generated source tag... elided to avoid confusion -- no columns will be considered hidden in this script -- DDL in procs will not count as declarations @SCHEMA_UPGRADE_SCRIPT;  Schema upgrade scripts need to see all the columns even the ones that would be logically deleted in normal mode. This is so that things like alter table add column can refer to real columns and drop table can refer to a table that shouldn't even be visible. Remember in CQL the declarations tell you the logical state of the universe and DLL mutations are expected to create that condition, so you should be dropping tables that are marked with @deleteCQL stores the current state of the universe in this table. -- schema crc -7714030317354747478  The schema crc is computed by hashing all the schema declarations in canonical form. That's everything in this next section. Facet Helpers​ CQL uses a set of four functions to manage a dictionary. The implementation is in cqlrt_common.c but it's really just a simple hash table that maps from a string key to a number. This functionality was added because over time the facets table can get pretty big and running a SQL query every time to read a single integer is not economical. -- declare facet helpers-- DECLARE facet_data TYPE LONG&lt;facet_data&gt; not null; DECLARE test_facets facet_data; DECLARE FUNCTION cql_facets_new() facet_data; DECLARE PROCEDURE cql_facets_delete(facets facet_data); DECLARE FUNCTION cql_facet_add(facets facet_data, facet TEXT NOT NULL, crc LONG NOT NULL) BOOL NOT NULL; DECLARE FUNCTION cql_facet_find(facets facet_data, facet TEXT NOT NULL) LONG NOT NULL;  Declaration Section​ Wherein all the necessary objects are declared... -- declare sqlite_master -- CREATE TABLE sqlite_master ( type TEXT NOT NULL, name TEXT NOT NULL, tbl_name TEXT NOT NULL, rootpage INTEGER NOT NULL, sql TEXT NOT NULL );  The sqlite_master table is built-in but it has to be introduced to CQL so that we can query it. Like all the other loose DDL declarations here there is no code generated for this. We are simply declaring tables. To create code you have to put the DDL in a proc. Normally DDL in procs also declares the table but since we may need the original version of a table created and the final version declared we have @schema_upgrade_script to help avoid name conflicts. -- declare full schema of tables and views to be upgraded -- CREATE TABLE foo( id INTEGER NOT NULL, rate LONG INT @DELETE(5), rate_2 LONG INT @DELETE(4, DeleteRate2Proc), id2 INTEGER DEFAULT 12345 @CREATE(4, CreateId2Proc), name TEXT @CREATE(5), name_2 TEXT @CREATE(6) ); CREATE TABLE table2( id INTEGER NOT NULL, name1 TEXT @CREATE(2, CreateName1Proc), name2 TEXT @CREATE(2, CreateName2Proc), name3 TEXT @CREATE(2), name4 TEXT @CREATE(2) ); CREATE TABLE added_table( id INTEGER NOT NULL, name1 TEXT, name2 TEXT @CREATE(4) ) @CREATE(3) @DELETE(5);  NOTE: all the tables are emitted including all the annotations. This lets us do the maximum validation when we compile this script. CREATE VIEW live_view AS SELECT * FROM foo; CREATE VIEW another_live_view AS SELECT * FROM foo; CREATE VIEW dead_view AS SELECT * FROM foo @DELETE(2);  These view declarations do very little. We only need the view names so we can legally drop the views. We create the views elsewhere. CREATE INDEX index_still_present ON table2 (name1, name2); CREATE INDEX index_going_away ON table2 (name3) @DELETE(3);  Just like views, these declarations introduce the index names and nothing else. CREATE TRIGGER trigger_one AFTER INSERT ON foo BEGIN DELETE FROM table2 WHERE table2.id = new.id; END;  We have only the one trigger; we declare it here. -- facets table declaration -- CREATE TABLE IF NOT EXISTS test_cql_schema_facets( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL );  This is where we will store everything we know about the current state of the schema. Below we define a few helper procs for reading and writing that table and reading sqlite_master -- saved facets table declaration -- CREATE TEMP TABLE test_cql_schema_facets_saved( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL );  We will snapshot the facets table at the start of the run so that we can produce a summary of the changes at the end of the run. This table will hold that snapshot. NOTE: the prefix &quot;test&quot; was specified when this file was built so all the methods and tables begin with test_. Helper Procedures​ -- helper proc for testing for the presence of a column/type CREATE PROCEDURE test_check_column_exists(table_name TEXT NOT NULL, decl TEXT NOT NULL, OUT present BOOL NOT NULL) BEGIN SET present := (SELECT EXISTS(SELECT * FROM sqlite_master WHERE tbl_name = table_name AND sql GLOB decl)); END;  check_column_exists inspects sqlite_master and returns true if a column matching decl exists. -- helper proc for creating the schema version table CREATE PROCEDURE test_create_cql_schema_facets_if_needed() BEGIN CREATE TABLE IF NOT EXISTS test_cql_schema_facets( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL ); END;  Here we actually create the cql_schema_facets table with DDL inside a proc. In a non-schema-upgrade script the above would give a name conflict. -- helper proc for saving the schema version table CREATE PROCEDURE test_save_cql_schema_facets() BEGIN DROP TABLE IF EXISTS test_cql_schema_facets_saved; CREATE TEMP TABLE test_cql_schema_facets_saved( facet TEXT NOT NULL PRIMARY KEY, version LONG INTEGER NOT NULL ); INSERT INTO test_cql_schema_facets_saved SELECT * FROM test_cql_schema_facets; END;  The save_sql_schema_facets procedure simply makes a snapshot of the current facets table. Later we use this snapshot to report the differences by joining these tables. -- helper proc for setting the schema version of a facet CREATE PROCEDURE test_cql_set_facet_version(_facet TEXT NOT NULL, _version LONG INTEGER NOT NULL) BEGIN INSERT OR REPLACE INTO test_cql_schema_facets (facet, version) VALUES(_facet, _version); END; -- helper proc for getting the schema version of a facet CREATE PROCEDURE test_cql_get_facet_version(_facet TEXT NOT NULL, out _version LONG INTEGER NOT NULL) BEGIN BEGIN TRY SET _version := (SELECT version FROM test_cql_schema_facets WHERE facet = _facet LIMIT 1 IF NOTHING -1); END TRY; BEGIN CATCH SET _version := -1; END CATCH; END;  The two procedures cql_get_facet_version and cql_set_facet_version do just what you would expect. Note the use of try and catch to return a default value if the select fails. There are two additional helper procedures that do essentially the same thing using a schema version index. These two methods exist only to avoid unnecessary repeated string literals in the output file which cause bloat. -- helper proc for getting the schema version CRC for a version index CREATE PROCEDURE test_cql_get_version_crc(_v INTEGER NOT NULL, out _crc LONG INTEGER NOT NULL) BEGIN SET _crc := cql_facet_find(test_facets, printf('cql_schema_v%d', _v)); END; -- helper proc for setting the schema version CRC for a version index CREATE PROCEDURE test_cql_set_version_crc(_v INTEGER NOT NULL, _crc LONG INTEGER NOT NULL) BEGIN INSERT OR REPLACE INTO test_cql_schema_facets (facet, version) VALUES('cql_schema_v'||_v, _crc); END;  As you can see, these procedures are effectively specializations of cql_get_facet_version and cql_set_facet_version where the facet name is computed from the integer. Triggers require some special processing. There are so-called &quot;legacy&quot; triggers that crept into the system. These begin with tr__ and they do not have proper tombstones. In fact some are from early versions of CQL before they were properly tracked. To fix any old databases that have these in them, we delete all triggers that start with tr__. Note we have to use the GLOB operator to do this, because _ is the LIKE wildcard. -- helper proc to reset any triggers that are on the old plan -- DECLARE PROCEDURE cql_exec_internal(sql TEXT NOT NULL) USING TRANSACTION; CREATE PROCEDURE test_cql_drop_legacy_triggers() BEGIN DECLARE C CURSOR FOR SELECT name from sqlite_master WHERE type = 'trigger' AND name GLOB 'tr__*'; LOOP FETCH C BEGIN call cql_exec_internal(printf('DROP TRIGGER %s;', C.name)); END; END;  Baseline Schema​ The 'baseline' or 'v0' schema is unannotated (no @create or @recreate). The first real schema management procedures are for creating and dropping these tables. CREATE PROCEDURE test_cql_install_baseline_schema() BEGIN CREATE TABLE foo( id INTEGER NOT NULL, rate LONG_INT, rate_2 LONG_INT ); CREATE TABLE table2( id INTEGER NOT NULL ); END;  -- helper proc for dropping baseline tables before installing the baseline schema CREATE PROCEDURE test_cql_drop_baseline_tables() BEGIN DROP TABLE IF EXISTS foo; DROP TABLE IF EXISTS table2; END;  Migration Procedures​ The next section declares the migration procedures that were in the schema. These are expected to be defined elsewhere. -- declared upgrade procedures if any DECLARE proc CreateName1Proc() USING TRANSACTION; DECLARE proc CreateName2Proc() USING TRANSACTION; DECLARE proc CreateId2Proc() USING TRANSACTION; DECLARE proc DeleteRate2Proc() USING TRANSACTION;  The code below will refer to these migration procedures. We emit a declaration so that we can use the names in context. NOTE: USING TRANSACTION when applied to a proc declaration simply means the proc will access the database so it needs to be provided with a sqlite3 *db parameter. Views​ -- drop all the views we know CREATE PROCEDURE test_cql_drop_all_views() BEGIN DROP VIEW IF EXISTS live_view; DROP VIEW IF EXISTS another_live_view; DROP VIEW IF EXISTS dead_view; END; -- create all the views we know CREATE PROCEDURE test_cql_create_all_views() BEGIN CREATE VIEW live_view AS SELECT * FROM foo; CREATE VIEW another_live_view AS SELECT * FROM foo; END;  View migration is done by dropping all views and putting all views back. NOTE: dead_view was not created, but we did try to drop it if it existed. Indices​ -- drop all the indices that are deleted or changing CREATE PROCEDURE test_cql_drop_all_indices() BEGIN IF cql_facet_find(test_facets, 'index_still_present_index_crc') != -6823087563145941851 THEN DROP INDEX IF EXISTS index_still_present; END IF; DROP INDEX IF EXISTS index_going_away; END; -- create all the indices we need CREATE PROCEDURE test_cql_create_indices() BEGIN IF cql_facet_find(test_facets, 'index_still_present_index_crc') != -6823087563145941851 THEN CREATE INDEX index_still_present ON table2 (name1, name2); CALL test_cql_set_facet_version('index_still_present_index_crc', -6823087563145941851); END IF; END;  Indices are processed similarly to views, however we do not want to drop indices that are not changing. Therefore we compute the CRC of the index definition. At the start of the script any indices that are condemned (e.g. index_going_away) are dropped as well as any that have a new CRC. At the end of migration, changed or new indices are (re)created using cql_create_indices. Triggers​ - drop all the triggers we know CREATE PROCEDURE test_cql_drop_all_triggers() BEGIN CALL test_cql_drop_legacy_triggers(); DROP TRIGGER IF EXISTS trigger_one; END; -- create all the triggers we know CREATE PROCEDURE test_cql_create_all_triggers() BEGIN CREATE TRIGGER trigger_one AFTER INSERT ON foo BEGIN DELETE FROM table2 WHERE table2.id = new.id; END; END;  Triggers are always dropped before migration begins and are re-instated quite late in the processing as we will see below. Caching the state of the facets​ To avoid selecting single rows out of the facets table repeatedly we introduce this procedure whose job is to harvest the facets table and store it in a dictionary. The helpers that do this were declared above. You've already seen usage of the facets in the code above. CREATE PROCEDURE test_setup_facets() BEGIN BEGIN TRY SET test_facets := cql_facets_new(); DECLARE C CURSOR FOR SELECT * from test_cql_schema_facets; LOOP FETCH C BEGIN LET added := cql_facet_add(test_facets, C.facet, C.version); END; END TRY; BEGIN CATCH -- if table doesn't exist we just have empty facets, that's ok END CATCH; END;  Main Migration Script​ The main script orchestrates everything. There are inline comments for all of it. The general order of events is: create schema facets table if neededcheck main schema crc; if it matches we're done here, otherwise continue... These operations are done in test_perform_needed_upgrades drop all viewsdrop condemned indicesfetch the current schema versionif version 0 then install the baseline schema (see below)for each schema version with changes do the following: create any tables that need to be created in this versionadd any columns that need to be added in this versionrun migration procs in this order: create tablecreate columndelete triggerdelete viewdelete indexdelete columndelete table drop any tables that need to be dropped in this versionmark schema upgraded to the current version so far, and proceed to the next versioneach partial step is also marked as completed so that it can be skipped if the script is run again create all the views(re)create any indices that changed and are not deadset the schema CRC to the current CRC That's it... the details are below. CREATE PROCEDURE test_perform_upgrade_steps() BEGIN DECLARE column_exists BOOL NOT NULL; DECLARE schema_version LONG INTEGER NOT NULL; -- dropping all views -- CALL test_cql_drop_all_views(); -- dropping condemned or changing indices -- CALL test_cql_drop_all_indices(); -- dropping condemned or changing triggers -- CALL test_cql_drop_all_triggers(); ---- install baseline schema if needed ---- CALL test_cql_get_version_crc(0, schema_version); IF schema_version != -9177754326374570163 THEN CALL test_cql_install_baseline_schema(); CALL test_cql_set_version_crc(0, -9177754326374570163); END IF; ---- upgrade to schema version 2 ---- CALL test_cql_get_version_crc(2, schema_version); IF schema_version != -6840158498294659234 THEN -- altering table table2 to add column name1 TEXT; CALL test_check_column_exists('table2', '*[( ]name1 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name1 TEXT; END IF; -- altering table table2 to add column name2 TEXT; CALL test_check_column_exists('table2', '*[( ]name2 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name2 TEXT; END IF; -- altering table table2 to add column name3 TEXT; CALL test_check_column_exists('table2', '*[( ]name3 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name3 TEXT; END IF; -- altering table table2 to add column name4 TEXT; CALL test_check_column_exists('table2', '*[( ]name4 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE table2 ADD COLUMN name4 TEXT; END IF; -- data migration procedures IF cql_facet_find(test_facets, 'CreateName1Proc') = -1 THEN CALL CreateName1Proc(); CALL test_cql_set_facet_version('CreateName1Proc', 2); END IF; IF cql_facet_find(test_facets, 'CreateName2Proc') = -1 THEN CALL CreateName2Proc(); CALL test_cql_set_facet_version('CreateName2Proc', 2); END IF; CALL test_cql_set_version_crc(2, -6840158498294659234); END IF; ---- upgrade to schema version 3 ---- CALL test_cql_get_version_crc(3, schema_version); IF schema_version != -4851321700834943637 THEN -- creating table added_table CREATE TABLE IF NOT EXISTS added_table( id INTEGER NOT NULL, name1 TEXT ); CALL test_cql_set_version_crc(3, -4851321700834943637); END IF; ---- upgrade to schema version 4 ---- CALL test_cql_get_version_crc(4, schema_version); IF schema_version != -6096284368832554520 THEN -- altering table added_table to add column name2 TEXT; CALL test_check_column_exists('added_table', '*[( ]name2 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE added_table ADD COLUMN name2 TEXT; END IF; -- altering table foo to add column id2 INTEGER; CALL test_check_column_exists('foo', '*[( ]id2 INTEGER*', column_exists); IF NOT column_exists THEN ALTER TABLE foo ADD COLUMN id2 INTEGER DEFAULT 12345; END IF; -- logical delete of column rate_2 from foo; -- no ddl -- data migration procedures IF cql_facet_find(test_facets, 'CreateId2Proc') = -1 THEN CALL CreateId2Proc(); CALL test_cql_set_facet_version('CreateId2Proc', 4); END IF; IF cql_facet_find(test_facets, 'DeleteRate2Proc') = -1 THEN CALL DeleteRate2Proc(); CALL test_cql_set_facet_version('DeleteRate2Proc', 4); END IF; CALL test_cql_set_version_crc(4, -6096284368832554520); END IF; ---- upgrade to schema version 5 ---- CALL test_cql_get_version_crc(5, schema_version); IF schema_version != 5720357430811880771 THEN -- altering table foo to add column name TEXT; CALL test_check_column_exists('foo', '*[( ]name TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE foo ADD COLUMN name TEXT; END IF; -- logical delete of column rate from foo; -- no ddl -- dropping table added_table DROP TABLE IF EXISTS added_table; CALL test_cql_set_version_crc(5, 5720357430811880771); END IF; ---- upgrade to schema version 6 ---- CALL test_cql_get_version_crc(6, schema_version); IF schema_version != 3572608284749506390 THEN -- altering table foo to add column name_2 TEXT; CALL test_check_column_exists('foo', '*[( ]name_2 TEXT*', column_exists); IF NOT column_exists THEN ALTER TABLE foo ADD COLUMN name_2 TEXT; END IF; CALL test_cql_set_version_crc(6, 3572608284749506390); END IF; CALL test_cql_create_all_views(); CALL test_cql_create_all_indices(); CALL test_cql_create_all_triggers(); CALL test_cql_set_facet_version('cql_schema_version', 6); CALL test_cql_set_facet_version('cql_schema_crc', -7714030317354747478); END;  We have one more helper that will look for evidence that we're trying to move backwards to a previous schema version. This is not supported. This procedure also arranges for the original facet versions to be saved and it proceduces a difference in facets after the upgrade is done. CREATE PROCEDURE test_perform_needed_upgrades() BEGIN -- check for downgrade -- IF cql_facet_find(test_facets, 'cql_schema_version') &gt; 6 THEN SELECT 'downgrade detected' facet; ELSE -- save the current facets so we can diff them later -- CALL test_save_cql_schema_facets(); CALL test_perform_upgrade_steps(); -- finally produce the list of differences SELECT T1.facet FROM test_cql_schema_facets T1 LEFT OUTER JOIN test_cql_schema_facets_saved T2 ON T1.facet = T2.facet WHERE T1.version is not T2.version; END IF; END;  This is the main function for upgrades, it checks only the master schema version. This function is separate so that the normal startup path doesn't have to have the code for the full upgrade case in it. This lets linker order files do a superior job (since full upgrade is the rare case). CREATE PROCEDURE test() BEGIN DECLARE schema_crc LONG INTEGER NOT NULL; -- create schema facets information table -- CALL test_create_cql_schema_facets_if_needed(); -- fetch the last known schema crc, if it's different do the upgrade -- CALL test_cql_get_facet_version('cql_schema_crc', schema_crc); IF schema_crc &lt;&gt; -7714030317354747478 THEN BEGIN TRY CALL test_setup_facets(); CALL test_perform_needed_upgrades(); END TRY; BEGIN CATCH CALL cql_facets_delete(test_facets); SET test_facets := 0; THROW; END CATCH; CALL cql_facets_delete(test_facets); SET test_facets := 0; ELSE -- some canonical result for no differences -- SELECT 'no differences' facet; END IF; END;  Temp Tables​ We had no temporary tables in this schema, but if there were some they get added to the schema after the upgrade check. A procedure like this one is generated: CREATE PROCEDURE test_cql_install_temp_schema() BEGIN CREATE TEMP TABLE tempy( id INTEGER ); END;  This entry point can be used any time you need the temp tables. But normally it is automatically invoked.  ---- install temp schema after upgrade is complete ---- CALL test_cql_install_temp_schema();  That logic is emitted at the end of the test procedure. "},{"title":"Schema Regions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#schema-regions","content":"Schema Regions are designed to let you declare your schema in logical regions whose dependencies are specified. It enforces the dependencies you specify creating errors if you attempt to break the declared rules. Schema regions allow you to generate upgrade scripts for parts of your schema that can compose and be guaranteed to remain self-consistent. Details​ In many cases schema can be factored into logical and independent islands. This is desireable for a number of reasons: so that the schema can go into different databasesso that the schema can be upgraded on a different scheduleso that &quot;not relevant&quot; schema can be omitted from distributionsso that parts of your schema that have no business knowing about each other can be prevented from taking dependencies on each other These all have very real applications: E.g. Your Application has an on-disk and an in-memory database​ This creates basically three schema regions: on disk: which cannot refer to the in-memory at allin-memory: which cannot refer to the on-disk schema at allcross-db: which refers to both, also in memory (optional) Your Application Needs To Upgrade Each of the Above​ There must be a separate upgrade script for both the island databases and yet a different one for the &quot;cross-db&quot; database Your Customer Doesn't Want The Kitchen Sink of Schema​ If you're making a library with database support, your customers likely want to be able to create databases that have only features they want; you will want logical parts within your schema that can be separated for cleanliness and distribution. Declaring Regions and Dependencies​ Schema Regions let you create logical groupings, you simply declare the regions you want and then start putting things into those regions. The regions form a directed acyclic graph -- just like C++ base classes. You create regions like this: @declare_schema_region root; @declare_schema_region extra using root;  The above simply declares the regions -- it doesn't put anything into them. In this case we now have a root region and an extra region. The root schema items will not be allowed to refer to anything in extra. Without regions, you could also ensure that the above is true by putting all the extra items afer the root in the input file but things can get more complicated than that in general, and the schema might also be in several files, complicating ordering as the option. Also, relying on order could be problematic as it is quite easy to put things in the wrong place (e.g. add a new root item after the extra items). Making this a bit more complicated, we could have: @declare_schema_region feature1 using extra; @declare_schema_region feature2 using extra; @declare_schema_region everything using feature1, feature2;  And now there are many paths to root from the everything region; that's ok but certainly it will be tricky to do all that with ordering. Using Regions​ An illustrative example, using the regions defined above: @begin_schema_region root; create table main( id integer, name text ); create view names as select name from main order by name; @end_schema_region; @begin_schema_region extra; create table details( id integer references main(id), details text ); create proc get_detail(id_ integer) begin select T1.id, T1.details, T2.name from details T1 inner join main T2 on T1.id = T2.id where T1.id = id_; end; @end_schema_region; @begin_schema_region feature1; create table f1( id integer references details(id), f1_info text ); create proc get_detail(id_ integer) begin select T1.id, T1.details, T2.name, f1_info from details T1 inner join f T2 on T1.id = T2.id inner join f1 on f1.id = T1.id where T1.id = id_; end; @end_schema_region; @begin_schema_region feature2; -- you can use details, and main but not f1 @end_schema_region;  With the structure above specified, even if a new contribution to the root schema appears later, the rules enforce that this region cannot refer to anything other than things in root. This can be very important if schema is being included via #include and might get pulled into the compilation in various orders. A feature area might also have a named public region that others things can depend on (e.g. some views) and private regions (e.g. some tables, or whatever). Region Visibility​ Schema regions do not provide additional name spaces -- the names of objects should be unique across all regions. In other words, regions do not hide or scope entity names; rather they create errors if inappropriate names are used. Case 1: The second line will fail semantic validation because table A already exists -- obvious standard name conflict create table A (id integer); create table A (id integer, name text);  Case 2: This fails for the same reason as case #1. Table A already exists @declare_region root; -- table A is in no region create table A (id integer); @begin_region root: -- this table A is in the root region, still an error create table A (id integer, name text); @end_region;  Case 3: Again fails for the same reason as case #1. Table A already exist in region extra, and you cannot define another table with the same name in another region. @declare_region root; @declare_region extra; @begin_region extra; -- so far so good create table A (id integer); @end_region; @begin_region root; -- no joy, this A conflicts with the previous A create table A (id integer, name text); @end_region;  Really the visibility rules couldn't be anything other than the above, as SQLite has no knowledge of regions at all and so any exotic name resolution would just doom SQLite statements to fail when they finally run. Exception for &quot;... LIKE &lt;table&gt;&quot; statement​ The rules above are enforced for all constructs except for where the syntactic sugar ... LIKE &lt;table&gt; forms, which can happen in a variety of statements. This form doesn't create a dependence on the table (but does create a dependence on its shape). When CQL generates output, the LIKE construct is replaced with the actual names of the columns it refers to. But these are independent columns, so this is simply a keystroke saver. The table (or view, cursor, etc.) reference will be gone. These cases below will succeed. @declare_region root; create table A (...); create view B (....); create procedure C {...} @begin_region root; create table AA(LIKE A); create table BB(LIKE B); create table CC(LIKE C); @end_region;  Note: this exception may end up causing maintenance problems and so it might be revisited in the future. Maintaining Schema in Pieces​ When creating upgrade scripts, using the --rt schema_upgrade flags you can add region options --include_regions a b c and --exclude_regions d e f per the following: Included regions: must be valid region names -- the base types are walked to compute all the regions that are &quot;in&quot;declarations are emitted in the upgrade for all of the &quot;in&quot; objects -- &quot;exclude&quot; does not affect the declarations Excluded regions: must be valid region names and indicate parts of schema that are upgraded elsewhere, perhaps with a seperate CQL run, a different automatic upgrade, or even a manual mechanismupgrade code will be generated for all the included schema, but not for the excluded regions and their contents Example: Referring to the regions above you might do something like this  # All of these also need a --global_proc param for the entry point but that's not relevant here cql --in schema.sql --cg shared.sql --rt schema_upgrade --include_regions extra cql --in schema.sql --cg f1.cql --rt schema_upgrade --include_regions feature1 --exclude_regions extra cql --in schema.sql --cg f2.cql --rt schema_upgrade --include_regions feature2 --exclude_regions extra  The first command generates all the shared schema for regions root and extra because extra contains root The second command declares all of root and extra so that the feature1 things can refer to them, however the upgrade code for these shared regions is not emitted. Only the upgrade for schema in feature1 is emitted. feature2 is completely absent. This will be ok because we know feature1 cannot depend on feature2 and extra is assumed to be upgraded elsewhere (such as in the previous line). The third command declares all of root and extra so that the feature2 things can refer to them, however the upgrade code for these shared regions is not emitted. Only the upgrade for schema in feature2 is emitted. feature1 is completely absent. Note that in the above examples, CQL is generating more CQL to be compiled again (a common pattern). The CQL upgrade scripts need to be compiled as usual to produce executable code. Thus the output of this form includes the schema declarations and executable DDL. Schema Not In Any Region​ For schema that is not in any region you might imagine that it is a special region &lt;none&gt; that depends on everything. So basically you can put anything there. Schema that is in any region cannot ever refer to schema that is in &lt;none&gt;. When upgrading, if any include regions are specified then &lt;none&gt; will not be emitted at all. If you want an upgrader for just &lt;none&gt; this is possible with an assortment of exclusions. You can always create arbitrary grouping regions to make this easier. A region named any that uses all other regions would make this simple. In general, best practice is that there is no schema in &lt;none&gt;, but since most SQL code has no regions some sensible meaning has to be given to DDL before it gets region encodings. Deployable Regions​ Given the above we note that some schema regions correspond to the way that we will deploy the schema. We want those bundles to be safe to deploy but to in order to be so we need a new notion -- a deployable region. To make this possible CQL includes the following: You can declare a region as deployable using @declare_deployable_regionCQL computes the covering of a deployable region: its transitive closure up to but not including any deployable regions it referencesNo region is allowed to depend on a region that is within the interior of a different deployable region, but you can depend on the deployable region itself Because of the above, each deployable region is in fact a well defined root for the regions it contains. The deployable region becomes the canonical way in which a bundle of regions (and their content) is deployed and any given schema item can be in only one deployable region. Motivation and Examples​ As we saw above, regions are logical groupings of tables/views/etc such that if an entity is in some region R then it is allowed to only refer to the things that R declared as dependencies D1, D2, etc. and their transitive closures. You can make as many logical regions as you like and you can make them as razor thin as you like; they have no physical reality but they let you make as many logical groups of things as you might want. Additionally, when we’re deploying schema you generally need to do it in several pieces. E.g. if we have tables that go in an in-memory database then defining a region that holds all the in-memory tables makes it easy to, say, put all those in-memory tables into a particular deployment script. Now we come to the reason for deployable regions. From CQL’s perspective, all regions are simply logical groups; some grouping is then meaningful to programmers but has no physical reality. This means you’re free to reorganize tables etc. as you see fit into new or different regions when things should move. Only, that’s not quite true. The fact that we deploy our schema in certain ways means while most logical moves are totally fine, if you were to move a table from, say, the main database region to the in-memory region you would be causing a major problem. Some installations may already have the table in the main area and there would be nothing left in the schema to tell CQL to drop the table from the main database -- the best you can hope for is the new location gets a copy of the table the old location keeps it and now there are name conflicts forever. So, the crux of the problem is this: We want to let you move schema freely between logical regions in whatever way makes sense to you, but once you pick the region you are going to deploy in, you cannot change that. To accomplish this, CQL needs to know that some of the regions are deployable regions and there have to be rules to make it all makes sense. Importantly, every region has to be contained in at most one deployable region. Since the regions form a DAG we must create an error if any region could ever roll up to two different deployable regions. The easiest way to describe this rule is “no peeking” – the contents of a deployable region are “private” they can refer to each other in any DAG shape but outside of the deployable region you can only refer to its root. So you can still compose them but each deployable region owns a well-defined covering. Note that you can make as many fine-grained deployable regions as you want; you don’t actually have to deploy them separately, but you get stronger rules about the sharing when you do. Here’s an example: Master Deployment 1 Feature 1 (Deployable) logical regions for feature 1 Core (Deployable) logical regions for core Feature 2 (Deployable) logical regions for feature 2 Core ... Master Deployment 2 Feature 1 (Deployable) ... Feature 3 (Deployable) logical regions for feature 3  In the above: none of the logical regions for feature 1, 2, 3 are allowed to refer to logical regions in any other feature, though any of them could refer to Core (but not directly to what is inside Core)within those regions you can make any set of groupings that makes sense and you can change them over time as you see fit, with some restrictionsany such regions are not allowed to move to a different Feature group (because those are deployment regions)the Master Deployment regions just group features in ways we’d like to deploy them; in this case there are two deployments: one that includes Feature 1 &amp; 2 and another that includes Feature 1 &amp; 3the deployable region boundaries are preventing Feature 1 regions from using Feature 2 regions in an ad hoc way (i.e. you can't cheat by taking a direct dependency on something inside a different feature), but both Features can use CoreFeature 3 doesn’t use Core but Core will still be in Master Deployment 2 due to Feature 1 Note that the deployable regions for Feature 1, 2, and 3 aren't actually deployed alone, but they are adding enforcement that makes the features cleaner Because of how upgrades work, “Core” could have its own upgrader. Then when you create the upgrader for Master Deployment 1 and 2, you can specify “exclude Core” in which case those tables are assumed to be updated independently. You could create as many or as few independently upgrade-able things with this pattern. Because regions are not allowed to &quot;peek&quot; inside of a deployable region, you can reorganize your logical regions without breaking other parts of the schema. Private Regions​ The above constructs create a good basis for creating and composing regions, but a key missing aspect is the ability to hide internal details in the logical groups. This becomes increasingly important as your desire to modularize schema grows; you will want to have certain parts that can change without worrying about breaking others and without fear that there are foreign keys and so forth referring to them. To accomplish this, CQL provides the ability to compose schema regions with the optional private keyword. In the following example there will be three regions creatively named r1, r2, and r3. Region r2 consumes r1 privately and therefore r3 is not allowed to use things in r1 even though it consumes r2. When creating an upgrade script for r3 you will still need (and will get) all of r2 and r1, but from a visibility perspective r3 can only directly depend on r2. @declare_schema_region r1; @declare_schema_region r2 using r1 private; @declare_schema_region r3 using r2; @begin_schema_region r1; create table r1_table(id integer primary key); @end_schema_region; @begin_schema_region r2; create table r2_table(id integer primary key references r1_table(id)); @end_schema_region; @begin_schema_region r3; -- this is OK create table r3_table_2(id integer primary key references r2_table(id)); -- this is an error, no peeking into r1 create table r3_table_1(id integer primary key references r1_table(id)); @end_schema_region;  As expected r2 is still allowed to use r1 because your private regions are not private from yourself. So you may think it’s easy to work around this privacy by simply declaring a direct dependency on r1 wherever you need it. @declare_schema_region my_sneaky_region using r1, other_stuff_I_need;  That would seem to make it all moot. However, this is where deployable regions come in. Once you bundle your logical regions in a deployable region there’s no more peeking inside the the deployable region. So we could strengthen the above to: @declare_deployable_region r2 using r1 private;  Once this is done it becomes an error to try to make new regions that peek into r2; you have to take all of r2 or none of it -- and you can’t see the private parts. Of course you can do region wrapping at any level so you can have as many subgroups as you like, whatever is useful. You can even add additional deployable regions that aren’t actually deployed to get the &quot;hardened&quot; grouping at no cost. So, in summary, to get true privacy, first make whatever logical regions you like that are helpful. Put privacy where you need/want it. Import logical regions as much as you want in your own bundle of regions. Then wrap that bundle up in a deployable region (they nest) and then your private regions are safe from unwanted usage. "},{"title":"Unsubscription and Resubscription Features​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#unsubscription-and-resubscription-features","content":"Any significant library that is centered around a database is likely to accrue significant amounts of schema to support its features. Often users of the library don’t want all its features and therefore don’t want all of its schema. CQL’s primary strategy is to allow the library author to divide the schema into regions and then the consumer of the library may generate a suitable schema deployer that deploys only the desired regions. You simply subscribe to the regions you want. The @unsub construct deals with the unfortunate situation of over-subscription. In the event that a customer has subscribed to regions that it turns out they don’t need, or if indeed the regions are not fine-grained enough, they may wish to (possibly much later) unsubscribe from particular tables or entire regions that they previously had included. Unfortunately it’s not so trivial as to simply remove the regions after the fact. The problem is that there might be billions of devices that already have the undesired tables and are paying the initialization costs for them. Affirmatively removing the tables is highly desirable and that means a forward-looking annotation is necessary to tell the upgrader to generate DROP statements at some point. Furthermore, a customer might decide at some point later that now is the time they need the schema in question, so resubcription also has to be possible. Unsubscription and Resubscription​ To accomplish this we add the following construct: @unsub(table_name);  The effects of a valid @unsub are as follows: The table is no longer accessible by statementsIf the table is marked @create, then “DROP IF EXISTS tablename” is emitted into the upgrade steps for _version_numberIf the table is @recreate the table is unconditionally dropped as though it had been deletedThe JSON includes the unsub details in a new subscriptions section The compiler ensures that the directives are valid and stay valid. Validations for @unsub(table):​ table must be a valid table nametable must not be already unsubscribedIf table must not be marked with @delete unsubscribing from a table after it’s been outright deleted is clearly a mistake For every child table -- those that mention this table using REFERENCES The child must be already deleted or unsubscribedThe deletion or unsubscription must have happened at a version &lt;= version table is marked unsubscribed for purposes of further analysis caution The legacy form @unsub(version, table) is supported but deprecated, and will soon be an error. The version is ignored. The legacy @resub directive is now an error; Resubscription is accomplished by simply removing the relevant @unsubdirective(s). Previous Schema validations for @unsub​ Unsubscriptions may be removed when they are no longer desired in order to resubscribe as long as this results in a valid chain of foreign keys. These validations are sufficient to guarantee a constistent logical history for unsubscriptions. "},{"title":"Chapter 11: Previous Schema Validation​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-11-previous-schema-validation","content":"As we saw in the previous chapter, CQL includes powerful schema management tools for creating automatic upgrade scripts for your databases. However, not all schema alterations are possible after-the-fact and so CQL also includes schema comparison tools to help you avoid problems as you version your schema over time. You can compare the previous version of a schema with the current version to do additional checks such as: the data type of a column may not changethe attributes of a column (e.g. nullable, default value) may not changecolumns can't be renamedcolumns can't be removed, only marked deletenew columns must be at the end of the table and marked with createcreated columns have to be created in a schema version &gt;= any that previously existed (no creating columns in the past)nothing other than new columns at the end may be added to a table (e.g. new PK/UK is right out)new tables must be marked create, deleted tables must be marked deletenew views must be marked create, deleted views must be marked deletenew indices must be marked create, deleted indices must be marked deletean item that was previously a table/view cannot turn into the other oneversion numbers in the annotations may not ever changeif any annotation has a migration proc associated with it, it cannot change to a different proc latercreated tables, views, indices have to be created in a schema version &gt;= any that previously existed (no creating tables in the past)there may be other checks not mentioned here When checking @recreate tables against the previous schema version for errors, these checks are done: suppress checking of any table facet changes in previous schema on recreate tables; you can do anything you wantallow new @recreate tables to appear with no @create neededallow a table to go from &quot;original schema&quot; (no annotation) to @recreate but not backallow a table to go from @recreate to @create at the current schema versionallow a table to go from recreate directly to @delete at the current schema versiondo not allow a table to go from @create or @delete state to @recreate All of these are statically checked. To use these tools, you must run CQL in a mode where it has both the proposed and existing schema in its input stream, then it can provide suitable errors if any unsupported change is about to happen. "},{"title":"Basic Usage​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#basic-usage","content":"The normal way that you do previous schema validation is to create an input file that provides both schema. This file may look something like this: -- prev_check.sql create table foo( id integer, new_field text @create(1) ); @previous_schema; create table foo( id integer );  So, here the old version of foo will be validated against the new version and all is well. A new nullable text field was added at the end. In practice these comparisons are likely to be done in a somewhat more maintainable way, like so: -- prev_check.sql #include &quot;table1.sql&quot; #include &quot;table2.sql&quot; #include &quot;table3.sql&quot; @previous_schema; #include &quot;previous.sql&quot;  Now importantly, in this configuration, everything that follows the @previous_schema directive does not actually contribute to the declared schema. This means the --rt schema result type will not see it. Because of this, you can do your checking operation like so: cc -E -x c prev_check.sql | cql --cg new_previous_schema.sql --rt schema  The above command will generate the schema in new_previous_schema and, if this command succeeds, it's safe to replace the existingprevious.sql with new_previous_schema. NOTE: you can bootstrap the above by leaving off the @previous_schema and what follows to get your first previous schema from the command above. Now, as you can imagine, comparing against the previous schema allows many more kinds of errors to be discovered. What follows is a large chunk of the CQL tests for this area taken from the test files themselves. For easy visibility I have brought each fragment of current and previous schema close to each other and I show the errors that are reported. We start with a valid fragment and go from there. Case 1 : No problemo​ create table foo( id integer not null, rate long int @delete(5, deletor), rate_2 long int @delete(4), id2 integer @create(4), name text @create(5), name_2 text @create(6) ); ------- create table foo( id integer not null, rate long int @delete(5, deletor), rate_2 long int @delete(4), id2 integer @create(4), name text @create(5), name_2 text @create(6) );  The table foo is the same! It doesn't get any easier than that. Case 2 : table create version changed​ create table t_create_version_changed(id integer) @create(1); ------- create table t_create_version_changed(id integer) @create(2); Error at sem_test_prev.sql:15 : in str : current create version not equal to previous create version for 't_create_version_changed'  You can't change the version a table was created in. Here the new schema says it appeared in version 1. The old schema says 2. Case 3 : table delete version changed​ create table t_delete_version_changed(id integer) @delete(1); ------- create table t_delete_version_changed(id integer) @delete(2); Error at sem_test_prev.sql:18 : in str : current delete version not equal to previous delete version for 't_delete_version_changed'  You can't change the version a table was deleted in. Here the new schema says it was gone in version 1. The old schema says 2. Case 4 : table not present in new schema​ -- t_not_present_in_new_schema is gone ------- create table t_not_present_in_new_schema(id integer); Error at sem_test_prev.sql:176 : in create_table_stmt : table was present but now it does not exist (use @delete instead) 't_not_present_in_new_schema'  So here t_not_present_in_new_schema was removed, it should have been marked with @delete. You don't remove tables. Case 5 : table is now a view​ create view t_became_a_view as select 1 id @create(6); ------- create table t_became_a_view(id integer); Error at sem_test_prev.sql:24 : in create_view_stmt : object was a table but is now a view 't_became_a_view'  Tables can't become views... Case 6 : table was in base schema, now created​ create table t_created_in_wrong_version(id integer) @create(1); ------- create table t_created_in_wrong_version(id integer); Error at sem_test_prev.sql:27 : in str : current create version not equal to previous create version for 't_created_in_wrong_version'  Here a version annotation is added after the fact. This item was already in the base schema. Case 7: table was in base schema, now deleted (ok)​ create table t_was_correctly_deleted(id integer) @delete(1); ------- create table t_was_correctly_deleted(id integer);  No errors here, just a regular delete. Case 8: column name changed​ create table t_column_name_changed(id_ integer); ------- create table t_column_name_changed(id integer); Error at sem_test_prev.sql:33 : in str : column name is different between previous and current schema 'id_'  You can't rename columns. We could support this but it's a bit of a maintenance nightmare and logical renames are possible easily without doing physical renames. Case 9 : column type changed​ create table t_column_type_changed(id real); ------- create table t_column_type_changed(id integer); Error at sem_test_prev.sql:36 : in str : column type is different between previous and current schema 'id'  You can't change the type of a column. Case 10 : column attribute changed​ create table t_column_attribute_changed(id integer not null); ------- create table t_column_attribute_changed(id integer); Error at sem_test_prev.sql:39 : in str : column type is different between previous and current schema 'id'  Change of column attributes counts as a change of type. Case 11: column version changed for delete​ create table t_column_delete_version_changed(id integer, id2 integer @delete(1)); ------- create table t_column_delete_version_changed(id integer, id2 integer @delete(2)); Error at sem_test_prev.sql:42 : in str : column current delete version not equal to previous delete version 'id2'  You can't change the delete version after it has been set. Case 12 : column version changed for create​ create table t_column_create_version_changed(id integer, id2 integer @create(1)); ------- create table t_column_create_version_changed(id integer, id2 integer @create(2)); Error at sem_test_prev.sql:45 : in str : column current create version not equal to previous create version 'id2'  You can't change the create version after it has been set. Case 13 : column default value changed​ create table t_column_default_value_changed(id integer, id2 integer not null default 2); ------- create table t_column_default_value_changed(id integer, id2 integer not null default 1); Error at sem_test_prev.sql:48 : in str : column current default value not equal to previous default value 'id2'  You can't change the default value after the fact. There's no alter statement that would allow this even though it does make some logical sense. Case 14 : column default value did not change (ok)​ create table t_column_default_value_ok(id integer, id2 integer not null default 1); ------- create table t_column_default_value_ok(id integer, id2 integer not null default 1);  No change. No error here. Case 15 : create table with additional attribute present and matching (ok)​ create table t_additional_attribute_present(a int not null, b int, primary key (a,b)); ------- create table t_additional_attribute_present(a int not null, b int, primary key (a,b));  No change. No error here. Case 16 : create table with additional attribute (doesn't match)​ create table t_additional_attribute_mismatch(a int not null, primary key (a)); ------- create table t_additional_attribute_mismatch(a int not null, b int, primary key (a,b)); Error at sem_test_prev.sql:57 : in pk_def : a table facet is different in the previous and current schema  This is an error because the additional attribute does not match the previous schema. Case 17 : column removed​ create table t_columns_removed(id integer); ------- create table t_columns_removed(id integer, id2 integer); Error at sem_test_prev.sql:255 : in col_def : items have been removed from the table rather than marked with @delete 't_columns_removed'  You can't remove columns from tables. You have to mark them with @delete instead. Case 18 : create table with added facet not present in the previous​ create table t_attribute_added(a int not null, primary key (a)); ------- create table t_attribute_added(a int not null); Error at sem_test_prev.sql:63 : in pk_def : table has a facet that is different in the previous and current schema 't_attribute_added'  Table facets like primary keys cannot be added after the fact. There is no way to do this in sqlite. Case 19 : create table with additional column and no @create​ create table t_additional_column(a int not null, b int); ------- create table t_additional_column(a int not null); Error at sem_test_prev.sql:66 : in col_def : table has columns added without marking them @create 't_additional_column'  If you add a new column like b above you have to mark it with @create in a suitable version. Case 20 : create table with additional column and `@create (ok)​ create table t_additional_column_ok(a int not null, b int @create(2), c int @create(6)); ------- create table t_additional_column_ok(a int not null, b int @create(2));  Column properly created. No errors here. Case 21 : create table with different flags (like TEMP)​ create TEMP table t_becomes_temp_table(a int not null, b int); ------- create table t_becomes_temp_table(a int not null, b int); Error at sem_test_prev.sql:72 : in create_table_stmt : table create statement attributes different than previous version 't_becomes_temp_table'  Table became a TEMP table, there is no way to generate an alter statement for that. Not allowed. Case 22 : create table and apply annotation (ok)​ create table t_new_table_ok(a int not null, b int) @create(6); ------- -- no previous version  No errors here; this is a properly created new table. Case 23 : create new table without annotation (error)​ create table t_new_table_no_annotation(a int not null, b int); ------- -- no previous version Error at sem_test_prev.sql:85 : in create_table_stmt : new table must be added with @create(6) or later 't_new_table_no_annotation'  This table was added with no annotation. It has to have an @create and be at least version 6, the current largest. Case 24 : create new table stale annotation (error)​ create table t_new_table_stale_annotation(a int not null, b int) @create(2); ------- -- no previous version Error at sem_test_prev.sql:91 : in create_table_stmt : new table must be added with @create(6) or later 't_new_table_stale_annotation'  The schema is already up to version 6. You can't then add a table in the past at version 2. Case 25 : add columns to table, marked @create and @delete​ create table t_new_table_create_and_delete(a int not null, b int @create(6) @delete(7)); ------- create table t_new_table_create_and_delete(a int not null); Error at sem_test_prev.sql:96 : in col_def : table has newly added columns that are marked both @create and @delete 't_new_table_create_and_delete'  Adding a column in the new version and marking it both create and delete is ... weird... don't do that. Technically you can do it (sigh) but it must be done one step at a time. Case 26 : add columns to table, marked @create correctly​ create table t_new_legit_column(a int not null, b int @create(6)); ------- create table t_new_legit_column(a int not null);  No errors here; new column added in legit version. Case 27 : create table with a create migration proc where there was none​ create table with_create_migrator(id integer) @create(1, ACreateMigrator); ------- create table with_create_migrator(id integer) @create(1); Error at sem_test_prev.sql:104 : in str : @create procedure changed in object 'with_create_migrator'  You can't add a create migration proc after the fact. Case 28 : create table with a different create migration proc​ create table with_create_migrator(id integer) @create(1, ACreateMigrator); ------- create table with_create_migrator(id integer) @create(1, ADifferentCreateMigrator); Error at sem_test_prev.sql:104 : in str : @create procedure changed in object 'with_create_migrator'  You can't change a create migration proc after the fact. Case 29 : create table with a delete migration proc where there was none​ create table with_delete_migrator(id integer) @delete(1, ADeleteMigrator); ------- create table with_delete_migrator(id integer) @delete(1); Error at sem_test_prev.sql:107 : in str : @delete procedure changed in object 'with_delete_migrator'  You can't add a delete migration proc after the fact. Case 30 : create table with a different delete migration proc​ create table with_delete_migrator(id integer) @delete(1, ADeleteMigrator); ------- create table with_delete_migrator(id integer) @delete(1, ADifferentDeleteMigrator); Error at sem_test_prev.sql:107 : in str : @delete procedure changed in object 'with_delete_migrator'  You can't change a delete migration proc after the fact. Case 31 : create a table which was a view in the previous schema​ create table view_becomes_a_table(id int); ------- create view view_becomes_a_table as select 1 X; Error at sem_test_prev.sql:110 : in create_table_stmt : object was a view but is now a table 'view_becomes_a_table'  Converting views to tables is not allowed. Case 32 : delete a view without marking it deleted​ --- no matching view in current schema ------- create view view_was_zomg_deleted as select 1 X; Error at sem_test_prev.sql:333 : in create_view_stmt : view was present but now it does not exist (use @delete instead) 'view_was_zomg_deleted'  Here the view was deleted rather than marking it with @delete, resulting in an error. Case 33 : create a new version of this view that is not temp​ create view view_was_temp_but_now_it_is_not as select 1 X; ------- create temp view view_was_temp_but_now_it_is_not as select 1 X; Error at sem_test_prev.sql:339 : in create_view_stmt : TEMP property changed in new schema for view 'view_was_temp_but_now_it_is_not'  A temp view became a view. This flag is not allowed to change. Side note: temp views are weird. Case 34 : create a new version of this view that was created in a different version​ create view view_with_different_create_version as select 1 X @create(3); ------- create view view_with_different_create_version as select 1 X @create(2); Error at sem_test_prev.sql:116 : in str : current create version not equal to previous create version for 'view_with_different_create_version'  You can't change the create version of a view after the fact. Case 35 : create an index that is now totally gone in the new schema​ --- no matching index in current schema ------- create index this_index_was_deleted_with_no_annotation on foo(id); Error at sem_test_prev.sql:349 : in create_index_stmt : index was present but now it does not exist (use @delete instead) 'this_index_was_deleted_with_no_annotation'  You have to use @delete on indices to remove them correctly. Case 36 : create a view with no annotation that is not in the previous schema​ create view view_created_with_no_annotation as select 1 X; ------- --- there is no previous version Error at sem_test_prev.sql:122 : in create_view_stmt : new view must be added with @create(6) or later 'view_created_with_no_annotation'  You have to use @create on views to create them correctly. Case 37 : index created in different version​ create index this_index_has_a_changed_attribute on foo(id) @create(2); ------- create index this_index_has_a_changed_attribute on foo(id) @create(1); Error at sem_test_prev.sql:125 : in str : current create version not equal to previous create version for 'this_index_has_a_changed_attribute'  You can't change the @create version of an index. Case 38 : create a new index but with no @create annotation​ create index this_index_was_created_with_no_annotation on foo(id); ------- --- there is no previous version Error at sem_test_prev.sql:130 : in create_index_stmt : new index must be added with @create(6) or later 'this_index_was_created_with_no_annotation'  You have to use @create on indices to make new ones. Case 39 : create a table with a column def that has a different create migrator proc​ create table create_column_migrate_test( id int, id2 int @create(2, ChangedColumnCreateMigrator) ); ------- create table create_column_migrate_test( id int, id2 int @create(2, PreviousColumnCreateMigrator) ); Error at sem_test_prev.sql:136 : in str : column @create procedure changed 'id2'  You can't change the @create migration stored proc on columns. Case 40 : create a table with a column def that has a different delete migrator proc​ create table delete_column_migrate_test( id int, id2 int @delete(2, ChangedColumnDeleteMigrator) ); ------- create table delete_column_migrate_test( id int, id2 int @delete(2, PreviousColumnDeleteMigrator) ); Error at sem_test_prev.sql:142 : in str : column @delete procedure changed 'id2'  You can't change the @delete migration stored proc on columns. NOTE: in addition to these errors, there are many more that do not require the previous schema which are also checked (not shown here). These comprise things like making sure the delete version is greater than the create version on any item. There is a lot of sensibility checking that can happen without reference to the previous schema. "},{"title":"Chapter 12: Testability Features​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-12-testability-features","content":"CQL includes a number of features to make it easier to create what you might call &quot;Test&quot; procedures. These primarily are concerned with loading up the database with dummy data, and/or validating the result of normal procedures that query the database. There are several interesting language features in these dimensions. "},{"title":"Dummy Data​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#dummy-data","content":"Test code can be needlessly brittle, especially when creating dummy data; any column changes typically cause all sorts of data insertion code to need to be repaired. In many cases the actual data values are completely uninteresting to the test -- any values would do. There are several strategies you can use to get good dummy data into your database in a more maintainable way. Simple Inserts With Dummy Data​ The simplest form uses a variant of the insert statement that fills in any missing columns with a seed value. An example might be something like the below: create proc dummy_user() begin insert into users () values () @dummy_seed(123) @dummy_nullables @dummy_defaults; end;  This statement causes all values including columns that are nullable or have a default value to get the value 123 for any numeric type and'column_name_123' for any text. If you omit the @dummy_nullables then any nullable fields will be null as usual. And likewise if you omit @dummy_defaults then any fields with a default value will use that value as usual. You might want any combination of these for your tests (null values are handy in your tests and default behavior is also handy.) The @dummy_seed expression provided can be anything that resolves to a non-null integer value, so it can be pretty flexible. You might use a while loop to insert a bunch of rows with the seed value being computed from the while loop variable. The form above is sort of like insert * into table in that it is giving dummy values for all columns but you can also specify some of the columns while using the seed value for others. Importantly, you can specify values you particularly want to control either for purposes of creating a more tailored test or because you need them to match existing or created rows in a table referenced by a foreign key. As an example: insert into users (id) values (1234) @dummy_seed(123) @dummy_nullables @dummy_defaults;  will provide dummy values for everything but the id column. Using WITH RECURSIVE​ Sometimes what you want to do is create a dummy result set without necessarily populating the database at all. If you have code that consumes a result set of a particular shape, it's easy enough to create a fake result set with a pattern something like this: create procedure dummy_stuff(lim integer not null) begin WITH RECURSIVE dummy(x) AS ( SELECT 1 UNION ALL SELECT x+1 FROM dummy WHERE x &lt; lim) SELECT x id, printf(&quot;name_%d&quot;, x) name, cast(x % 2 as bool) is_cool, x * 1.3 as rate, x etc1, x etc2 FROM dummy; end;  The first part of the above creates a series of numbers from 1 to lim. The second uses those values to create dummy columns. Any result shape can be generated in this fashion. You get data like this from the above: 1|name_1|1|1.3|1|1 2|name_2|0|2.6|2|2 3|name_3|1|3.9|3|3 4|name_4|0|5.2|4|4 5|name_5|1|6.5|5|5 6|name_6|0|7.8|6|6 7|name_7|1|9.1|7|7 8|name_8|0|10.4|8|8 9|name_9|1|11.7|9|9 10|name_10|0|13.0|10|10  The result of the select statement is itself quite flexible and if more dummy data is what you wanted, this form can be combined withINSERT ... FROM SELECT... to create dummy data in real tables. And of course once you have a core query you could use it in a variety of ways, combined with cursors or any other strategy to select out pieces and insert them into various tables. Using Temporary Tables​ If you need an API to create very flexible dummy data with values of your choice you can use temporary tables and a series of helper procedures. First, create a table to hold the results. You can of course make this table however you need to but the like construct in the table creation is especially helpful; it creates columns in the table that match the name and type of the named object. For instance like my_proc is shorthand for the column names ands of the shape that my_proc returns. This is perfect for emulating the results of my_proc. create proc begin_dummy() begin drop table if exists my_dummy_data; -- the shape of my_dummy_data matches the columns -- returned by proc_I_want_to_emulate create temp table my_dummy_data( like proc_I_want_to_emulate; ); end;  Next, you will need a procedure that accepts and writes a single row to your temp table. You can of course write this all explicitly but the testing support features provide more support to make things easier; In this example, arguments of the procedure will exactly match the output of the procedure we emulating, one argument for each column the proc returns. The insert statement gets its values from the arguments. create proc add_dummy(like proc_I_want_to_emulate) begin insert into my_dummy_data from arguments; end;  This allows you to create the necessary helper methods automatically even if the procedure changes over time. Next we need a procedure to get our result set. create proc get_dummy() begin select * from my_dummy_data; end;  And finally, some cleanup. create proc cleanup_dummy() begin drop table if exists my_dummy_data; end;  Again the temp table could be combined with INSERT INTO ...FROM SELECT... to create dummy data in real tables. Other Considerations​ Wrapping your insert statements in try/catch can be very useful if there may be dummy data conflicts. In test code searching for a new suitable seed is pretty easy. Alternatively set seed := 1 + (select max(id) from foo);  could be very useful. Many alternatives are also possible. The dummy data features are not suitable for use in production code, only tests. But the LIKE features are generally useful for creating contract-like behavior in procs and there are reasonable uses for them in production code. Complex Result Set Example​ Here's a more complicated example that can be easily rewritten using the sugar features. This method is designed to return a single-row result set that can be used to mock a method. I've replaced the real fields with 'f1, 'f2' etc. CREATE PROCEDURE test_my_subject( f1_ LONG INTEGER NOT NULL, f2_ TEXT NOT NULL, f3_ INTEGER NOT NULL, f4_ LONG INTEGER NOT NULL, f5_ TEXT, f6_ TEXT, f7_ TEXT, f8_ BOOL NOT NULL, f9_ TEXT, f10_ TEXT ) BEGIN DECLARE data_cursor CURSOR LIKE my_subject; FETCH data_cursor() FROM VALUES (f1_, f2_, f3_, f4_, f5_, f6_, f7_, f8_, f9_, f10); OUT data_cursor; END;  This can be written much more maintainably as: CREATE PROCEDURE test_my_subject(like my_subject) BEGIN DECLARE C CURSOR LIKE my_subject; FETCH C FROM ARGUMENTS; OUT C; END;  Naturally, real columns have much longer names and there are often many more than 10. "},{"title":"Autotest Attributes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#autotest-attributes","content":"Some of the patterns described above are so common that CQL offers a mechanism to automatically generate those test procedures. Temporary Table Pattern​ The attributes dummy_table, dummy_insert, and dummy_select can be used together to create and populate temp tables. Example: To create a dummy row set for sample_proc, add the cql:autotest attribute with dummy_table, dummy_insert, and dummy_select values. create table foo( id integer not null, name text not null ); @attribute(cql:autotest=(dummy_table, dummy_insert, dummy_select)) create proc sample_proc(foo int) begin select * from Foo; end;  dummy_table generates procedures for creating and dropping a temp table with the same shape as sample_proc. CREATE PROC open_sample_proc() BEGIN CREATE TEMP TABLE test_sample_proc(LIKE sample_proc); END; CREATE PROC close_sample_proc() BEGIN DROP test_sample_proc; END;  The dummy_insert attribute generates a procedure for inserting into the temp table. CREATE PROC insert_sample_proc(LIKE sample_proc) BEGIN INSERT INTO test_sample_proc FROM ARGUMENTS; END;  The dummy_select attribute generates procedures for selecting from the temp table. CREATE PROC select_sample_proc() BEGIN SELECT * FROM test_sample_proc; END;  It's interesting to note that the generated test code does not ever need to mention the exact columns it is emulating because it can always use like, *, and from arguments in a generic way. When compiled, the above will create C methods that can create, drop, insert, and select from the temp table. They will have the following signatures: CQL_WARN_UNUSED cql_code open_sample_proc( sqlite3 *_Nonnull _db_); CQL_WARN_UNUSED cql_code close_sample_proc( sqlite3 *_Nonnull _db_); CQL_WARN_UNUSED cql_code insert_sample_proc( sqlite3 *_Nonnull _db_, cql_int32 id_, cql_string_ref _Nonnull name_); CQL_WARN_UNUSED cql_code select_sample_proc_fetch_results( sqlite3 *_Nonnull _db_, select_sample_proc_result_set_ref _Nullable *_Nonnull result_set);  Single Row ResultSet​ In some cases, using four APIs to generate fake data can be verbose. In the case that only a single row of data needs to be faked, the dummy_result_set attribute can be more convenient. Example: @attribute(cql:autotest=(dummy_result_set)) create proc sample_proc() begin select id from Foo; end;  Will generate the following procedure CREATE PROC generate_sample_proc_row(LIKE sample_proc) BEGIN DECLARE curs CURSOR LIKE sample_proc; FETCH curs FROM ARGUMENTS; OUT curs; END;  Which generates this C API: void generate_sample_proc_row_fetch_results( generate_sample_proc_row_rowset_ref _Nullable *_Nonnull result_set, string_ref _Nonnull foo_, int64_t bar_);  These few test helpers are useful in a variety of scenarios and can save you a lot of typing and maintenance. They evolve automatically as the code changes, always matching the signature of the attributed procedure. Generalized Dummy Test Pattern​ The most flexible test helper is the dummy_test form. This is far more advanced than the simple helpers above. While the choices above were designed to help you create fake result sets pretty easily, dummy_test goes much further letting you set up arbitrary schema and data so that you can run your procedure on actual data. The dummy_test code generator uses the features above to do its job and like the other autotest options, it works by automatically generating CQL code from your procedure definition. However, you get a lot more code in this mode. It's easiest to study an example so let's begin there. To understand dummy_test we'll need a more complete example, so we start with this simple two-table schema with a trigger and some indices. To this we add a very small procedure that we might want to test. create table foo( id integer not null primary key, name text ); create table bar( id integer not null primary key references foo(id), data text ); create index foo_index on foo(name); create index bar_index on bar(data); create temp trigger if not exists trigger1 before delete on foo begin delete from foo where name = 'this is so bogus'; end; @attribute(cql:autotest=( dummy_table, dummy_insert, dummy_select, dummy_result_set, (dummy_test, (bar, (data), ('plugh')))) ) create proc the_subject() begin select * from bar; end;  As you can see, we have two tables, foo and bar; the foo table has a trigger; both foo and bar have indices. This schema is very simple, but of course it could be a lot more complicated, and real cases typically are. The procedure we want to test is creatively called the_subject. It has lots of test attributes on it. We've already discussed dummy_table, dummy_insert, dummy_select, and dummy_result_set above but as you can see they can be mixed in with dummy_test. Now let's talk about dummy_test. First you'll notice that annotation has additional sub-attributes; the attribute grammar is sufficiently flexible such that, in principle, you could represent an arbitrary LISP program, so the instructions can be very detailed. In this case, the attribute provides table and column names, as well as sample data. We'll discuss that when we get to the population code. First let's dispense with the attributes we already discussed -- since we had all the attributes, the output will include those helpers, too. Here they are again: -- note that the code does not actually call the test subject -- this declaration is used so that CQL will know the shape of the result DECLARE PROC the_subject () (id INTEGER NOT NULL, data TEXT); CREATE PROC open_the_subject() BEGIN CREATE TEMP TABLE test_the_subject(LIKE the_subject); END; CREATE PROC close_the_subject() BEGIN DROP TABLE test_the_subject; END; CREATE PROC insert_the_subject(LIKE the_subject) BEGIN INSERT INTO test_the_subject FROM ARGUMENTS; END; CREATE PROC select_the_subject() BEGIN SELECT * FROM test_the_subject; END; CREATE PROC generate_the_subject_row(LIKE the_subject) BEGIN DECLARE curs CURSOR LIKE the_subject; FETCH curs FROM ARGUMENTS; OUT curs; END;  That covers what we had before, so, what's new? Actually, quite a bit. We'll begin with the easiest: CREATE PROC test_the_subject_create_tables() BEGIN CREATE TABLE IF NOT EXISTS foo( id INTEGER NOT NULL PRIMARY KEY, name TEXT ); CREATE TABLE IF NOT EXISTS bar( id INTEGER NOT NULL PRIMARY KEY REFERENCES foo (id), data TEXT ); END;  Probably the most important of all the helpers, test_the_subject_create_tables will create all the tables you need to run the procedure. Note that in this case, even though the subject code only references bar, CQL determined that foo is also needed because of the foreign key. The symmetric drop procedure is also generated: CREATE PROC test_the_subject_drop_tables() BEGIN DROP TABLE IF EXISTS bar; DROP TABLE IF EXISTS foo; END;  Additionally, in this case there were triggers and indices. This caused the creation of helpers for those aspects. CREATE PROC test_the_subject_create_indexes() BEGIN CREATE INDEX bar_index ON bar (data); CREATE INDEX foo_index ON foo (name); END; CREATE PROC test_the_subject_create_triggers() BEGIN CREATE TEMP TRIGGER IF NOT EXISTS trigger1 BEFORE DELETE ON foo BEGIN DELETE FROM foo WHERE name = 'this is so bogus'; END; END; CREATE PROC test_the_subject_drop_indexes() BEGIN DROP INDEX IF EXISTS bar_index; DROP INDEX IF EXISTS foo_index; END; CREATE PROC test_the_subject_drop_triggers() BEGIN DROP TRIGGER IF EXISTS trigger1; END;  If there are no triggers or indices, the corresponding create/drop methods will not be generated. With these helpers available, when writing test code you can then choose if you want to create just the tables, or the tables and indices, or tables and indices and triggers by invoking the appropriate combination of helper methods. Since all the implicated triggers and indices are automatically included, even if they change over time, maintenance is greatly simplified. Note that in this case the code simply reads from one of the tables, but in general the procedure under test might make modifications as well. Test code frequently has to read back the contents of the tables to verify that they were modified correctly. So these additional helper methods are also included: CREATE PROC test_the_subject_read_foo() BEGIN SELECT * FROM foo; END; CREATE PROC test_the_subject_read_bar() BEGIN SELECT * FROM bar; END;  These procedures will allow you to easily create result sets with data from the relevant tables which can then be verified for correctness. Of course if more tables were implicated, those would have been included as well. As you can see, the naming always follows the convention test_[YOUR_PROCEDURE]_[helper_type] Finally, the most complicated helper is the one that used that large annotation. Recall that we provided the fragment (dummy_test, (bar, (data), ('plugh')))) to the compiler. This fragment helped to produce this last helper function: CREATE PROC test_the_subject_populate_tables() BEGIN INSERT OR IGNORE INTO foo(id) VALUES(1) @dummy_seed(123); INSERT OR IGNORE INTO foo(id) VALUES(2) @dummy_seed(124) @dummy_nullables @dummy_defaults; INSERT OR IGNORE INTO bar(data, id) VALUES('plugh', 1) @dummy_seed(125); INSERT OR IGNORE INTO bar(id) VALUES(2) @dummy_seed(126) @dummy_nullables @dummy_defaults; END;  In general the populate_tables helper will fill all implicated tables with at least two rows of data. It uses the dummy data features discussed earlier to generate the items using a seed. Recall that if @dummy_seed is present in an insert statement then any missing columns are generated using that value, either as a string, or as an integer (or true/false for a boolean). Note that the second of the two rows that is generated also specifies @dummy_nullables and @dummy_defaults. This means that even nullable columns, and columns with a default value will get the non-null seed instead. So you get a mix of null/default/explicit values loaded into your tables. Of course blindly inserting data doesn't quite work. As you can see, the insert code used the foreign key references in the schema to figure out the necessary insert order and the primary key values for foo were automatically specified so that they could then be used again in bar. Lastly, the autotest attribute included explicit test values for the table bar, and in particular the data column has the value 'plugh'. So the first row of data for table bar did not use dummy data for the data column but rather used 'plugh'. In general, the dummy_test annotation can include any number of tables, and for each table you can specify any of the columns and you can have any number of tuples of values for those columns. NOTE: if you include primary key and/or foreign key columns among the explicit values, it's up to you to ensure that they are valid combinations. SQLite will complain as usual if they are not, but the CQL compiler will simply emit the data you asked for. Generalizing the example a little bit, we could use the following: (dummy_test, (foo, (name), ('fred'), ('barney'), ('wilma'), ('betty')), (bar, (id, data), (1, 'dino'), (2, 'hopparoo'))))  to generate this population: CREATE PROC test_the_subject_populate_tables() BEGIN INSERT OR IGNORE INTO foo(name, id) VALUES('fred', 1) @dummy_seed(123); INSERT OR IGNORE INTO foo(name, id) VALUES('barney', 2) @dummy_seed(124) @dummy_nullables @dummy_defaults; INSERT OR IGNORE INTO foo(name, id) VALUES('wilma', 3) @dummy_seed(125); INSERT OR IGNORE INTO foo(name, id) VALUES('betty', 4) @dummy_seed(126) @dummy_nullables @dummy_defaults; INSERT OR IGNORE INTO bar(id, data) VALUES(1, 'dino') @dummy_seed(127); INSERT OR IGNORE INTO bar(id, data) VALUES(2, 'hopparoo') @dummy_seed(128) @dummy_nullables @dummy_defaults; END;  And of course if the annotation is not flexible enough, you can write your own data population. The CQL above results in the usual C signatures. For instance: CQL_WARN_UNUSED cql_code test_the_subject_populate_tables(sqlite3 *_Nonnull _db_);  So, it's fairly easy to call from C/C++ test code or from CQL test code. Cross Procedure Limitations​ Generally it's not possible to compute table usages that come from called procedures. This is because to do so you need to see the body of the called procedure and typically that body is in a different translation -- and is therefore not available. A common workaround for this particular problem is to create a dummy procedure that explicitly uses all of the desired tables. This is significantly easier than creating all the schema manually and still gets you triggers and indices automatically. Something like this: @attribute(cql:autotest=(dummy_test)) create proc use_my_stuff() begin let x := select 1 from t1, t2, t3, t4, t5, t6, etc..; end;  The above can be be done as a macro if desired. But in any case use_my_stuff simply and directly lists the desired tables. Using this approach you can have one set of test helpers for an entire unit rather than one per procedure. This is often desirable and the maintenance is not too bad. You just use the use_my_stuff test helpers everywhere. "},{"title":"Chapter 13: JSON Output​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-13-json-output","content":"To help facilitate additional tools that might want to depend on CQL input files further down the toolchain, CQL includes a JSON output format for SQL DDL as well as stored procedure information, including special information for a single-statement DML. &quot;Single-statement DML&quot; refers to those stored procedures that consist of a single insert, select, update, or delete. Even though such procedures comprise just one statement, good argument binding can create very powerful DML fragments that are re-usable. Many CQL stored procedures are of this form (in practice maybe 95% are just one statement.) To use CQL in this fashion, the sequence will be something like the below. See Appendix 1 for command line details. cql --in input.sql --rt json_schema --cg out.json  The output contains many different sections for the various types of entities that CQL can process. There is a full description of the possible outputs available at https://cgsql.dev/json-diagram. In the balance of this chapter we'll deal with the contents of the sections and their meaning rather than the specifics of the format, which are better described with the grammar above. "},{"title":"Tables​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#tables","content":"The &quot;tables&quot; section has zero or more tables, each table is comprised of these fields: name : the table namecrc : the schema CRC for the entire table definition, including columns and constraintsisTemp : true if this is a temporary tableifNotExists : true if the table was created with &quot;if not exists&quot;withoutRowid : true if the table was created using &quot;without rowid&quot;isAdded : true if the table has an @create directive addedVersion : optional, the schema version number in the @create directive isDeleted : true if the table was marked with @delete or is currently unsubscribed deletedVersion : optional, the schema version number in the @delete directive isRecreated : true if the table is marked with @recreate recreateGroupName : optional, if the @recreate attribute specifies a group name, it is present here unsubscribedVersion : optional, if the table was last unsubscribed, the version number when this happenedresubscribedVersion : optional, if the table was last resubscribed, the version number when this happenedregion information : optional, see the section on Region Infoindices : optional, a list of the names of the indices on this table, see the indices sectionattributes : optional, see the section on attributes, they appear in many placescolumns : an array of column definitions, see the section on columnsprimaryKey : a list of column names, possibly empty if no primary keyprimaryKeySortOrders : a list of corresponding sort orders, possibly empty, for each column of the primary key if specifiedprimaryKeyName : optional, the name of the primary key, if it has oneforeignKeys : a list of foreign keys for this table, possibly empty, see the foreign keys sectionuniqueKeys : a list of unique keys for this table, possibly empty, see the unique keys sectioncheckExpressions : a list of check expressions for this table, possibly empty, see the check expression section Example: @attribute(an_attribute=(1,('foo', 'bar'))) CREATE TABLE foo( id INTEGER, name TEXT );  generates:  { &quot;name&quot; : &quot;foo&quot;, &quot;CRC&quot; : &quot;-1869326768060696459&quot;, &quot;isTemp&quot; : 0, &quot;ifNotExists&quot; : 0, &quot;withoutRowid&quot; : 0, &quot;isAdded&quot; : 0, &quot;isDeleted&quot; : 0, &quot;isRecreated&quot;: 0, &quot;indices&quot; : [ &quot;foo_name&quot; ], &quot;attributes&quot; : [ { &quot;name&quot; : &quot;an_attribute&quot;, &quot;value&quot; : [1, [&quot;foo&quot;, &quot;bar&quot;]] } ], &quot;columns&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0, &quot;isAdded&quot; : 0, &quot;isDeleted&quot; : 0, &quot;isPrimaryKey&quot; : 0, &quot;isUniqueKey&quot; : 0, &quot;isAutoIncrement&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0, &quot;isAdded&quot; : 0, &quot;isDeleted&quot; : 0, &quot;isPrimaryKey&quot; : 0, &quot;isUniqueKey&quot; : 0, &quot;isAutoIncrement&quot; : 0 } ], &quot;primaryKey&quot; : [ ], &quot;primaryKeySortOrders&quot; : [ ], &quot;foreignKeys&quot; : [ ], &quot;uniqueKeys&quot; : [ ], &quot;checkExpressions&quot; : [ ] }  "},{"title":"Region Information​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#region-information","content":"Region Information can appear on many entities, it consists of two optional elements: region : optional, the name of the region in which the entity was defineddeployedInRegion : optional, the deployment region in which that region is located "},{"title":"Attributes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#attributes","content":"Miscellaneous attributes can be present on virtual every kind of entity. They are optional. The root node introduces the attributes: attributes : a list at least one attribute Each attribute is a name and value pair: name : any string attribute names are often compound like &quot;cql:shared_fragment&quot;they are otherwise simple identifiers value : any attribute value Each attribute value can be: any literalan array of attribute values Since the attribute values can nest its possible to represent arbitrarily complex data types in an attribute. You can even represent a LISP program. "},{"title":"Global attributes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#global-attributes","content":"While the most common use case for attributes is to be attached to other entities (e.g., tables, columns), CQL also lets you define &quot;global&quot; attributes, which are included in the top level attributes section of the JSON output. To specify global attributes you can declare a variable ending with the suffix &quot;database&quot; and attach attributes to it. CQL will merge together all the attributes from all the variables ending with &quot;database&quot; and place them in the attributes section of the JSON output. The main usage of global attributes is as a way to propagate configurations across an entire CQL build. You can, for instance, include these attributes in some root file that you #include in the rest of your CQL code, and by doing this these attributes will be visible everywhere else. Example: @attribute(attribute_1 = &quot;value_1&quot;) @attribute(attribute_2 = &quot;value_2&quot;) declare database object; @attribute(attribute_3 = &quot;value_3&quot;) declare some_other_database object;  Generates:  { &quot;attributes&quot;: [ { &quot;name&quot;: &quot;attribute_1&quot;, &quot;value&quot;: &quot;value_1&quot; }, { &quot;name&quot;: &quot;attribute_2&quot;, &quot;value&quot;: &quot;value_2&quot; }, { &quot;name&quot;: &quot;attribute_3&quot;, &quot;value&quot;: &quot;value_3&quot; } ] }  "},{"title":"Foreign Keys​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#foreign-keys","content":"Foreign keys appear only in tables, the list of keys contains zero or more entries of this form: name : optional, the name of the foreign key if specifiedcolumns : the names of the constrained columns in the current table (the &quot;child&quot; table)referenceTable : the name of the table that came after REFERENCES in the foreign keyreferenceColumns : the constraining columns in the referenced tableonUpdate : the ON UPDATE action (e.g. &quot;CASCADE&quot;, &quot;NO ACTION&quot;, etc.)onDelete : the ON DELETE action (e.g. &quot;CASCADE&quot;, &quot;NO ACTION&quot;, etc.)isDeferred : boolean, indicating the deferred or not deferred setting for this foreign key "},{"title":"Unique Keys​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#unique-keys","content":"Unique keys appear only in tables, the list of keys contains zero or more entries of this form: name: optional, the name of the unique key if specifiedcolumns: a list of 1 or more constrained column namessortOrders: a list of corresponding sort orders for the columns "},{"title":"Check Expressions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#check-expressions","content":"Check Expressions appear only in tables, the list of keys contains zero or more entries of this form: name : optional, the name of the unique key if specifiedcheckExpr : the check expression in plain textcheckExprArgs: an array of zero or more local variables that should be bound to the ? items in the check expression The checkExprArgs will almost certainly be the empty list []. In the exceedingly rare situation that the table in question was defined in a procedure and some of parts of the check expression were arguments to that procedure then the check expression is not fully known until that procedure runs and some of its literals will be decided at run time. This is an extraordinary choice but technically possible. "},{"title":"Columns​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#columns","content":"Columns are themselves rather complex, there are 1 or more of them in each table. The table will have a list of records of this form: name : the name of the columnsattributes : optional, see the section on attributes, they appear in many placestype : the column type (e.g. bool, real, text, etc.)kind : optional, if the type is qualified by a discriminator such as int&lt;task_id&gt; it appears hereisSensitive : optional, indicates a column that holds sensitive information such as PIIisNotNull : true if the column is not nullisAdded : true if the column has an @create directive addedVersion : optional, the schema version number in the @create directive isDeleted : true if the column was marked with @delete deletedVersion : optional, the schema version number in the @delete directive defaultValue : optional, can be any literal, the default value of the columncollate : optional, the collation string (e.g. nocase)checkExpr : optional, the check expression for this column (see the related section)isPrimaryKey : true if the column was marked with PRIMARY KEYisUniqueKey : true if the column was marked with UNIQUEisAutoIncrement : true if the column was marked with AUTOINCREMENT "},{"title":"Virtual Tables​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#virtual-tables","content":"The &quot;virtualTables&quot; section is very similar to the &quot;tables&quot; section with zero or more virtual table entries. Virtual table entries are the same as table entries with the following additions: module : the name of the module that manages this virtual tableisEponymous : true if the virtual table was declared eponymousisVirtual : always true for virtual tables The JSON schema for these items was designed to be as similar as possible so that typically the same code can handle both with possibly a few extra tests of the isVirtual field. "},{"title":"Views​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#views-1","content":"The views section contains the list of all views in the schema, it is zero or more view entires of this form. name : the view namecrc : the schema CRC for the entire view definitionisTemp : true if this is a temporary viewisDeleted : true if the view was marked with @delete deletedVersion : optional, the schema version number in the @delete directive region information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placesprojection : an array of projected columns from the view, the view result if you will, see the section on projectionsselect : the text of the select statement that defined the viewselectArgs : the names of arguments any unbound expressions (&quot;?&quot;) in the viewdependencies : several lists of tables and how they are used in the view, see the section on dependencies Note that the use of unbound expressions in a view truly extraordinary so selectArgs is essentially always going to be an empty list. Example: CREATE VIEW MyView AS SELECT * FROM foo  Generates:  { &quot;name&quot; : &quot;MyView&quot;, &quot;CRC&quot; : &quot;5545408966671198580&quot;, &quot;isTemp&quot; : 0, &quot;isDeleted&quot; : 0, &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;select&quot; : &quot;SELECT id, name FROM foo&quot;, &quot;selectArgs&quot; : [ ], &quot;fromTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ] }  "},{"title":"Projections​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#projections","content":"A projection defines the output shape of something that can return a table-like value such as a view or a procedure. The projection consists of a list of one or more projected columns, each of which is: name : the name of the result column (e.g. in select 2 as foo) the name is &quot;foo&quot;type : the type of the column (e.g. text, real, etc.)kind : optional, the discriminator of the type if it has one (e.g. if the result is an int&lt;job_id&gt; the kind is &quot;job_id&quot;)isSensitive : optional, true if the result is sensitive (e.g. PII or something like that)isNotNull : true if the result is known to be not null "},{"title":"Dependencies​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#dependencies","content":"The dependencies section appears in many entities, it indicates things that were used by the object and how they were used. Most of the fields are optional, some fields are impossible in some contexts (e.g. inserts can happen inside of views). insertTables : optional, a list of tables into which values were insertedupdateTables : optional, a list of tables whose values were updateddeleteTables : optional, a list of tables which had rows deletedfromTables : optional, a list of tables that appeared in a FROM clause (maybe indirectly inside a VIEW or CTE)usesProcedures : optional, a list of procedures that were accessed via CALL (not shared fragments, those are inlined)usesViews : optional, a list of views which were accessed (these are recursively visited to get to tables)usesTables : the list of tables that were used in any way at all by the current entity (i.e. the union of the previous table sections) "},{"title":"Indices​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#indices-1","content":"The indices section contains the list of all indices in the schema, it is zero or more view entires of this form: name : the index namecrc : the schema CRC for the entire index definitiontable : the name of the table with this indexisUnique : true if this is a unique indexifNotExists : true if this index was created with IF NOT EXISTSisDeleted : true if the view was marked with @delete deletedVersion : optional, the schema version number in the @delete directive region information : optional, see the section on Region Infowhere : optional, if this is partial index then this has the partial index where expressionattributes : optional, see the section on attributes, they appear in many placescolumns : the list of column names in the indexsortOrders : the list of corresponding sort orders Example: create index foo_name on foo(name);  Generates:  { &quot;name&quot; : &quot;foo_name&quot;, &quot;CRC&quot; : &quot;6055860615770061843&quot;, &quot;table&quot; : &quot;foo&quot;, &quot;isUnique&quot; : 0, &quot;ifNotExists&quot; : 0, &quot;isDeleted&quot; : 0, &quot;columns&quot; : [ &quot;name&quot; ], &quot;sortOrders&quot; : [ &quot;&quot; ] }  "},{"title":"Procedures​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedures","content":"The next several sections: QueriesInsertsGeneral InsertsUpdatesDeletesGeneral All provide information about various types of procedures. Some &quot;simple&quot; procedures that consist only of the type of statement correspond to their section (and some other rules) present additional information about their contents. This can sometimes be useful. All the sections define certain common things about procedures so that basic information is available about all procedures. This is is basically the contents of the &quot;general&quot; section which deals with procedures that have a complex body of which little can be said. "},{"title":"Queries​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#queries","content":"The queries section corresponds to the stored procedures that are a single SELECT statement with no fragments. The fields of a query record are: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placesprojection : an array of projected columns from the procedure, the view if you will, see the section on projectionsstatement : the text of the select statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statement Example: create proc p(name_ text) begin select * from foo where name = name_; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;name_&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;fromTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;statement&quot; : &quot;SELECT id, name FROM foo WHERE name = ?&quot;, &quot;statementArgs&quot; : [ &quot;name_&quot; ] }  "},{"title":"Procedure Arguments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedure-arguments","content":"Procedure arguments have several generalities that don't come up very often but are important to describe. The argument list of a procedure is 0 or more arguments of the form: name : the argument name, any valid identifierargOrigin : either the name repeated if it's just a name or a 3 part string if it came from a bundle, see belowtype : the type of the argument (e.g. text, real, etc.)kind : optional, the discriminated type if any e.g. in int&lt;job_id&gt; it's &quot;job_id&quot;isSensitive : optional, true if the argument is marked with @sensitive (e.g. it has PII etc.)isNotNull : true if the argument is declared not null An example of a simple argument was shown above, if we change the example a little bit to use the argument bundle syntax (even though it's overkill) we can see the general form of argOrigin. Example: create proc p(a_foo like foo) begin select * from foo where name = a_foo.name or id = a_foo.id; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;a_foo_id&quot;, &quot;argOrigin&quot; : &quot;a_foo foo id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;a_foo_name&quot;, &quot;argOrigin&quot; : &quot;a_foo foo name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;fromTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;statement&quot; : &quot;SELECT id, name FROM foo WHERE name = ? OR id = ?&quot;, &quot;statementArgs&quot; : [ &quot;a_foo_name&quot;, &quot;a_foo_id&quot; ] }  Note the synthetic names a_foo_id and a_foo_name the argOrigin indicates that the bundle name is a_foowhich could have been anything, the shape was foo and the column in foo was id or name as appropriate. The JSON is often used to generate glue code to call procedures from different languages. The argOrigin can be useful if you want to codegen something other normal arguments in your code. "},{"title":"General Inserts​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#general-inserts","content":"The general insert section corresponds to the stored procedures that are a single INSERT statement with no fragments. The fields of a general insert record are: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placestable : the name of the table the procedure inserts intostatement : the text of the select statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statementstatementType : there are several insert forms such as &quot;INSERT&quot;, &quot;INSERT OR REPLACE&quot;, &quot;REPLACE&quot;, etc. the type is encoded here General inserts does not include the inserted values because they are not directly extractable in general. This form is used if one of these is true: insert from multiple value rowsinsert from a select statementinsert using a WITH clauseinsert using the upsert clause If fragments are in use then even &quot;generalInsert&quot; cannot capture everything and &quot;general&quot; must be used (see below). Example: create proc p() begin insert into foo values (1, &quot;foo&quot;), (2, &quot;bar&quot;); end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;args&quot; : [ ], &quot;insertTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;INSERT INTO foo(id, name) VALUES(1, 'foo'), (2, 'bar')&quot;, &quot;statementArgs&quot; : [ ], &quot;statementType&quot; : &quot;INSERT&quot;, &quot;columns&quot; : [ &quot;id&quot;, &quot;name&quot; ] }  "},{"title":"Simple Inserts​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#simple-inserts","content":"The vanilla inserts section can be used for procedures that just insert a single row. This is a very common case and if the JSON is being used to drive custom code generation it is useful to provide the extra information. The data in this section is exactly the same as the General Inserts section except that includes the inserted values. The &quot;values&quot; property has this extra information. Each value in the values list corresponds 1:1 with a column and has this form: value : the expression for this valuevalueArgs: the array of procedure arguments that should replace the &quot;?&quot; entries in the value Example: create proc p(like foo) begin insert into foo from arguments; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;foo name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;id_&quot;, &quot;argOrigin&quot; : &quot;foo id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 } ], &quot;insertTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;INSERT INTO foo(id, name) VALUES(?, ?)&quot;, &quot;statementArgs&quot; : [ &quot;id_&quot;, &quot;name_&quot; ], &quot;statementType&quot; : &quot;INSERT&quot;, &quot;columns&quot; : [ &quot;id&quot;, &quot;name&quot; ], &quot;values&quot; : [ { &quot;value&quot; : &quot;?&quot;, &quot;valueArgs&quot; : [ &quot;id_&quot; ] }, { &quot;value&quot; : &quot;?&quot;, &quot;valueArgs&quot; : [ &quot;name_&quot; ] } ] }  "},{"title":"Updates​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#updates","content":"The updates section corresponds to the stored procedures that are a single UPDATE statement with no fragments. The fields of an update record are: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placestable : the name of the table the procedure inserts intostatement : the text of the update statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statement Example: create proc p(like foo) begin update foo set name = name_ where id = id_; end;  Generates:  { &quot;name&quot; : &quot;p&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;foo name&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 }, { &quot;name&quot; : &quot;id_&quot;, &quot;argOrigin&quot; : &quot;foo id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 } ], &quot;updateTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;UPDATE foo SET name = ? WHERE id = ?&quot;, &quot;statementArgs&quot; : [ &quot;name_&quot;, &quot;id_&quot; ] }  "},{"title":"Deletes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#deletes","content":"The deletes section corresponds to the stored procedures that are a single DELETE statement with no fragments. The fields of a delete record are exactly the same as those of update. Those are the basic fields needed to bind any statement. Example: create proc delete_proc (name_ text) begin delete from foo where name like name_; end;  Generates:  { &quot;name&quot; : &quot;delete_proc&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 3, &quot;args&quot; : [ { &quot;name&quot; : &quot;name_&quot;, &quot;argOrigin&quot; : &quot;name_&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 0 } ], &quot;deleteTables&quot; : [ &quot;foo&quot; ], &quot;usesTables&quot; : [ &quot;foo&quot; ], &quot;table&quot; : &quot;foo&quot;, &quot;statement&quot; : &quot;DELETE FROM foo WHERE name LIKE ?&quot;, &quot;statementArgs&quot; : [ &quot;name_&quot; ] }  "},{"title":"General​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#general","content":"And finally the section for procedures that were encountered that are not one of the simple prepared statement forms. The principle reasons for being in this category are: the procedure has out argumentsthe procedure uses something other than a single DML statementthe procedure has no projection (no result of any type)the procedure uses shared fragments and hence has complex argument binding The fields of a general procedure are something like a union of update and delete and query but with no statement info. The are as follows: name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredargs : complex procedure arguments see the relevant sectiondependencies : several lists of tables and how they are used in the view, see the section on dependenciesregion information : optional, see the section on Region Infoattributes : optional, see the section on attributes, they appear in many placesprojection : optional, an array of projected columns from the procedure, the view if you will, see the section on projectionsresult_contract : optional,table : the name of the table the procedure inserts intostatement : the text of the update statement that is the body of the procedurestatementArgs : a list of procedure arguments (possibly empty) that should be used to replace the corresponding &quot;?&quot; parameters in the statementusesDatabase : true if the procedure requires you to pass in a sqlite connection to call it The result contract is at most one of these: hasSelectResult : true if the procedure generates its projection using SELECThasOutResult: true if the procedure generates its projection using OUThasOutUnionResult: true if the procedure generates its projection using OUT UNION A procedure that does not produce a result set in any way will set none of these and have no projection entry. Example: create proc with_complex_args (inout arg real) begin set arg := (select arg+1 as a); select &quot;foo&quot; bar; end;  Generates:  { &quot;name&quot; : &quot;with_complex_args&quot;, &quot;definedInFile&quot; : &quot;x&quot;, &quot;definedOnLine&quot; : 1, &quot;args&quot; : [ { &quot;binding&quot; : &quot;inout&quot;, &quot;name&quot; : &quot;arg&quot;, &quot;argOrigin&quot; : &quot;arg&quot;, &quot;type&quot; : &quot;real&quot;, &quot;isNotNull&quot; : 0 } ], &quot;usesTables&quot; : [ ], &quot;projection&quot; : [ { &quot;name&quot; : &quot;bar&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 1 } ], &quot;hasSelectResult&quot; : 1, &quot;usesDatabase&quot; : 1 }  Complex Procedure Arguments​ The complex form of the arguments allows for an optional &quot;binding&quot; binding : optional, if present it can take the value &quot;out&quot; or &quot;inout&quot; if absent then binding is the usual &quot;in&quot; Note that atypical binding forces procedures into the &quot;general&quot; section. "},{"title":"Interfaces​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#interfaces","content":"name : the name of the proceduredefinedInFile : the file that contains the procedure (the path is as it was specified to CQL so it might be relative or absolute)definedOnLine : the line number of the file where the procedure is declaredattributes : optional, see the section on attributes, they appear in many placesprojection: An array of projections. See the section on projections Example declare interface interface1 (id integer);  Generates:  { &quot;name&quot; : &quot;interface1&quot;, &quot;definedInFile&quot; : &quot;x.sql&quot;, &quot;definedOnLine&quot; : 1, &quot;projection&quot; : [ { &quot;name&quot; : &quot;id&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 0 } ] }  "},{"title":"Procecdure Declarations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procecdure-declarations","content":"The declareProcs section contains a list of procedure declaractions. Each declaration is of the form: name : the name of the procedureargs : procedure arguments see the relevant sectionattributes : optional, see the section on attributes, they appear in many placesprojection : An array of projections. See the section on projectionsusesDatabase : true if the procedure requires you to pass in a sqlite connection to call it "},{"title":"Function Declarations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#function-declarations","content":"The declareFuncs section contains a list of function declarations, Each declaration is of the form: name : the name of the functionargs : see the relevant sectionattributes : optional, see the section on attributes, they appear in many placesreturnType : see the relevant section below.createsObject : true if the function will create a new object (e.g. declare function dict_create() create object;) "},{"title":"Return Type​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#return-type","content":"type : base type of the return value (e.g. INT, LONG)kind : optional, if the type is qualified by a discriminator such as int&lt;task_id&gt; it appears hereisSensitive : optional, true if the result is sensitive (e.g. PII)isNotNull : true if the result is known to be not null "},{"title":"Regions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#regions","content":"The regions section contains a list of all the region definitions. Each region is of the form: name : the name of the regionisDeployableRoot : is this region itself a deployment region (declared with @declare_deployable_region)deployedInRegion : name, the deployment region that contains this region or &quot;(orphan)&quot; if none note that deploymentRegions form a forest using : a list of zero or more parent regionsusingPrivately: a list of zero more more booleans, one corresponding to each region the boolean is true if the inheritance is private, meaning that sub-regions cannot see the contents of the inherited region There are more details on regions and the meaning of these terms in Chapter 10. "},{"title":"Ad Hoc Migrations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#ad-hoc-migrations","content":"This section lists all of the declared ad hoc migrations. Each entry is of the form: name : the name of the procedure to be called for the migration stepcrc : the CRC of this migration step, a hash of the callattributes : optional, see the section on attributes, they appear in many places Exactly one of: version: optional, any positive integer, the version at which the migration runs, ORonRecreateOf: optional, if present indicates that the migration runs when the indicated group is recreated There are more details on ad hoc migrations in Chapter 10. "},{"title":"Enums​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#enums","content":"This section list all the enumeration types and values. Each entry is of the form: name : the name of the enumerationtype : the base type of the enumeration (e.g. INT, LONG)isNotNull: always true, all enum values are not null (here for symmetry with other uses of &quot;type&quot;)values: a list of legal enumeration values Each enumeration value is of the form: name : the name of the valuevalue : a numeric literal Example: declare enum an_enumeration integer ( x = 5, y = 12 );  Generates:  { &quot;name&quot; : &quot;an_enumeration&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;isNotNull&quot; : 1, &quot;values&quot; : [ { &quot;name&quot; : &quot;x&quot;, &quot;value&quot; : 5 }, { &quot;name&quot; : &quot;y&quot;, &quot;value&quot; : 12 } ] }  "},{"title":"Constant Groups​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#constant-groups","content":"This section list all the constant groups and values. Each entry is of the form: name : the name of the constant groupvalues: a list of declared constant values, this can be of mixed type Each constant value is of the form: name : the name of the constanttype : the base type of the constant (e.g. LONG, REAL, etc.)kind : optional, the type kind of the constant (this can be set with a CAST on a literal, e.g. CAST(1 as int&lt;job_id&gt;))isNotNull : true if the constant type is not null (which is anything but the NULL literal)value : the numeric or string literal value of the constant Example: declare const group some_constants ( x = cast(5 as integer&lt;job_id&gt;), y = 12.0, z = 'foo' );  Generates:  { &quot;name&quot; : &quot;some_constants&quot;, &quot;values&quot; : [ { &quot;name&quot; : &quot;x&quot;, &quot;type&quot; : &quot;integer&quot;, &quot;kind&quot; : &quot;job_id&quot;, &quot;isNotNull&quot; : 1, &quot;value&quot; : 5 }, { &quot;name&quot; : &quot;y&quot;, &quot;type&quot; : &quot;real&quot;, &quot;isNotNull&quot; : 1, &quot;value&quot; : 1.200000e+01 }, { &quot;name&quot; : &quot;z&quot;, &quot;type&quot; : &quot;text&quot;, &quot;isNotNull&quot; : 1, &quot;value&quot; : &quot;foo&quot; } ] }  "},{"title":"Subscriptions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#subscriptions","content":"This section list all the schema subscriptions in order of appearance. Each entry is of the form: type : always &quot;unsub&quot; at this timetable : the target of the subscription directiveversion : the version at which this operation is to happen (always 1 at this time) This section is a little more complicated than it needs to be becasue of the legacy/deprecated @resub directive. At this point only the table name is relevant. The version is always 1 and the type is always &quot;unsub&quot;. Example: @unsub(foo);  Generates:  { &quot;type&quot; : &quot;unsub&quot;, &quot;table&quot; : &quot;foo&quot;, &quot;version&quot; : 1 }  "},{"title":"Summary​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#summary","content":"These sections general provide all the information about everything that was declared in a translation unit. Typically not the full body of what was declared but its interface. The schema information provide the core type and context while the procedure information illuminates the code that was generated and how you might call it. "},{"title":"Chapter 14: CQL Query Fragments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-14-cql-query-fragments","content":"caution CQL base fragments, extension fragments, and assembly fragments are now deprecated and will be removed. Please use shared fragments instead. CQL Query fragments are the most sophisticated rewrite CQL offers for productivity. The idea is that a very large query can be represented in &quot;fragments&quot; that add columns or add rows based on the original &quot;core&quot; query. The final query will be an assembled rewrite of all the fragments chained together. Specifically, the motivation for this is that you can have a &quot;core&quot; query that fetches the essential columns for some UI design and then you can add query extension fragments that add new/additional columns for some new set of features. The core and extended columns can be in their own fragment and they can be compiled independently. The result of this is that any errors are in much smaller and easier to understand fragments rather than in some monster &quot;fetch everything&quot; query; any given extension does not have to know all the details of all the other extensions and can take a limited dependency on even the core query. It's easiest to illustrate this with an example so let's begin there. Let's first start with this very simple schema: create table my_table( id integer primary key, name text not null, rate real not null ); create table added_rows( like my_table -- sugar to duplicate the columns of my_table ); create table added_columns( id integer references my_table(id), data text );  Typically there would be a lot more columns but where you see flag1 and flag2 appear in fragments you can imagine any number of additional columns of any type. So we can keep the examples simple. "},{"title":"Base Query Fragments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#base-query-fragments","content":"The base fragment might look something like this: @attribute(cql:base_fragment=base_frag) create proc base_frag_template(id_ integer not null) begin with base_frag(*) as (select * from my_table where my_table.id = id_) select * from base_frag; end;  Here are the essential aspects: the base fragment is given a name, it can be anything, probably something that describes the purpose of the fragmentsthe procedure name can be anything at allthe procedure must consist of exactly one with...select statementthe fragment name must be the one and only CTE in the select statementyou must select all the columns from the CTE Note the syntax helper base_frag(*) is just shorthand to avoid retyping all the column names of my_table. The interesting part is (select * from my_table where my_table.id = id_) which could have been any select statement of your choice. Everything else in the procedure must follow the designated format, and the format is enforced due to the presence of @attribute(cql:base_fragment=base_frag). The point of putting everything on rails like this is that all base fragments will look the same and it will be clear how to transform any base fragment into the final query when it is assembled with its extensions. Note: the base fragment produces no codegen at all. There is no base_frag_template procedure in the output. This is just a template. Also, the name of the procedure cannot be base_frag this name will be used by the assembly fragment later. Really any descriptive unique name will do since the name does not appear in the output at all. "},{"title":"Extension Query Fragments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#extension-query-fragments","content":"Adding Columns​ The most common thing that an extension might want to do is add columns to the result. There can be any number of such extensions in the final assembly. Here's a simple example that adds one column. @attribute(cql:extension_fragment=base_frag) create proc adds_columns(id_ integer not null) begin with base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate), col_adder_frag(*) as ( select base_frag.*, added_columns.data from base_frag left outer join added_columns on base_frag.id = added_columns.id) select * from col_adder_frag; end;  Again there are some important features to this extension and they are largely completely constrained, i.e. you must follow the pattern. the attribute indicates extension_fragment and the name (here base_frag) must have been previously declared in a base_fragmentthe procedure name can be any unique name other than base_frag - it corresponds to this particular extension's purposethe procedure arguments must be identical to those in the base fragmentthe first CTE must match the base_fragment attribute value, base_frag in this caseyou do not need to repeat the full select statement for base_frag, any surrogate with the same column names and types will do the base fragment code might include a #define to make this easier e.g. #define base_frags_core as base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate) doing so will make maintenance easier if new columns are added to the base fragment there must be exactly one additional CTE it may have any unique descriptive name you likeit must begin with select base_frags.* with the appropriate CTE name matching the base fragment CTEit must add at least one column (or it would be uninteresting)it may not have any clause other than the first from (e.g. no where, having, limit etc.) if any of these were allowed they would remove or re-order rows in the base query which is not allowedthe from clause often includes nested selects which have no restrictions it must select from the base fragment name and left outer join to wherever it likes to get optional additional columns because of this the additional column(s) will certainly be a nullable type in the projection the final select must be of the form select * from col_adder_frag with the appropriate namekeeping all this in mind, the interesting bit happens here: left outer join added_columns on base_frag.id = added_columns.id this is where you get the data for your additional column using values in the core columns This fragment can be (and should be) compiled in its own compiland while using #include to get the base fragment only. This will result in code gen for the accessor functions for a piece of the overall query -- the part this extension knows about. Importantly code that uses this extension's data does not need or want to know about any other extensions that may be present, thereby keeping dependencies under control. The C signatures generated would look like this: extern cql_int32 adds_columns_get_id( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nonnull adds_columns_get_name( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_double adds_columns_get_rate( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nullable adds_columns_get_data( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 adds_columns_result_count( base_frag_result_set_ref _Nonnull result_set);  Even if there were dozens of other extensions, the functions for reading those columns would not be declared in the header for this extension. Any given extension &quot;sees&quot; only the core columns plus any columns it added. Adding Rows​ Query extensions also frequently want to add additional rows to the main result set, based on the data that is already present. The second form of extension allows for this; it is similarly locked in form. Here is an example: @attribute(cql:extension_fragment=base_frag) create proc adds_rows(id_ integer not null) begin with base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate), row_adder_frag(*) as ( select * from base_frag union all select * from added_rows) select * from row_adder_frag; end;  Let's review the features of this second template form: there is a surrogate for the core querythere is a mandatory second CTEthe second CTE is a compound query with any number of branches, all union allthe first branch must be select * from base_frag (the base fragment) to ensure that the original rows remain this is also why all the branches must be union all this form cannot add new columnsthe extension CTE may not include order by or limit because that might reorder or remove rows of the baseany extensions of this form must come before those of the left outer join form for a given base fragment which ironically means row_adder_frag has to come before col_adder_frag the usual restrictions are in place on compound selects (same type and number of columns) to ensure a consistent resultthe final select after the CTE section must be exactly in the form select * from row_adder_frag which is the name of the one and only additional CTE with no other clauses or options in practice only the CTE will be used to create the final assembly, so even if you did change the final select to something else it would be moot The signatures generated for this will look something like so: extern cql_int32 adds_rows_get_id( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nonnull adds_rows_get_name( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_double adds_rows_get_rate( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 adds_rows_result_count( base_frag_result_set_ref _Nonnull result_set);  which gives you access to the core columns. Again this fragment can and should be compiled standalone with only the declaration for the base fragment in the same translation unit to get the cleanest possible output. This is so that consumers of this extension do not &quot;see&quot; other extensions which may or may not be related and may or may not always be present. Assembling the Fragments​ With all the fragments independently declared they need to be unified to create one final query. This is where the major rewriting happens. The assembly_fragment looks something like this: @attribute(cql:assembly_fragment=base_frag) create proc base_frag(id_ integer not null) begin with base_frag(*) as (select 1 id, &quot;name&quot; name, 1.0 rate) select * from base_frag; end;  It will always be as simple as this; all the complexity is in the fragments. the assembly_fragment name must match the core fragment namethe procedure arguments must be identical to the base fragment argumentsthe procedure must have the same name as the assembly fragment (base_frag in this case) the code that was generated for the previous fragments anticipates this and makes reference to what will be generated herethis is enforced the assembled query is what you run to get the result set, this has real code behind it the other fragments only produce result set readers that call into the helper methods to get columns there is a surrogate for the core fragment as usualall of CTE section will ultimately be replaced with the fragments chained togetherthe final select should be of the form select * from your_frags but it can include ordering and/or filtering, this statement will be present in final codegen, the final order is usually defined here When compiling the assembly fragment, you should include the base, and all the other fragments, and the assembly template. The presence of the assembly_fragment will cause codegen for the extension fragments to be suppressed. The assembly translation unit only contains the assembly query as formed from the fragments. Now let's look at how the query is rewritten, the process is pretty methodical. After rewriting the assembly looks like this: CREATE PROC base_frag (id_ INTEGER NOT NULL) BEGIN WITH base_frag (id, name, rate) AS (SELECT * FROM my_table WHERE my_table.id = id_), row_adder_frag (id, name, rate) AS (SELECT * FROM base_frag UNION ALL SELECT * FROM added_rows), col_adder_frag (id, name, rate, data) AS (SELECT row_adder_frag.*, added_columns.data FROM row_adder_frag LEFT OUTER JOIN added_columns ON row_adder_frag.id = added_columns.id) SELECT * FROM col_adder_frag; END;  Let's dissect this part by part. Each CTE serves a purpose: the core CTE was replaced by the CTE in the base_fragment, and it appears directlynext, the first extension was added as a CTE referring to the base fragment just as before recall that the first extension has to be row_adder_frag, as that type must come firstlooking at the chain you can see why it would be hard to write a correct fragment if it came after columns were added next the second extension was added as a CTE all references to the base fragment were replaced with references to row_adder_fragthe extra column names in the CTE were added such that all previous column names are introduced this process continues until all extensions are exhaustedthe final select statement reads all the columns from the last extension CTE and includes and ordering and so forth that was present in the assembly query The result of all this is a single query that gets all the various columns that were requested in all the extensions and all the union all operations play out as written. The extensions are emitted in the order that they appear in the translation unit with the assembly, which again must have the row adding extensions first. This facility provides considerable ability to compose a large query, but each fragment can be independently checked for errors so that nobody ever has to debug the (possibly monstrous) overall result. Fragments can be removed simply by excluding them from the final assembly (with e.g. #ifdefs, or build rules) With the rewrite of the assembly_fragment complete, the codegen for that procedure is the normal codegen for a procedure with a single select. As always, Java and Objective C codegen on these pieces will produce suitable wrappers for the C. The output code for the assembly fragment generates these reading functions: extern cql_int32 base_frag_get_id( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_string_ref _Nonnull base_frag_get_name( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_double base_frag_get_rate( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); // used by adds_columns_get_data() to read its data extern cql_string_ref _Nullable __PRIVATE__base_frag_get_data( base_frag_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 base_frag_result_count( base_frag_result_set_ref _Nonnull result_set);  These are exactly what you would get for a normal query except that the pieces that came from extensions are marked PRIVATE. Those methods should not be used directly but instead the methods generated for each extension proc should be used. Additionally, to create the result set, as usual. extern CQL_WARN_UNUSED cql_code base_frag_fetch_results( sqlite3 *_Nonnull _db_, base_frag_result_set_ref _Nullable *_Nonnull result_set, cql_int32 id_);  With the combined set of methods you can create a variety of assembled queries from extensions in a fairly straightforward way. "},{"title":"Shared Fragments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#shared-fragments","content":"Shared fragments do not have the various restrictions that the &quot;extension&quot; style fragments have. While extensions were created to allow a single query to be composed by authors that did not necessarily work with each other, and therefore they are full of restrictions on the shape, shared queries instead are designed to give you maximum flexibility in how the fragments are re-used. You can think of them as being somewhat like a parameterized view, but the parameters are both value parameters and type parameters. In Java or C#, a shared fragments might have had an invocation that looked something like this: `my_fragment(1,2)&lt;table1, table2&gt;. As with the other fragment types the common table expression (CTE) is the way that they plug in. It's helpful to consider a real example: split_text(tok) AS ( WITH RECURSIVE splitter(tok,rest) AS ( SELECT '' tok, IFNULL( some_variable_ || ',', '') rest UNION ALL SELECT substr(rest, 1, instr(rest, ',') - 1) tok, substr(rest, instr(rest, ',') + 1) rest FROM splitter WHERE rest != '' ) SELECT tok from splitter where tok != '' )  This text might appear in dozens of places where a comma separated list needs to be split into pieces and there is no good way to share the code between these locations. CQL is frequently used in conjunction with the C-pre-processor so you could come up with something using the #define construct but this is problematic for several reasons: the compiler does not then know that the origin of the text really is the same thus it has no clue that sharing the text of the string might be a good idea any error messages happen in the context of the use of the macro not the definitionbonus: a multi-line macro like the above gets folded into one line so any error messages are impenetrableif you try to compose such macros it only gets worse; it's more code duplication and harder error casesany IDE support for syntax coloring and so forth will be confused by the macro as it's not part of the language None of this is any good but the desire to create helpers like this is real both for correctness and for performance. To make these things possible, we introduce the notion of shared fragments. We need to give them parameters and the natural way to create a select statement that is bindable in CQL is the procedure. So the shape we choose looks like this: @attribute(cql:shared_fragment) CREATE PROC split_text(value TEXT) BEGIN WITH RECURSIVE splitter(tok,rest) AS ( SELECT '' tok, IFNULL( value || ',', '') rest UNION ALL SELECT substr(rest, 1, instr(rest, ',') - 1) tok, substr(rest, instr(rest, ',') + 1) rest FROM tokens WHERE rest != '' ) SELECT tok from splitter where tok != '' END;  The introductory attribute @attribute(cql:shared_fragment) indicates that the procedure is to produce no code, but rather will be inlined as a CTE in other locations. To use it, we introduce the ability to call a procedure as part of a CTE declaration. Like so: WITH result(v) as (call split_text('x,y,z')) select * from result;  Once the fragment has been defined, the statement above could appear anywhere, and of course the text 'x,y,z' need not be constant. For instance: CREATE PROC print_parts(value TEXT) BEGIN DECLARE C CURSOR FOR WITH result(v) as (CALL split_text('x,y,z')) SELECT * from result; LOOP FETCH C BEGIN CALL printf(&quot;%s\\n&quot;, C.v); END; END;  Fragments are also composable, so for instance, we might also want some shared code that extracts comma separated numbers. We could do this: @attribute(cql:shared_fragment) CREATE PROC ids_from_string(value TEXT) BEGIN WITH result(v) as (CALL split_text(value)) SELECT CAST(v as LONG) as id from result; END;  Now we could write: CREATE PROC print_ids(value TEXT) BEGIN DECLARE C CURSOR FOR WITH result(id) as (CALL ids_from_string('1,2,3')) SELECT * from result; LOOP FETCH C BEGIN CALL printf(&quot;%ld\\n&quot;, C.id); END; END;  Of course these are very simple examples but in principle you can use the generated tables in whatever way is necessary. For instance, here's a silly but illustrative example: /* This is a bit silly */ CREATE PROC print_common_ids(value TEXT) BEGIN DECLARE C CURSOR FOR WITH v1(id) as (CALL ids_from_string('1,2,3')), v2(id) as (CALL ids_from_string('2,4,6')) SELECT * from v1 INTERSECT SELECT * from v2; LOOP FETCH C BEGIN CALL printf(&quot;%ld\\n&quot;, C.id); END; END;  With a small amount of dynamism in the generation of the SQL for the above, it's possible to share the body of v1 and v2. SQL will of course see the full expansion but your program only needs one copy no matter how many times you use the fragment anywhere in the code. So far we have illustrated the &quot;parameter&quot; part of the flexibility. Now let's look at the &quot;generics&quot; part; even though it's overkill for this example, it should still be illustrative. You could imagine that the procedure we wrote above ids_from_string might do something more complicated, maybe filtering out negative ids, ids that are too big, or that don't match some pattern, whatever the case might be. You might want these features in a variety of contexts, maybe not just starting from a string to split. We can rewrite the fragment in a &quot;generic&quot; way like so: @attribute(cql:shared_fragment) CREATE PROC ids_from_string_table() BEGIN WITH source(v) LIKE (select &quot;x&quot; v) SELECT CAST(v as LONG) as id from source; END;  Note the new construct for a CTE definition: inside a fragment we can use &quot;LIKE&quot; to define a plug-able CTE. In this case we used a select statement to describe the shape the fragment requires. We could also have used a name source(*) LIKE shape_name just like we use shape names when describing cursors. The name can be any existing view, table, a procedure with a result, etc. Any name that describes a shape. Now when the fragment is invoked, you provide the actual data source (some table, view, or CTE) and that parameter takes the role of &quot;values&quot;. Here's a full example: CREATE PROC print_ids(value TEXT) BEGIN DECLARE C CURSOR FOR WITH my_data(*) as (CALL split_text(value)), my_numbers(id) as (CALL ids_from_string_table() USING my_data AS source) SELECT id from my_numbers; LOOP FETCH C BEGIN CALL printf(&quot;%ld\\n&quot;, C.id); END; END;  We could actually rewrite the previous simple id fragment as follows: @attribute(cql:shared_fragment) CREATE PROC ids_from_string(value TEXT) BEGIN WITH tokens(v) as (CALL split_text(value)) ids(id) as (CALL ids_from_string_table() USING tokens as source) SELECT * from ids; END;  And actually we have a convenient name we could use for the shape we need so we could have used the shape syntax to define ids_from_string_table. @attribute(cql:shared_fragment) CREATE PROC ids_from_string_table() BEGIN WITH source(*) LIKE split_text SELECT CAST(tok as LONG) as id from source; END;  These examples have made very little use of the database but of course normal data is readily available, so shared fragments can make a great way to provide access to complex data with shareable, correct code. For instance, you could write a fragment that provides the ids of all open businesses matching a name from a combination of tables. This is similar to what you could do with a VIEW plus a WHERE clause but: such a system can give you well controlled combinations known to work wellthere is no schema required, so your database load time can still be fastparameterization is not limited to filtering VIEWs after the fact&quot;generic&quot; patterns are available, allowing arbitrary data sources to be filtered, validated, augmentedeach fragment can be tested separately with its own suite rather than only in the context of some larger thingcode generation can be more economical because the compiler is aware of what is being shared In short, shared fragments can help with the composition of any complicated kinds of queries. If you're producing an SDK to access a data set, they are indispensible. Creating and Using Valid Shared Fragments​ When creating a fragment the following rules are enforced: the fragment many not have any out argumentsit must consist of exactly one valid select statement (but see future forms below)it may use the LIKE construct in CTE definitions to create placeholder shapes this form is illegal outside of shared fragments (otherwise how would you bind it) the LIKE form may only appear in top level CTE expressions in the fragmentthe fragment is free to use other fragments, but it may not call itself calling itself would result in infinite inlining Usage of a fragment is always introduced by a &quot;call&quot; to the fragment name in a CTE body. When using a fragment the following rules are enforced. the provided parameters must create a valid procedure call just like normal procedure calls i.e. the correct number and type of arguments the provided parameters may not use nested (SELECT ...) expressions this could easily create fragment building within fragment building which seems not worth the complexityif database access is required in the parameters simply wrap it in a helper procedure the optional USING clause must specify each required table parameter exactly once and no other tables a fragment that requires table parameters be invoked without a USING clause every actual table provided must match the column names of the corresponding table parameter i.e. in USING my_data AS values the actual columns in my_data must be the same as in the values parameterthe columns need not be in the same order each column in any actual table must be &quot;assignment compatible&quot; with its corresponding column in the parameters i.e. the actual type could be converted to the formal type using the same rules as the := operatorthese are the same rules used for procedure calls, for instance, where the call is kind of like assigning the actual parameter values to the formal parameter variables the provided table values must not conflict with top level CTEs in the shared fragment exception: the top level CTEs that were parameters do not create conflictse.g. it's common to do values(*) as (CALL something() using source as source) - here the caller's &quot;source&quot; takes the value of the fragment's &quot;source&quot;, which is not a true conflicthowever, the caller's source might itself have been a parameter in which case the value provided could create an inner conflict all these problems are easily avoided with a simple naming convention for parameters so that real arguments never look like parameter names and parameter forwarding is apparente.g. USING _source AS _source makes it clear that a parameter is being forwarded and _source is not likely to conflict with real table or view names Note that when shared fragments are used, the generated SQL has the text split into parts, with each fragment and its surroundings separated, therefore the text of shared fragments is shared(!) between usages if normal linker optimizations for text folding are enabled (common in production code.) "},{"title":"Shared Fragments with Conditionals​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#shared-fragments-with-conditionals","content":"Shared fragments use dynamic assembly of the text to do the sharing but it is also possible to create alternative texts. There are many instances where it is desirable to not just replace parameters but use, for instance, an entirely different join sequence. Without shared fragments, the only way to accomplish this is to fork the desired query at the topmost level (because SQLite has no internal possibility of &quot;IF&quot; conditions.) This is expensive in terms of code size and also cognitive load because the entire alternative sequences have to be kept carefully in sync. Macros can help with this but then you get the usual macro maintenance problems, including poor diagnostics. And of course there is no possibility to share the common parts of the text of the code if it is forked. However, conditional shared fragments allow forms like this: @attribute(cql:shared_fragment) CREATE PROC ids_from_string(val TEXT) BEGIN IF val IS NULL OR val IS '' THEN SELECT 0 id WHERE 0; -- empty result ELSE WITH tokens(v) as (CALL split_text(val)) ids(id) as (CALL ids_from_string_table() USING tokens as source) SELECT * from ids; END IF; END;  Now we can do something like:  ids(*) AS (CALL ids_from_string(str))  In this case, if the string val is empty then SQLite will not see the complex comma splitting code, and instead will see the trivial case select 0 id where 0. The code in a conditional fragment might be entirely different between the branches removing unnecessary code, or swapping in a new experimental cache in your test environment, or anything like that. The generalization is simply this: instead of just one select statement there is one top level &quot;IF&quot; statementeach statement list of the IF must be exactly one select statementthere must be an ELSE clausethe select statements must be type compatible, just like in a normal procedureany table parameters with the same name in different branches must have the same type otherwise it would be impossible to provide a single actual table for those table parameters With this additional flexibility a wide variety of SQL statements can be constructed economically and maintainability. Importantly, consumers of the fragments need not deal with all these various alternate possibilities but they can readily create their own useful combinations out of building blocks. Ultimately, from SQLite's perspective, all of these shared fragment forms result in nothing more complicated than a chain of CTE expressions. See Appendix 8 for an extensive section on best practices around fragments and common table expressions in general. tip If you use CQL's query planner on shared fragments with conditionals, the query planner will only analyze the first branch by default. You need to use @attribute(cql:query_plan_branch=[integer]) to modify the behaviour. Read Query Plan Generation for details. "},{"title":"Shared Fragments as Expressions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#shared-fragments-as-expressions","content":"The shared fragment system also has the ability to create re-usable expression-style fragments giving you something like SQL inline functions. These do come with some performance cost so they should be used for larger fragments. In many systems a simple shared fragment would not compete well with an equivalent #define. Expression fragments shine when: the fragment is quite largeits used frequently (hence providing significant space savings)the arguments are complex, potentially used many times in the expression From a raw performance perspective, the best you can hope for with any of the fragment approaches is a &quot;tie&quot; on speed compared do directly inlining equivalent SQL or using a macro to do the same. However, from a correctness and space perspective it is very possible to come out ahead. It's fair to say that expression fragments have the greatest overhead compared to the other types and so they are best used in cases where the size benefits are going to be important. Syntax​ An expression fragment is basically a normal shared fragment with no top-level FROM clause that generates a single column. A typical one might look like this: -- this isn't very exciting because regular max would do the job @attribute(cql:shared_fragment) create proc max_func(x integer, y integer) begin select case when x &gt;= y then x else y end; end;  The above can be used in the context of a SQL statement like so: select max_func(T1.column1, T1.column2) the_max from foo T1;  Discussion​ The consequence of the above is that the body of max_func is inlined into the generated SQL. However, like the other shared fragments, this is done in such a way that the text can be shared between instances so you only pay for the cost of the text of the SQL in your program one time, no matter how many time you use it. In particular, for the above, the compiler will generate the following SQL: select ( select case when x &gt;= y then x else y end from (select T1.column1 x, column2 y))  But each line will be its own string literal, so, more accurately, it will concatenate the following three strings: &quot;select (&quot;, // string1 &quot; select case when x &gt;= y then x else y end&quot;, // string2 &quot; from (select T1.column1 x, column2 y))&quot; // string3  Importantly, string2 is fixed for any given fragment. The only thing that changes is string3, i.e., the arguments. In any modern C compilation system, the linker will unify the string2 literal across all translation units so you only pay for the cost of that text one time. It also means that the text of the arguments appears exactly one time, no matter how complex they are. For these benefits, we pay the cost of the select wrapper on the arguments. If the arguments are complex that &quot;cost&quot; is negative. Consider the following: select max_func((select max(T.m) from T), (select max(U.m) from U))  A direct expansion of the above would result in something like this: case when (select max(T.m) from T) &gt;= (select max(U.m) from U) then (select max(T.m) from T) else (select max(U.m) from U) end;  The above could be accomplished with a simple #define style macro. However, the expression fragment generates the following code: select ( select case when x &gt;= y then x else y end from select (select max(T.m) from T) x, (select max(U.m) from U) y))  Expression fragments can nest, so you could write: @attribute(cql:shared_fragment) create proc max3_func(x integer, y integer, z integer) begin select max_func(x, max_func(y, z)); end;  Again, this particular example is a waste because regular max would already do the job better. To give another example, common mappings from one kind of code to another using case/when can be written and shared this way: -- this sort of thing happens all the time @attribute(cql:shared_fragment) create proc remap(x integer not null) begin select case x when 1 then 1001 when 2 then 1057 when 3 then 2010 when 4 then 2011 else 9999 end; end;  In the following: select remap(T1.c), remap(T2.d), remap(T3.e) from T1, T2, T3... etc.  The text for remap will appear three times in the generated SQL query but only one time in your binary. Restrictions​ the fragment must consist of exactly one simple select statement no FROM, WHERE, HAVING, etc. -- the result is an expression the select list must have exactly one value Note: the expression can be a nested SELECT which could then have all the usual SELECT elements the usual shared fragment rules apply, e.g. no out-parameters, exactly one statement, etc. Additional Notes​ A simpler syntax might have been possible but expression fragments are only interesting in SQL contexts where (among other things) normal procedure and function calls are not available. So the select keyword makes it clear to the coder and the compiler that the expression will be evaluated by SQLite and the rules for what is allowed to go in the expression are the SQLite rules. The fragment has no FROM clause because we're trying to produce an expression, not a table-value with one column. If you want a table-value with one column, the original shared fragments solution already do exactly that. Expression fragments give you a solution for sharing code in, say, the WHERE clause of a larger select statement. Commpared to something like #define max_func(x,y) case when (x) &gt;= (y) then x else y end;  The macro does give you a ton of flexibility, but it has many problems: if the macro has an error, you see the error in the call site with really bad diagnostic infothe compiler doesn't know that the sharing is going on so it won't be able to share text between call sitesthe arguments can be evaluated many times each which could be expensive, bloaty, or wrongthere is no type-checking of arguments to the macro so you may or may not get compilation errors after expansionyou have to deal with all the usual pre-processor hazards In general, macros can be used (as in C and C++) as an alternative to expression fragments, especially for small fragments. But, this gets to be a worse idea as such macros grow. For larger cases, C and C++ provide inline functions -- CQL provides expression fragments. "},{"title":"Chapter 15: Query Plan Generation​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#chapter-15-query-plan-generation","content":"CQL offers a way to run SQLite's EXPLAIN QUERY PLAN command for all the SQL statements used in a CQL file using a set of special compilation steps. Generating query plans is inherently complicated. Any given stored procedure might include many SQL statements, each of which has a plan. To get the plans, those statements must be executed in the appropriate mode. In order for them to execute, whatever tables, views, and user-defined functions they use must exist. The statements can have any number of parameters, those have to be swapped out because they might come from anywhere. When run in --rt query_plan mode, the compiler accomplishes all of this by analyzing the original code and creating entirely new code. Running this new code creates the schema and, with the appropriate transforms, runs all the SQL statements in the original to give us the query plans. The process requires many steps as we'll see below. "},{"title":"Query Plan Generation Compilation Steps​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#query-plan-generation-compilation-steps","content":"tip The following steps are used in ./repl/go_query_plan.sh, you can run it to get a quick demonstration of this feature in action. The rest of the section explains how query plan generation works and some of its quirks. To execute query plans for a given CQL file, the following commands need to be run: CQL_FILE= # The CQL file to compile CQL_ROOT_DIR= # Path to cql directory CQL=$CQL_ROOT_DIR/out/cql # Generate Query Plan Script $CQL --in $CQL_FILE --rt query_plan --cg go-qp.sql # Generate UDF stubs $CQL --in $CQL_FILE --rt udf --cg go-qp-udf.h go-qp-udf.c # Compile and link CQL artifacts, with a main C file query_plan_test.c $CQL --in go-qp.sql --cg go-qp.h go-qp.c --dev cc -I$CQL_ROOT_DIR -I. -c $CQL_ROOT_DIR/query_plan_test.c go-qp.c go-qp-udf.c cc -I$CQL_ROOT_DIR -I. -O -o go_query_plan go-qp.o go-qp-udf.o query_plan_test.o $CQL_ROOT_DIR/cqlrt.c -lsqlite3 # Run and generate query plans ./go_query_plan  Contrary to what one might expect, rather than providing query plans directly, CQL uses --rt query_plan to generate a second CQL script that returns query plans for each SQL statement used. A separate command, --rt udf is required to generate any stubbed user defined functions that are used in the original script. Afterwards, the generated query plan script, udf stubs needs to compiled like any CQL file and run by a &quot;main&quot; function that needs to be created separately. The CQL repository provides the file query_plan_test.c that can be used as the &quot;main&quot; function, otherwise you can make your own. ::note When compiling the CQL file generated by --rt query_plan, the --dev flag is required. ::: "},{"title":"Special Handling of CQL features in Query Plan Generation​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#special-handling-of-cql-features-in-query-plan-generation","content":"CQL's query planner generator modifies the usage of the following features to allow SQLite run EXPLAIN QUERY PLAN successfully: VariablesUser Defined FunctionsConditionals in Shared Fragments caution Generating query plans of CQL files that use table valued functions, or virtual tables is not supported. Variables​ Variables used in SQL statements are stubbed into constants. The exact value varies depending on the type of the variable, but it is always equivalent to some form of &quot;1&quot;. original.sql ... SELECT * FROM my_table WHERE id = x; ...  query_plan.sql ... EXPLAIN QUERY PLAN SELECT * FROM my_table WHERE my_table.id = nullable(1); ...  User Defined Functions​ Read Functions on details about Function Types. Since the implementation of UDFs in a CQL file do not affect SQLite query plans, CQL's query plan script expects stubs generated by cql --rt udf to be used instead. Conditionals in Shared Fragments​ Read CQL Query Fragments on details about shared fragments Only one branch of a conditional is chosen for query plan analysis. By default this will be the first branch, which is the initial SELECT statement following the IF conditional. The branch to analyze can be configured with the cql:query_plan_branch @attribute. Here's an example of cql:query_plan_branch being used: original.sql @attribute(cql:shared_fragment) @attribute(cql:query_plan_branch=1) CREATE PROC frag2(y int) BEGIN IF y == 2 THEN SELECT 10 b; ELSE IF y == -1 THEN SELECT 20 b; ELSE SELECT 30 b; END IF; END;  query_plan.sql EXPLAIN QUERY PLAN SELECT 20 b;  Setting cql:query_plan_branch=1 selects the second branch. Providing cql:query_plan_branch=2 instead would yield the ELSE clause SELECT 30 b. cql:query_plan_branch=0 would yield SELECT 10 b, which is the same as the default behaviour. "},{"title":"Appendix 1: Command Line Options​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-1-command-line-options","content":"CQL has a variety of command line (CLI) options but many of them are only interesting for cql development. Nonetheless this is a comprehensive list: note CQL is often used after the c pre-processor is run so this kind of invocation is typical: cc -E -x c foo.sql | cql [args]  "},{"title":"With No Options​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#with-no-options","content":"emits a usage message "},{"title":"--in file​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--in-file","content":"reads the given file for the input instead of stdinthe input should probably have already been run through the C pre-processor as abovereturns non-zero if the file fails to parse Example: cql --in test.sql  "},{"title":"--sem​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--sem","content":"performs semantic analysis on the input file ONLYthe return code is zero if there are no errors Example: cql --in sem_test.sql --sem  "},{"title":"--ast​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--ast","content":"walks the AST and prints it to stdout in human readable text formmay be combined with --sem (semantic info will be included) Example cql --in sem_test.sql --sem --ast &gt;sem_ast.out  "},{"title":"--echo​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--echo","content":"walks the AST and emits the text of a program that would create itthis has the effect of &quot;beautifying&quot; badly formatted input or &quot;canonicalizing&quot; it some sensible indenting is added but it might not be the original indentingextraneous whitespace, parens, etc. are removed may be combined with --sem (in which case you see the source after any rewrites for sugar)this also validates that the input can be parsed Example cql --in test.sql --echo &gt;test.out # test.out is &quot;equivalent&quot; to test.sql  "},{"title":"--dot​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--dot","content":"prints the internal AST to stdout in DOT format for graph visualizationthis is really only interesting for small graphs for discussion as it rapidly gets insane Example: cql --dot --in dottest.sql  "},{"title":"--cg output1 output2 ...​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--cg-output1-output2-","content":"any number of output files may be needed for a particular result type, two is commonthe return code is zero if there are no errorsany --cg option implies --sem Example: cql --in foo.sql --cg foo.h foo.c  "},{"title":"--nolines​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--nolines","content":"Suppress the # directives for lines. Useful if you need to debug the C code. Example: cql --in test.sql --nolines --cg foo.h foo.c  "},{"title":"--global_proc name​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--global_proc-name","content":"any loose SQL statements not in a stored proc are gathered and put into a procedure of the given namewhen generating a schema migrate script the global proc name is used as a prefix on all of the artifacts so that there can be several independent migrations linked into a single executable "},{"title":"--compress​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--compress","content":"for use with the C result type, (or any similar types added to the runtime array in your compiler)string literals for the SQL are broken into &quot;fragments&quot; the DML is then represented by an array of fragmentssince DML is often very similar there is a lot of token sharing possiblethe original string is recreated at runtime from the fragments and then executedcomments show the original string inline for easier debugging and searching NOTE: different result types require a different number of output files with different meanings "},{"title":"--test​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--test","content":"some of the output types can include extra diagnostics if --test is includedthe test output often makes the outputs badly formed so this is generally good for humans only "},{"title":"--dev​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--dev","content":"some codegen features only make sense during development, this enables dev mode to turn those one ** example: explain query plan "},{"title":"--c_include_namespace​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--c_include_namespace","content":"for the C codegen runtimes, it determines the header namespace (as in #include &quot;namespace/file.h&quot;) that goes into the output C fileif this option is used, it is prefixed to the first argment to --cg to form the include path in the C fileif absent there is no &quot;namespace/&quot; prefix "},{"title":"--c_include_path​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--c_include_path","content":"for the C codegen runtimes, it determines the full header path (as in #include &quot;your_arg&quot;) that goes into the output C fileif this option is used, the first argment to --cg controls only the output path and does not appear in include path at allthis form overrides --c_include_namespace if both are specified "},{"title":"--objc_c_include_path​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#--objc_c_include_path","content":"for ObjC codegen runtimes that need to refer to the generated C code, this represents the header of the C generated code that will be used during inclusion from the ObjC file "},{"title":"Result Types (--rt *)​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#result-types---rt-","content":"These are the various outputs the compiler can produce. --rt c​ requires two output files (foo.h and foo.c)this is the standard C compilation of the sql file --cqlrt foo.h​ emits #include &quot;foo.h&quot; into the C output instead of #include &quot;cqlrt.h&quot; --generate_type_getters​ changes C output for CQL result sets so that the field readers used shared functions to get fields of a certain typethis style of codegen makes result-sets more interoperable with each other if they have similar shape so it can be useful --generate_exports​ adds an additional output fileexample: `--in foo.sql --generate_exports --rt c --cg foo.h foo.c foo_exports.sqlthe output file foo_exports.sql includes procedure declarations for the contents of foo.sqlbasically automatically generates the CQL header file you need to access the procedures in the input from some other fileif it were C it would be like auto-generating foo.h from foo.c --rt objc​ objective C wrappers for result sets produced by the stored procedures in the inputthese depend on the output of a standard codegen run so this is additiverequires one output file (foo.h) --rt schema​ produces the canonical schema for the given input filesstored procedures etc. are removedwhitespace etc. is removedsuitable for use to create the next or first &quot;previous&quot; schema for schema validationrequires one output file --rt schema_upgrade​ produces a CQL schema upgrade script which can then be compiled with CQL itselfsee the chapter on schema upgrade/migration: Chapter 10requires one output file (foo.sql) --include_regions a b c​ the indicated regions will be declaredused with --rt schema_upgrade or --rt schemain the upgrade case excluded regions will not be themselves upgraded, but may be referred, to by things that are being upgraded --exclude_regions x y z​ the indicated regions will still be declared but the upgrade code will be suppressed, the presumption being that a different script already upgrades x y zused with --rt schema_upgrade --min_schema_version n​ the schema upgrade script will not include upgrade steps for schema older than the version specified --schema_exclusive​ the schema upgrade script assumes it owns all the schema in the database, it aggressively removes other things --rt json_schema​ produces JSON output suitable for consumption by downstream codegenthe JSON includes a definition of the various entities in the inputsee the section on JSON output for details --rt query_plan​ produces CQL output which can be re-compiled by CQL as normal inputthe output consists of a set of procedures that will emit all query plans for the DML that was in the inputsee also --rt udf and Chapter 15 --rt stats​ produces a simple .csv file with node count information for AST nodes per procedure in the inputrequires one output file (foo.csv) --rt udf​ produces stub UDF implementations for all UDFS that were seen in the inputthis output is suitable for use with --rt query_plan so that SQL with UDFs will run in a simple contextrequires two output files (e.g. udfs.h and udfs.c)See also Chapter 15 "},{"title":"Appendix 2: CQL Grammar​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-2-cql-grammar","content":"What follows is taken from a grammar snapshot with the tree building rules removed. It should give a fair sense of the syntax of CQL (but not semantic validation). Snapshot as of Fri Mar 3 21:02:20 PST 2023 "},{"title":"Operators and Literals​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#operators-and-literals","content":"These are in order of priority lowest to highest &quot;UNION ALL&quot; &quot;UNION&quot; &quot;INTERSECT&quot; &quot;EXCEPT&quot; &quot;:=&quot; &quot;OR&quot; &quot;AND&quot; &quot;NOT&quot; &quot;BETWEEN&quot; &quot;NOT BETWEEN&quot; &quot;&lt;&gt;&quot; &quot;!=&quot; '=' &quot;==&quot; &quot;LIKE&quot; &quot;NOT LIKE&quot; &quot;GLOB&quot; &quot;NOT GLOB&quot; &quot;MATCH&quot; &quot;NOT MATCH&quot; &quot;REGEXP&quot; &quot;NOT REGEXP&quot; &quot;IN&quot; &quot;NOT IN&quot; &quot;IS NOT&quot; &quot;IS&quot; &quot;IS TRUE&quot; &quot;IS FALSE&quot; &quot;IS NOT TRUE&quot; &quot;IS NOT FALSE&quot; &quot;ISNULL&quot; &quot;NOTNULL&quot; '&lt;' '&gt;' &quot;&gt;=&quot; &quot;&lt;=&quot; &quot;&lt;&lt;&quot; &quot;&gt;&gt;&quot; '&amp;' '|' '+' '-' '*' '/' '%' &quot;||&quot; &quot;COLLATE&quot; &quot;UMINUS&quot; '~'  NOTE: The above varies considerably from the C binding order!!! Literals: ID /* a name */ STRLIT /* a string literal in SQL format e.g. 'it''s sql' */ CSTRLIT /* a string literal in C format e.g. &quot;hello, world\\n&quot; */ BLOBLIT /* a blob literal in SQL format e.g. x'12ab' */ INTLIT /* integer literal */ LONGLIT /* long integer literal */ REALLIT /* floating point literal */  "},{"title":"Statement/Type Keywords​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#statementtype-keywords","content":"&quot;@ATTRIBUTE&quot; &quot;@BEGIN_SCHEMA_REGION&quot; &quot;@BLOB_CREATE_KEY&quot; &quot;@BLOB_CREATE_VAL&quot; &quot;@BLOB_GET_KEY&quot; &quot;@BLOB_GET_KEY_TYPE&quot; &quot;@BLOB_GET_VAL&quot; &quot;@BLOB_GET_VAL_TYPE&quot; &quot;@BLOB_UPDATE_KEY&quot; &quot;@BLOB_UPDATE_VAL&quot; &quot;@CREATE&quot; &quot;@DECLARE_DEPLOYABLE_REGION&quot; &quot;@DECLARE_SCHEMA_REGION&quot; &quot;@DELETE&quot; &quot;@DUMMY_SEED&quot; &quot;@ECHO&quot; &quot;@EMIT_CONSTANTS&quot; &quot;@EMIT_ENUMS&quot; &quot;@EMIT_GROUP&quot; &quot;@END_SCHEMA_REGION&quot; &quot;@ENFORCE_NORMAL&quot; &quot;@ENFORCE_POP&quot; &quot;@ENFORCE_PUSH&quot; &quot;@ENFORCE_RESET&quot; &quot;@ENFORCE_STRICT&quot; &quot;@EPONYMOUS&quot; &quot;@FILE&quot; &quot;@PREVIOUS_SCHEMA&quot; &quot;@PROC&quot; &quot;@RC&quot; &quot;@RECREATE&quot; &quot;@SCHEMA_AD_HOC_MIGRATION&quot; &quot;@SCHEMA_UPGRADE_SCRIPT&quot; &quot;@SCHEMA_UPGRADE_VERSION&quot; &quot;@SENSITIVE&quot; &quot;@UNSUB&quot; &quot;ABORT&quot; &quot;ACTION&quot; &quot;ADD&quot; &quot;AFTER&quot; &quot;ALL&quot; &quot;ALTER&quot; &quot;ARGUMENTS&quot; &quot;AS&quot; &quot;ASC&quot; &quot;AUTOINCREMENT&quot; &quot;BEFORE&quot; &quot;BEGIN&quot; &quot;BLOB&quot; &quot;BY&quot; &quot;CALL&quot; &quot;CASCADE&quot; &quot;CASE&quot; &quot;CAST&quot; &quot;CATCH&quot; &quot;CHECK&quot; &quot;CLOSE&quot; &quot;COLUMN&quot; &quot;COLUMNS&quot; &quot;COMMIT&quot; &quot;CONST&quot; &quot;CONSTRAINT&quot; &quot;CONTEXT COLUMN&quot; &quot;CONTEXT TYPE&quot; &quot;CONTINUE&quot; &quot;CREATE&quot; &quot;CROSS&quot; &quot;CURRENT ROW&quot; &quot;CURSOR HAS ROW&quot; &quot;CURSOR&quot; &quot;DECLARE&quot; &quot;DEFAULT&quot; &quot;DEFERRABLE&quot; &quot;DEFERRED&quot; &quot;DELETE&quot; &quot;DESC&quot; &quot;DISTINCT&quot; &quot;DISTINCTROW&quot; &quot;DO&quot; &quot;DROP&quot; &quot;ELSE IF&quot; &quot;ELSE&quot; &quot;ENCODE&quot; &quot;END&quot; &quot;ENUM&quot; &quot;EXCLUDE CURRENT ROW&quot; &quot;EXCLUDE GROUP&quot; &quot;EXCLUDE NO OTHERS&quot; &quot;EXCLUDE TIES&quot; &quot;EXCLUSIVE&quot; &quot;EXISTS&quot; &quot;EXPLAIN&quot; &quot;FAIL&quot; &quot;FETCH&quot; &quot;FILTER&quot; &quot;FIRST&quot; &quot;FOLLOWING&quot; &quot;FOR EACH ROW&quot; &quot;FOR&quot; &quot;FOREIGN&quot; &quot;FROM BLOB&quot; &quot;FROM&quot; &quot;FUNC&quot; &quot;FUNCTION&quot; &quot;GROUP&quot; &quot;GROUPS&quot; &quot;HAVING&quot; &quot;HIDDEN&quot; &quot;IF&quot; &quot;IGNORE&quot; &quot;IMMEDIATE&quot; &quot;INDEX&quot; &quot;INITIALLY&quot; &quot;INNER&quot; &quot;INOUT&quot; &quot;INSERT&quot; &quot;INSTEAD&quot; &quot;INT&quot; &quot;INTEGER&quot; &quot;INTERFACE&quot; &quot;INTO&quot; &quot;JOIN&quot; &quot;KEY&quot; &quot;LAST&quot; &quot;LEAVE&quot; &quot;LEFT&quot; &quot;LET&quot; &quot;LIMIT&quot; &quot;LONG&quot; &quot;LONG_INT&quot; &quot;LONG_INTEGER&quot; &quot;LOOP&quot; &quot;NO&quot; &quot;NOT DEFERRABLE&quot; &quot;NOTHING&quot; &quot;NULL&quot; &quot;NULLS&quot; &quot;OBJECT&quot; &quot;OF&quot; &quot;OFFSET&quot; &quot;ON CONFLICT&quot; &quot;ON&quot; &quot;OPEN&quot; &quot;ORDER&quot; &quot;OUT&quot; &quot;OUTER&quot; &quot;OVER&quot; &quot;PARTITION&quot; &quot;PRECEDING&quot; &quot;PRIMARY&quot; &quot;PRIVATE&quot; &quot;PROC&quot; &quot;PROCEDURE&quot; &quot;QUERY PLAN&quot; &quot;RAISE&quot; &quot;RANGE&quot; &quot;REAL&quot; &quot;RECURSIVE&quot; &quot;REFERENCES&quot; &quot;RELEASE&quot; &quot;RENAME&quot; &quot;REPLACE&quot; &quot;RESTRICT&quot; &quot;RETURN&quot; &quot;RIGHT&quot; &quot;ROLLBACK&quot; &quot;ROWID&quot; &quot;ROWS&quot; &quot;SAVEPOINT&quot; &quot;SELECT&quot; &quot;SET&quot; &quot;SIGN FUNCTION&quot; &quot;STATEMENT&quot; &quot;SWITCH&quot; &quot;TABLE&quot; &quot;TEMP&quot; &quot;TEXT&quot; &quot;THEN&quot; &quot;THROW&quot; &quot;TO&quot; &quot;TRANSACTION&quot; &quot;TRIGGER&quot; &quot;TRY&quot; &quot;TYPE&quot; &quot;TYPE_CHECK&quot; &quot;UNBOUNDED&quot; &quot;UNIQUE&quot; &quot;UPDATE&quot; &quot;UPSERT&quot; &quot;USING&quot; &quot;VALUES&quot; &quot;VAR&quot; &quot;VIEW&quot; &quot;VIRTUAL&quot; &quot;WHEN&quot; &quot;WHERE&quot; &quot;WHILE&quot; &quot;WINDOW&quot; &quot;WITH&quot; &quot;WITHOUT&quot;  "},{"title":"Rules​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#rules","content":"Note that in many cases the grammar is more generous than the overall language and errors have to be checked on top of this, often this is done on purpose because even when it's possible it might be very inconvenient to do checks with syntax. For example the grammar cannot enforce non-duplicate ids in id lists, but it could enforce non-duplicate attributes in attribute lists. It chooses to do neither as they are easily done with semantic validation. Thus the grammar is not the final authority on what constitutes a valid program but it's a good start.  program: opt_stmt_list ; opt_stmt_list: /*nil*/ | stmt_list ; stmt_list: stmt ';' | stmt_list stmt ';' ; stmt: misc_attrs any_stmt ; any_stmt: alter_table_add_column_stmt | begin_schema_region_stmt | begin_trans_stmt | blob_get_key_type_stmt | blob_get_val_type_stmt | blob_get_key_stmt | blob_get_val_stmt | blob_create_key_stmt | blob_create_val_stmt | blob_update_key_stmt | blob_update_val_stmt | call_stmt | close_stmt | commit_return_stmt | commit_trans_stmt | continue_stmt | create_index_stmt | create_proc_stmt | create_table_stmt | create_trigger_stmt | create_view_stmt | create_virtual_table_stmt | declare_deployable_region_stmt | declare_enum_stmt | declare_const_stmt | declare_group_stmt | declare_select_func_no_check_stmt | declare_func_stmt | declare_out_call_stmt | declare_proc_no_check_stmt | declare_proc_stmt | declare_interface_stmt | declare_schema_region_stmt | declare_vars_stmt | declare_forward_read_cursor_stmt | declare_fetched_value_cursor_stmt | declare_type_stmt | delete_stmt | drop_index_stmt | drop_table_stmt | drop_trigger_stmt | drop_view_stmt | echo_stmt | emit_enums_stmt | emit_group_stmt | emit_constants_stmt | end_schema_region_stmt | enforce_normal_stmt | enforce_pop_stmt | enforce_push_stmt | enforce_reset_stmt | enforce_strict_stmt | explain_stmt | select_nothing_stmt | fetch_call_stmt | fetch_stmt | fetch_values_stmt | fetch_cursor_from_blob_stmt | guard_stmt | if_stmt | insert_stmt | leave_stmt | let_stmt | loop_stmt | out_stmt | out_union_stmt | out_union_parent_child_stmt | previous_schema_stmt | proc_savepoint_stmt | release_savepoint_stmt | return_stmt | rollback_return_stmt | rollback_trans_stmt | savepoint_stmt | select_stmt | schema_ad_hoc_migration_stmt | schema_unsub_stmt | schema_upgrade_script_stmt | schema_upgrade_version_stmt | set_stmt | switch_stmt | throw_stmt | trycatch_stmt | update_cursor_stmt | update_stmt | upsert_stmt | while_stmt | with_delete_stmt | with_insert_stmt | with_update_stmt | with_upsert_stmt ; explain_stmt: &quot;EXPLAIN&quot; opt_query_plan explain_target ; opt_query_plan: /* nil */ | &quot;QUERY PLAN&quot; ; explain_target: select_stmt | update_stmt | delete_stmt | with_delete_stmt | with_insert_stmt | insert_stmt | upsert_stmt | drop_table_stmt | drop_view_stmt | drop_index_stmt | drop_trigger_stmt | begin_trans_stmt | commit_trans_stmt ; previous_schema_stmt: &quot;@PREVIOUS_SCHEMA&quot; ; schema_upgrade_script_stmt: &quot;@SCHEMA_UPGRADE_SCRIPT&quot; ; schema_upgrade_version_stmt: &quot;@SCHEMA_UPGRADE_VERSION&quot; '(' &quot;integer-literal&quot; ')' ; set_stmt: &quot;SET&quot; name &quot;:=&quot; expr | &quot;SET&quot; name &quot;FROM&quot; &quot;CURSOR&quot; name ; let_stmt: &quot;LET&quot; name &quot;:=&quot; expr ; version_attrs_opt_recreate: /* nil */ | &quot;@RECREATE&quot; opt_delete_plain_attr | &quot;@RECREATE&quot; '(' name ')' opt_delete_plain_attr | version_attrs ; opt_delete_plain_attr: /* nil */ | &quot;@DELETE&quot; ; opt_version_attrs: /* nil */ | version_attrs ; version_attrs: &quot;@CREATE&quot; version_annotation opt_version_attrs | &quot;@DELETE&quot; version_annotation opt_version_attrs ; opt_delete_version_attr: /* nil */ | &quot;@DELETE&quot; version_annotation ; drop_table_stmt: &quot;DROP&quot; &quot;TABLE&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;TABLE&quot; name ; drop_view_stmt: &quot;DROP&quot; &quot;VIEW&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;VIEW&quot; name ; drop_index_stmt: &quot;DROP&quot; &quot;INDEX&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;INDEX&quot; name ; drop_trigger_stmt: &quot;DROP&quot; &quot;TRIGGER&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;TRIGGER&quot; name ; create_virtual_table_stmt: &quot;CREATE&quot; &quot;VIRTUAL&quot; &quot;TABLE&quot; opt_vtab_flags name &quot;USING&quot; name opt_module_args &quot;AS&quot; '(' col_key_list ')' opt_delete_version_attr ; opt_module_args: /* nil */ | '(' misc_attr_value_list ')' | '(' &quot;ARGUMENTS&quot; &quot;FOLLOWING&quot; ')' ; create_table_prefix_opt_temp: &quot;CREATE&quot; opt_temp &quot;TABLE&quot; ; create_table_stmt: create_table_prefix_opt_temp opt_if_not_exists name '(' col_key_list ')' opt_no_rowid version_attrs_opt_recreate ; opt_temp: /* nil */ | &quot;TEMP&quot; ; opt_if_not_exists: /* nil */ | &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; ; opt_no_rowid: /* nil */ | &quot;WITHOUT&quot; &quot;ROWID&quot; ; opt_vtab_flags: /* nil */ | &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; | &quot;@EPONYMOUS&quot; | &quot;@EPONYMOUS&quot; &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; | &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; &quot;@EPONYMOUS&quot; ; col_key_list: col_key_def | col_key_def ',' col_key_list ; col_key_def: col_def | pk_def | fk_def | unq_def | check_def | shape_def ; check_def: &quot;CONSTRAINT&quot; name &quot;CHECK&quot; '(' expr ')' | &quot;CHECK&quot; '(' expr ')' ; shape_exprs : shape_expr ',' shape_exprs | shape_expr ; shape_expr: name | '-' name ; shape_def: shape_def_base | shape_def_base '(' shape_exprs ')' ; shape_def_base: &quot;LIKE&quot; name | &quot;LIKE&quot; name &quot;ARGUMENTS&quot; ; col_name: name ; misc_attr_key: name | name ':' name ; misc_attr_value_list: misc_attr_value | misc_attr_value ',' misc_attr_value_list ; misc_attr_value: name | any_literal | const_expr | '(' misc_attr_value_list ')' | '-' num_literal | '+' num_literal ; misc_attr: &quot;@ATTRIBUTE&quot; '(' misc_attr_key ')' | &quot;@ATTRIBUTE&quot; '(' misc_attr_key '=' misc_attr_value ')' ; misc_attrs: /* nil */ | misc_attr misc_attrs ; col_def: misc_attrs col_name data_type_any col_attrs ; pk_def: &quot;CONSTRAINT&quot; name &quot;PRIMARY&quot; &quot;KEY&quot; '(' indexed_columns ')' opt_conflict_clause | &quot;PRIMARY&quot; &quot;KEY&quot; '(' indexed_columns ')' opt_conflict_clause ; opt_conflict_clause: /* nil */ | conflict_clause ; conflict_clause: &quot;ON CONFLICT&quot; &quot;ROLLBACK&quot; | &quot;ON CONFLICT&quot; &quot;ABORT&quot; | &quot;ON CONFLICT&quot; &quot;FAIL&quot; | &quot;ON CONFLICT&quot; &quot;IGNORE&quot; | &quot;ON CONFLICT&quot; &quot;REPLACE&quot; ; opt_fk_options: /* nil */ | fk_options ; fk_options: fk_on_options | fk_deferred_options | fk_on_options fk_deferred_options ; fk_on_options: &quot;ON&quot; &quot;DELETE&quot; fk_action | &quot;ON&quot; &quot;UPDATE&quot; fk_action | &quot;ON&quot; &quot;UPDATE&quot; fk_action &quot;ON&quot; &quot;DELETE&quot; fk_action | &quot;ON&quot; &quot;DELETE&quot; fk_action &quot;ON&quot; &quot;UPDATE&quot; fk_action ; fk_action: &quot;SET&quot; &quot;NULL&quot; | &quot;SET&quot; &quot;DEFAULT&quot; | &quot;CASCADE&quot; | &quot;RESTRICT&quot; | &quot;NO&quot; &quot;ACTION&quot; ; fk_deferred_options: &quot;DEFERRABLE&quot; fk_initial_state | &quot;NOT DEFERRABLE&quot; fk_initial_state ; fk_initial_state: /* nil */ | &quot;INITIALLY&quot; &quot;DEFERRED&quot; | &quot;INITIALLY&quot; &quot;IMMEDIATE&quot; ; fk_def: &quot;CONSTRAINT&quot; name &quot;FOREIGN&quot; &quot;KEY&quot; '(' name_list ')' fk_target_options | &quot;FOREIGN&quot; &quot;KEY&quot; '(' name_list ')' fk_target_options ; fk_target_options: &quot;REFERENCES&quot; name '(' name_list ')' opt_fk_options ; unq_def: &quot;CONSTRAINT&quot; name &quot;UNIQUE&quot; '(' indexed_columns ')' opt_conflict_clause | &quot;UNIQUE&quot; '(' indexed_columns ')' opt_conflict_clause ; opt_unique: /* nil */ | &quot;UNIQUE&quot; ; indexed_column: expr opt_asc_desc ; indexed_columns: indexed_column | indexed_column ',' indexed_columns ; create_index_stmt: &quot;CREATE&quot; opt_unique &quot;INDEX&quot; opt_if_not_exists name &quot;ON&quot; name '(' indexed_columns ')' opt_where opt_delete_version_attr ; name: &quot;ID&quot; | &quot;TEXT&quot; | &quot;TRIGGER&quot; | &quot;ROWID&quot; | &quot;REPLACE&quot; | &quot;KEY&quot; | &quot;VIRTUAL&quot; | &quot;TYPE&quot; | &quot;HIDDEN&quot; | &quot;PRIVATE&quot; | &quot;FIRST&quot; | &quot;LAST&quot; ; opt_name: /* nil */ | name ; name_list: name | name ',' name_list ; opt_name_list: /* nil */ | name_list ; cte_binding_list: cte_binding | cte_binding ',' cte_binding_list ; cte_binding: name name | name &quot;AS&quot; name ; col_attrs: /* nil */ | &quot;NOT&quot; &quot;NULL&quot; opt_conflict_clause col_attrs | &quot;PRIMARY&quot; &quot;KEY&quot; opt_conflict_clause col_attrs | &quot;PRIMARY&quot; &quot;KEY&quot; opt_conflict_clause &quot;AUTOINCREMENT&quot; col_attrs | &quot;DEFAULT&quot; '-' num_literal col_attrs | &quot;DEFAULT&quot; '+' num_literal col_attrs | &quot;DEFAULT&quot; num_literal col_attrs | &quot;DEFAULT&quot; const_expr col_attrs | &quot;DEFAULT&quot; str_literal col_attrs | &quot;COLLATE&quot; name col_attrs | &quot;CHECK&quot; '(' expr ')' col_attrs | &quot;UNIQUE&quot; opt_conflict_clause col_attrs | &quot;HIDDEN&quot; col_attrs | &quot;@SENSITIVE&quot; col_attrs | &quot;@CREATE&quot; version_annotation col_attrs | &quot;@DELETE&quot; version_annotation col_attrs | fk_target_options col_attrs ; version_annotation: '(' &quot;integer-literal&quot; ',' name ')' | '(' &quot;integer-literal&quot; ',' name ':' name ')' | '(' &quot;integer-literal&quot; ')' ; opt_kind: /* nil */ | '&lt;' name '&gt;' ; data_type_numeric: &quot;INT&quot; opt_kind | &quot;INTEGER&quot; opt_kind | &quot;REAL&quot; opt_kind | &quot;LONG&quot; opt_kind | &quot;BOOL&quot; opt_kind | &quot;LONG&quot; &quot;INTEGER&quot; opt_kind | &quot;LONG&quot; &quot;INT&quot; opt_kind | &quot;LONG_INT&quot; opt_kind | &quot;LONG_INTEGER&quot; opt_kind ; data_type_any: data_type_numeric | &quot;TEXT&quot; opt_kind | &quot;BLOB&quot; opt_kind | &quot;OBJECT&quot; opt_kind | &quot;OBJECT&quot; '&lt;' name &quot;CURSOR&quot; '&gt;' | &quot;OBJECT&quot; '&lt;' name &quot;SET&quot; '&gt;' | &quot;ID&quot; ; data_type_with_options: data_type_any | data_type_any &quot;NOT&quot; &quot;NULL&quot; | data_type_any &quot;@SENSITIVE&quot; | data_type_any &quot;@SENSITIVE&quot; &quot;NOT&quot; &quot;NULL&quot; | data_type_any &quot;NOT&quot; &quot;NULL&quot; &quot;@SENSITIVE&quot; ; str_literal: str_chain ; str_chain: str_leaf | str_leaf str_chain ; str_leaf: &quot;sql-string-literal&quot; | &quot;c-string-literal&quot; ; num_literal: &quot;integer-literal&quot; | &quot;long-literal&quot; | &quot;real-literal&quot; | &quot;TRUE&quot; | &quot;FALSE&quot; ; const_expr: &quot;CONST&quot; '(' expr ')' ; any_literal: str_literal | num_literal | &quot;NULL&quot; | &quot;@FILE&quot; '(' str_literal ')' | &quot;@PROC&quot; | &quot;sql-blob-literal&quot; ; raise_expr: &quot;RAISE&quot; '(' &quot;IGNORE&quot; ')' | &quot;RAISE&quot; '(' &quot;ROLLBACK&quot; ',' expr ')' | &quot;RAISE&quot; '(' &quot;ABORT&quot; ',' expr ')' | &quot;RAISE&quot; '(' &quot;FAIL&quot; ',' expr ')' ; call: name '(' arg_list ')' opt_filter_clause | name '(' &quot;DISTINCT&quot; arg_list ')' opt_filter_clause | basic_expr ':' name '(' arg_list ')' ; basic_expr: name | &quot;@RC&quot; | name '.' name | any_literal | const_expr | '(' expr ')' | call | window_func_inv | raise_expr | '(' select_stmt ')' | '(' select_stmt &quot;IF&quot; &quot;NOTHING&quot; expr ')' | '(' select_stmt &quot;IF&quot; &quot;NOTHING&quot; &quot;OR&quot; &quot;NULL&quot; expr ')' | '(' select_stmt &quot;IF&quot; &quot;NOTHING&quot; &quot;THROW&quot;')' | &quot;EXISTS&quot; '(' select_stmt ')' | &quot;CASE&quot; expr case_list &quot;END&quot; | &quot;CASE&quot; expr case_list &quot;ELSE&quot; expr &quot;END&quot; | &quot;CASE&quot; case_list &quot;END&quot; | &quot;CASE&quot; case_list &quot;ELSE&quot; expr &quot;END&quot; | &quot;CAST&quot; '(' expr &quot;AS&quot; data_type_any ')' | &quot;TYPE_CHECK&quot; '(' expr &quot;AS&quot; data_type_with_options ')' math_expr: basic_expr | math_expr '&amp;' math_expr | math_expr '|' math_expr | math_expr &quot;&lt;&lt;&quot; math_expr | math_expr &quot;&gt;&gt;&quot; math_expr | math_expr '+' math_expr | math_expr '-' math_expr | math_expr '*' math_expr | math_expr '/' math_expr | math_expr '%' math_expr | math_expr &quot;IS NOT TRUE&quot; | math_expr &quot;IS NOT FALSE&quot; | math_expr &quot;ISNULL&quot; | math_expr &quot;NOTNULL&quot; | math_expr &quot;IS TRUE&quot; | math_expr &quot;IS FALSE&quot; | '-' math_expr | '+' math_expr | '~' math_expr | &quot;NOT&quot; math_expr | math_expr '=' math_expr | math_expr &quot;==&quot; math_expr | math_expr '&lt;' math_expr | math_expr '&gt;' math_expr | math_expr &quot;&lt;&gt;&quot; math_expr | math_expr &quot;!=&quot; math_expr | math_expr &quot;&gt;=&quot; math_expr | math_expr &quot;&lt;=&quot; math_expr | math_expr &quot;NOT IN&quot; '(' expr_list ')' | math_expr &quot;NOT IN&quot; '(' select_stmt ')' | math_expr &quot;IN&quot; '(' expr_list ')' | math_expr &quot;IN&quot; '(' select_stmt ')' | math_expr &quot;LIKE&quot; math_expr | math_expr &quot;NOT LIKE&quot; math_expr | math_expr &quot;MATCH&quot; math_expr | math_expr &quot;NOT MATCH&quot; math_expr | math_expr &quot;REGEXP&quot; math_expr | math_expr &quot;NOT REGEXP&quot; math_expr | math_expr &quot;GLOB&quot; math_expr | math_expr &quot;NOT GLOB&quot; math_expr | math_expr &quot;BETWEEN&quot; math_expr &quot;AND&quot; math_expr | math_expr &quot;NOT BETWEEN&quot; math_expr &quot;AND&quot; math_expr | math_expr &quot;IS NOT&quot; math_expr | math_expr &quot;IS&quot; math_expr | math_expr &quot;||&quot; math_expr | math_expr &quot;COLLATE&quot; name ; expr: math_expr | expr &quot;AND&quot; expr | expr &quot;OR&quot; expr ; case_list: &quot;WHEN&quot; expr &quot;THEN&quot; expr | &quot;WHEN&quot; expr &quot;THEN&quot; expr case_list ; arg_expr: '*' | expr | shape_arguments ; arg_list: /* nil */ | arg_expr | arg_expr ',' arg_list ; expr_list: expr | expr ',' expr_list ; shape_arguments: &quot;FROM&quot; name | &quot;FROM&quot; name shape_def | &quot;FROM&quot; &quot;ARGUMENTS&quot; | &quot;FROM&quot; &quot;ARGUMENTS&quot; shape_def ; column_calculation: &quot;COLUMNS&quot; '(' col_calcs ')' | &quot;COLUMNS&quot; '(' &quot;DISTINCT&quot; col_calcs ')' ; col_calcs: col_calc | col_calc ',' col_calcs ; col_calc: name | shape_def | name shape_def | name '.' name ; call_expr: expr | shape_arguments ; call_expr_list: call_expr | call_expr ',' call_expr_list ; cte_tables: cte_table | cte_table ',' cte_tables ; cte_decl: name '(' name_list ')' | name '(' '*' ')' ; shared_cte: call_stmt | call_stmt &quot;USING&quot; cte_binding_list ; cte_table: cte_decl &quot;AS&quot; '(' select_stmt ')' | cte_decl &quot;AS&quot; '(' shared_cte')' | '(' call_stmt ')' | '(' call_stmt &quot;USING&quot; cte_binding_list ')' | cte_decl &quot;LIKE&quot; '(' select_stmt ')' | cte_decl &quot;LIKE&quot; name ; with_prefix: &quot;WITH&quot; cte_tables | &quot;WITH&quot; &quot;RECURSIVE&quot; cte_tables ; with_select_stmt: with_prefix select_stmt_no_with ; select_nothing_stmt: &quot;SELECT&quot; &quot;NOTHING&quot; ; select_stmt: with_select_stmt | select_stmt_no_with ; select_stmt_no_with: select_core_list opt_orderby opt_limit opt_offset ; select_core_list: select_core | select_core compound_operator select_core_list ; values: '(' insert_list ')' | '(' insert_list ')' ',' values ; select_core: &quot;SELECT&quot; select_opts select_expr_list opt_from_query_parts opt_where opt_groupby opt_having opt_select_window | &quot;VALUES&quot; values ; compound_operator: &quot;UNION&quot; | &quot;UNION ALL&quot; | &quot;INTERSECT&quot; | &quot;EXCEPT&quot; ; window_func_inv: name '(' arg_list ')' opt_filter_clause &quot;OVER&quot; window_name_or_defn ; opt_filter_clause: /* nil */ | &quot;FILTER&quot; '(' opt_where ')' ; window_name_or_defn: window_defn | name ; window_defn: '(' opt_partition_by opt_orderby opt_frame_spec ')' ; opt_frame_spec: /* nil */ | frame_type frame_boundary_opts frame_exclude ; frame_type: &quot;RANGE&quot; | &quot;ROWS&quot; | &quot;GROUPS&quot; ; frame_exclude: /* nil */ | &quot;EXCLUDE NO OTHERS&quot; | &quot;EXCLUDE CURRENT ROW&quot; | &quot;EXCLUDE GROUP&quot; | &quot;EXCLUDE TIES&quot; ; frame_boundary_opts: frame_boundary | &quot;BETWEEN&quot; frame_boundary_start &quot;AND&quot; frame_boundary_end ; frame_boundary_start: &quot;UNBOUNDED&quot; &quot;PRECEDING&quot; | expr &quot;PRECEDING&quot; | &quot;CURRENT ROW&quot; | expr &quot;FOLLOWING&quot; ; frame_boundary_end: expr &quot;PRECEDING&quot; | &quot;CURRENT ROW&quot; | expr &quot;FOLLOWING&quot; | &quot;UNBOUNDED&quot; &quot;FOLLOWING&quot; ; frame_boundary: &quot;UNBOUNDED&quot; &quot;PRECEDING&quot; | expr &quot;PRECEDING&quot; | &quot;CURRENT ROW&quot; ; opt_partition_by: /* nil */ | &quot;PARTITION&quot; &quot;BY&quot; expr_list ; opt_select_window: /* nil */ | window_clause ; window_clause: &quot;WINDOW&quot; window_name_defn_list ; window_name_defn_list: window_name_defn | window_name_defn ',' window_name_defn_list ; window_name_defn: name &quot;AS&quot; window_defn ; region_spec: name | name &quot;PRIVATE&quot; ; region_list: region_spec ',' region_list | region_spec ; declare_schema_region_stmt: &quot;@DECLARE_SCHEMA_REGION&quot; name | &quot;@DECLARE_SCHEMA_REGION&quot; name &quot;USING&quot; region_list ; declare_deployable_region_stmt: &quot;@DECLARE_DEPLOYABLE_REGION&quot; name | &quot;@DECLARE_DEPLOYABLE_REGION&quot; name &quot;USING&quot; region_list ; begin_schema_region_stmt: &quot;@BEGIN_SCHEMA_REGION&quot; name ; end_schema_region_stmt: &quot;@END_SCHEMA_REGION&quot; ; schema_unsub_stmt: &quot;@UNSUB&quot; '(' name ')' ; schema_ad_hoc_migration_stmt: &quot;@SCHEMA_AD_HOC_MIGRATION&quot; version_annotation | &quot;@SCHEMA_AD_HOC_MIGRATION&quot; &quot;FOR&quot; &quot;@RECREATE&quot; '(' name ',' name ')' ; emit_enums_stmt: &quot;@EMIT_ENUMS&quot; opt_name_list ; emit_group_stmt: &quot;@EMIT_GROUP&quot; opt_name_list ; emit_constants_stmt: &quot;@EMIT_CONSTANTS&quot; name_list ; opt_from_query_parts: /* nil */ | &quot;FROM&quot; query_parts ; opt_where: /* nil */ | &quot;WHERE&quot; expr ; opt_groupby: /* nil */ | &quot;GROUP&quot; &quot;BY&quot; groupby_list ; groupby_list: groupby_item | groupby_item ',' groupby_list ; groupby_item: expr ; opt_asc_desc: /* nil */ | &quot;ASC&quot; opt_nullsfirst_nullslast | &quot;DESC&quot; opt_nullsfirst_nullslast ; opt_nullsfirst_nullslast: /* nil */ | &quot;NULLS&quot; &quot;FIRST&quot; | &quot;NULLS&quot; &quot;LAST&quot; ; opt_having: /* nil */ | &quot;HAVING&quot; expr ; opt_orderby: /* nil */ | &quot;ORDER&quot; &quot;BY&quot; orderby_list ; orderby_list: orderby_item | orderby_item ',' orderby_list ; orderby_item: expr opt_asc_desc ; opt_limit: /* nil */ | &quot;LIMIT&quot; expr ; opt_offset: /* nil */ | &quot;OFFSET&quot; expr ; select_opts: /* nil */ | &quot;ALL&quot; | &quot;DISTINCT&quot; | &quot;DISTINCTROW&quot; ; select_expr_list: select_expr | select_expr ',' select_expr_list | '*' ; select_expr: expr opt_as_alias | name '.' '*' | column_calculation ; opt_as_alias: /* nil */ | as_alias ; as_alias: &quot;AS&quot; name | name ; query_parts: table_or_subquery_list | join_clause ; table_or_subquery_list: table_or_subquery | table_or_subquery ',' table_or_subquery_list ; join_clause: table_or_subquery join_target_list ; join_target_list: join_target | join_target join_target_list ; table_or_subquery: name opt_as_alias | '(' select_stmt ')' opt_as_alias | '(' shared_cte ')' opt_as_alias | table_function opt_as_alias | '(' query_parts ')' ; join_type: /*nil */ | &quot;LEFT&quot; | &quot;RIGHT&quot; | &quot;LEFT&quot; &quot;OUTER&quot; | &quot;RIGHT&quot; &quot;OUTER&quot; | &quot;INNER&quot; | &quot;CROSS&quot; ; join_target: join_type &quot;JOIN&quot; table_or_subquery opt_join_cond ; opt_join_cond: /* nil */ | join_cond ; join_cond: &quot;ON&quot; expr | &quot;USING&quot; '(' name_list ')' ; table_function: name '(' arg_list ')' ; create_view_stmt: &quot;CREATE&quot; opt_temp &quot;VIEW&quot; opt_if_not_exists name &quot;AS&quot; select_stmt opt_delete_version_attr ; with_delete_stmt: with_prefix delete_stmt ; delete_stmt: &quot;DELETE&quot; &quot;FROM&quot; name opt_where ; opt_insert_dummy_spec: /*nil*/ | &quot;@DUMMY_SEED&quot; '(' expr ')' dummy_modifier ; dummy_modifier: /* nil */ | &quot;@DUMMY_NULLABLES&quot; | &quot;@DUMMY_DEFAULTS&quot; | &quot;@DUMMY_NULLABLES&quot; &quot;@DUMMY_DEFAULTS&quot; | &quot;@DUMMY_DEFAULTS&quot; &quot;@DUMMY_NULLABLES&quot; ; insert_stmt_type: &quot;INSERT&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;REPLACE&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;IGNORE&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;ROLLBACK&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;ABORT&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;FAIL&quot; &quot;INTO&quot; | &quot;REPLACE&quot; &quot;INTO&quot; ; with_insert_stmt: with_prefix insert_stmt ; opt_column_spec: /* nil */ | '(' opt_name_list ')' | '(' shape_def ')' ; from_shape: &quot;FROM&quot; &quot;CURSOR&quot; name opt_column_spec | &quot;FROM&quot; name opt_column_spec | &quot;FROM&quot; &quot;ARGUMENTS&quot; opt_column_spec ; insert_stmt: insert_stmt_type name opt_column_spec select_stmt opt_insert_dummy_spec | insert_stmt_type name opt_column_spec from_shape opt_insert_dummy_spec | insert_stmt_type name &quot;DEFAULT&quot; &quot;VALUES&quot; | insert_stmt_type name &quot;USING&quot; select_stmt | insert_stmt_type name &quot;USING&quot; expr_names opt_insert_dummy_spec ; insert_list_item: expr | shape_arguments ; insert_list: /* nil */ | insert_list_item | insert_list_item ',' insert_list ; basic_update_stmt: &quot;UPDATE&quot; opt_name &quot;SET&quot; update_list opt_from_query_parts opt_where ; with_update_stmt: with_prefix update_stmt ; update_stmt: &quot;UPDATE&quot; name &quot;SET&quot; update_list opt_from_query_parts opt_where opt_orderby opt_limit ; update_entry: name '=' expr ; update_list: update_entry | update_entry ',' update_list ; with_upsert_stmt: with_prefix upsert_stmt ; upsert_stmt: insert_stmt &quot;ON CONFLICT&quot; conflict_target &quot;DO&quot; &quot;NOTHING&quot; | insert_stmt &quot;ON CONFLICT&quot; conflict_target &quot;DO&quot; basic_update_stmt ; update_cursor_stmt: &quot;UPDATE&quot; &quot;CURSOR&quot; name opt_column_spec &quot;FROM&quot; &quot;VALUES&quot; '(' insert_list ')' | &quot;UPDATE&quot; &quot;CURSOR&quot; name opt_column_spec from_shape | &quot;UPDATE&quot; &quot;CURSOR&quot; name &quot;USING&quot; expr_names ; conflict_target: /* nil */ | '(' indexed_columns ')' opt_where ; function: &quot;FUNC&quot; | &quot;FUNCTION&quot; ; declare_out_call_stmt: &quot;DECLARE&quot; &quot;OUT&quot; call_stmt ; declare_enum_stmt: &quot;DECLARE&quot; &quot;ENUM&quot; name data_type_numeric '(' enum_values ')' ; enum_values: enum_value | enum_value ',' enum_values ; enum_value: name | name '=' expr ; declare_const_stmt: &quot;DECLARE&quot; &quot;CONST&quot; &quot;GROUP&quot; name '(' const_values ')' ; declare_group_stmt: &quot;DECLARE&quot; &quot;GROUP&quot; name &quot;BEGIN&quot; simple_variable_decls &quot;END&quot; ; simple_variable_decls: declare_vars_stmt ';' | declare_vars_stmt ';' simple_variable_decls ; const_values: const_value | const_value ',' const_values ; const_value: name '=' expr ; declare_select_func_no_check_stmt: &quot;DECLARE&quot; &quot;SELECT&quot; function name &quot;NO&quot; &quot;CHECK&quot; data_type_with_options | &quot;DECLARE&quot; &quot;SELECT&quot; function name &quot;NO&quot; &quot;CHECK&quot; '(' typed_names ')' ; declare_func_stmt: &quot;DECLARE&quot; function name '(' func_params ')' data_type_with_options | &quot;DECLARE&quot; &quot;SELECT&quot; function name '(' params ')' data_type_with_options | &quot;DECLARE&quot; function name '(' func_params ')' &quot;CREATE&quot; data_type_with_options | &quot;DECLARE&quot; &quot;SELECT&quot; function name '(' params ')' '(' typed_names ')' ; procedure: &quot;PROC&quot; | &quot;PROCEDURE&quot; ; declare_proc_no_check_stmt: &quot;DECLARE&quot; procedure name &quot;NO&quot; &quot;CHECK&quot; ; declare_proc_stmt: &quot;DECLARE&quot; procedure name '(' params ')' | &quot;DECLARE&quot; procedure name '(' params ')' '(' typed_names ')' | &quot;DECLARE&quot; procedure name '(' params ')' &quot;USING&quot; &quot;TRANSACTION&quot; | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; '(' typed_names ')' | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; '(' typed_names ')' &quot;USING&quot; &quot;TRANSACTION&quot; | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; &quot;UNION&quot; '(' typed_names ')' | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; &quot;UNION&quot; '(' typed_names ')' &quot;USING&quot; &quot;TRANSACTION&quot; ; declare_interface_stmt: &quot;DECLARE&quot; &quot;INTERFACE&quot; name '(' typed_names ')' | &quot;INTERFACE&quot; name '(' typed_names ')' ; create_proc_stmt: &quot;CREATE&quot; procedure name '(' params ')' &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; inout: &quot;IN&quot; | &quot;OUT&quot; | &quot;INOUT&quot; ; typed_name: name data_type_with_options | shape_def | name shape_def ; typed_names: typed_name | typed_name ',' typed_names ; func_param: param | name &quot;CURSOR&quot; ; func_params: /* nil */ | func_param | func_param ',' func_params ; param: name data_type_with_options | inout name data_type_with_options | shape_def | name shape_def ; params: /* nil */ | param | param ',' params ; declare_value_cursor: &quot;DECLARE&quot; name &quot;CURSOR&quot; shape_def | &quot;CURSOR&quot; name shape_def | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;LIKE&quot; select_stmt | &quot;CURSOR&quot; name &quot;LIKE&quot; select_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;LIKE&quot; '(' typed_names ')' | &quot;CURSOR&quot; name &quot;LIKE&quot; '(' typed_names ')' ; declare_forward_read_cursor_stmt: &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; select_stmt | &quot;CURSOR&quot; name &quot;FOR&quot; select_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; explain_stmt | &quot;CURSOR&quot; name &quot;FOR&quot; explain_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; call_stmt | &quot;CURSOR&quot; name &quot;FOR&quot; call_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; expr | &quot;CURSOR&quot; name &quot;FOR&quot; expr ; declare_fetched_value_cursor_stmt: &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FETCH&quot; &quot;FROM&quot; call_stmt | &quot;CURSOR&quot; name &quot;FETCH&quot; &quot;FROM&quot; call_stmt ; declare_type_stmt: &quot;DECLARE&quot; name &quot;TYPE&quot; data_type_with_options | &quot;TYPE&quot; name data_type_with_options ; declare_vars_stmt: &quot;DECLARE&quot; name_list data_type_with_options | &quot;VAR&quot; name_list data_type_with_options | declare_value_cursor ; call_stmt: &quot;CALL&quot; name '(' ')' | &quot;CALL&quot; name '(' call_expr_list ')' | &quot;CALL&quot; name '(' '*' ')' ; while_stmt: &quot;WHILE&quot; expr &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; switch_stmt: &quot;SWITCH&quot; expr switch_case switch_cases | &quot;SWITCH&quot; expr &quot;ALL&quot; &quot;VALUES&quot; switch_case switch_cases ; switch_case: &quot;WHEN&quot; expr_list &quot;THEN&quot; stmt_list | &quot;WHEN&quot; expr_list &quot;THEN&quot; &quot;NOTHING&quot; ; switch_cases: switch_case switch_cases | &quot;ELSE&quot; stmt_list &quot;END&quot; | &quot;END&quot; ; loop_stmt: &quot;LOOP&quot; fetch_stmt &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; leave_stmt: &quot;LEAVE&quot; ; return_stmt: &quot;RETURN&quot; ; rollback_return_stmt: &quot;ROLLBACK&quot; &quot;RETURN&quot; ; commit_return_stmt: &quot;COMMIT&quot; &quot;RETURN&quot; ; throw_stmt: &quot;THROW&quot; ; trycatch_stmt: &quot;BEGIN&quot; &quot;TRY&quot; opt_stmt_list &quot;END&quot; &quot;TRY&quot; ';' &quot;BEGIN&quot; &quot;CATCH&quot; opt_stmt_list &quot;END&quot; &quot;CATCH&quot; ; continue_stmt: &quot;CONTINUE&quot; ; fetch_stmt: &quot;FETCH&quot; name &quot;INTO&quot; name_list | &quot;FETCH&quot; name ; fetch_cursor_from_blob_stmt: &quot;FETCH&quot; name &quot;FROM BLOB&quot; expr ; fetch_values_stmt: &quot;FETCH&quot; name opt_column_spec &quot;FROM&quot; &quot;VALUES&quot; '(' insert_list ')' opt_insert_dummy_spec | &quot;FETCH&quot; name opt_column_spec from_shape opt_insert_dummy_spec | &quot;FETCH&quot; name &quot;USING&quot; expr_names opt_insert_dummy_spec ; expr_names: expr_name | expr_name ',' expr_names ; expr_name: expr as_alias ; fetch_call_stmt: &quot;FETCH&quot; name opt_column_spec &quot;FROM&quot; call_stmt ; close_stmt: &quot;CLOSE&quot; name ; out_stmt: &quot;OUT&quot; name ; out_union_stmt: &quot;OUT&quot; &quot;UNION&quot; name ; out_union_parent_child_stmt: &quot;OUT&quot; &quot;UNION&quot; call_stmt &quot;JOIN&quot; child_results ; child_results: child_result | child_result &quot;AND&quot; child_results ; child_result: call_stmt &quot;USING&quot; '(' name_list ')' | call_stmt &quot;USING&quot; '(' name_list ')' &quot;AS&quot; name ; if_stmt: &quot;IF&quot; expr &quot;THEN&quot; opt_stmt_list opt_elseif_list opt_else &quot;END&quot; &quot;IF&quot; ; opt_else: /* nil */ | &quot;ELSE&quot; opt_stmt_list ; elseif_item: &quot;ELSE IF&quot; expr &quot;THEN&quot; opt_stmt_list ; elseif_list: elseif_item | elseif_item elseif_list ; opt_elseif_list: /* nil */ | elseif_list ; control_stmt: commit_return_stmt | continue_stmt | leave_stmt | return_stmt | rollback_return_stmt | throw_stmt guard_stmt: &quot;IF&quot; expr control_stmt ; transaction_mode: /* nil */ | &quot;DEFERRED&quot; | &quot;IMMEDIATE&quot; | &quot;EXCLUSIVE&quot; ; begin_trans_stmt: &quot;BEGIN&quot; transaction_mode &quot;TRANSACTION&quot; | &quot;BEGIN&quot; transaction_mode ; rollback_trans_stmt: &quot;ROLLBACK&quot; | &quot;ROLLBACK&quot; &quot;TRANSACTION&quot; | &quot;ROLLBACK&quot; &quot;TO&quot; savepoint_name | &quot;ROLLBACK&quot; &quot;TRANSACTION&quot; &quot;TO&quot; savepoint_name | &quot;ROLLBACK&quot; &quot;TO&quot; &quot;SAVEPOINT&quot; savepoint_name | &quot;ROLLBACK&quot; &quot;TRANSACTION&quot; &quot;TO&quot; &quot;SAVEPOINT&quot; savepoint_name ; commit_trans_stmt: &quot;COMMIT&quot; &quot;TRANSACTION&quot; | &quot;COMMIT&quot; ; proc_savepoint_stmt: procedure &quot;SAVEPOINT&quot; &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; savepoint_name: &quot;@PROC&quot; | name ; savepoint_stmt: &quot;SAVEPOINT&quot; savepoint_name ; release_savepoint_stmt: &quot;RELEASE&quot; savepoint_name | &quot;RELEASE&quot; &quot;SAVEPOINT&quot; savepoint_name ; echo_stmt: &quot;@ECHO&quot; name ',' str_literal ; alter_table_add_column_stmt: &quot;ALTER&quot; &quot;TABLE&quot; name &quot;ADD&quot; &quot;COLUMN&quot; col_def ; create_trigger_stmt: &quot;CREATE&quot; opt_temp &quot;TRIGGER&quot; opt_if_not_exists trigger_def opt_delete_version_attr ; trigger_def: name trigger_condition trigger_operation &quot;ON&quot; name trigger_action ; trigger_condition: /* nil */ | &quot;BEFORE&quot; | &quot;AFTER&quot; | &quot;INSTEAD&quot; &quot;OF&quot; ; trigger_operation: &quot;DELETE&quot; | &quot;INSERT&quot; | &quot;UPDATE&quot; opt_of ; opt_of: /* nil */ | &quot;OF&quot; name_list ; trigger_action: opt_foreachrow opt_when_expr &quot;BEGIN&quot; trigger_stmts &quot;END&quot; ; opt_foreachrow: /* nil */ | &quot;FOR EACH ROW&quot; ; opt_when_expr: /* nil */ | &quot;WHEN&quot; expr ; trigger_stmts: trigger_stmt | trigger_stmt trigger_stmts ; trigger_stmt: trigger_update_stmt ';' | trigger_insert_stmt ';' | trigger_delete_stmt ';' | trigger_select_stmt ';' ; trigger_select_stmt: select_stmt_no_with ; trigger_insert_stmt: insert_stmt ; trigger_delete_stmt: delete_stmt ; trigger_update_stmt: basic_update_stmt ; enforcement_options: &quot;FOREIGN&quot; &quot;KEY&quot; &quot;ON&quot; &quot;UPDATE&quot; | &quot;FOREIGN&quot; &quot;KEY&quot; &quot;ON&quot; &quot;DELETE&quot; | &quot;JOIN&quot; | &quot;UPSERT&quot; &quot;STATEMENT&quot; | &quot;WINDOW&quot; function | &quot;WITHOUT&quot; &quot;ROWID&quot; | &quot;TRANSACTION&quot; | &quot;SELECT&quot; &quot;IF&quot; &quot;NOTHING&quot; | &quot;INSERT&quot; &quot;SELECT&quot; | &quot;TABLE&quot; &quot;FUNCTION&quot; | &quot;ENCODE&quot; &quot;CONTEXT COLUMN&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;INTEGER&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;LONG_INTEGER&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;REAL&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;BOOL&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;TEXT&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;BLOB&quot; | &quot;IS TRUE&quot; | &quot;CAST&quot; | &quot;SIGN FUNCTION&quot; | &quot;CURSOR HAS ROW&quot; | &quot;UPDATE&quot; &quot;FROM&quot; ; enforce_strict_stmt: &quot;@ENFORCE_STRICT&quot; enforcement_options ; enforce_normal_stmt: &quot;@ENFORCE_NORMAL&quot; enforcement_options ; enforce_reset_stmt: &quot;@ENFORCE_RESET&quot; ; enforce_push_stmt: &quot;@ENFORCE_PUSH&quot; ; enforce_pop_stmt: &quot;@ENFORCE_POP&quot; ; opt_use_offset: /* nil */ | &quot;OFFSET&quot; ; blob_get_key_type_stmt: &quot;@BLOB_GET_KEY_TYPE&quot; name ; blob_get_val_type_stmt: &quot;@BLOB_GET_VAL_TYPE&quot; name ; blob_get_key_stmt: &quot;@BLOB_GET_KEY&quot; name opt_use_offset ; blob_get_val_stmt: &quot;@BLOB_GET_VAL&quot; name opt_use_offset ; blob_create_key_stmt: &quot;@BLOB_CREATE_KEY&quot; name opt_use_offset ; blob_create_val_stmt: &quot;@BLOB_CREATE_VAL&quot; name opt_use_offset ; blob_update_key_stmt: &quot;@BLOB_UPDATE_KEY&quot; name opt_use_offset ; blob_update_val_stmt: &quot;@BLOB_UPDATE_VAL&quot; name opt_use_offset ;  "},{"title":"Appendix 3: Control Directives​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-3-control-directives","content":"The control directives are those statements that begin with @ and they are distinguished from other statements because they influence the compiler rather than the program logic. Some of these are of great importance and discussed elsewhere. The complete list (as of this writing) is: @ENFORCE_STRICT@ENFORCE_NORMAL These enable or disable more strict semanic checking the sub options are FOREIGN KEY ON UPDATE: all FK's must choose some ON UPDATE strategyFOREIGN KEY ON DELETE: all FK's must choose some ON DELETE strategyPROCEDURE: all procedures must be declared before they are called (eliminating the vanilla C call option)JOIN: all joins must be ANSI style, the form FROM A,B is not allowed (replace with A INNER JOIN BWINDOW FUNC: window functions are disallowed (useful if targeting old versions of SQLite)UPSERT STATEMENT: the upsert form is disallowed (useful if targeting old versions of SQLite) @SENSITIVE marks a column or variable as 'sensitive' for privacy purposes, this behaves somewhat like nullability (See Chapter 3) in that it is radioactive, contaminating anything it touchesthe intent of this annotation is to make it clear where sensitive data is being returned or consumed in your proceduresthis information appears in the JSON output for further codegen or for analysis (See Chapter 13) @DECLARE_SCHEMA_REGION@DECLARE_DEPLOYABLE_REGION@BEGIN_SCHEMA_REGION@END_SCHEMA_REGION These directives control the declaration of schema regions and allow you to place things into those regions -- see Chapter 10 @SCHEMA_AD_HOC_MIGRATION Allows for the creation of a ad hoc migration step at a given schema version, (See Chapter 10) @ECHO Emits text into the C output stream, useful for emiting things like function prototypes or preprocessor directivese.g. `echo C, '#define foo bar' @RECREATE@CREATE@DELETE used to mark the schema version where an object is created or deleted, or alternatively indicate the the object is always dropped and recreated when it changes (See Chapter 10) @SCHEMA_UPGRADE_VERSION used to indicate that the code that follows is part of a migration script for the indicated schema versionthis has the effect of making the schema appear to be how it existed at the indicated versionthe idea here is that migration procedures operate on previous versions of the schema where (e.g.) some columns/tables hadn't been deleted yet @PREVIOUS_SCHEMA indicates the start of the previous version of the schema for comparison (See Chapter 11) @SCHEMA_UPGRADE_SCRIPT CQL emits a schema upgrade script as part of its upgrade features, this script declares tables in their final form but also creates the same tables as they existed when they were first createdthis directive instructs CQL to ignore the incompatible creations, the first declaration controlsthe idea here is that the upgrade script is in the business of getting you to the finish line in an orderly fashion and some of the interim steps are just not all the way there yetnote that the upgrade script recapitulates the version history, it does not take you directly to the finish line, this is so that all instances get to the same place the same way (and this fleshes out any bugs in migration) @DUMMY_NULLABLES@DUMMY_DEFAULTS@DUMMY_SEED these control the creation of dummy data for insert and fetch statements (See Chapters 5 and 12) @FILE a string literal that corresponds to the current file name with a prefix stripped (to remove build lab junk in the path) @ATTRIBUTE the main purpose of @attribute is to appear in the JSON output so that it can control later codegen stages in whatever way you deem appropriate the nested nature of attribute values is sufficiently flexible than you could encode an arbitrary LISP program in an attribute, so really anything you might need to express is possible there are a number of attributes known to the compiler which I list below (complete as of this writing) cql:autodrop=(table1, table2, ...) when present the indicated tables, which must be temp tables, are dropped when the results of the procedure have been fetched into a rowset cql:identity=(column1, column2, ...) the indicated columns are used to create a row comparator for the rowset corresponding to the procedure, this appears in a C macro of the form procedure_name_row_same(rowset1, row1, rowset2, row2) cql:suppress_getters the annotated procedure should not emit its related column getter functions. Useful if you only indend to call the procedure from CQL.Saves code generation and removes the possibility of C code using the getters. cql:suppress_result_set the annotated procedure should not emit its related &quot;fetch results&quot; function. Useful if you only indend to call the procedure from CQL.Saves code generation and removes the possibility of C code using the result set or getters.Implies cql:suppress_getters; since there is no result set, getters would be redundant.Note: an OUT UNION procedure cannot have a suppressed result set since all such a procedure does is produce a result set. This attribute is ignored for out union procedures. cql:private the annotated procedure will be static in the generated C Because the generated function is static it cannot be called from other modules and therefore will not go in any CQL exports file (that would be moot since you couldn't call it).This attribute also implies cql:suppress_result_set since only CQL code in the same translation unit could possibly call it and hence the result set procedure is useless to other C code. cql:generate_copy the code generation for the annotated procedure will produce a [procedure_name]_copy function that can make complete or partial copies of its result set. cql:base_fragment=frag_name used for base fragments (See Chapter 14) cql:extension_fragment=frag_name used for extension fragments (See Chapter 14) cql:assembly_fragment=frag_name used for assembly fragments (See Chapter 14) cql:shared_fragment is used to create shared fragments (See Chapter 14) cql:no_table_scan for query plan processing, indicates that the table in question should never be table scanned in any plan (for better diagnostics) cql:autotest=([many forms]) declares various autotest features (See Chapter 12) cql:query_plan_branch=[integer] is used by the query plan generator to determine which conditional branch to use in query plan analysis when a shared fragment that contains an IF statement is used. (See Chapter 15) cql:alias_of=[c_function_name] are used on function declarations to declare a function or procedure in CQL that calls a function of a different name. This is intended to used for aliasing native (C) functions. Both the aliased function name and the original function name may be declared in CQL at the same time. Note that the compiler does not enforce any consistency in typing between the original and aliased functions. "},{"title":"Appendix 4: CQL Error Codes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-4-cql-error-codes","content":""},{"title":"CQL0001: operands must be an integer type, not real​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0001-operands-must-be-an-integer-type-not-real","content":"integer math operators like &lt;&lt; &gt;&gt; &amp; and | are not compatible with real-valued arguments  "},{"title":"CQL0002: left operand cannot be an object in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0002-left-operand-cannot-be-an-object-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on objects. Basically comparison is all you can do.  "},{"title":"CQL0003: left operand cannot be an object in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0003-left-operand-cannot-be-an-object-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on objects. Basically comparison is all you can do.  "},{"title":"CQL0004: left operand cannot be a blob in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0004-left-operand-cannot-be-a-blob-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on blobs. Basically comparison is all you can do.  "},{"title":"CQL0005: right operand cannot be a blob in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0005-right-operand-cannot-be-a-blob-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on blobs. Basically comparison is all you can do.  "},{"title":"CQL0007: left operand cannot be a string in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0007-left-operand-cannot-be-a-string-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on strings. Basically comparison is all you can do.  "},{"title":"CQL0008: right operand cannot be a string in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0008-right-operand-cannot-be-a-string-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on strings. Basically comparison is all you can do.  "},{"title":"CQL0009: incompatible types in expression 'subject'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0009-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required a TEXT as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0010: incompatible types in expression 'subject'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0010-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required an OBJECT as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0011: incompatible types in expression 'subject'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0011-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required a BLOB as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0012: incompatible types in expression 'subject'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0012-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required a numeric as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0013: cannot assign/copy possibly null expression to not null target 'target'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0013-cannot-assigncopy-possibly-null-expression-to-not-null-target-target","content":"Here assign/copy can be the simplest case of assigning to a local variable or an OUT parameter but this error also appears when calling functions. You should think of the IN arguments as requiring that the actual argument be assignable to the formal variable and OUT arguments requiring that the formal be assignable to the actual argument variable.  "},{"title":"CQL0014: cannot assign/copy sensitive expression to not null target 'target'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0014-cannot-assigncopy-sensitive-expression-to-not-null-target-target","content":"Here assign/copy can be the simplest case of assigning to a local variable or an OUT parameter but this error also appears when calling functions. You should think of the IN arguments as requiring that the actual argument be assignable to the formal variable and OUT arguments requiring that the formal be assignable to the actual argument variable.  "},{"title":"CQL0015: expected numeric expression 'context'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0015-expected-numeric-expression-context","content":"Many SQL clauses require a numeric expression such as WHERE/HAVING/LIMIT/OFFSET. This expression indicates the expression in the given context is not a numeric.  "},{"title":"CQL0016: duplicate table name in join 'table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0016-duplicate-table-name-in-join-table","content":"When this error is produced it means the result of the join would have the same table twice with no disambiguation between the two places. The conflicting name is provided. To fix this, make an alias both tables. e.g. SELECT T1.id AS parent_id, T2.id AS child_id FROM foo AS T1 INNER JOIN foo AS T2 ON T1.id = T2.parent_id;   "},{"title":"CQL0017: index was present but now it does not exist (use @delete instead) 'index'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0017-index-was-present-but-now-it-does-not-exist-use-delete-instead-index","content":"The named index is in the previous schema bit it is not in the current schema. All entities need some kind of tombstone in the schema so that they can be correctly deleted if they are still present.  "},{"title":"CQL0018: duplicate index name 'index'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0018-duplicate-index-name-index","content":"An index with the indicated name already exists.  "},{"title":"CQL0019: create index table name not found 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0019-create-index-table-name-not-found-table_name","content":"The table part of a CREATE INDEX statement was not a valid table name.  "},{"title":"CQL0020: duplicate constraint name in table 'constraint_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0020-duplicate-constraint-name-in-table-constraint_name","content":"A table contains two constraints with the same name.  "},{"title":"CQL0021: foreign key refers to non-existent table 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0021-foreign-key-refers-to-non-existent-table-table_name","content":"The table in a foreign key REFERENCES clause is not a valid table.  "},{"title":"CQL0022: exact type of both sides of a foreign key must match (expected expected_type; found actual_type) 'key_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0022-exact-type-of-both-sides-of-a-foreign-key-must-match-expected-expected_type-found-actual_type-key_name","content":"The indicated foreign key has at least one column with a different type than corresponding column in the table it references. This usually means that you have picked the wrong table or column in the foreign key declaration.  "},{"title":"CQL0023: number of columns on both sides of a foreign key must match​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0023-number-of-columns-on-both-sides-of-a-foreign-key-must-match","content":"The number of column in the foreign key must be the same as the number of columns specified in the foreign table. This usually means a column is missing in the REFERENCES part of the declaration.  CQL0024: no longer in use  "},{"title":"CQL0025: version number in annotation must be positive​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0025-version-number-in-annotation-must-be-positive","content":"In an @create or @delete annotation, the version number must be &gt; 0. This error usually means there is a typo in the version number.  "},{"title":"CQL0026: duplicate version annotation​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0026-duplicate-version-annotation","content":"There can only be one @create, @delete, or @recreate annotation for any given table/column. More than one @create is redundant. This error usually means the @create was cut/paste to make an @delete and then not edited or something like that.  "},{"title":"CQL0027: a procedure can appear in only one annotation 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0027-a-procedure-can-appear-in-only-one-annotation-procedure_name","content":"The indicated migration procedure e.g. the foo in @create(5, foo) appears in another annotation. Migration steps should happen exactly once. This probably means the annotation was cut/paste and the migration proc was not removed.  "},{"title":"CQL0028: FK reference must be exactly one column with the correct type 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0028-fk-reference-must-be-exactly-one-column-with-the-correct-type-column_name","content":"When a foreign key is specified in the column definition it is the entire foreign key. That means the references part of the declaration can only be for that one column. If you need more columns, you have to declare the foreign key independently.  "},{"title":"CQL0029: autoincrement column must be [LONG_]INTEGER PRIMARY KEY 'column name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0029-autoincrement-column-must-be-long_integer-primary-key-column-name","content":"SQLite is very fussy about autoincrement columns. The column in question must be either a LONG INTEGER or an INTEGER and it must be PRIMARY KEY. In fact, CQL will rewrite LONG INTEGER into INTEGER because only that exact form is supported, but SQLite INTEGERs can hold LONG values so that's ok. Any other autoincrement form results in this error.  "},{"title":"CQL0030: a column attribute was specified twice on the same column 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0030-a-column-attribute-was-specified-twice-on-the-same-column-column_name","content":"This error indicates a pattern like &quot;id text not null not null&quot; was found. The same attribute shouldn't appear twice.  "},{"title":"CQL0031: column can't be primary key and also unique key 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0031-column-cant-be-primary-key-and-also-unique-key-column","content":"In a column definition, the column can only be marked with at most one of PRIMARY KEY or UNIQUE  "},{"title":"CQL0032: created columns must be at the end and must be in version order\", 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0032-created-columns-must-be-at-the-end-and-must-be-in-version-order-column","content":"The SQLite ALTER TABLE ADD COLUMN statement is used to add new columns to the schema. This statement puts the columns at the end of the table. In order to make the CQL schema align as closely as possible to the actual sqlite schema you will get you are required to add columns where SQLite will put them. This will help a lot if you ever connect to such a database and start doing select * from &lt;somewhere with creates&gt;  "},{"title":"CQL0033: columns in a table marked @recreate cannot have @create or @delete, 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0033-columns-in-a-table-marked-recreate-cannot-have-create-or-delete-column","content":"If the table is using the @recreate plan then you can add and remove columns (and other things freely) you don't need to mark columns with @create or @delete just add/remove them. This error prevents the build up of useless annotations.  "},{"title":"CQL0034: create/delete version numbers can only be applied to columns that are nullable or have a default value 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0034-createdelete-version-numbers-can-only-be-applied-to-columns-that-are-nullable-or-have-a-default-value-column","content":"Any new column added to a schema must have a default value or be nullable so that its initial state is clear and so that all existing insert statements do not have to be updated to include it. Either make the column nullable or give it a default value. Similarly, any column being deleted must be nullable or have a default value. The column can't actually be deleted (not all versions of SQLite support this) so it will only be &quot;deprecated&quot;. But if the column is not null and has no default then it would be impossible to write a correct insert statement for the table with the deleted column. As a consequence you can neither add nor remove columns that are not null and have no default.  "},{"title":"CQL0035: column delete version can't be <= column create version\", 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0035-column-delete-version-cant-be--column-create-version-column","content":"You can't @delete a column in a version before it was even created. Probably there is a typo in one or both of the versions.  "},{"title":"CQL0036: column delete version can't be <= the table create version 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0036-column-delete-version-cant-be--the-table-create-version-column","content":"The indicated column is being deleted in a version that is before the table it is found in was even created. Probably there is a typo in the delete version.  "},{"title":"CQL0037: column delete version can't be >= the table delete version​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0037-column-delete-version-cant-be--the-table-delete-version","content":"The indicated column is being deleted in a version that is after the table has already been deleted. This would be redundant. Probably one or both have a typo in their delete version.  "},{"title":"CQL0038: column create version can't be <= the table create version 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0038-column-create-version-cant-be--the-table-create-version-column","content":"The indicated column is being created in a version that is before the table it is found in was even created. Probably there is a typo in the delete version.  "},{"title":"CQL0039: column create version can't be >= the table delete version 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0039-column-create-version-cant-be--the-table-delete-version-column","content":"The indicated column is being created in a version that that is after it has already been supposedly deleted. Probably there is a typo in one or both of the version numbers.  "},{"title":"CQL0040: table can only have one autoinc column 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0040-table-can-only-have-one-autoinc-column-column","content":"The indicated column is the second column to be marked with AUTOINCREMENT in its table. There can only be one such column.  "},{"title":"CQL0041: tables cannot have object columns 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0041-tables-cannot-have-object-columns-column","content":"The OBJECT data type is only for use in parameters and local variables. SQLite has no storage for object references. The valid data types include INTEGER, LONG INTEGER, REAL, BOOL, TEXT, BLOB  "},{"title":"CQL0042: left operand must be a string in 'LIKE/MATCH/GLOB'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0042-left-operand-must-be-a-string-in-likematchglob","content":"The indicated operator can only be used to compare two strings.  "},{"title":"CQL0043: right operand must be a string in 'LIKE/MATCH/GLOB'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0043-right-operand-must-be-a-string-in-likematchglob","content":"The indicated operator can only be used to compare two strings.  "},{"title":"CQL0044: operator may only appear in the context of a SQL statement 'MATCH'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0044-operator-may-only-appear-in-the-context-of-a-sql-statement-match","content":"The MATCH operator is a complex sqlite primitive. It can only appear within SQL expressions. See the CQL documentation about it being a two-headed-beast when it comes to expression evaluation.  "},{"title":"CQL0045: blob operand not allowed in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0045-blob-operand-not-allowed-in-operator","content":"None of the unary math operators e.g. '-' and '~' allow blobs as an operand.  "},{"title":"CQL0046: object operand not allowed in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0046-object-operand-not-allowed-in-operator","content":"None of the unary math operators e.g. '-' and '~' allow objects as an operand.  "},{"title":"CQL0047: string operand not allowed in 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0047-string-operand-not-allowed-in-operator","content":"None of the unary math operators e.g. '-' and '~' allow strings as an operand.  "},{"title":"CQL0051: argument can only be used in count() ''​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0051-argument-can-only-be-used-in-count-","content":"The '' special operator can only appear in the COUNT function. e.g. `select count() from some_table` It is not a valid function argument in any other context.  "},{"title":"CQL0052: select * cannot be used with no FROM clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0052-select--cannot-be-used-with-no-from-clause","content":"Select statements of the form select 1, 'foo'; are valid but select '*'; is not. The * shortcut for columns only makes sense if there is something to select from. e.g. select * from some_table; is valid.  "},{"title":"CQL0053: select [table].* cannot be used with no FROM clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0053-select-table-cannot-be-used-with-no-from-clause","content":"Select statements of the form select 1, 'foo'; are valid but select 'T.*'; is not. The T.* shortcut for all the columns from table T only makes sense if there is something to select form. e.g. select T.* from some_table T; is valid.  "},{"title":"CQL0054: table not found 'table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0054-table-not-found-table","content":"The indicated table was used in a select statement like select T.* from ... but no such table was present in the FROM clause.  "},{"title":"CQL0055: all columns in the select must have a name​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0055-all-columns-in-the-select-must-have-a-name","content":"Referring to the select statement on the failing line, that select statement was used in a context where all the columns must have a name. Examples include defining a view, a cursor, or creating a result set from a procedure. The failing code might look something like this.select 1, 2 B; it needs to look like this select 1 A, 2 B;  "},{"title":"CQL0056: NULL expression has no type to imply a needed type 'variable'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0056-null-expression-has-no-type-to-imply-a-needed-type-variable","content":"In some contexts the type of a constant is used to imply the type of the expression. The NULL literal cannot be used in such contexts because it has no specific type. In a SELECT statement the NULL literal has no type. If the type of the column cannot be inferred then it must be declared specifically. In a LET statement, the same situation arises LET x := NULL; doesn't specify what type 'x' is to be. You can fix this error by changing the NULL to something like CAST(NULL as TEXT). A common place this problem happens is in defining a view or returning a result set from a stored procedure. In those cases all the columns must have a name and a type.  "},{"title":"CQL0057: if multiple selects, all must have the same column count​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0057-if-multiple-selects-all-must-have-the-same-column-count","content":"If a stored procedure might return one of several result sets, each of the select statements it might return must have the same number of columns. Likewise, if several select results are being combined with UNION or UNION ALL they must all have the same number of columns.  "},{"title":"CQL0058: if multiple selects, all column names must be identical so they have unambiguous names; error in column N: 'X' vs. 'Y'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0058-if-multiple-selects-all-column-names-must-be-identical-so-they-have-unambiguous-names-error-in-column-n-x-vs-y","content":"If a stored procedure might return one of several result sets, each of the select statements must have the same column names for its result. Likewise, if several select results are being combined with UNION or UNION ALL they must all have the same column names. This is important so that there can be one unambiguous column name for every column for group of select statements. e.g. select 1 A, 2 B union select 3 A, 4 C;  Would provoke this error. In this case the error would report that the problem was in column 2 and that error was 'B' vs. 'C'  "},{"title":"CQL0059: a variable name might be ambiguous with a column name, this is an anti-pattern 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0059-a-variable-name-might-be-ambiguous-with-a-column-name-this-is-an-anti-pattern-name","content":"The referenced name is the name of a local or a global in the same scope as the name of a column. This can lead to surprising results as it is not clear which name takes priority (previously the variable did rather than the column, now it's an error). example: create proc foo(id integer) begin -- this is now an error, in all cases the argument would have been selected select id from bar T1 where T1.id != id; end;  To correct this, rename the local/global. Or else pick a more distinctive column name, but usually the local is the problem.  "},{"title":"CQL0060: referenced table can be independently recreated so it cannot be used in a foreign key, 'referenced_table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0060-referenced-table-can-be-independently-recreated-so-it-cannot-be-used-in-a-foreign-key-referenced_table","content":"The referenced table is marked recreate so it must be in the same recreate group as the current table or in a recreate group that does not introduce a cyclic foreign key dependency among recreate groups. Otherwise, the referenced table might be recreated away leaving all the foreign key references in current table as orphans. So we check the following: If the referenced table is marked recreate then any of the following result in CQL0060 the containing table is not recreate at all (non-recreate table can't reference recreate tables at all), ORthe new foreign key dependency between the referenced table and the current table introduces a cycle The referenced table is a recreate table and one of the 4 above conditions was not met. Either don't reference it or else put the current table and the referenced table into the same recreate group.  "},{"title":"CQL0061: if multiple selects, all columns must be an exact type match (expected expected_type; found actual_type) 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0061-if-multiple-selects-all-columns-must-be-an-exact-type-match-expected-expected_type-found-actual_type-column","content":"In a stored proc with multiple possible selects providing the result, all of the columns of all the selects must be an exact type match. e.g. if x then select 1 A, 2 B else select 3 A, 4.0 B; end if;  Would provoke this error. In this case 'B' would be regarded as the offending column and the error is reported on the second B.  "},{"title":"CQL0062: if multiple selects, all columns must be an exact type match (including nullability) (expected expected_type; found actual_type) 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0062-if-multiple-selects-all-columns-must-be-an-exact-type-match-including-nullability-expected-expected_type-found-actual_type-column","content":"In a stored proc with multiple possible selects providing the result, all of the columns of all the selects must be an exact type match. This error indicates that the specified column differs by nullability.  "},{"title":"CQL0063: can't mix and match out statement with select/call for return values 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0063-cant-mix-and-match-out-statement-with-selectcall-for-return-values-procedure_name","content":"If the procedure is using SELECT to create a result set it cannot also use the OUT keyword to create a one-row result set.  "},{"title":"CQL0064: object variables may not appear in the context of a SQL statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0064-object-variables-may-not-appear-in-the-context-of-a-sql-statement","content":"SQLite doesn't understand object references, so that means you cannot try to use a variable or parameter of type object inside of a SQL statement. e.g. create proc foo(X object) begin select X is null; end;  In this example X is an object parameter, but even to use X for an is null check in a select statement would require binding an object which is not possible. On the other hand this compiles fine. create proc foo(X object, out is_null bool not null) begin set is_null := X is null; end;  This is another example of XQL being a two-headed beast.  "},{"title":"CQL0065: identifier is ambiguous 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0065-identifier-is-ambiguous-name","content":"There is more than one variable/column with indicated name in equally near scopes. The most common reason for this is that there are two column in a join with the same name and that name has not been qualified elsewhere. e.g. SELECT A FROM (SELECT 1 AS A, 2 AS B) AS T1 INNER JOIN (SELECT 1 AS A, 2 AS B) AS T2;  There are two possible columns named A. Fix this by using T1.A or T2.A.  "},{"title":"CQL0066: if a table is marked @recreate, its indices must be in its schema region 'index_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0066-if-a-table-is-marked-recreate-its-indices-must-be-in-its-schema-region-index_name","content":"If a table is marked @recreate that means that when it changes it is dropped and created during schema maintenance. Of course when it is dropped its indices are also dropped. So the indices must also be recreated when the table changes. So with such a table it makes no sense to have indices that are in a different schema region. This can only work if they are all always visible together. Tables on the @create plan are not dropped so their indices can be maintained separately. So they get a little extra flexibility. To fix this error move the offending index into the same schema region as the table. And probably put them physically close for maintenance sanity.  "},{"title":"CQL0067: cursor was not used with 'fetch [cursor]' 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0067-cursor-was-not-used-with-fetch-cursor--cursor_name","content":"The code is trying to access fields in the named cursor but the automatic field generation form was not used so there are no such fields. e.g. declare a integer; declare b integer; declare C cursor for select 1 A, 2 B; fetch C into a, b; -- C.A and C.B not created (!) if (C.A) then -- error ... end if;  Correct usage looks like this: declare C cursor for select 1 A, 2 B; fetch C; -- automatically creates C.A and C.B if (C.A) then ... end if;   "},{"title":"CQL0068: field not found in cursor 'field'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0068-field-not-found-in-cursor-field","content":"The indicated field is not a valid field in a cursor expression. e.g. declare C cursor for select 1 A, 2 B; fetch C; -- automatically creates C.A and C.B if (C.X) then -- C has A and B, but no X ... end if;   "},{"title":"CQL0069: name not found 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0069-name-not-found-name","content":"The indicated name could not be resolved in the scope in which it appears. Probably there is a typo. But maybe the name you need isn't available in the scope you are trying to use it in.  "},{"title":"CQL0070: incompatible object type 'incompatible_type'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0070-incompatible-object-type-incompatible_type","content":"Two expressions of type object are holding a different object type e.g. declare x object&lt;Foo&gt;; declare y object&lt;Bar&gt;; set x := y;  Here the message would report that 'Bar' is incompatible. The message generally refers to the 2nd object type as the first one was ok by default then the second one caused the problem.  "},{"title":"CQL0071: first operand cannot be a blob in 'BETWEEN/NOT BETWEEN'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0071-first-operand-cannot-be-a-blob-in-betweennot-between","content":"The BETWEEN operator works on numerics and strings only.  "},{"title":"CQL0072: first operand cannot be a blob in 'BETWEEN/NOT BETWEEN'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0072-first-operand-cannot-be-a-blob-in-betweennot-between","content":"The BETWEEN operator works on numerics and strings only.  "},{"title":"CQL0073: CAST may only appear in the context of SQL statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0073-cast-may-only-appear-in-the-context-of-sql-statement","content":"The CAST function does highly complex and subtle conversions, including date/time functions and other things. It's not possibly to emulate this accurately and there is no sqlite helper to do the job directly from a C call. Consequently it's only supported in the context of CQL statements. It can be used in normal expressions by using the nested SELECT form (select ...)  "},{"title":"CQL0074: too few arguments provided 'coalesce'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0074-too-few-arguments-provided-coalesce","content":"There must be at least two arguments in a call to coalesce.  "},{"title":"CQL0075: incorrect number of arguments 'ifnull'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0075-incorrect-number-of-arguments-ifnull","content":"The ifnull function requires exactly two arguments.  "},{"title":"CQL0076: NULL literal is useless in function 'ifnull/coalesce'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0076-null-literal-is-useless-in-function-ifnullcoalesce","content":"Adding a NULL literal to IFNULL or COALESCE is a no-op. It's most likely an error.  "},{"title":"CQL0077: encountered arg known to be not null before the end of the list, rendering the rest useless 'expression'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0077-encountered-arg-known-to-be-not-null-before-the-end-of-the-list-rendering-the-rest-useless-expression","content":"In an IFNULL or COALESCE call, only the last argument may be known to be not null. If a not null argument comes earlier in the list, then none of the others could ever be used. That is almost certainly an error. The most egregious form of this error is if the first argument is known to be not null in which case the entireIFNULL or COALESCE can be removed.  "},{"title":"CQL0078: [not] in (select ...) is only allowed inside of select lists, where, on, and having clauses​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0078-not-in-select--is-only-allowed-inside-of-select-lists-where-on-and-having-clauses","content":"The (select...) option for IN or NOT IN only makes sense in certain expression contexts. Other uses are most likely errors. It cannot appear in a loose expression because it fundamentally requires sqlite to process it.  "},{"title":"CQL0079: function got incorrect number of arguments 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0079-function-got-incorrect-number-of-arguments-name","content":"The indicated function was called with the wrong number of arguments. There are various functions supported each with different rules. See the SQLite documentation for more information about the specified function.  "},{"title":"CQL0080: function may not appear in this context 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0080-function-may-not-appear-in-this-context-name","content":"Many functions can only appear in certain contexts. For instance most aggregate functions are limited to the select list or the HAVING clause. They cannot appear in, for instance, a WHERE, or ON clause. The particulars vary by function.  "},{"title":"CQL0081: aggregates only make sense if there is a FROM clause 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0081-aggregates-only-make-sense-if-there-is-a-from-clause-name","content":"The indicated aggregate function was used in a select statement with no tables. For instance select MAX(7);  Doesn't make any sense.  "},{"title":"CQL0082: argument must be numeric​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0082-argument-must-be-numeric","content":"The argument of function must be numeric.  "},{"title":"CQL0083: argument must be numeric 'SUM'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0083-argument-must-be-numeric-sum","content":"The argument of SUM must be numeric.  "},{"title":"CQL0084: second argument must be a string in function 'group_concat'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0084-second-argument-must-be-a-string-in-function-group_concat","content":"The second argument of group_concat is the separator, it must be a string. The first argument will be converted to a string.  "},{"title":"CQL0085: all arguments must be strings 'strftime'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0085-all-arguments-must-be-strings-strftime","content":"The strftime function does complex data formatting. All the arguments are strings. See the sqlite documentation for more details on the options (there are many).  "},{"title":"CQL0086: first argument must be a string in function 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0086-first-argument-must-be-a-string-in-function-function","content":"The first argument of the function is the formatting string. The other arguments are variable and many complex conversions will apply.  "},{"title":"CQL0087: first argument must be of type real 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0087-first-argument-must-be-of-type-real-function","content":"The first argument of the function (e.g. round) should be of type 'real'.  "},{"title":"CQL0088: user function may not appear in the context of a SQL statement 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0088-user-function-may-not-appear-in-the-context-of-a-sql-statement-function_name","content":"External C functions declared with declare function ... are not for use in sqlite. They may not appear inside statements.  "},{"title":"CQL0089: user function may only appear in the context of a SQL statement 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0089-user-function-may-only-appear-in-the-context-of-a-sql-statement-function_name","content":"SQLite user defined functions (or builtins) declared with declare select function may only appear inside of sql statements. In the case of user defined functions they must be added to sqlite by the appropriate C APIs before they can be used in CQL stored procs (or any other context really). See the sqlite documentation on how to add user defined functions. Create Or Redefine SQL Functions  "},{"title":"CQL0090: object<T SET> has a T that is not a procedure with a result set, 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0090-objectt-set-has-a-t-that-is-not-a-procedure-with-a-result-set-name","content":"The data type object&lt;T SET&gt; refers to the shape of a result set of a particular procedure. In this case the indicated name is not such a procedure. The most likely source of this problem is that there is a typo in the indicated name. Alternatively the name might be a valid shape like a cursor name or some other shape name but it's a shape that isn't coming from a procedure.  CQL0091: -- generalized so that this is not an error anymore  "},{"title":"CQL0092: RAISE may only be used in a trigger statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0092-raise-may-only-be-used-in-a-trigger-statement","content":"SQLite only supports this kind of control flow in the context of triggers, certain trigger predicates might need to unconditionally fail and complex logic can be implemented in this way. However this sort of thing is not really recommended. In any case this is not a general purpose construct.  "},{"title":"CQL0093: RAISE 2nd argument must be a string​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0093-raise-2nd-argument-must-be-a-string","content":"Only forms with a string as the second argument are supported by SQLite.  "},{"title":"CQL0094: function not yet implemented 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0094-function-not-yet-implemented-function","content":"The indicated function is not implemented in CQL. Possibly you intended to declare it with declare function as an external function or declare select function as a sqlite builtin. Note not all sqlite builtins are automatically declared.  "},{"title":"CQL0095: table/view not defined 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0095-tableview-not-defined-name","content":"The indicated name is neither a table nor a view. It is possible that the table/view is now deprecated with @delete and therefore will appear to not exist in the current context.  "},{"title":"CQL0096: join using column not found on the left side of the join 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0096-join-using-column-not-found-on-the-left-side-of-the-join-column_name","content":"In the JOIN ... USING(x,y,z) form, all the columns in the using clause must appear on both sides of the join. Here the indicated name is not present on the left side of the join.  "},{"title":"CQL0097: join using column not found on the right side of the join 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0097-join-using-column-not-found-on-the-right-side-of-the-join-column_name","content":"In the JOIN ... USING(x,y,z) form, all the columns in the using clause must appear on both sides of the join. Here the indicated name is not present on the right side of the join.  "},{"title":"CQL0098: left/right column types in join USING(...) do not match exactly 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0098-leftright-column-types-in-join-using-do-not-match-exactly-column_name","content":"In the JOIN ... USING(x,y,z) form, all the columns in the using clause must appear on both sides of the join and have the same data type. Here the data types differ in the named column.  "},{"title":"CQL0099: HAVING clause requires GROUP BY clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0099-having-clause-requires-group-by-clause","content":"The HAVING clause makes no sense unless there is also a GROUP BY clause. SQLite enforces this as does CQL.  "},{"title":"CQL0100: duplicate common table name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0100-duplicate-common-table-name-name","content":"In a WITH clause, the indicated common table name was defined more than once.  "},{"title":"CQL0101: too few column names specified in common table expression 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0101-too-few-column-names-specified-in-common-table-expression-name","content":"In a WITH clause the indicated common table expression doesn't include enough column names to capture the result of the select statement it is associated with. e.g. WITH foo(a) as (SELECT 1 A, 2 B) ...`  The select statement produces two columns the foo declaration specifies one.  "},{"title":"CQL0102: too many column names specified in common table expression 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0102-too-many-column-names-specified-in-common-table-expression-name","content":"In a WITH clause the indicated common table expression has more column names than the select expression it is associated with. e.g. WITH foo(a, b) as (SELECT 1) ... `  The select statement produces one column the foo declaration specifies two.  "},{"title":"CQL0103: duplicate table/view name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0103-duplicate-tableview-name-name","content":"The indicated table or view must be unique in its context. The version at the indicated line number is a duplicate of a previous declaration.  "},{"title":"CQL0104: view was present but now it does not exist (use @delete instead) 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0104-view-was-present-but-now-it-does-not-exist-use-delete-instead-name","content":"During schema validation, CQL found a view that used to exist but is now totally gone. The correct procedure is to mark the view with @delete (you can also make it stub with the same name to save a little space). This is necessary so that CQL can know what views should be deleted on client devices during an upgrade. If the view is eradicated totally there would be no way to know that the view should be deleted if it exists.  "},{"title":"CQL0105: object was a view but is now a table 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0105-object-was-a-view-but-is-now-a-table-name","content":"Converting a view into a table, or otherwise creating a table with the same name as a view is not legal.  "},{"title":"CQL0106: trigger was present but now it does not exist (use @delete instead) 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0106-trigger-was-present-but-now-it-does-not-exist-use-delete-instead-name","content":"During schema validation, CQL found a trigger that used to exist but is now totally gone. The correct procedure is to mark the trigger with @delete (you can also make it stub with the same name to save a little space). This is necessary so that CQL can know what triggers should be deleted on client devices during an upgrade. If the trigger is eradicated totally there would be no way to know that the trigger should be deleted if it exists. That would be bad.  "},{"title":"CQL0107: delete version can't be <= create version 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0107-delete-version-cant-be--create-version-name","content":"Attempting to declare that an object has been deleted before it was created is an error. Probably there is a typo in one or both of the version numbers of the named object.  "},{"title":"CQL0108: table in drop statement does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0108-table-in-drop-statement-does-not-exist-table_name","content":"The indicated table was not declared anywhere. Note that CQL requires that you declare all tables you will work with, even if all you intend to do with the table is drop it. When you put a CREATE TABLE statement in global scope this only declares a table, it doesn't actually create the table. See the documentation on DDL for more information.  "},{"title":"CQL0109: cannot drop a view with drop table 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0109-cannot-drop-a-view-with-drop-table-view_name","content":"The object named in a DROP TABLE statement must be a table, not a view.  "},{"title":"CQL0110: view in drop statement does not exist 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0110-view-in-drop-statement-does-not-exist-view_name","content":"The indicated view was not declared anywhere. Note that CQL requires that you declare all views you will work with, even if all you intend to do with the view is drop it. When you put a CREATE VIEW statement in global scope this only declares a view, it doesn't actually create the view. See the documentation on DDL for more information.  "},{"title":"CQL0111: cannot drop a table with drop view 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0111-cannot-drop-a-table-with-drop-view-name","content":"The object named in a DROP VIEW statement must be a view, not a table.  "},{"title":"CQL0112: index in drop statement was not declared 'index_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0112-index-in-drop-statement-was-not-declared-index_name","content":"The indicated index was not declared anywhere. Note that CQL requires that you declare all indices you will work with, even if all you intend to do with the index is drop it. When you put a CREATE INDEX statement in global scope this only declares an index, it doesn't actually create the index. See the documentation on DDL for more information.  "},{"title":"CQL0113: trigger in drop statement was not declared 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0113-trigger-in-drop-statement-was-not-declared-name","content":"The indicated trigger was not declared anywhere. Note that CQL requires that you declare all triggers you will work with, even if all you intend to do with the trigger is drop it. When you put a CREATE TRIGGER statement in global scope this only declares a trigger, it doesn't actually create the trigger. See the documentation on DDL for more information.  "},{"title":"CQL0114: current schema can't go back to recreate semantics for 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0114-current-schema-cant-go-back-to-recreate-semantics-for-table_name","content":"The indicated table was previously marked with @create indicating it has precious content and should be upgraded carefully. The current schema marks the same table with @recreate meaning it has discardable content and should be upgraded by dropping it and recreating it. This transition is not allowed. If the table really is non-precious now you can mark it with @delete and then make a new similar table with @recreate. This really shouldn't happen very often if at all. Probably the error is due to a typo or wishful thinking.  "},{"title":"CQL0115: current create version not equal to previous create version for 'table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0115-current-create-version-not-equal-to-previous-create-version-for-table","content":"The indicated table was previously marked with @create at some version (x) and now it is being created at some different version (y !=x ). This not allowed (if it were then objects might be created in the wrong/different order during upgrade which would cause all kinds of problems).  "},{"title":"CQL0116: current delete version not equal to previous delete version for 'table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0116-current-delete-version-not-equal-to-previous-delete-version-for-table","content":"The indicated table was previously marked with @delete at some version (x) and now it is being deleted at some different version (y != x). This not allowed (if it were then objects might be deleted in the wrong/different order during upgrade which would cause all kinds of problems).  "},{"title":"CQL0117: @delete procedure changed in object 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0117-delete-procedure-changed-in-object-table_name","content":"The @delete attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0118: @create procedure changed in object 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0118-create-procedure-changed-in-object-table_name","content":"The @create attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0119: column name is different between previous and current schema 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0119-column-name-is-different-between-previous-and-current-schema-name","content":"Since there is no sqlite operation that allows for columns to be renamed, attempting to rename a column is not allowed. NOTE: you can also get this error if you remove a column entirely, or add a column in the middle of the list somewhere. Since columns (also) cannot be reordered during upgrade, CQL expects to find all the columns in exactly the same order in the previous and new schema. Any reordering, or deletion could easily look like an erroneous rename. New columns must appear at the end of any existing columns.  "},{"title":"CQL0120: column type is different between previous and current schema 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0120-column-type-is-different-between-previous-and-current-schema-name","content":"It is not possible to change the data type of a column during an upgrade, SQLite provides no such options. Attempting to do so results in an error. This includes nullability.  "},{"title":"CQL0121: column current create version not equal to previous create version 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0121-column-current-create-version-not-equal-to-previous-create-version-name","content":"The indicated column was previously marked with @create at some version (x) and now it is being created at some different version (y !=x ). This not allowed (if it were then objects might be created in the wrong/different order during upgrade which would cause all kinds of problems). "},{"title":"CQL0122: column current delete version not equal to previous delete version 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0122-column-current-delete-version-not-equal-to-previous-delete-version-name","content":"The indicated column was previously marked with @delete at some version (x) and now it is being deleted at some different version (y != x). This not allowed (if it were then objects might be deleted in the wrong/different order during upgrade which would cause all kinds of problems).  "},{"title":"CQL0123: column @delete procedure changed 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0123-column-delete-procedure-changed-name","content":"The @delete attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0124: column @create procedure changed 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0124-column-create-procedure-changed-name","content":"The @create attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0125: column current default value not equal to previous default value 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0125-column-current-default-value-not-equal-to-previous-default-value-column","content":"The default value of a column may not be changed in later versions of the schema. There is no SQLite operation that would allow this.  "},{"title":"CQL0126: table was present but now it does not exist (use @delete instead) 'table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0126-table-was-present-but-now-it-does-not-exist-use-delete-instead-table","content":"During schema validation, CQL found a table that used to exist but is now totally gone. The correct procedure is to mark the table with @delete. This is necessary so that CQL can know what tables should be deleted on client devices during an upgrade. If the table is eradicated totally there would be no way to know that the table should be deleted if it exists. That would be bad.  "},{"title":"CQL0127: object was a table but is now a view 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0127-object-was-a-table-but-is-now-a-view-name","content":"The indicated object was a table in the previous schema but is now a view in the current schema. This transformation is not allowed.  "},{"title":"CQL0128: table has a column that is different in the previous and current schema 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0128-table-has-a-column-that-is-different-in-the-previous-and-current-schema-column","content":"The indicated column changed in one of its more exotic attributes, examples: its FOREIGN KEY rules changed in some wayits PRIMARY KEY status changedits UNIQUE status changed Basically the long form description of the column is now different and it isn't different in one of the usual way like type or default value. This error is the catch all for all the other ways a column could change such as &quot;the FK rule for what happens when an update fk violation occurs is now different&quot; -- there are dozens of such errors and they aren't very helpful anyway.  "},{"title":"CQL0129: a column was removed from the table rather than marked with @delete 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0129-a-column-was-removed-from-the-table-rather-than-marked-with-delete-column_name","content":"During schema validation, CQL found a column that used to exist but is now totally gone. The correct procedure is to mark the column with @delete. This is necessary so that CQL can know what columns existed during any version of the schema, thereby allowing them to be used in migration scripts during an upgrade. If the column is eradicated totally there would be no way to know that the exists, and should no longer be used. That would be bad. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0130: table has columns added without marking them @create 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0130-table-has-columns-added-without-marking-them-create-column_name","content":"The indicated column was added but it was not marked with @create. The table in question is not on the @recreate plan so this is an error. Add a suitable @create annotation to the column declaration.  "},{"title":"CQL0131: table has newly added columns that are marked both @create and @delete 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0131-table-has-newly-added-columns-that-are-marked-both-create-and-delete-column_name","content":"The indicated column was simultaneously marked @create and @delete. That's surely some kind of typo. Creating a column and deleting it in the same version is weird.  "},{"title":"CQL0132: table has a facet that is different in the previous and current schema 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0132-table-has-a-facet-that-is-different-in-the-previous-and-current-schema-table_name","content":"The indicated table has changes in one of its non-column features. These changes might be: a primary key declarationa unique key declarationa foreign key declaration None of these are allowed to change. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0133: non-column facets have been removed from the table 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0133-non-column-facets-have-been-removed-from-the-table-name","content":"The error indicates that the table has had some stuff removed from it. The &quot;stuff&quot; might be: a primary key declarationa unique key declarationa foreign key declaration Since there is no way to change any of the constraints after the fact, they may not be changed at all if the table is on the @create plan. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0134: table has a new non-column facet in the current schema 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0134-table-has-a-new-non-column-facet-in-the-current-schema-table_name","content":"The error indicates that the table has had some stuff added to it. The &quot;stuff&quot; might be: a primary key declarationa unique key declarationa foreign key declaration Since there is no way to change any of the constraints after the fact, they may not be changed at all if the table is on the @create plan. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0135: table create statement attributes different than previous version 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0135-table-create-statement-attributes-different-than-previous-version-table_name","content":"The 'flags' on the CREATE TABLE statement changed between versions. These flags capture the options like theTEMP in CREATE TEMP TABLE and the IF NOT EXISTS. Changing these is not allowed.  "},{"title":"CQL0136: trigger already exists 'trigger_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0136-trigger-already-exists-trigger_name","content":"Trigger names may not be duplicated. Probably there is copy/pasta going on here.  "},{"title":"CQL0137: table/view not found 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0137-tableview-not-found-name","content":"In a CREATE TRIGGER statement, the indicated name is neither a table or a view. Either a table or a view was expected in this context.  "},{"title":"CQL0138: a trigger on a view must be the INSTEAD OF form 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0138-a-trigger-on-a-view-must-be-the-instead-of-form-name","content":"In a CREATE TRIGGER statement, the named target of the trigger was a view but the trigger type is not INSTEAD OF. Only INSTEAD OF can be applied to views because views are not directly mutable so none of the other types make sense. e.g. there can be no delete operations, on a view, so BEFORE DELETE or AFTER DELETE are not really a thing.  "},{"title":"CQL0139: temp objects may not have versioning annotations 'object_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0139-temp-objects-may-not-have-versioning-annotations-object_name","content":"The indicated object is a temporary. Since temporary do not survive sessions it makes no sense to try to version them for schema upgrade. They are always recreated on demand. If you need to remove one, simply delete it entirely, it requires no tombstone.  "},{"title":"CQL0140: columns in a temp table may not have versioning attributes 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0140-columns-in-a-temp-table-may-not-have-versioning-attributes-column_name","content":"The indicated column is part of a temporary table. Since temp tables do not survive sessions it makes no sense to try to version their columns for schema upgrade. They are always recreated on demand.  "},{"title":"CQL0141: table has an AUTOINCREMENT column; it cannot also be WITHOUT ROWID 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0141-table-has-an-autoincrement-column-it-cannot-also-be-without-rowid-table_name","content":"SQLite uses its ROWID internally for AUTOINCREMENT columns. Therefore WITHOUT ROWID is not a possibility if AUTOINCREMENT is in use.  "},{"title":"CQL0142: duplicate column name 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0142-duplicate-column-name-column_name","content":"In a CREATE TABLE statement, the indicated column was defined twice. This is probably a copy/pasta issue.  "},{"title":"CQL0143: more than one primary key in table 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0143-more-than-one-primary-key-in-table-table_name","content":"The indicated table has more than one column with the PRIMARY KEY attribute or multiple PRIMARY KEY constraints, or a combination of these things. You'll have to decide which one is really intended to be primary.  "},{"title":"CQL0144: cannot alter a view 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0144-cannot-alter-a-view-view_name","content":"In an ALTER TABLE statement, the table to be altered is actually a view. This is not allowed.  "},{"title":"CQL0144: table in alter statement does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0144-table-in-alter-statement-does-not-exist-table_name","content":"In an ALTER TABLE statement, the table to be altered was not defined, or perhaps was marked with @delete and is no longer usable in the current schema version. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0145: version annotations not valid in alter statement 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0145-version-annotations-not-valid-in-alter-statement-column_name","content":"In an ALTER TABLE statement, the attributes on the column may not include @create or @delete. Those annotations go on the columns declaration in the corresponding CREATE TABLE statement. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0146: adding an auto increment column is not allowed 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0146-adding-an-auto-increment-column-is-not-allowed-column_name","content":"In an ALTER TABLE statement, the attributes on the column may not include AUTOINCREMENT. SQLite does not support the addition of new AUTOINCREMENT columns. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0147: adding a not nullable column with no default value is not allowed 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0147-adding-a-not-nullable-column-with-no-default-value-is-not-allowed-column_name","content":"In an ALTER TABLE statement the attributes on the named column must include a default value or else the column must be nullable. This is so that SQLite knows what value to put on existing rows when the column is added and also so that any existing insert statements will not suddenly all become invalid. If the column is nullable or has a default value then the existing insert statements that don't specify the column will continue to work, using either NULL or the default. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0148: added column must already be reflected in declared schema, with @create, exact name match required 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0148-added-column-must-already-be-reflected-in-declared-schema-with-create-exact-name-match-required-column_name","content":"In CQL loose schema is a declaration, it does not actually create anything unless placed inside of a procedure. A column that is added with ALTER TABLE is not actually declared as part of the schema by the ALTER. Rather the schema declaration is expected to include any columns you plan to add. Normally the way this all happens is that you put @create notations on a column in the schema and the automatic schema upgrader then creates suitable ALTER TABLE statements to arrange for that column to be added. If you manually write an ALTER TABLE statement it isn't allowed to add columns at whim; in some sense it must be creating the reality already described in the declaration. This is exactly what the automated schema upgrader does -- it declares the end state and then alters the world to get to that state. It's important to remember that from CQL's perspective the schema is fixed for any given compilation, so runtime alterations to it are not really part of the type system. They can't be. Even DROP TABLE does not remove the table from type system -- it can't -- the most likely situation is that you are about to recreate that same table again for another iteration with the proc that creates it. This particular error is saying that the column you are trying to add does not exist in the declared schema. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0149: added column must be an exact match for the column type declared in the table 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0149-added-column-must-be-an-exact-match-for-the-column-type-declared-in-the-table-column_name","content":"In CQL loose schema is a declaration, it does not actually create anything unless placed inside of a procedure. A column that is added with ALTER TABLE is not actually declared as part of the schema by the ALTER. Rather the schema declaration is expected to include any columns you plan to add. Normally the way this all happens is that you put @create notations on a column in the schema and the automatic schema upgrader then creates suitable ALTER TABLE statements to arrange for that column to be added. If you manually write an ALTER TABLE statement it isn't allowed to add columns at whim; in some sense it must be creating the reality already described in the declaration. This is exactly what the automated schema upgrader does -- it declares the end state and then alters the world to get to that state. It's important to remember that from CQL's perspective the schema is fixed for any given compilation, so runtime alterations to it are not really part of the type system. They can't be. Even DROP TABLE does not remove the table from type system -- it can't -- the most likely situation is that you are about to recreate that same table again for another iteration with the proc that creates it. This particular error is saying that the column you are trying to add exists in the declared schema, but its definition is different than you have specified in the ALTER TABLE statement. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0150: expected numeric expression in IF predicate​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0150-expected-numeric-expression-in-if-predicate","content":"In an IF statement the condition (predicate) must be a numeric. The body of the IF runs if the value is not null and not zero.  "},{"title":"CQL0151: table in delete statement does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0151-table-in-delete-statement-does-not-exist-table_name","content":"In a DELETE statement, the indicated table does not exist. Probably it's a spelling mistake, or else the table has been marked with @delete and may no longer be used in DELETE statements.  "},{"title":"CQL0152: cannot delete from a view 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0152-cannot-delete-from-a-view-view_name","content":"In a DELETE statement, the target of the delete must be a table, but the indicated name is a view.  "},{"title":"CQL0153: duplicate target column name in update statement 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0153-duplicate-target-column-name-in-update-statement-column_name","content":"In an UPDATE statement, you can only specify any particular column to update once. e.g. UPDATE coordinates set x = 1, x = 3; will produce this error. UPDATE coordinates set x = 1, y = 3; might be correct. This error is most likely caused by a typo or a copy/pasta of the column names, especially if they were written one per line.  "},{"title":"CQL0154: table in update statement does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0154-table-in-update-statement-does-not-exist-table_name","content":"In an UPDATE statement, the target table does not exist. Probably it's a spelling mistake, or else the table has been marked with @delete and may no longer be used in UPDATE statements.  "},{"title":"CQL0155: cannot update a view 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0155-cannot-update-a-view-view_name","content":"In an UPDATE statement, the target of the update must be a table but the name of a view was provided.  "},{"title":"CQL0156: seed expression must be a non-nullable integer​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0156-seed-expression-must-be-a-non-nullable-integer","content":"The INSERT statement statement supports the notion of synthetically generated values for dummy data purposes. A 'seed' integer is used to derive the values. That seed (in the @seed() position) must be a non-null integer. The most common reason for this error is that the seed is an input parameter and it was not declared NOT NULL.  "},{"title":"CQL0157: count of columns differs from count of values​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0157-count-of-columns-differs-from-count-of-values","content":"In an INSERT statement of the form INSERT INTO foo(a, b, c) VALUES(x, y, z) the number of values (x, y, z) must be the same as the number of columns (a, b, c). Note that there are many reasons you might not have to specify all the columns of the table but whichever columns you do specify should have values.  "},{"title":"CQL0158: required column missing in INSERT statement 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0158-required-column-missing-in-insert-statement-column_name","content":"In an INSERT statement such as INSERT INTO foo(a,b,c) VALUES(x,yz) this error is indicating that there is a column in foo (the one indicated in the error) which was not in the list (i.e. not one of a, b, c) and that column is neither nullable, nor does it have a default value. In order to insert a row a value must be provided. To fix this include the indicated column in your insert statement.  "},{"title":"CQL0159: cannot add an index to a virtual table 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0159-cannot-add-an-index-to-a-virtual-table-table_name","content":"Adding an index to a virtual table isn't possible, the virtual table includes whatever indexing its module provides, no further indexing is possible. From the SQLite documentation: &quot;One cannot create additional indices on a virtual table. (Virtual tables can have indices but that must be built into the virtual table implementation. Indices cannot be added separately using CREATE INDEX statements.)&quot;  "},{"title":"CQL0160: table in insert statement does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0160-table-in-insert-statement-does-not-exist-table_name","content":"In an INSERT statement attempting to insert into the indicated table name is not possible because there is no such table. This error might happen because of a typo, or it might happen because the indicated table has been marked with @delete and is logically hidden.  "},{"title":"CQL0161: cannot insert into a view 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0161-cannot-insert-into-a-view-view_name","content":"In an INSERT statement attempting to insert into the indicated name is not possible because that name is a view not a table. Inserting into views is not supported.  "},{"title":"CQL0162: cannot add a trigger to a virtual table 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0162-cannot-add-a-trigger-to-a-virtual-table-table_name","content":"Adding a trigger to a virtual table isn't possible. From the SQLite documentation: &quot;One cannot create a trigger on a virtual table.&quot;  "},{"title":"CQL0163: FROM ARGUMENTS construct is only valid inside a procedure​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0163-from-arguments-construct-is-only-valid-inside-a-procedure","content":"Several statements support the FROM ARGUMENTS sugar format like INSERT INTO foo(a,b,c) FROM ARGUMENTS which causes the arguments of the current procedure to be used as the values. This error is complaining that you have used this form but the statement does not occur inside of a procedure so there can be no arguments. This form does not make sense outside of any procedure.  "},{"title":"CQL0164: cannot use ALTER TABLE on a virtual table 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0164-cannot-use-alter-table-on-a-virtual-table-table_name","content":"This is not supported by SQLite. From the SQLite documentation: &quot;One cannot run ALTER TABLE ... ADD COLUMN commands against a virtual table.&quot;  "},{"title":"CQL0165: fetch values is only for value cursors, not for sqlite cursors 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0165-fetch-values-is-only-for-value-cursors-not-for-sqlite-cursors-cursor_name","content":"Cursors come in two flavors. There are &quot;statement cursors&quot; which are built from something like this: declare C cursor for select * from foo; fetch C; -- or -- fetch C into a, b, c;  That is, they come from a SQLite statement and you can fetch values from that statement. The second type comes from procedural values like this. declare C cursor like my_table; fetch C from values(1, 2, 3);  In the second example C's data type will be the same as the columns in my_table and we will fetch its values from 1,2,3 -- this version has no database backing at all, it's just data. This error says that you declared the cursor in the first form (with a SQL statement) but then you tried to fetch it using the second form, the one for data. These forms don't mix. If you need a value cursor for a row you can copy data from one cursor into another.  "},{"title":"CQL0166: count of columns differs from count of values​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0166-count-of-columns-differs-from-count-of-values","content":"In a value cursor, declared something like this: declare C cursor like my_table; fetch C from values(1, 2, 3);  The type of the cursor ( in this case from my_table) requires a certain number of columns, but that doesn't match the number that were provided in the values. To fix this you'll need to add/remove values so that the type match.  "},{"title":"CQL0167: required column missing in FETCH statement 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0167-required-column-missing-in-fetch-statement-column_name","content":"In a value cursor, declared something like this: declare C cursor like my_table; fetch C(a,b,c) from values(1, 2, 3);  This error is saying that there is some other field in the table 'd' and it was not specified in the values. Nor was there a usable dummy data for that column that could be used. You need to provide a value for the missing column.  "},{"title":"CQL0168: CQL has no good way to generate dummy blobs; not supported for now​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0168-cql-has-no-good-way-to-generate-dummy-blobs-not-supported-for-now","content":"In a value cursor with dummy data specified, one of the columns in the cursor is of type blob. There's no good way to create dummy data for blobs so that isn't supported.  "},{"title":"CQL0169: enum not found 'enum_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0169-enum-not-found-enum_name","content":"The indicated name was used in a context where an enumerated type name was expected but there is no such type. Perhaps the enum was not included (missing a #include) or else there is a typo.  "},{"title":"CQL0170: cast is redundant, remove to reduce code size 'expression'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0170-cast-is-redundant-remove-to-reduce-code-size-expression","content":"The operand of the CAST expression is already the type that it is being cast to. The cast will do nothing but waste space in the binary and make the code less clear. Remove it.  "},{"title":"CQL0171: name not found 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0171-name-not-found-name","content":"In a scoped name list, like the columns of a cursor (for a fetch), or the columns of a particular table (for an index) a name appeared that did not belong to the universe of legal names. Trying to make a table index using a column that is not in the table would produce this error. There are many instances where a list of names belongs to some limited scope.  "},{"title":"CQL0172: name list has duplicate name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0172-name-list-has-duplicate-name-name","content":"In a scoped name list, like the columns of a cursor (for a fetch), or the columns of a particular table (for an index) a name appeared twice in the list where the names must be unique. Trying to make a table index using the same column twice would produce this error.  "},{"title":"CQL0173: variable not found 'variable_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0173-variable-not-found-variable_name","content":"In a SET statement, the target of the assignment is not a valid variable name in that scope.  "},{"title":"CQL0174: cannot set a cursor 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0174-cannot-set-a-cursor-cursor_name","content":"In a SET statement, the target of the assignment is a cursor variable, you cannot assign to a cursor variable.  "},{"title":"CQL0175: duplicate parameter name 'parameter_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0175-duplicate-parameter-name-parameter_name","content":"In a parameter list for a function or a procedure, the named parameter appears more than once. The formal names for function arguments must be unique.  "},{"title":"CQL0176: indicated procedure or group already has a recreate action 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0176-indicated-procedure-or-group-already-has-a-recreate-action-name","content":"There can only be one migration rule for a table or group, the indicated item already has such an action. If you need more than one migration action you can create a containing procedure that dispatches to other migrators.  "},{"title":"CQL0177: global constants must be either constant numeric expressions or string literals 'constant_definition'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0177-global-constants-must-be-either-constant-numeric-expressions-or-string-literals-constant_definition","content":"Global constants must be either a combination other constants for numeric expressions or else string literals. The indicated expression was not one of those. This can happen if the expression uses variables, or has other problems that prevent it from evaluating, or if a function is used that is not supported.  "},{"title":"CQL0178: proc has no result 'like_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0178-proc-has-no-result-like_name","content":"In an argument list, the LIKE construct was used to create arguments that are the same as the return type of the named procedure. However the named procedure does not produce a result set and therefore has no columns to mimic. Probably the name is wrong.  "},{"title":"CQL0179: shared fragments must consist of exactly one top level statement 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0179-shared-fragments-must-consist-of-exactly-one-top-level-statement-procedure_name","content":"Any shared fragment can have only one statement. There are three valid forms -- IF/ELSE, WITH ... SELECT, and SELECT. This error indicates the named procedure, which is a shared fragment, has more than one statement.  "},{"title":"CQL0180: duplicate column name in result not allowed 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0180-duplicate-column-name-in-result-not-allowed-column_name","content":"In a procedure that returns a result either with a loose SELECT statement or in a place where the result of a SELECT is captured with a FETCH statement the named column appears twice in the projection of the SELECT in question. The column names must be unique in order to have consistent cursor field names or consistent access functions for the result set of the procedure. One instance of the named column must be renamed with something like select T1.foo first_foo, T2.foo second_foo.  "},{"title":"CQL0181: autodrop temp table does not exist 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0181-autodrop-temp-table-does-not-exist-name","content":"In a cql:autodrop annotation, the given name is unknown entirely.  "},{"title":"CQL0182: autodrop target is not a table 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0182-autodrop-target-is-not-a-table-name","content":"In a cql:autodrop annotation, the given name is not a table (it's probably a view).  "},{"title":"CQL0183: autodrop target must be a temporary table 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0183-autodrop-target-must-be-a-temporary-table-name","content":"In a cql:autodrop annotation, the given name is a table but it is not a temp table. The annotation is only valid on temp tables, it's not for &quot;durable&quot; tables.  "},{"title":"CQL0184: stored procedures cannot be nested 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0184-stored-procedures-cannot-be-nested-name","content":"The CREATE PROCEDURE statement may not appear inside of another stored procedure. The named procedure appears in a nested context.  "},{"title":"CQL0185: proc name conflicts with func name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0185-proc-name-conflicts-with-func-name-name","content":"In a CREATE PROCEDURE statement, the given name conflicts with an already declared function (DECLARE FUNCTION or DECLARE SELECT FUNCTION). You'll have to choose a different name.  "},{"title":"CQL0186: duplicate stored proc name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0186-duplicate-stored-proc-name-name","content":"In a CREATE PROCEDURE statement, the indicated name already corresponds to a created (not just declared) stored procedure. You'll have to choose a different name.  "},{"title":"CQL0187: @schema_upgrade_version not declared or doesn't match upgrade version N for proc 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0187-schema_upgrade_version-not-declared-or-doesnt-match-upgrade-version-n-for-proc-name","content":"The named procedure was declared as a schema migration procedure in an @create or @delete annotation for schema version N. In order to correctly type check such a procedure it must be compiled in the context of schema version N. This restriction is required so that the tables and columns the procedure sees are the ones that existed in version N not the ones that exist in the most recent version as usual. To create this condition, the procedure must appear in a file that begins with the line: @schema_upgrade_version &lt;N&gt;;  And this declaration must come before any CREATE TABLE statements. If there is no such declaration, or if it is for the wrong version, then this error will be generated.  "},{"title":"CQL0188: procedure is supposed to do schema migration but it doesn't have any DML 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0188-procedure-is-supposed-to-do-schema-migration-but-it-doesnt-have-any-dml-name","content":"The named procedure was declared as a schema migration procedure in an @create or @delete annotation, however the procedure does not have any DML in it. That can't be right. Some kind of data reading and writing is necessary.  "},{"title":"CQL0189: procedure declarations/definitions do not match 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0189-procedure-declarationsdefinitions-do-not-match-name","content":"The named procedure was previously declared with a DECLARE PROCEDURE statement but when the CREATE PROCEDURE was encountered, it did not match the previous declaration.  "},{"title":"CQL0190: duplicate column name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0190-duplicate-column-name-name","content":"In a context with a typed name list (e.g. id integer, t text) the named column occurs twice. Typed name lists happen in many contexts, but a common one is the type of the result in a declared procedure statement or declared function statement.  "},{"title":"CQL0191: declared functions must be top level 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0191-declared-functions-must-be-top-level-function_name","content":"A DECLARE FUNCTION statement for the named function is happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure.  "},{"title":"CQL0192: func name conflicts with proc name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0192-func-name-conflicts-with-proc-name-name","content":"The named function in a DECLARE FUNCTION statement conflicts with an existing declared or created procedure. One or the other must be renamed to resolve this issue.  "},{"title":"CQL0193: duplicate function name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0193-duplicate-function-name-name","content":"The named function in a DECLARE FUNCTION statement conflicts with an existing declared function, or it was declared twice. One or the other declaration must be renamed or removed to resolve this issue.  "},{"title":"CQL0194: declared procedures must be top level 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0194-declared-procedures-must-be-top-level-name","content":"A DECLARE PROCEDURE statement for the named procedure is itself happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure.  "},{"title":"CQL0195: proc name conflicts with func name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0195-proc-name-conflicts-with-func-name-name","content":"The named procedure in a DECLARE PROCEDURE statement conflicts with an existing declared function. One or the other declaration must be renamed or removed to resolve this issue.  "},{"title":"CQL0196: procedure declarations/definitions do not match 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0196-procedure-declarationsdefinitions-do-not-match-name","content":"The named procedure was previously declared with a DECLARE PROCEDURE statement. Now there is another declaration and it does not match the previous declaration  "},{"title":"CQL0197: duplicate variable name in the same scope 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0197-duplicate-variable-name-in-the-same-scope-name","content":"In a DECLARE statement, a variable of the same name already exists in that scope. Note that CQL does not have block level scope, all variables are procedure level, so they are in scope until the end of the procedure. To resolve this problem, either re-use the old variable if appropriate or rename the new variable.  "},{"title":"CQL0198: global variable hides table/view name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0198-global-variable-hides-tableview-name-name","content":"In a DECLARE statement, the named variable is a global (declared outside of any procedure) and has the same name as a table or view. This creates a lot of confusion and is therefore disallowed. To correct the problem, rename the variable. Global variables generally are problematic, but sometimes necessary.  "},{"title":"CQL0199: cursor requires a procedure that returns a result set via select 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0199-cursor-requires-a-procedure-that-returns-a-result-set-via-select-procedure_name","content":"In a DECLARE statement that declares a CURSOR FOR CALL the procedure that is being called does not produce a result set with the SELECT statement. As it has no row results it is meaningless to try to put a cursor on it. Probably the error is due to a copy/pasta of the procedure name.  "},{"title":"CQL0200: variable is not a cursor 'another_cursor'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0200-variable-is-not-a-cursor-another_cursor","content":"In a DECLARE statement that declares a CURSOR LIKE another cursor, the indicated name is a variable but it is not a cursor, so we cannot make another cursor like it. Probably the error is due to a typo in the 'like_name'.  "},{"title":"CQL0201: expanding FROM ARGUMENTS, there is no argument matching 'required_arg'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0201-expanding-from-arguments-there-is-no-argument-matching-required_arg","content":"In an INSERT or FETCH statement using the form FROM ARGUMENTS(LIKE [name])The shape [name] had columns that did not appear in as arguments to the current procedure. Maybe arguments are missing or maybe the name in the like part is the wrong name.  "},{"title":"CQL0202: must be a cursor, proc, table, or view 'like_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0202-must-be-a-cursor-proc-table-or-view-like_name","content":"In a DECLARE statement that declares a CURSOR LIKE some other name, the indicated name is not the name of any of the things that might have a valid shape to copy, like other cursors, procedures, tables, or views. Probably there is a typo in the name.  "},{"title":"CQL0203: cursor requires a procedure that returns a cursor with OUT 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0203-cursor-requires-a-procedure-that-returns-a-cursor-with-out-cursor_name","content":"In the DECLARE [cursor_name] CURSOR FETCH FROM CALL &lt;something&gt; form, the code is trying to create the named cursor by calling a procedure that doesn't actually produce a single row result set with the OUT statement. The procedure is valid (that would be a different error) so it's likely that the wrong procedure is being called rather than being an outright typo. Or perhaps the procedure was changed such that it no longer produces a single row result set. This form is equivalent to: DECLARE [cursor_name] LIKE procedure; FETCH [cursor_name] FROM CALL procedure(args);  It's the declaration that's failing here, not the call.  CQL0204 : Unused.  "},{"title":"CQL0205: not a cursor 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0205-not-a-cursor-name","content":"The indicated name appeared in a context where the name of a cursor was expected, but the name does not refer to a cursor.  "},{"title":"CQL0206: duplicate name in list 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0206-duplicate-name-in-list-name","content":"There are many contexts where a list of names appears in the CQL grammar and the list must not contain duplicate names. Some examples are: the column names in a JOIN ... USING(x,y,z,...) clausethe fetched variables in a FETCH [cursor] INTO x,y,z... statementthe column names listed in a common table expression CTE(x,y,z,...) as (SELECT ...)the antecedent schema region names in @declare_schema_region &lt;name&gt; USING x,y,z,... The indicated name was duplicated in such a context.  "},{"title":"CQL0207: expected a variable name for OUT or INOUT argument 'param_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0207-expected-a-variable-name-for-out-or-inout-argument-param_name","content":"In a procedure call, the indicated parameter of the procedure is an OUT or INOUT parameter but the call site doesn't have a variable in that position in the argument list. Example: declare proc foo(out x integer); -- the constant 1 cannot be used in the out position when calling foo call foo(1); '   "},{"title":"CQL0208: shared fragments cannot have any out or in/out parameters 'param_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0208-shared-fragments-cannot-have-any-out-or-inout-parameters-param_name","content":"A shared fragment will be expanded into the body of a SQL select statement, as such it can have no side-effects such as out arguments.  "},{"title":"CQL0209: proc out parameter: arg must be an exact type match (expected expected_type; found actual_type) 'param_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0209-proc-out-parameter-arg-must-be-an-exact-type-match-expected-expected_type-found-actual_type-param_name","content":"In a procedure call, the indicated parameter is in an 'out' position, it is a viable local variable but it is not an exact type match for the parameter. The type of variable used to capture out parameters must be an exact match. declare proc foo(out x integer); create proc bar(out y real) begin call foo(y); -- y is a real variable, not an integer. end;  The above produces: CQL0209: proc out parameter: arg must be an exact type match (expected integer; found real) 'y'   "},{"title":"CQL0210: proc out parameter: arg must be an exact type match (even nullability)​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0210-proc-out-parameter-arg-must-be-an-exact-type-match-even-nullability","content":"(expected expected_type; found actual_type) 'variable_name' In a procedure call, the indicated parameter is in an 'out' position, it is a viable local variable of the correct type but the nullability does not match. The type of variable used to capture out parameters must be an exact match. declare proc foo(out x integer not null); create proc bar(out y integer) begin call foo(y); -- y is nullable but foo is expecting not null. end;  The above produces: CQL0210: proc out parameter: arg must be an exact type match (even nullability) (expected integer notnull; found integer) 'y'   "},{"title":"CQL0211: procedure without trailing OUT parameter used as function 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0211-procedure-without-trailing-out-parameter-used-as-function-procedure_name","content":"In a function call, the target of the function call was a procedure, procedures can be used like functions but their last parameter must be marked out. That will be the return value. In this case the last argument was not marked as out and so the call is invalid. Example: declare proc foo(x integer); create proc bar(out y integer) begin set y := foo(); -- foo does not have an out argument at the end end;   "},{"title":"CQL0212: too few arguments provided to procedure 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0212-too-few-arguments-provided-to-procedure-name","content":"In a procedure call to the named procedure, not enough arguments were provided to make the call. One or more arguments may have been omitted or perhaps the called procedure has changed such that it now requires more arguments.  "},{"title":"CQL0213: procedure had errors, can't call. 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0213-procedure-had-errors-cant-call-procedure_name","content":"In a procedure call to the named procedure, the target of the call had compilation errors. As a consequence this call cannot be checked and therefore must be marked in error, too. Fix the errors in the named procedure.  "},{"title":"CQL0214: procedures with results can only be called using a cursor in global context 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0214-procedures-with-results-can-only-be-called-using-a-cursor-in-global-context-name","content":"The named procedure results a result set, either with the SELECT statement or the OUT statement. However it is being called from outside of any procedure. Because of this, its result cannot then be returned anywhere. As a result, at the global level the result must be capture with a cursor. Example: create proc foo() begin select * from bar; end; call foo(); -- this is not valid declare cursor C for call foo(); -- C captures the result of foo, this is ok.   "},{"title":"CQL0215: value cursors are not used with FETCH C, or FETCH C INTO 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0215-value-cursors-are-not-used-with-fetch-c-or-fetch-c-into-cursor_name","content":"In a FETCH statement of the form FETCH [cursor] or FETCH [cursor] INTO the named cursor is a value cursor. These forms of the FETCH statement apply only to statement cursors. Example:good -- value cursor shaped like a table declare C cursor for select * from bar; --ok, C is fetched from the select results fetch C;  Example: bad -- value cursor shaped like a table declare C cursor like bar; -- invalid, there is no source for fetching a value cursor fetch C; -- ok assuming bar is made up of 3 integers fetch C from values(1,2,3);  statement cursors come from SQL statements and can be fetchedvalue cursors are of a prescribed shape and can only be loaded from value sources  "},{"title":"CQL0216: FETCH variable not found 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0216-fetch-variable-not-found-cursor_name","content":"In a FETCH statement, the indicated name, which is supposed to be a cursor, is not in fact a valid name at all. Probably there is a typo in the name. Or else the declaration is entirely missing.  "},{"title":"CQL0217: number of variables did not match count of columns in cursor 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0217-number-of-variables-did-not-match-count-of-columns-in-cursor-cursor_name","content":"In a FETCH [cursor] INTO [variables] the number of variables specified did not match the number of columns in the named cursor. Perhaps the source of the cursor (a select statement or some such) has changed.  "},{"title":"CQL0218: continue must be inside of a 'loop' or 'while' statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0218-continue-must-be-inside-of-a-loop-or-while-statement","content":"The CONTINUE statement may only appear inside of looping constructs. CQL only has two LOOP FETCH ... and WHILE  "},{"title":"CQL0219: leave must be inside of a 'loop', 'while', or 'switch' statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0219-leave-must-be-inside-of-a-loop-while-or-switch-statement","content":"The LEAVE statement may only appear inside of looping constructs or the switch statement. CQL has two loop types: LOOP FETCH ... and WHILE and of course the SWITCH statement. The errant LEAVE statement is not in any of those.  "},{"title":"CQL0220: savepoint has not been mentioned yet, probably wrong 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0220-savepoint-has-not-been-mentioned-yet-probably-wrong-name","content":"In a ROLLBACK statement that is rolling back to a named savepoint, the indicated savepoint was never mentioned before. It should have appeared previously in a SAVEPOINT statement. This probably means there is a typo in the name.  "},{"title":"CQL0221: savepoint has not been mentioned yet, probably wrong 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0221-savepoint-has-not-been-mentioned-yet-probably-wrong-name","content":"In a RELEASE SAVEPOINT statement that is rolling back to a named savepoint, the indicated savepoint was never mentioned before. It should have appeared previously in a SAVEPOINT statement. This probably means there is a typo in the name.  "},{"title":"CQL0222: out cursor statement only makes sense inside of a procedure​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0222-out-cursor-statement-only-makes-sense-inside-of-a-procedure","content":"The statement form OUT [cursor_name] makes a procedure that returns a single row result set. It doesn't make any sense to do this outside of any procedure because there is no procedure to return that result. Perhaps the OUT statement was mis-placed.  "},{"title":"CQL0223: cursor was not fetched with the auto-fetch syntax 'fetch [cursor]' 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0223-cursor-was-not-fetched-with-the-auto-fetch-syntax-fetch-cursor-cursor_name","content":"The statement form OUT [cursor_name] makes a procedure that returns a single row result set that corresponds to the current value of the cursor. If the cursor never held values directly then there is nothing to return. Example: declare C cursor for select * from bar; out C; -- error C was never fetched declare C cursor for select * from bar; fetch C into x, y, z; -- error C was used to load x, y, z so it's not holding any data out C; declare C cursor for select * from bar; -- create storage in C to hold bar columns (e.g. C.x, C,y, C.z) fetch C; -- ok, C holds data out C;   "},{"title":"CQL0224: a CALL statement inside SQL may call only a shared fragment i.e. @attribute(cql:shared_fragment)​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0224-a-call-statement-inside-sql-may-call-only-a-shared-fragment-ie-attributecqlshared_fragment","content":"Inside of a WITH clause you can create a CTE by calling a shared fragment like so: WITH my_shared_something(*) AS (CALL shared_proc(5)) SELECT * from my shared_something;  or you can use a nested select expression like  SELECT * FROM (CALL shared_proc(5)) as T;  However shared_proc must define a shareable fragment, like so: @attribute(cql:shared_fragment) create proc shared_proc(lim_ integer) begin select * from somewhere limit lim_; end;  Here the target of the CALL is not a shared fragment.  "},{"title":"CQL0225: switching to previous schema validation mode must be outside of any proc​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0225-switching-to-previous-schema-validation-mode-must-be-outside-of-any-proc","content":"The @previous_schema directive says that any schema that follows should be compared against what was declared before this point. This gives CQL the opportunity to detect changes in schema that are not supportable. The previous schema directive must be outside of any stored procedure. Example: @previous_schema; -- ok here create proc foo() begin @previous schema; -- nope end;   "},{"title":"CQL0226: schema upgrade declaration must be outside of any proc​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0226-schema-upgrade-declaration-must-be-outside-of-any-proc","content":"The @schema_upgrade_script directive tells CQL that the code that follows is intended to upgrade schema from one version to another. This kind of script is normally generated by the --rt schema_upgrade option discussed elsewhere. When processing such a script, a different set of rules are used for DDL analysis. In particular, it's normal to declare the final versions of tables but have DDL that creates the original version and more DDL to upgrade them from wherever they are to the final version (as declared). Ordinarily these duplicate definitions would produce errors. This directive allows those duplications. This error is reporting that the directive happened inside of a stored procedure, this is not allowed. Example: @schema_upgrade_script; -- ok here create proc foo() begin @schema_upgrade_script; -- nope end;   "},{"title":"CQL0227: schema upgrade declaration must come before any tables are declared​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0227-schema-upgrade-declaration-must-come-before-any-tables-are-declared","content":"The @schema_upgrade_script directive tells CQL that the code that follows is intended to upgrade schema from one version to another. This kind of script is normally generated by the --rt schema_upgrade option discussed elsewhere. When processing such a script, a different set of rules are used for DDL analysis. In particular, it's normal to declare the final versions of tables but have DDL that creates the original version and more DDL to upgrade them from wherever they are to the final version (as declared). Ordinarily these duplicate definitions would produce errors. This directive allows those duplications. In order to do its job properly the directive must come before any tables are created with DDL. This error tells you that the directive came too late in the stream. Or perhaps there were two such directives and one is late in the stream.  "},{"title":"CQL0228: schema upgrade version must be a positive integer​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0228-schema-upgrade-version-must-be-a-positive-integer","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that the version you are trying to view is not a positive integer version (e.g version -2)  "},{"title":"CQL0229: schema upgrade version declaration may only appear once​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0229-schema-upgrade-version-declaration-may-only-appear-once","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that a second @schema_upgrade_version directive has been found.  "},{"title":"CQL0230: schema upgrade version declaration must be outside of any proc​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0230-schema-upgrade-version-declaration-must-be-outside-of-any-proc","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that the @schema_upgrade_version directive was found inside of a stored procedure. This is not allowed.  "},{"title":"CQL0231: schema upgrade version declaration must come before any tables are declared​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0231-schema-upgrade-version-declaration-must-come-before-any-tables-are-declared","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that the @schema_upgrade_version directive came after tables were already declared. This is not allowed, the directive must come before any DDL.  "},{"title":"CQL0232: nested select expression must return exactly one column​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0232-nested-select-expression-must-return-exactly-one-column","content":"In a SELECT expression like set x := (select id from bar) the select statement must return exactly one column as in the example provided. Note that a runtime error will ensue if the statement returns zero rows, or more than one row, so this form is very limited. To fix this error change your select statement to return exactly one column. Consider how many rows you will get very carefully also, that cannot be checked at compile time.  "},{"title":"CQL0233: procedure previously declared as schema upgrade proc, it can have no args 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0233-procedure-previously-declared-as-schema-upgrade-proc-it-can-have-no-args-procedure_name","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive that procedure will be called during schema migration with no context available. Therefore, the schema migration proc is not allowed to have any arguments.  "},{"title":"CQL0234: autodrop annotation can only go on a procedure that returns a result set 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0234-autodrop-annotation-can-only-go-on-a-procedure-that-returns-a-result-set-procedure_name","content":"The named procedure has the autodrop annotation (to automatically drop a temporary table) but the procedure in question doesn't return a result set so it has no need of the autodrop feature. The purpose that that feature is to drop the indicated temporary tables once all the select results have been fetched.  "},{"title":"CQL0235: too many arguments provided to procedure 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0235-too-many-arguments-provided-to-procedure-procedure_name","content":"In a CALL statement, or a function call, the named procedure takes fewer arguments than were provided. This error might be due to some copy/pasta going on or perhaps the argument list of the procedure/function changed to fewer items. To fix this, consult the argument list and adjust the call accordingly.  "},{"title":"CQL0236: autodrop annotation can only go on a procedure that uses the database 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0236-autodrop-annotation-can-only-go-on-a-procedure-that-uses-the-database-name","content":"The named procedure has the autodrop annotation (to automatically drop a temporary table) but the procedure in question doesn't even use the database at all, much less the named table. This annotation is therefore redundant.  "},{"title":"CQL0237: strict FK validation requires that some ON UPDATE option be selected for every foreign key​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0237-strict-fk-validation-requires-that-some-on-update-option-be-selected-for-every-foreign-key","content":"@enforce_strict has been use to enable strict foreign key enforcement. When enabled every foreign key must have an action for the ON UPDATE rule. You can specify NO ACTION but you can't simply leave the action blank.  "},{"title":"CQL0238: strict FK validation requires that some ON DELETE option be selected for every foreign key​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0238-strict-fk-validation-requires-that-some-on-delete-option-be-selected-for-every-foreign-key","content":"@enforce_strict has been use to enable strict foreign key enforcement. When enabled every foreign key must have an action for the ON DELETE rule. You can specify NO ACTION but you can't simply leave the action blank.  "},{"title":"CQL0239: 'annotation' column does not exist in result set 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0239-annotation-column-does-not-exist-in-result-set-column_name","content":"The @attribute(cql:identity=(col1, col2, ...)) form has been used to list the identity columns of a stored procedures result set. These columns must exist in the result set and they must be unique. The indicated column name is not part of the result of the procedure that is being annotated. The @attribute(cql:vault_sensitive=(col1, col2, ...) form has been used to list the columns of a stored procedures result set. These columns must exist in the result set. The indicated column name will be encoded if they are sensitive and the cursor that produced the result_set is a DML.  "},{"title":"CQL0240: identity annotation can only go on a procedure that returns a result set 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0240-identity-annotation-can-only-go-on-a-procedure-that-returns-a-result-set-procedure_name","content":"The @attribute(cql:identity=(col1, col2,...)) form has been used to list the identity columns of a stored procedures result set. These columns must exist in the result set and they must be unique. In this case, the named procedure doesn't even return a result set. Probably there is a copy/pasta going on. The identity attribute can likely be removed.  "},{"title":"CQL0241: CONCAT may only appear in the context of SQL statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0241-concat-may-only-appear-in-the-context-of-sql-statement","content":"The SQLite || operator has complex string conversion rules making it impossible to faithfully emulate. Since there is no helper function for doing concatenations, CQL choses to support this operator only in contexts where it will be evaluated by SQLite. That is, inside of some SQL statement. Examples: declare X text; set X := 'foo' || 'bar'; -- error set X := (select 'foo' || 'bar'); -- ok  If concatenation is required in some non-sql context, use the (select ..) expression form to let SQLite do the evaluation.  "},{"title":"CQL0242: lossy conversion from type 'type'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0242-lossy-conversion-from-type-type","content":"There is an explicit (set x := y) or implicit assignment (e.g. conversion of a parameter) where the storage for the target is a smaller numeric type than the expression that is being stored. This usually means a variable that should have been declared LONG is instead declared INTEGER or that you are typing to pass a LONG to a procedure that expects an INTEGER  "},{"title":"CQL0243: blob operand must be converted to string first in '||'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0243-blob-operand-must-be-converted-to-string-first-in-","content":"We explicitly do not wish to support string concatenation for blobs that holds non-string data. If the blob contains string data, make your intent clear by converting it to string first using CAST before doing the concatenation.  "},{"title":"CQL0244: unknown schema region 'region'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0244-unknown-schema-region-region","content":"In a @declare_schema_region statement one of the USING regions is not a valid region name. Or in @begin_schema_region the region name is not valid. This probably means there is a typo in your code.  "},{"title":"CQL0245: schema region already defined 'region'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0245-schema-region-already-defined-region","content":"The indicated region was previously defined, it cannot be redefined.  "},{"title":"CQL0246: schema regions do not nest; end the current region before starting a new one​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0246-schema-regions-do-not-nest-end-the-current-region-before-starting-a-new-one","content":"Another @begin_schema_region directive was encountered before the previous @end_schema_region was found.  "},{"title":"CQL0247: you must begin a schema region before you can end one​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0247-you-must-begin-a-schema-region-before-you-can-end-one","content":"An @end_schema_region directive was encountered but there was no corresponding @begin_schema_region directive.  "},{"title":"CQL0248: schema region directives may not appear inside of a procedure​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0248-schema-region-directives-may-not-appear-inside-of-a-procedure","content":"All of the *_schema_region directives must be used at the top level of your program, where tables are typically declared. They do not belong inside of procedures. If you get this error, move the directive out of the procedure near the DDL that it affects.  "},{"title":"CQL0249: function is not a table-valued-function 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0249-function-is-not-a-table-valued-function-function_name","content":"The indicated identifier appears in the context of a table, it is a function, but it is not a table-valued function. Either the declaration is wrong (use something like declare select function foo(arg text) (id integer, t text)) or the name is wrong. Or both.  "},{"title":"CQL0250: table-valued function not declared 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0250-table-valued-function-not-declared-function_name","content":"In a select statement, there is a reference to the indicated table-valued-function. For instance: -- the above error happens if my_function has not been declared -- as a table valued function select * from my_function(1,2,3);  However , my_function has not been declared as a function at all. A correct declaration might look like this: declare select function my_function(a int, b int, c int) (x int, y text);  Either there is a typo in the name or the declaration is missing, or both...  "},{"title":"CQL0251: fragment must end with exactly 'SELECT * FROM CTE'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0251-fragment-must-end-with-exactly-select--from-cte","content":"Query fragments have an exact prescription for their shape. This prescription includes select * from CTE as the final query where CTE is the common table expression that they define. This the error message includes the specific name that is required in this context.  "},{"title":"CQL0252: @PROC literal can only appear inside of procedures​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0252-proc-literal-can-only-appear-inside-of-procedures","content":"An @PROC literal was used outside of any procedure. It cannot be resolved if it isn't inside a procedure.  "},{"title":"CQL0253: base fragment must have only a single CTE named the same as the fragment 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0253-base-fragment-must-have-only-a-single-cte-named-the-same-as-the-fragment-name","content":"Query fragments have an exact prescription for their shape. This prescription includes select * from CTE where CTE is the single common table expression with the same name as the base query. This error says that the final select came from something other than the single CTE that is the base name or there was more than one CTE in the fragment. You can also get this error if you have an extension fragment but you accidentally marked it as a base fragment.  "},{"title":"CQL0254: switching to previous schema validation mode not allowed if @schema_upgrade_version was used​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0254-switching-to-previous-schema-validation-mode-not-allowed-if-schema_upgrade_version-was-used","content":"When authoring a schema migration script (a stored proc named in an @create or @delete annotation) you must create that procedure in a file that is marked with @schema_upgrade_verison specifying the schema version it is upgrading. If you do this, then the proc (correctly) only sees the schema as it existed at that version. However that makes the schema unsuitable for analysis using @previous_schema because it could be arbitrarily far in the past. This error prevents you from combining those features. Previous schema validation should only happen against the current schema.  "},{"title":"CQL0255: fragment name is not a previously declared base fragment 'bad_fragment_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0255-fragment-name-is-not-a-previously-declared-base-fragment-bad_fragment_name","content":"In an extension or assembly fragment declaration, the specified base fragment name has not been previously defined and that is not allowed. Probably there is a typo, or the declarations are in the wrong order. The base fragment has to come first.  "},{"title":"CQL0256: fragment name conflicts with existing base fragment 'NAME'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0256-fragment-name-conflicts-with-existing-base-fragment-name","content":"Extension query fragment can only be created with a custom procedure name different from all existing base fragment names otherwise we throw this error.  "},{"title":"CQL0257: argument must be a string or numeric in 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0257-argument-must-be-a-string-or-numeric-in-function","content":"The indicated function (usually min or max) only works on strings and numerics. NULL literals, blobs, or objects are not allowed in this context.  "},{"title":"CQL0258: extension fragment must add exactly one CTE; found extra named 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0258-extension-fragment-must-add-exactly-one-cte-found-extra-named-name","content":"The extension fragment includes more than one additional CTE, it must have exactly one. In the following example, ext2 is not valid, you have to stop at ext1 -- example bad extension fragment @attribute(cql:extension_fragment=core) create proc test_bad_extension_fragment_three() begin with core(x,y,z) as (select 1,nullable(&quot;a&quot;),nullable(3L)), ext1(x,y,z,a) as (select core.*, extra1.* from core left outer join extra1), ext2(x,y,z,b) as (select core.*, extra2.* from core left outer join extra2) select * from ext2; end;   "},{"title":"CQL0259: extension fragment CTE must select T.* from base CTE​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0259-extension-fragment-cte-must-select-t-from-base-cte","content":"For the select expression in extension fragment, it must include all columns from the base table. This error indicates the select expression doesn't select from the base table. It should look like this select core.*, ... from core  Here core is the name of its base table.  "},{"title":"CQL0260: extension fragment CTE must be a simple left outer join from 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0260-extension-fragment-cte-must-be-a-simple-left-outer-join-from-table_name","content":"Extension fragments may add columns to the result, to do this without lose any rows you must always left outer join to the new data that you with to include. There is a specific prescription for this. It has to look like this: @attribute(cql:extension_fragment=core) create proc an_extension() begin with core(x,y,z) as (select 1,nullable(&quot;a&quot;),nullable(3L)), ext1(x,y,z,a) as (select core.*, extra_column from core left outer join extra_stuff), select * from ext1; end;  Here extension ext1 is adding extra_column which came from extra_stuff. There could have been any desired join condition or indeed any join expression at all but it has to begin with from core left outer join so that all the core columns will be present and now rows can be removed.  "},{"title":"CQL0261: cursor did not originate from a SQLite statement, it only has values 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0261-cursor-did-not-originate-from-a-sqlite-statement-it-only-has-values-cursor_name","content":"The form:  SET [name] FROM CURSOR [cursor_name]  Is used to wrap a cursor in an object so that it can be returned for forwarded. This is the so-called &quot;boxing&quot; operation on the cursor. The object can then be &quot;unboxed&quot; later to make a cursor again. However the point of this is to keep reading forward on the cursor perhaps in another procedure. You can only read forward on a cursor that has an associated SQLite statement. That is the cursor was created with something like this  DECLARE [name] CURSOR FOR SELECT ... | CALL ...  If the cursor isn't of this form it's just values, you can't move it forward and so &quot;boxing&quot; it is of no value. Hence not allowed. You can return the cursor values with OUT instead.  "},{"title":"CQL0262: LIKE ... ARGUMENTS used on a procedure with no arguments 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0262-like--arguments-used-on-a-procedure-with-no-arguments-procedure_name","content":"The LIKE [procedure] ARGUMENTS form creates a shape for use in a cursor or procedure arguments. The indicated name is a procedure with no arguments so it cannot be used to make a shape.  "},{"title":"CQL0263: non-ANSI joins are forbidden if strict join mode is enabled.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0263-non-ansi-joins-are-forbidden-if-strict-join-mode-is-enabled","content":"You can enable strict enforcement of joins to avoid the form select * from A, B;  which sometimes confuses people (the above is exactly the same as select * from A inner join B on 1;  Usually there are constraints on the join also in the WHERE clause but there don't have to be. @enforce_strict join turns on this mode.  "},{"title":"CQL0264: duplicate assembly fragments of base fragment​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0264-duplicate-assembly-fragments-of-base-fragment","content":"For each base fragment, it only allows to exist one assembly fragment of that base fragment.  "},{"title":"CQL0265: assembly fragment can only have one CTE​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0265-assembly-fragment-can-only-have-one-cte","content":"Assembly fragment can only have one base table CTE.  "},{"title":"CQL0266: extension fragment name conflicts with existing fragment​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0266-extension-fragment-name-conflicts-with-existing-fragment","content":"Two or more extension fragments share the same name.  "},{"title":"CQL0267: extension fragments of same base fragment share the same cte column​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0267-extension-fragments-of-same-base-fragment-share-the-same-cte-column","content":"Two or more extension fragments which have the same base fragments share the same name for one of their unique columns. E.g. the base table is core(x,y) and one extension table is plugin_one(x,y,z) and another is plugin_two(x,y,z). Here, z in both extension fragments share the same name but may refer to different values.  "},{"title":"CQL0268: extension/assembly fragment must have the CTE columns same as the base fragment​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0268-extensionassembly-fragment-must-have-the-cte-columns-same-as-the-base-fragment","content":"Extension and assembly fragments have an exact prescription for their shape. For each extension and assembly fragment, the first CTE must be a stub for their base table. This error means this stub in extension/assembly fragment differs from the definition of the base table.  "},{"title":"CQL0269: at least part of this unique key is redundant with previous unique keys​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0269-at-least-part-of-this-unique-key-is-redundant-with-previous-unique-keys","content":"The new unique key must have at least one column that is not in a previous key AND it must not have all the columns from any previous key. e.g: create table t1 ( a int, b long, c text, d real, UNIQUE (a, b), UNIQUE (a, b, c), -- INVALID (a, b) is already unique key UNIQUE (b, a), -- INVALID (b, a) is the same as (a, b) UNIQUE (c, d, b, a), -- INVALID subset (b, a) is already unique key UNIQUE (a), -- INVALID a is part of (a, b) key UNIQUE (a, c), -- VALID UNIQUE (d), -- VALID UNIQUE (b, d) -- VALID );   "},{"title":"CQL0270: use FETCH FROM for procedures that returns a cursor with OUT 'cursor'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0270-use-fetch-from-for-procedures-that-returns-a-cursor-with-out-cursor","content":"If you are calling a procedure that returns a value cursor (using OUT) then you accept that cursor using the pattern DECLARE C CURSOR FETCH FROM CALL foo(...);  The pattern DECLARE C CURSOR FOR CALL foo(...);  Is used for procedures that provide a full select statement. Note that in the former cause you don't then use fetch on the cursor. There is at most one row anyway and it's fetched for you so a fetch would be useless. In the second case you fetch as many rows as there are and/or you want.  "},{"title":"CQL0271: OFFSET clause may only be used if LIMIT is also present​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0271-offset-clause-may-only-be-used-if-limit-is-also-present","content":"select * from foo offset 1;  Is not supported by SQLite. OFFSET may only be used if LIMIT is also present. Also, both should be small because offset is not cheap. There is no way to do offset other than to read and ignore the indicated number of rows. So something like offset 1000 is always horrible.  "},{"title":"CQL0272: columns referenced in the foreign key statement should match exactly a unique key in parent table​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0272-columns-referenced-in-the-foreign-key-statement-should-match-exactly-a-unique-key-in-parent-table","content":"If you're creating a table t2 with foreign keys on table t1, then the set of t1's columns reference in the foreign key statement for table t2 should be: A primary key in t1 e.g: create table t1(a text primary key); create table t2(a text primary key, foreign key(a) references t1(a));  A unique key in t1 e.g: create table t1(a text unique); create table t2(a text primary key, foreign key(a) references t1(a));  A group of unique key in t1 e.g: create table t1(a text, b int, unique(a, b)); create table t2(a text, b int, foreign key(a, b) references t1(a, b));  A group of primary key in t1 e.g: create table t1(a text, b int, primary key(a, b)); create table t2(a text, b int, foreign key(a, b) references t1(a, b));  A unique index in t1 e.g: create table t1(a text, b int); create unique index unq on t1(a, b); create table t2(a text, b int, foreign key(a, b) references t1(a, b));   "},{"title":"CQL0273: autotest attribute has incorrect format (...) in 'dummy_test'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0273-autotest-attribute-has-incorrect-format--in-dummy_test","content":"In a cql:autotest annotation, the given dummy_test info (table name, column name, column value) has incorrect format.  "},{"title":"CQL0274: autotest attribute 'dummy_test' has non existent table​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0274-autotest-attribute-dummy_test-has-non-existent-table","content":"In a cql:autotest annotation, the given table name for dummy_test attribute does not exist.  "},{"title":"CQL0275: autotest attribute 'dummy_test' has non existent column​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0275-autotest-attribute-dummy_test-has-non-existent-column","content":"In a cql:autotest annotation, the given column name for dummy_test attribute does not exist.  "},{"title":"CQL0276: autotest attribute 'dummy_test' has invalid value type in​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0276-autotest-attribute-dummy_test-has-invalid-value-type-in","content":"In a cql:autotest annotation, the given column value's type for dummy_test attribute does not match the column type.  "},{"title":"CQL0277: autotest has incorrect format​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0277-autotest-has-incorrect-format","content":"In a cql:autotest annotation, the format is incorrect.  "},{"title":"CQL0278: autotest attribute name is not valid​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0278-autotest-attribute-name-is-not-valid","content":"In a cql:autotest annotation, the given attribute name is not valid.  "},{"title":"CQL0279: columns referenced in an UPSERT conflict target must exactly match a unique key the target table​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0279-columns-referenced-in-an-upsert-conflict-target-must-exactly-match-a-unique-key-the-target-table","content":"If you're doing an UPSERT on table T, the columns listed in the conflict target should be: A primary key in TA unique key in TA group of unique key in TA group of primary key in TA unique index in T  "},{"title":"CQL0280: upsert statement requires a where clause if the insert clause uses select​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0280-upsert-statement-requires-a-where-clause-if-the-insert-clause-uses-select","content":"When the INSERT statement to which the UPSERT is attached takes its values from a SELECT statement, there is a potential parsing ambiguity. The SQLite parser might not be able to tell if the ON keyword is introducing the UPSERT or if it is the ON clause of a join. To work around this, the SELECT statement should always include a WHERE clause, even if that WHERE clause is just WHERE 1 (always true). Note: The CQL parser doesn't have this ambiguity because it treats &quot;ON CONFLICT&quot; as a single token so this is CQL reporting that SQLite might have trouble with the query as written. e.g: insert into foo select id from bar where 1 on conflict(id) do nothing;   "},{"title":"CQL0281: upsert statement does not include table name in the update statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0281-upsert-statement-does-not-include-table-name-in-the-update-statement","content":"The UPDATE statement of and UPSERT should not include the table name because the name is already known from the INSERT statement part of the UPSERT e.g: insert into foo select id from bar where 1 on conflict(id) do update set id=10;   "},{"title":"CQL0282: update statement require table name​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0282-update-statement-require-table-name","content":"The UPDATE statement should always include a table name except if the UPDATE statement is part of an UPSERT statement. e.g: update foo set id=10; insert into foo(id) values(1) do update set id=10;   "},{"title":"CQL0283: upsert syntax only support INSERT INTO​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0283-upsert-syntax-only-support-insert-into","content":"The INSERT statement part of an UPSERT statement can only uses INSERT INTO ... e.g: insert into foo(id) values(1) on conflict do nothing; insert into foo(id) values(1) on conflict do update set id=10;   "},{"title":"CQL0284: ad hoc schema migration directive must provide a procedure to run​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0284-ad-hoc-schema-migration-directive-must-provide-a-procedure-to-run","content":"@schema_ad_hoc_migration must provide both a version number and a migrate procedure name. This is unlike the other version directives like @create where the version number is optional. This is because the whole point of this migrator is to invoke a procedure of your choice.  "},{"title":"CQL0285: ad hoc schema migration directive version number changed 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0285-ad-hoc-schema-migration-directive-version-number-changed-procedure_name","content":"In @schema_ad_hoc_migration you cannot change the version number of the directive once it has been added to the schema because this could cause inconsistencies when upgrading. You can change the body of the method if you need to but this is also not recommended because again there could be inconsistencies. However careful replacement and compensation is possible. This is like going to 110% on the reactor... possible, but not recommended.  "},{"title":"CQL0286: ad hoc schema migration directive was removed; this is not allowed 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0286-ad-hoc-schema-migration-directive-was-removed-this-is-not-allowed-procedure_name","content":"An @schema_ad_hoc_migration cannot be removed because it could cause inconsistencies on upgrade. You can change the body of the method if you need to but this is also not recommended because again there could be inconsistencies. However careful replacement and compensation is possible. This is like going to 110% on the reactor... possible, but not recommended.  "},{"title":"CQL0287: extension/assembly fragment must add stub \"​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0287-extensionassembly-fragment-must-add-stub-","content":" "},{"title":"CQL0288: extension/assembly fragment stub for base CTE column must be \"​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0288-extensionassembly-fragment-stub-for-base-cte-column-must-be-","content":" "},{"title":"CQL0289: upsert statement are forbidden if strict upsert statement mode is enabled​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0289-upsert-statement-are-forbidden-if-strict-upsert-statement-mode-is-enabled","content":"@enforce_strict has been use to enable strict upsert statement enforcement. When enabled all sql statement should not use the upsert statement. This is because sqlite version running in some iOS and Android version is old. Upsert statement was added to sqlite in the version 3.24.0 (2018-06-04).  "},{"title":"CQL0290: fragments can only have one statement in the statement list and it must be a WITH...SELECT​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0290-fragments-can-only-have-one-statement-in-the-statement-list-and-it-must-be-a-withselect","content":"All of the extendable query fragment types consist of a procedure with exactly one statement and that statement is a WITH...SELECT statement. If you have more than one statement or some other type of statement you'll get this error.  "},{"title":"CQL0291: region links into the middle of a deployable region; you must point to the root of <deployable_region> not into the middle: <error_region>​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0291-region-links-into-the-middle-of-a-deployable-region-you-must-point-to-the-root-of-deployable_region-not-into-the-middle-error_region","content":"Deployable regions have an &quot;inside&quot; that is in some sense &quot;private&quot;. In order to keep the consistent (and independently deployable) you can't peek into the middle of such a region, you have to depend on the root (i.e. &lt;deployable_region&gt; itself). This allows the region to remain independently deployable and for its internal logical regions to be reorganized in whatever manner makes sense. To fix this error probably you should change error_region so that it depends directly on deployable_region  "},{"title":"CQL0292: explain statement is only available in dev mode because its result set may vary between sqlite versions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0292-explain-statement-is-only-available-in-dev-mode-because-its-result-set-may-vary-between-sqlite-versions","content":"The EXPLAIN statement is intended for interactive debugging only. It helps engineer understand how Sqlite will execute their query and the cost attached to it. This is why this grammar is only available in dev mode in CQL and should never be used in production.  "},{"title":"CQL0293: only [EXPLAIN QUERY PLAN ...] statement is supported​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0293-only-explain-query-plan--statement-is-supported","content":"CQL only support [EXPLAIN QUERY PLAN stmt] sql statement.  "},{"title":"CQL0294: window function invocations can only appear in the select list of a select statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0294-window-function-invocations-can-only-appear-in-the-select-list-of-a-select-statement","content":"Not all SQLite builtin function can be used as a window function.  "},{"title":"CQL0295: window name is not defined​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0295-window-name-is-not-defined","content":"Window name referenced in the select list should be defined in the Window clause of the same select statement.  "},{"title":"CQL0296: window name definition is not used​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0296-window-name-definition-is-not-used","content":"Window name defined in the window clause of a select statement should always be used within that statement.  "},{"title":"CQL0297: FROM [shape] is redundant if column list is empty​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0297-from-shape-is-redundant-if-column-list-is-empty","content":"In this form: insert into YourTable() FROM your_cursor; The () means no columns are being specified, the cursor will never be used. The only source of columns is maybe dummy data (if it was specified) or the default values or null. In no case will the cursor be used. If you really want this use FROM VALUES() and don't implicate a cursor or an argument bundle.  "},{"title":"CQL0298: cannot read from a cursor without fields 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0298-cannot-read-from-a-cursor-without-fields-cursor_name","content":"The cursor in question has no storage associated with it. It was loaded with something like: fetch C into x, y, z; You can only use a cursor as a source of data if it was fetched with its own storage like fetch C This results in a structure for the cursor. This gives you C.x, C.y, C.z etc. If you fetched the cursor into variables then you have to use the variables for any inserting.  "},{"title":"CQL0299: [cursor] has too few fields, 'shape_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0299-cursor-has-too-few-fields-shape_name","content":"The named shape was used in a fetch statement but the number of columns fetched is smaller than the number required by the statement we are processing. If you need to use the cursor plus some other data then you can't use this form, you'll have to use each field individually like from values(C.x, C.y, C.z, other_stuff). The shape with too few fields might be the source or the target of the statement.  "},{"title":"CQL0300: argument must be an integer (between 1 and max integer) in function 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0300-argument-must-be-an-integer-between-1-and-max-integer-in-function-function_name","content":"The argument of the function should be an integer.  "},{"title":"CQL0301: second argument must be an integer (between 0 and max integer) in function 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0301-second-argument-must-be-an-integer-between-0-and-max-integer-in-function-function_name","content":"The second argument of the function should be an integer between 0 and INTEGER_MAX.  "},{"title":"CQL0302: first and third arguments must be compatible in function 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0302-first-and-third-arguments-must-be-compatible-in-function-function_name","content":"The first and third arguments of the function have to be of the same type because the third argument provide a default value in cause the first argument is NULL.  "},{"title":"CQL0303: second argument must be an integer between 1 and max integer in function 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0303-second-argument-must-be-an-integer-between-1-and-max-integer-in-function-function_name","content":"The second argument of the function must be and integer between 1 and INTEGER_MAX.  "},{"title":"CQL0304: DISTINCT may only be used with one explicit argument in an aggregate function​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0304-distinct-may-only-be-used-with-one-explicit-argument-in-an-aggregate-function","content":"The keyword DISTINCT can only be used with one argument in an aggregate function.  "},{"title":"CQL0305: DISTINCT may only be used in function that are aggregated or user defined​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0305-distinct-may-only-be-used-in-function-that-are-aggregated-or-user-defined","content":"Only aggregated functions and user defined functions can use the keyword DISTINCT. Others type of functions are not allowed to use it.  "},{"title":"CQL0306: FILTER clause may only be used in function that are aggregated or user defined​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0306-filter-clause-may-only-be-used-in-function-that-are-aggregated-or-user-defined","content":" "},{"title":"CQL0307: return statement should be in a procedure and not at the top level​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0307-return-statement-should-be-in-a-procedure-and-not-at-the-top-level","content":"There are basically two checks here both of which have to do with the &quot;nesting level&quot; at which the return occurs. A loose return statement (not in a procedure) is meaningless so that produce an error. There is nothing to return from. If the return statement is not inside of an &quot;if&quot; or something like that then it will run unconditionally. Nothing should follow the return (see CQL0308) so if we didn't fall afoul of CQL0308 and we're at the top level then the return is the last thing in the proc, in which case it is totally redundant. Both these situations produce an error.  "},{"title":"CQL0308: statement should be the last thing in a statement list​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0308-statement-should-be-the-last-thing-in-a-statement-list","content":"Control flow will exit the containing procedure after a return statement, so any statements that follow in its statement list will certainly not run. So the return statement must be the last statement, otherwise there are dead/unreachable statements which is most likely done by accident. To fix this probably the things that came after the return should be deleted. Or alternately there was a condition on the return that should have been added but wasn't, so the return should have been inside a nested statement list (like the body of an if maybe).  "},{"title":"CQL0309: new table must be added with @create([number]) or later 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0309-new-table-must-be-added-with-createnumber-or-later-table_name","content":"The indicated table was newly added -- it is not present in the previous schema. However the version number it was added at is in the past. The new table must appear at the current schema version or later. That version is provided in the error message. To fix this, change the @create annotation on the table to be at the indicated version or later.  "},{"title":"CQL0310: new column must be added with @create([number]) or later\" 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0310-new-column-must-be-added-with-createnumber-or-later-column_name","content":"The indicated column was newly added -- it is not present in the previous schema. However the version number it was added at is in the past. The new column must appear at the current schema version or later. That version is provided in the error message. To fix this, change the @create annotation on the table to be at the indicated version or later.  "},{"title":"CQL0311: object's deployment region changed from '<previous_region>' to '<current_region>' 'object_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0311-objects-deployment-region-changed-from-previous_region-to-current_region-object_name","content":"An object may not move between deployment regions, because users of the schema will depend on its contents. New objects can be added to a deployment region but nothing can move from one region to another. The indicated object appears to be moving.  "},{"title":"CQL0312: window function invocation are forbidden if strict window function mode is enabled​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0312-window-function-invocation-are-forbidden-if-strict-window-function-mode-is-enabled","content":"@enforce_strict has been use to enable strict window function enforcement. When enabled all sql statement should not invoke window function. This is because sqlite version running in some iOS version is old. Window function was added to SQLite in the version 3.25.0 (2018-09-15).  "},{"title":"CQL0313: blob literals may only appear in the context of a SQL statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0313-blob-literals-may-only-appear-in-the-context-of-a-sql-statement","content":"CQL (currently) limits use of blob literals to inside of SQL fragments. There's no easy way to get a blob constant variable into the data section so any implementation would be poor. These don't come up very often in any case so this is a punt basically. You can fake it with (select x'1234') which makes it clear that you are doing something expensive. This is not recommended. Better to pass the blob you need into CQL rather than cons it from a literal. Within SQL it's just text and SQLite does the work as usual so that poses no problems. And of course non-literal blobs (as args) work find and are bound as usual.  "},{"title":"CQL0314: select function does not require a declaration, it is a CQL built-in​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0314-select-function-does-not-require-a-declaration-it-is-a-cql-built-in","content":"CQL built-in function does not require a select function declaration. You can used it directly in your SQL statement.  "},{"title":"CQL0315: mandatory column with no default value in INSERT INTO name DEFAULT VALUES statement.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0315-mandatory-column-with-no-default-value-in-insert-into-name-default-values-statement","content":"Columns on a table must have default value or be nullable in order to use INSERT INTO &lt;table&gt; DEFAULT VALUES statement.  "},{"title":"CQL0316: upsert-clause is not compatible with DEFAULT VALUES​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0316-upsert-clause-is-not-compatible-with-default-values","content":"INSERT statement with DEFAULT VALUES can not be used in a upsert statement. This form is not supported by SQLite.  "},{"title":"CQL0317: char function arguments must be integer​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0317-char-function-arguments-must-be-integer","content":"All parameters of the built-In scalar CQL functions char(...) must be of type integer.  "},{"title":"CQL0318: more than one fragment annotation on procedure 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0318-more-than-one-fragment-annotation-on-procedure-procedure_name","content":"The indicated procedure has several cql:*_fragment annotations such as cql:base_fragment and cql:extension_fragment. You can have at most one of these. example: @attribute(cql:assembly_fragment=foo) @attribute(cql:base_fragment=goo) create proc mixed_frag_types3(id_ integer) begin ... end;   "},{"title":"CQL0319: name of the assembly procedure must match the name of the base fragment 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0319-name-of-the-assembly-procedure-must-match-the-name-of-the-base-fragment-procedure_name","content":"The name of the procedure that carries the assembly attribute (cql:assembly_fragment) has to match the name of the base fragment. This is because the code that is generated for the extension fragments refers to some shared code that is generated in the assembly fragment. If the assembly fragment were allowed to have a distinct name the linkage could never work. example: -- correct example -- note: 'foo' must be a valid base fragment, declared elsewhere @attribute(cql:assembly_fragment=foo) create proc foo(id_ integer) begin ... end; -- incorrect example @attribute(cql:assembly_fragment=foo) create proc bar(id_ integer) begin ... end;   "},{"title":"CQL0320: extension fragment CTE must have a FROM clause and no other top level clauses 'frag_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0320-extension-fragment-cte-must-have-a-from-clause-and-no-other-top-level-clauses-frag_name","content":"In the extension fragment form that uses LEFT OUTER JOIN to add columns you cannot include top level restrictions/changes like WHERE, ORDER BY, LIMIT and so forth. Any of these would remove or reorder the rows from the core fragment and that is not allowed, you can only add columns. Hence you must have a FROM clause and you can have no other top level clauses. You can use any clauses you like in a nested select to get your additional columns. Note: you could potentially add rows with a LEFT OUTER JOIN and a generous ON clause. That's allowed but not recommended. The ON clause can't be forbidden because it's essential in the normal case.  "},{"title":"CQL0321: migration proc not allowed on object 'object_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0321-migration-proc-not-allowed-on-object-object_name","content":"The indicated name is an index or a trigger. These objects may not have a migration script associated with them when they are deleted. The reason for this is that both these types of objects are attached to a table and the table itself might be deleted. If the table is deleted it becomes impossible to express even a tombstone for the deleted trigger or index without getting errors. As a consequence the index/trigger must be completely removed. But if there had been a migration procedure on it then some upgrade sequences would have run it, but others would not (anyone who upgraded after the table was deleted would not get the migration procedure). To avoid this problem, migration procedures are not allowed on indices and triggers.  "},{"title":"CQL0322: fragment parameters must be exactly '[arguments]' 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0322-fragment-parameters-must-be-exactly-arguments-procedure_name","content":"The named procedure is an extension fragment or an assembly fragment. It must have exactly the same arguments as the base fragment. These arguments are provided. Recall that the code for the procedure that does the select comes from the assembly fragment, so its arguments are in some sense the only ones that matter. But the extension fragments are also allowed to use the arguments. Of course we must ensure that the extension fragments do not use any arguments that aren't in the assembly, and so the only choice we have is to make sure the extensions conform to the base. And so for that to work the assembly also has to conform to the base. So the base fragment must define the args for all the other fragments. You could imagine a scheme where the extension fragments are allowed to use a subset of the parameters defined in the base but if that were the case you might have names that mean different things in different fragments and then you could get errors or different semantics when the fragments were assembled. To avoid all of these problems, and for simplicity, we demand that the arguments of all fragments match exactly.  "},{"title":"CQL0323: calls to undeclared procedures are forbidden; declaration missing or typo 'procedure'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0323-calls-to-undeclared-procedures-are-forbidden-declaration-missing-or-typo-procedure","content":"If you get this error it means that there is a typo in the name of the procedure you are trying to call, or else the declaration for the procedure is totally missing. Maybe a necessary #include needs to be added to the compiland. Previously if you attempted to call an unknown CQL would produce a generic function call. If you need to do this, especially a function with varargs, then you must declare the function with something like: DECLARE PROCEDURE printf NO CHECK; This option only works for void functions. For more complex signatures check DECLARE FUNCTION and DECLARE SELECT FUNCTION. Usually these will require a simple wrapper to call from CQL. In all cases there must be some kind of declaration,to avoid mysterious linker failures or argument signature mismatches.  "},{"title":"CQL0324: referenced table was created in a later version so it cannot be used in a foreign key 'referenced_table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0324-referenced-table-was-created-in-a-later-version-so-it-cannot-be-used-in-a-foreign-key-referenced_table","content":"In a foreign key, we enforce the following rules: @recreate tables can see any version they like, if the name is in scope that's good enoughother tables may only &quot;see&quot; the same version or an earlier version. Normal processing can't actually get into this state because if you tried to create the referencing table with the smaller version number first you would get errors because the name of the referenced table doesn't yet exist. But if you created them at the same time and you made a typo in the version number of the referenced table such that it was accidentally bigger you'd create a weirdness. So we check for that situation here and reject it to prevent that sort of typo. If you see this error there is almost certainly a typo in the version number of the referenced table; it should be fixed.  "},{"title":"CQL0325: ok_table_scan attribute must be a name​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0325-ok_table_scan-attribute-must-be-a-name","content":"The values for the attribute ok_table_scan can only be names. CQL attributes can have a variety of values but in this case the attribute refers to the names of tables so no other type of attribute is reasonable.  "},{"title":"CQL0326: table name in ok_table_scan does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0326-table-name-in-ok_table_scan-does-not-exist-table_name","content":"The names provided to ok_table_scan attribute should be names of existing tables. The attribute indicates tables that are ok to scan in this procedure even though they are typically not ok to scan due to 'no_table_scan'. Therefore the attribute must refer to an existing table. There is likely a typo in the the table name that needs to be corrected.  "},{"title":"CQL0327: a value should not be assigned to 'attribute_name' attribute​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0327-a-value-should-not-be-assigned-to-attribute_name-attribute","content":"The attribute attribute_name doesn't take a value. When marking a statement with @attribute(cql:&lt;attribute_name&gt;) there is no need for an attribute value.  "},{"title":"CQL0328: 'attribute_name' attribute may only be added to a 'statement_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0328-attribute_name-attribute-may-only-be-added-to-a-statement_name","content":"The attribute_name attribute can only be assigned to specific statements. The marking @attribute(cql:&lt;attribute_name&gt;) only makes sense on specific statement. It's likely been put somewhere strange, If it isn't obviously on the wrong thing, look into possibly how the source is after macro expansion.  "},{"title":"CQL0329: ok_table_scan attribute can only be used in a create procedure statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0329-ok_table_scan-attribute-can-only-be-used-in-a-create-procedure-statement","content":"The ok_table_scan can only be placed on a create procedure statement. The marking @attribute(cql:ok_table_scan=...) indicates that the procedure may scan the indicated tables. This marking doesn't make sense on other kinds of statements.  "},{"title":"CQL0330: fragment must start with exactly 'SELECT * FROM CTE'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0330-fragment-must-start-with-exactly-select--from-cte","content":"Query fragments have an exact prescription for their shape. This prescription includes select * from CTE in the first branch of the UNION ALL operator when using that form. Here CTE is the common table expression that they define. This the error message includes the specific name that is required in this context.  "},{"title":"CQL0331: extension fragment CTE must have not have ORDER BY or LIMIT clauses 'frag_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0331-extension-fragment-cte-must-have-not-have-order-by-or-limit-clauses-frag_name","content":"In the extension fragment form that uses UNION ALL to add rows you cannot include the top level operators ORDER BY, or LIMIT Any of these would remove or reorder the rows from the core fragment and that is not allowed, you can only add new rows. You can use any clauses you like in a nested selects as they will not remove rows from the base query.  "},{"title":"CQL0332: all extension fragments that use UNION ALL must come before those that use LEFT OUTER JOIN 'frag_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0332-all-extension-fragments-that-use-union-all-must-come-before-those-that-use-left-outer-join-frag_name","content":"Query fragments that add rows using the UNION ALL form have no way to refer columns that may have been added before them in the part of the query that adds rows (the second and subsequent branches of UNION ALL). As a result, in order to get a assembled query that makes sense the row-adding form must always come before any columns were added. Hence all of these fragments must come before any of the LEFT OUTER JOIN form. If you get this error, you should re-order your fragments such that the UNION ALL form comes before any LEFT OUTER JOIN fragments. The offending fragment is named in the error.  "},{"title":"CQL0333: all the compound operators in this CTE must be UNION ALL​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0333-all-the-compound-operators-in-this-cte-must-be-union-all","content":"The compound operators in CTE must and always be an UNION ALL.  "},{"title":"CQL0334: @dummy_seed @dummy_nullables @dummy_defaults many only be used with a single VALUES row​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0334-dummy_seed-dummy_nullables-dummy_defaults-many-only-be-used-with-a-single-values-row","content":"Dummy insert feature makes only sense when it's used in a VALUES clause that is not part of a compound select statement. "},{"title":"CQL0336: select statement with VALUES clause requires a non empty list of values​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0336-select-statement-with-values-clause-requires-a-non-empty-list-of-values","content":"VALUES clause requires at least a value for each of the values list. Empty values list are not supported.  "},{"title":"CQL0337: number of columns values for each row should be identical in VALUES clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0337-number-of-columns-values-for-each-row-should-be-identical-in-values-clause","content":"The number of values for each values list in VALUES clause should always be the same.  "},{"title":"CQL0338: name of a migration procedure may not end in '_crc' 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0338-name-of-a-migration-procedure-may-not-end-in-_crc-procedure_name","content":"To avoid name conflicts in the upgrade script, migration procedures are not allowed to end in '_crc' this suffix is reserved for internal use.  "},{"title":"CQL0339: WITHOUT ROWID tables are forbidden if strict without rowid mode is enabled​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0339-without-rowid-tables-are-forbidden-if-strict-without-rowid-mode-is-enabled","content":"@enforce_strict has been used to enable strict WITHOUT ROWID enforcement. When enabled no CREATE TABLE statement can have WITHOUT ROWID clause.  "},{"title":"CQL0340: FROM ARGUMENTS used in a procedure with no arguments 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0340-from-arguments-used-in-a-procedure-with-no-arguments-procedure_name","content":"The named procedure has a call that uses the FROM ARGUMENTS pattern but it doesn't have any arguments. This is almost certainly a cut/paste from a different location that needs to be adjusted.  "},{"title":"CQL0341: argument must be a variable in function 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0341-argument-must-be-a-variable-in-function-function_name","content":"The argument for the CQL builtin function 'function_name' should always be a variable. It can not be an expression for example  "},{"title":"CQL0342: cursor arguments must have identical column count 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0342-cursor-arguments-must-have-identical-column-count-function_name","content":"The number of column in the cursor arguments must be identical to accurately do diffing between two cursors.  "},{"title":"CQL0343: all arguments must be blob 'cql_get_blob_size'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0343-all-arguments-must-be-blob-cql_get_blob_size","content":"The argument for the CQL builtin function cql_get_blob_size should always be of type blob  "},{"title":"CQL0344: argument must be a nullable type (but not constant NULL) in 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0344-argument-must-be-a-nullable-type-but-not-constant-null-in-function","content":"Functions like ifnull_crash only make sense if the argument is nullable. If it's already not null the operation is uninteresting/redundant. The most likely cause is that the function call in question is vestigial and you can simply remove it.  "},{"title":"CQL0345: arguments must be of type blob 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0345-arguments-must-be-of-type-blob-function_name","content":"The indicated function accepts only a single argument of type blob.  "},{"title":"CQL0346: expression must be of type object<T cursor> where T is a valid shape name 'variable'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0346-expression-must-be-of-type-objectt-cursor-where-t-is-a-valid-shape-name-variable","content":"It's possible to take the statement associated with a statement cursor and store it in an object variable. Using the form: declare C cursor for X;  The object variable 'X' must be declared as follows: declare X object&lt;T cursor&gt;;  Where T refers to a named object with a shape, like a table, a view, or a stored procedure that returns a result set. This type T must match the shape of the cursor exactly i.e. having the column names and types. The reverse operation, storing a statement cursor in a variable is also possible with this form: set X from cursor C;  This has similar constraints on the variable X. This error indicates that the variable in question (X in this example) is not a typed object variable so it can't be the source of a cursor, or accept a cursor. See Chapter 5 of the CQL Programming Language.  "},{"title":"CQL0347: select function may not return type OBJECT 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0347-select-function-may-not-return-type-object-function_name","content":"The indicated function was declared with DECLARE SELECT FUNCTION meaning it is to be used in the context of SQLite statements. However, SQLite doesn't understand functions that return type object at all. Therefore declaration is illegal. When working with pointer type through SQLite it is often possibly to encode the object as an long integer assuming it can pass through unchanged with no retain/release semantics or any such thing. If that is practical you can move objects around by returning long integers.  "},{"title":"CQL0348: collate applied to a non-text column 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0348-collate-applied-to-a-non-text-column-column_name","content":"Collation order really only makes sense on text fields. Possibly blob fields but we're taking a stand on blob for now. This can be relaxed later if that proves to be a mistake. For now, only text  "},{"title":"CQL0349: column definitions may not come after constraints 'column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0349-column-definitions-may-not-come-after-constraints-column_name","content":"In a CREATE TABLE statement, the indicated column name came after a constraint. SQLite expects all the column definitions to come before any constraint definitions. You must move the offending column definition above the constraints.  "},{"title":"CQL0350: statement must appear inside of a PROC SAVEPOINT block​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0350-statement-must-appear-inside-of-a-proc-savepoint-block","content":"The ROLLBACK RETURN and COMMIT RETURN forms are only usable inside of a PROC SAVEPOINT block because they rollback or commit the savepoint that was created at the top level.  "},{"title":"CQL0351: statement should be in a procedure and at the top level​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0351-statement-should-be-in-a-procedure-and-at-the-top-level","content":"The indicated statement may only appear inside procedure and not nested. The classic example of this is the PROC SAVEPOINT form which can only be used at the top level of procedures.  "},{"title":"CQL0352: use COMMIT RETURN or ROLLBACK RETURN in within a proc savepoint block​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0352-use-commit-return-or-rollback-return-in-within-a-proc-savepoint-block","content":"The normal RETURN statement cannot be used inside of PROC SAVEPOINT block, you have to indicate if you want to commit or rollback the savepoint when you return. This makes it impossible to forget to do so which is in some sense the whole point of PROC SAVEPOINT.  "},{"title":"CQL0353: evaluation of constant failed​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0353-evaluation-of-constant-failed","content":"The constant expression could not be evaluated. This is most likely because it includes an operator that is not supported or a function call which is not support. Very few functions can be used in constant expressions The supported functions include iif, which is rewritten; abs; ifnull, nullif, and coalesce.  "},{"title":"CQL0354: duplicate enum member 'enum_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0354-duplicate-enum-member-enum_name","content":"While processing a declare enum statement the indicated member of the enum appeared twice. This is almost certainly a copy/paste of the same enum member twice.  "},{"title":"CQL0355: evaluation failed 'enum_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0355-evaluation-failed-enum_name","content":"While processing a declare enum statement the indicated member of the enum could not be evaluated as a constant expression. There could be a non-constant in the expression or there could be a divide-by-zero error.  "},{"title":"CQL0356: enum definitions do not match 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0356-enum-definitions-do-not-match-name","content":"The two described declare enum statements have the same name but they are not identical. The error output contains the full text of both declarations to compare.  "},{"title":"CQL0357: enum does not contain 'enum_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0357-enum-does-not-contain-enum_name","content":"The indicated member is not part of the enumeration.  "},{"title":"CQL0358: declared enums must be top level 'enum'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0358-declared-enums-must-be-top-level-enum","content":"A DECLARE ENUM statement for the named enum is happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure.  "},{"title":"CQL0359: duplicate type declaration 'type_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0359-duplicate-type-declaration-type_name","content":"The name of a declared type should always be unique.  "},{"title":"CQL0360: unknown type 'type_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0360-unknown-type-type_name","content":"The indicated name is not a valid type name.  "},{"title":"CQL0361: return data type in a create function declaration can only be Text, Blob or Object​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0361-return-data-type-in-a-create-function-declaration-can-only-be-text-blob-or-object","content":"Return data type in a create function definition can only be TEXT, BLOB or OBJECT. These are the only reference types and so CREATE makes sense only with those types. An integer, for instance, can't start with a +1 reference count.  "},{"title":"CQL0362: HIDDEN column attribute must be the first attribute if present​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0362-hidden-column-attribute-must-be-the-first-attribute-if-present","content":"In order to ensure that SQLite will parse HIDDEN as part of the type it has to come before any other attributes like NOT NULL. This limitation is due to the fact that CQL and SQLite use slightly different parsing approaches for attributes and in SQLite HIDDEN isn't actually an attribute. The safest place to put the attribute is right after the type name and before any other attributes as it is totally unambiguous there so CQL enforces this.  "},{"title":"CQL0363: all arguments must be names 'vault_sensitive'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0363-all-arguments-must-be-names-vault_sensitive","content":"vault_sensitive attribution only allow names. Integer, string literal, c string or blob are not allowed, only IDs should be provided.  "},{"title":"CQL0364: vault_sensitive annotation can only go on a procedure that uses the database​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0364-vault_sensitive-annotation-can-only-go-on-a-procedure-that-uses-the-database","content":"The named procedure has the vault_sensitive annotation to automatically encode sensitive value in the result set. Encoding value require the database, but the procedure in question doesn't even use the database at all. This annotation is therefore useless.  "},{"title":"CQL0365: @enforce_pop used but there is nothing to pop​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0365-enforce_pop-used-but-there-is-nothing-to-pop","content":"Each @enforce_pop should match an @enforce_push, but there is nothing to pop on the stack now.  "},{"title":"CQL0366: transaction operations disallowed while STRICT TRANSACTION enforcement is on​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0366-transaction-operations-disallowed-while-strict-transaction-enforcement-is-on","content":"@enforce_strict transaction has been used, while active no transaction operations are allowed. Savepoints may be used. This is typically done to prevent transactions from being used in any ad hoc way because they don't nest and typically need to be used with some &quot;master plan&quot; in mind.  "},{"title":"CQL0367: an attribute was specified twice 'attribute_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0367-an-attribute-was-specified-twice-attribute_name","content":"In the indicated type declaration, the indicated attribute was specified twice. This is almost certainly happening because the line in question looks like thisdeclare x type_name not null; but type_name is already not null.  "},{"title":"CQL0368: strict select if nothing requires that all (select ...) expressions include 'if nothing'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0368-strict-select-if-nothing-requires-that-all-select--expressions-include-if-nothing","content":"@enforce_strict select if nothing has been enabled. This means that select expressions must includeif nothing throw (the old default) if nothing [value] or if nothing or null [value]. This options exists because commonly the case where a row does not exist is not handled correctly when (select ...) is used without the if nothing options. If your select expression uses a built-in aggregate function, this check may not be enforced because they can always return a row. But there are exceptions. The check is still enforced when one of the following is in the expression: a GROUP BY clausea LIMIT that evaluates to less than 1, or is a variablean OFFSET clauseYou have a min or max function with more than 1 argument. Those are scalar functions.  "},{"title":"CQL0369: (SELECT ... IF NOTHING) construct is for use in top level expressions, not inside of other DML​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0369-select--if-nothing-construct-is-for-use-in-top-level-expressions-not-inside-of-other-dml","content":"This form allows for error control of (select...) expressions. But SQLite does not understand the form at all, so it can only appear at the top level of expressions where CQL can strip it out. Here are some examples: good:  set x := (select foo from bar where baz if nothing 0); if (select foo from bar where baz if nothing 1) then ... end if;  bad:  select foo from bar where (select something from somewhere if nothing null); delete from foo where (select something from somewhere if nothing 1);  Basically if you are already in a SQL context, the form isn't usable because SQLite simply doesn't understand if nothing at all. This error makes it so that you'll get a build time failure from CQL rather than a run time failure from SQLite.  "},{"title":"CQL0370: due to a memory leak bug in old SQLite versions, the select part of an insert must not have a top level join or compound operator. Use WITH and a CTE, or a nested select to work around this.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0370-due-to-a-memory-leak-bug-in-old-sqlite-versions-the-select-part-of-an-insert-must-not-have-a-top-level-join-or-compound-operator-use-with-and-a-cte-or-a-nested-select-to-work-around-this","content":"There is an unfortunate memory leak in older versions of SQLite (research pending on particular versions, but 3.28.0 has it). It causes this pattern to leak: -- must be autoinc table create table x ( pk integer primary key autoincrement ); -- any join will do (this is a minimal repro) insert into x select NULL pk from (select 1) t1 inner join (select 1) t2;  You can workaround this with a couple of fairly simple rewrites. This form is probably the cleanest. with cte (pk) as (select .. anything you need) insert into x select * from cte;  Simply wrapping your desired select in a nested select also suffices. So long as the top level is simple. insert into x select * from ( select anything you need.... );   "},{"title":"CQL0371: table valued function used in a left/right/cross context; this would hit a SQLite bug. Wrap it in a CTE instead.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0371-table-valued-function-used-in-a-leftrightcross-context-this-would-hit-a-sqlite-bug--wrap-it-in-a-cte-instead","content":"This error is generated by @enforce_strict table function. It is there to allow safe use of Table Valued Functions (TVFs) even though there was a bug in SQLite prior to v 3.31.0 when joining against them. The bug appears when the TVF is on the right of a left join. For example: select * from foo left join some_tvf(1);  In this case the join becomes an INNER join even though you wrote a left join. Likewise select * from some_tvf(1) right join foo;  Becomes an inner join even though you wrote a right join. The same occurs when a TVF is on either side of a cross join. The workaround is very simple. You don't want the TVF to be the target of the join directly. Instead: with tvf_(*) as (select * from some_tvf(1)) select * from foo left join tvf_;  OR select * from foo left join (select * from some_tvf(1));   "},{"title":"CQL0372: SELECT ... IF NOTHING OR NULL NULL is redundant; use SELECT ... IF NOTHING NULL instead.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0372-select--if-nothing-or-null-null-is-redundant-use-select--if-nothing-null-instead","content":"It is always the case that SELECT ... IF NOTHING OR NULL NULL is equivalent to SELECT ... IF NOTHING NULL. As such, do not do this: select foo from bar where baz if nothing or null null  Do this instead: select foo from bar where baz if nothing null   "},{"title":"CQL0373: comparing against NULL always yields NULL; use IS and IS NOT instead.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0373-comparing-against-null-always-yields-null-use-is-and-is-not-instead","content":"Attempting to check if some value x is NULL via x = NULL or x == NULL, or isn't NULL via x &lt;&gt; NULL or x != NULL, will always produce NULL regardless of the value of x. Instead, use x IS NULL or x IS NOT NULL to get the expected boolean result.  "},{"title":"CQL0374: SELECT expression is equivalent to NULL.​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0374-select-expression-is-equivalent-to-null","content":"CQL found a redundant select operation (e.g., set x := (select NULL);). There is no need to write a select expression that always evaluates to NULL. Simply use NULL instead (e.g., set x := NULL;).  CQL 0375 : unused, this was added to prevent merge conflicts at the end on literally every checkin  CQL 0376 : unused, this was added to prevent merge conflicts at the end on literally every checkin  "},{"title":"CQL0377: table transitioning from @recreate to @create must use @create(nn,cql:from_recreate) 'table name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0377-table-transitioning-from-recreate-to-create-must-use-createnncqlfrom_recreate-table-name","content":"The indicated table is moving from @recreate to @create meaning it will now be schema managed in an upgradable fashion. When this happens end-user databases might have some stale version of the table from a previous installation. This stale version must get a one-time cleanup in order to ensure that the now current schema is correctly applied. The cql:from_recreate annotation does this. It is required because otherwise there would be no record that this table &quot;used to be recreate&quot; and therefore might have an old version still in the database. A correct table might look something like this: create table correct_migration_to_create( id integer primary key, t text ) @create(7, cql:from_recreate);   "},{"title":"CQL0378: built-in migration procedure not valid in this context 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0378-built-in-migration-procedure-not-valid-in-this-context-name","content":"The indicated name is a valid built-in migration procedure but it is not valid on this kind of item. For instance cql:from_recreate can only be applied to tables.  "},{"title":"CQL0379: unknown built-in migration procedure 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0379-unknown-built-in-migration-procedure-name","content":"Certain schema migration steps are built-in. Currently the only one is cql:from_recreate for moving to @create from @recreate. Others may be added in the future. The cql: prefix ensures that this name cannot conflict with a valid user migration procedure.  "},{"title":"CQL0380: WHEN expression cannot be evaluated to a constant​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0380-when-expression-cannot-be-evaluated-to-a-constant","content":"In a SWITCH statement each expression each expression in a WHEN clause must be made up of constants and simple numeric math operations. See the reference on the const(..) expression for the valid set. It's most likely that a variable or function call appears in the errant expression.  "},{"title":"CQL0381: case expression must be a not-null integral type​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0381-case-expression-must-be-a-not-null-integral-type","content":"The SWITCH statement can only switch over integers or long integers. It will be translated directly to the C switch statement form. TEXT, REAL, BLOB, BOOL, and OBJECT cannot be used in this way.  "},{"title":"CQL0382: type of a WHEN expression is bigger than the type of the SWITCH expression​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0382-type-of-a-when-expression-is-bigger-than-the-type-of-the-switch-expression","content":"The WHEN expression evaluates to a LONG INTEGER but the expression in the SWITCH is INTEGER.  "},{"title":"CQL0383: switch ... ALL VALUES is useless with an ELSE clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0383-switch--all-values-is-useless-with-an-else-clause","content":"The ALL VALUES form of switch means that: the SWITCH expression is an enumerated typethe WHEN cases will completely cover the values of the enum If you allow the ELSE form then ALL VALUES becomes meaningless because of course they are all covered. So with ALL VALUES there can be no ELSE. You can list items that have no action with this form:  WHEN 10, 15 THEN NOTHING -- explicitly do nothing in these cases so they are still covered  No code is generated for such cases.  "},{"title":"CQL0384: switch statement did not have any actual statements in it​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0384-switch-statement-did-not-have-any-actual-statements-in-it","content":"Either there were no WHEN clauses at all, or they were all WHEN ... THEN NOTHING so there is no actual code to execute. You need to add some cases that do work.  "},{"title":"CQL0385: WHEN clauses contain duplicate values 'value'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0385-when-clauses-contain-duplicate-values-value","content":"In a SWITCH statement all of the values in the WHEN clauses must be unique. The indicated errant entry is a duplicate.  "},{"title":"CQL0386: SWITCH ... ALL VALUES is used but the switch expression is not an enum type​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0386-switch--all-values-is-used-but-the-switch-expression-is-not-an-enum-type","content":"In a SWITCH statement with ALL VALUES specified the switch expression was not an enumerated type.ALL VALUES is used to ensure that there is a case for every value of an enumerated type so this switch cannot be so checked. Either correct the expression, or remove ALL VALUES.  "},{"title":"CQL0387: a value exists in the enum that is not present in the switch 'enum_member'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0387-a-value-exists-in-the-enum-that-is-not-present-in-the-switch-enum_member","content":"In a SWITCH statement with ALL VALUES specified the errant enum member did not appear in any WHEN clause. All members must be specified when ALL VALUES is used.  "},{"title":"CQL0388: a value exists in the switch that is not present in the enum 'numeric_value'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0388-a-value-exists-in-the-switch-that-is-not-present-in-the-enum-numeric_value","content":"In a SWITCH statement with ALL VALUES specified the errant integer value appeared in in a WHEN clause. This value is not part of the members of the enum. Note that enum members that begin with '_' are ignored as they are, by convention, considered to be pseudo-members. e.g. in declare enum v integer (v0 = 0, v1 =1, v2 =2, _count = 3) _count is a pseudo-member. The errant entry should probably be removed. Alternatively, ALL VALUES isn't appropriate as the domain of the switch is actually bigger than the domain of the enumeration. One of these changes must happen.  "},{"title":"CQL0389: DECLARE OUT requires that the procedure be already declared​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0389-declare-out-requires-that-the-procedure-be-already-declared","content":"The purpose of the DECLARE OUT form is to automatically declare the out parameters for that procedure. This cannot be done if the type of the procedure is not yet known.  "},{"title":"CQL0390: DECLARE OUT CALL used on a procedure with no missing OUT arguments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0390-declare-out-call-used-on-a-procedure-with-no-missing-out-arguments","content":"The DECLARE OUT CALL form was used, but the procedure has no OUT arguments that need any implicit declaration. Either they have already all been declared or else there are noOUT arguments at all, or even no arguments of any kind.  "},{"title":"CQL0391: CLOSE cannot be used on a boxed cursor​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0391-close-cannot-be-used-on-a-boxed-cursor","content":"When a cursor is boxed—i.e., wrapped in an object—the lifetime of the box and underlying statement are automatically managed via reference counting. Accordingly, it does not make sense to manually call CLOSE on such a cursor as it may be retained elsewhere. Instead, to allow the box to be freed and the underlying statement to be finalized, set all references to the cursor to NULL. Note: As with all other objects, boxed cursors are automatically released when they fall out of scope. You only have to set a reference to NULL if you want to release the cursor sooner, for some reason.  "},{"title":"CQL0392: when deleting a virtual table you must specify @delete(nn, cql:module_must_not_be_deleted_see_docs_for_CQL0392) as a reminder not to delete the module for this virtual table​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0392-when-deleting-a-virtual-table-you-must-specify-deletenn-cqlmodule_must_not_be_deleted_see_docs_for_cql0392-as-a-reminder-not-to-delete-the-module-for-this-virtual-table","content":"When the schema upgrader runs, if the virtual table is deleted it will attempt to do DROP TABLE IF EXISTS on the indicated table. This table is a virtual table. SQLite will attempt to initialize the table even when you simply try to drop it. For that to work the module must still be present. This means modules can never be deleted! This attribute is here to remind you of this fact so that you are not tempted to delete the module for the virtual table when you delete the table. You may, however, replace it with a shared do-nothing stub. The attribute itself does nothing other than hopefully cause you to read this documentation.  "},{"title":"CQL0393: not deterministic user function cannot appear in a constraint expression 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0393-not-deterministic-user-function-cannot-appear-in-a-constraint-expression-function_name","content":"CHECK expressions and partial indexes (CREATE INDEX with a WHERE clause) require that the expressions be deterministic. User defined functions may or may not be deterministic. Use @attribute(cql:deterministic) on a UDF declaration (declare select function...) to mark it deterministic and allow its use in an index.  "},{"title":"CQL0394: nested select expressions may not appear inside of a constraint expression​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0394-nested-select-expressions-may-not-appear-inside-of-a-constraint-expression","content":"SQLite does not allow the use of correlated subqueries or other embedded select statements inside of a CHECK expression or the WHERE clauses of a partial index. This would require additional joins on every such operation which would be far too expensive.  "},{"title":"CQL0395: table valued functions may not be used in an expression context 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0395-table-valued-functions-may-not-be-used-in-an-expression-context-function_name","content":"A table valued function should be used like a table e.g. -- this is right select * from table_valued_func(5);  Not like a value e.g. -- this is wrong select table_valued_func(5); -- this is also wrong select 1 where table_valued_func(5) = 3;   "},{"title":"CQL0396: versioning attributes may not be used on DDL inside a procedure​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0396-versioning-attributes-may-not-be-used-on-ddl-inside-a-procedure","content":"If you are putting DDL inside of a procedure then that is going to run regardless of any @create,@delete, or @recreate attributes; DDL in entires do not get versioning attributes, attributes are reserved for schema declarations outside of any procedure.  "},{"title":"CQL0397: object is an orphan because its table is deleted. Remove rather than @delete 'object_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0397-object-is-an-orphan-because-its-table-is-deleted-remove-rather-than-delete-object_name","content":"This error is about either a trigger or an index. In both cases you are trying to use @delete on the index/trigger but the table that the named object is based on is itself deleted, so the object is an orphan. Because of this, the orphaned object doesn't need, or no longer needs, an @delete tombstone because when the table is dropped, all of its orphaned indices and triggers will also be dropped. To fix this error, remove the named object entirely rather than marking it @delete. Note: if the index/trigger was previously deleted and now the table is also deleted, it is now safe to remove the index/trigger @delete tombstone and this error reminds you to do so.  "},{"title":"CQL0398: a compound select cannot be ordered by the result of an expression​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0398-a-compound-select-cannot-be-ordered-by-the-result-of-an-expression","content":"When specifying an ORDER BY for a compound select, you may only order by indices (e.g., 3) or names (e.g., foo) that correspond to an output column, not by the result of an arbitrary expression (e.g., foo + bar). For example, this is allowed: SELECT x, y FROM t0 UNION ALL select x, y FROM t1 ORDER BY y  The equivalent using an index is also allowed: SELECT x, y FROM t0 UNION ALL select x, y FROM t1 ORDER BY 2  This seemingly equivalent version containing an arbitrary expression, however, is not: SELECT x, y FROM t0 UNION ALL select x, y FROM t1 ORDER BY 1 + 1;   "},{"title":"CQL0399: table must leave @recreate management with @create(nn) or later 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0399-table-must-leave-recreate-management-with-createnn-or-later-table_name","content":"The indicated table changed from @recreate to @create but it did so in a past schema version. The change must happen in the current schema version. That version is indicated by the value of nn. To fix this you can change the @create annotation so that it matches the number in this error message  "},{"title":"CQL0400: encode context column can't be sensitive​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0400-encode-context-column-cant-be-sensitive","content":"The encode context column will be used to encode sensitive fields, it can't be exposed to encode functions  "},{"title":"CQL0401: encode context column must be specified if strict encode context column mode is enabled​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0401-encode-context-column-must-be-specified-if-strict-encode-context-column-mode-is-enabled","content":"encode context column must be specified in vault_sensitive attribute with format: @attribute(cql:vault_sensitive=(encode_context_col, (col1, col2, ...))  "},{"title":"CQL0402: encode context column in vault_sensitive attribute must match the specified type in strict mode​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0402-encode-context-column-in-vault_sensitive-attribute-must-match-the-specified-type-in-strict-mode","content":"encode context column must match the specified type in vault_sensitive attribute with format: @attribute(cql:vault_sensitive=(encode_context_col, (col1, col2, ...))  "},{"title":"CQL0403: operator may not be used because it is not supported on old versions of SQLite, 'operator'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0403-operator-may-not-be-used-because-it-is-not-supported-on-old-versions-of-sqlite-operator","content":"The indicated operator has been suppressed with @enforce_strict is true because it is not available on older versions of sqlite.  "},{"title":"CQL0404: procedure cannot be both a normal procedure and an unchecked procedure, 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0404-procedure-cannot-be-both-a-normal-procedure-and-an-unchecked-procedure-procedure_name","content":"The construct: DECLARE PROCEDURE printf NO CHECK;  Is used to tell CQL about an external procedure that might take any combination of arguments. The canonical example isprintf. All the arguments are converted from CQL types to basic C types when making the call (e.g. TEXT variables become temporary C strings). Once a procedure has been declared in this way it can't then also be declared as a normal CQL procedure via CREATE or DECLARE PROCEDURE. Likewise a normal procedure can't be redeclared with the NO CHECKpattern.  "},{"title":"CQL0405: procedure of an unknown type used in an expression 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0405-procedure-of-an-unknown-type-used-in-an-expression-procedure_name","content":"If a procedure has no known type—that is, it was originally declared with NO CHECK, and has not been subsequently re-declared with DECLARE FUNCTION orDECLARE SELECT FUNCTION—it is not possible to use it in an expression. You must either declare the type before using it or call the procedure outside of an expression via a CALL statement: DECLARE PROCEDURE some_external_proc NO CHECK; -- This works even though `some_external_proc` has no known type -- because we're using a CALL statement. CALL some_external_proc(&quot;Hello!&quot;); DECLARE FUNCTION some_external_proc(t TEXT NOT NULL) INT NOT NULL; -- Now that we've declared the type, we can use it in an expression. let result := some_external_proc(&quot;Hello!&quot;);   "},{"title":"CQL0406: substr uses 1 based indices, the 2nd argument of substr may not be zero\"​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0406-substr-uses-1-based-indices-the-2nd-argument-of-substr-may-not-be-zero","content":"A common mistake with substr is to assume it uses zero based indices like C does. It does not. In fact the result when using 0 as the second argument is not well defined. If you want the firstn characters of a string you use substr(haystack, 1, n).  CQL 0407 : unused, this was added to prevent merge conflicts at the end on literally every checkin  "},{"title":"CQL0408: encode context column can be only specified once​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0408-encode-context-column-can-be-only-specified-once","content":"The encode context column can be only specified once in @vault_sensitive attribute  "},{"title":"CQL0409: cannot use IS NULL or IS NOT NULL on a value of a NOT NULL type 'nonnull_expr'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0409-cannot-use-is-null-or-is-not-null-on-a-value-of-a-not-null-type-nonnull_expr","content":"If the left side of an IS NULL or IS NOT NULL expression is of a NOT NULLtype, the answer will always be the same (FALSE or TRUE, respectively). Such a check often indicates confusion that may lead to unexpected behavior (e.g., checking, incorrectly, if a cursor has a row via cursor IS NOT NULL). NOTE: Cursor fields of cursors without a row and uninitialized variables of a NOT NULL reference type are exceptions to the above rule: Something may be NULL even if it is of a NOT NULL type in those cases. CQL will eventually eliminate these exceptions. In the cursor case, one can check whether or not a cursor has a row by using the cursor-as-boolean-expression syntax (e.g., IF cursor THEN ... END IF;, IF NOT cursor ROLLBACK RETURN;, et cetera). In the uninitialized variables case, writing code that checks for initialization is not recommended (and, indeed, use before initialization will soon be impossible anyway): One should simply always initialize the variable.  CQL 0410 : unused, this was added to prevent merge conflicts at the end on literally every checkin  "},{"title":"CQL0411: duplicate flag in substitution 'flag'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0411-duplicate-flag-in-substitution-flag","content":"The same flag cannot be used more than once per substitution within a format string.  "},{"title":"CQL0412: cannot combine '+' flag with space flag​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0412-cannot-combine--flag-with-space-flag","content":"It is not sensible to use both the + flag and the space flag within the same substitution (e.g., %+ d) as it is equivalent to just using the + flag (e.g., %+d).  "},{"title":"CQL0413: width required when using flag in substitution 'flag'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0413-width-required-when-using-flag-in-substitution-flag","content":"The flag used (- or 0) for a substitution within a format string does not make sense unless accompanied by a width (e.g., %-10d).  "},{"title":"CQL0414: 'l' length specifier has no effect; consider 'll' instead​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0414-l-length-specifier-has-no-effect-consider-ll-instead","content":"The use of the l length specifier within a format string, e.g. %ld, has no effect in SQLite. If the argument is to be a LONG, use ll instead (e.g.,%lld). If the argument is to be an INTEGER, simply omit the length specifier entirely (e.g., %d).  "},{"title":"CQL0415: length specifier cannot be combined with '!' flag​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0415-length-specifier-cannot-be-combined-with--flag","content":"Length specifiers are only for use with integer type specifiers (e.g. %lld) and the ! flag is only for use with non-integer type specifiers (e.g. %!10sand %!f). It therefore makes no sense to use both within the same substitution.  "},{"title":"CQL0416: type specifier not allowed in CQL 'type_specifier'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0416-type-specifier-not-allowed-in-cql-type_specifier","content":"The type specifier used is accepted by SQLite, but it would be either useless or unsafe if used within the context of CQL.  "},{"title":"CQL0417: unrecognized type specifier 'type_specifier'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0417-unrecognized-type-specifier-type_specifier","content":"The type specifier used within the format string is not known to SQLite.  "},{"title":"CQL0418: type specifier combined with inappropriate flags 'type_specifier'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0418-type-specifier-combined-with-inappropriate-flags-type_specifier","content":"The type specifier provided does not make sense given one or more flags that appear within the same substitution. For example, it makes no sense to have a substitution like %+u: the + indicates the sign of the number will be shown, while the u indicates the number will be shown as an unsigned integer.  "},{"title":"CQL0419: type specifier cannot be combined with length specifier 'type_specifier'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0419-type-specifier-cannot-be-combined-with-length-specifier-type_specifier","content":"The type specifier provided cannot be used with a length specifier. For example,%lls makes no sense because ll only makes sense with integer types and sis a type specifier for strings.  "},{"title":"CQL0420: incomplete substitution in format string​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0420-incomplete-substitution-in-format-string","content":"The format string ends with a substitution that is incomplete. This can be the case if a format string ends with a % (e.g., &quot;%d %s %&quot;). If the intent is to have a literal % printed, use %% instead (e.g., &quot;%d %s %%&quot;`).  "},{"title":"CQL0421: first argument must be a string literal 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0421-first-argument-must-be-a-string-literal-function","content":"The first argument to the function must be a string literal.  "},{"title":"CQL0422: more arguments provided than expected by format string 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0422-more-arguments-provided-than-expected-by-format-string-function","content":"More arguments were provided to the function than its format string indicates are necessary. The most likely cause for this problem is that the format string is missing a substitution.  "},{"title":"CQL0423: fewer arguments provided than expected by format string 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0423-fewer-arguments-provided-than-expected-by-format-string-function","content":"Fewer arguments were provided to the function than its format string indicates are necessary. The most likely cause for this problem is that an argument was accidentally omitted. "},{"title":"CQL0424: procedure with INOUT parameter used as function 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0424-procedure-with-inout-parameter-used-as-function-procedure_name","content":"If a procedure has an INOUT parameter, it cannot be used as a function: It may only be called via a CALL statement. "},{"title":"CQL0425: procedure with non-trailing OUT parameter used as function 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0425-procedure-with-non-trailing-out-parameter-used-as-function-procedure_name","content":"For a procedure to be used as a function, it must have exactly one OUTparameter, and that parameter must be the last parameter of the procedure. In all other cases, procedures with one or more OUT parameters may only be called via a CALL statement.  "},{"title":"CQL0426: OUT or INOUT argument cannot be used again in same call 'variable'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0426-out-or-inout-argument-cannot-be-used-again-in-same-call-variable","content":"When a variable is passed as an OUT or INOUT argument, it may not be used as another argument within the same procedure call. It can, however, be used within a subexpression of another argument. For example: CREATE PROC some_proc(IN a TEXT, OUT b TEXT) BEGIN ... END DECLARE t TEXT; -- This is NOT legal. CALL some_proc(t, t); -- This, however, is perfectly fine. CALL some_proc(some_other_proc(t), t);   "},{"title":"CQL0427: LIKE CTE form may only be used inside a shared fragment at the top level i.e. @attribute(cql:shared_fragment) 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0427-like-cte-form-may-only-be-used-inside-a-shared-fragment-at-the-top-level-ie-attributecqlshared_fragment-procedure_name","content":"When creating a shared fragment you can specify &quot;table parameters&quot; by defining their shape like so: @attribute(cql:shared_fragment) create proc shared_proc(lim_ integer) begin with source(*) LIKE any_shape select * from source limit lim_; end;  However this LIKE form only makes sense withing a shared fragment, and only as a top level CTE in such a fragment. So either: the LIKE appeared outside of any procedurethe LIKE appeared in a procedure, but that procedure is not a shared fragmentthe LIKE appeared in a nested WITH clause  "},{"title":"CQL0428: duplicate binding of table in CALL/USING clause 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0428-duplicate-binding-of-table-in-callusing-clause-table_name","content":"In a CALL clause to access a shared fragment there is a duplicate table name in the USING portion. Example: my_cte(*) AS (call my_fragment(1) USING something as param1, something_else as param1),  Here param1 is supposed to take on the value of both something and something_else. Each parameter may appear only once in the USING clause. "},{"title":"CQL0429: called procedure has no table arguments but a USING clause is present 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0429-called-procedure-has-no-table-arguments-but-a-using-clause-is-present-procedure_name","content":"In a CALL clause to access a shared fragment there are table bindings but the shared fragment that is being called does not have any table bindings. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin select * from a_location limit lim; end; -- here we try to use my_fragment with table parameter but it has none with my_cte(*) AS (call my_fragment(1) USING something as param) select * from my_cte;   "},{"title":"CQL0430: no actual table was provided for the table parameter 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0430-no-actual-table-was-provided-for-the-table-parameter-table_name","content":"In a CALL clause to access a shared fragment the table bindings are missing a table parameter. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE source_shape select * from source limit lim; end; -- here we try to use my_fragment but no table was specified to play the role of &quot;source&quot; with my_cte(*) AS (call my_fragment(1)) select * from my_cte;   "},{"title":"CQL0431: an actual table was provided for a table parameter that does not exist 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0431-an-actual-table-was-provided-for-a-table-parameter-that-does-not-exist-table_name","content":"In a CALL clause to access a shared fragment the table bindings refer to a table parameter that does not exist. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE source_shape select * from source limit lim; end; -- here we try to use my_fragment but there is a table name &quot;soruce&quot; that doesn't match source with my_cte(*) AS (call my_fragment(1) USING something as soruce) select * from my_cte;   "},{"title":"CQL0432: table provided must have the same number of columns as the table parameter 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0432-table-provided-must-have-the-same-number-of-columns-as-the-table-parameter-table_name","content":"In a CALL clause to access a shared fragment the table bindings are trying to use a table that has the wrong number of columns. The column count, names, and types must be compatible. Extra columns for instance are not allowed because they might create ambiguities that were not present in the shared fragment. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE (select 1 x, 2 y) select * from source limit lim; end; -- here we try to use my_fragment but we provided 3 columns not 2 with my_source(*) AS (select 1 x, 2 y, 3 z), my_cte(*) AS (call my_fragment(1) USING my_source as source) select * from my_cte;  Here my_fragment wants a source table with 2 columns (x, y). But 3 were provided.  "},{"title":"CQL0433: table argument 'formal_name' requires column 'column_name' but it is missing in provided table 'actual_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0433-table-argument-formal_name-requires-column-column_name-but-it-is-missing-in-provided-table-actual_name","content":"In a CALL clause to access a shared fragment the table bindings are trying to use a table that is missing a required column. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE (select 1 x, 2 y) select * from source limit lim; end; -- here we try to use my_fragment but we passed in a table with (w,x) not (x,y) with my_source(*) AS (select 1 w, 2 x), my_cte(*) AS (call my_fragment(1) USING my_source as source) select * from my_cte;   "},{"title":"CQL0434: shared fragments may not be called outside of a SQL statement 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0434-shared-fragments-may-not-be-called-outside-of-a-sql-statement-procedure_name","content":"The indicated name is the name of a shared fragment, these fragments may be used inside of SQL code (e.g. select statements) but they have no meaning in a normal call outside of a SQL statement. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin select * from somewhere limit lim; end; call my_fragment();  Here my_fragment is being used like a normal procedure. This is not valid. A correct use of a fragment might look something like this: with (call my_fragment()) select * from my_fragment;   "},{"title":"CQL0435: must use qualified form to avoid ambiguity with alias 'column'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0435-must-use-qualified-form-to-avoid-ambiguity-with-alias-column","content":"In a SQLite SELECT expression, WHERE, GROUP BY, HAVING, and WINDOWclauses see the columns of the FROM clause before they see any aliases in the expression list. For example, assuming some table t has columns x and y, the following two expressions are equivalent: SELECT x AS y FROM t WHERE y &gt; 100 SELECT x AS y FROM t WHERE t.y &gt; 100  In the first expression, the use of y &gt; 100 makes it seem as though the yreferred to could be the y resulting from x as y in the expression list, but that is not the case. To avoid such confusion, CQL requires the use of the qualified form t.y &gt; 100 instead.  "},{"title":"CQL0436: alias referenced from WHERE, GROUP BY, HAVING, or WINDOW clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0436-alias-referenced-from-where-group-by-having-or-window-clause","content":"Unlike many databases (e.g., PostgreSQL and SQL Server), SQLite allows the aliases of a SELECT expression list to be referenced from clauses that are evaluated before the expression list. It does this by replacing all such alias references with the expressions to which they are equivalent. For example, assuming t does not have a column x, the following two expressions are equivalent: SELECT a + b AS x FROM t WHERE x &gt; 100 SELECT a + b AS x FROM t WHERE a + b &gt; 100  This can be convenient, but it is also error-prone. As mentioned above, the above equivalency only holds if x is not a column in t: If x is a column in t, the WHERE clause would be equivalent to t.x &gt; 100 instead, and there would be no syntactically obvious way to know this without first manually determining all of the columns present in t. To avoid such confusion, CQL disallows referencing expression list aliases fromWHERE, GROUP BY, HAVING, and WINDOW clauses altogether. Instead, one should simply use the expression to which the alias is equivalent (as is done in the second example above).  "},{"title":"CQL0437: common table name shadows previously declared table or view 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0437-common-table-name-shadows-previously-declared-table-or-view-name","content":"The name of a common table expression may not shadow a previously declared table or view. To rectify the problem, simply use a different name.  "},{"title":"CQL0438: variable possibly used before initialization 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0438-variable-possibly-used-before-initialization-name","content":"The variable indicated must be initialized before it is used because it is of a reference type (BLOB, OBJECT, or TEXT) that is also NOT NULL. CQL is usually smart enough to issue this error only in cases where initialization is truly lacking. Be sure to verify that the variable will be initialized before it is used for all possible code paths.  "},{"title":"CQL0439: nonnull reference OUT parameter possibly not always initialized 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0439-nonnull-reference-out-parameter-possibly-not-always-initialized-name","content":"The parameter indicated must be initialized before the procedure returns because it is of a reference type (BLOB, OBJECT, or TEXT) that is also NOT NULL. CQL is usually smart enough to issue this error only in cases where initialization is truly lacking. Be sure to verify that the parameter will be initialized both before the end of the procedure and before all cases ofRETURN and ROLLBACK RETURN. (Initialization before THROW is not required.)  "},{"title":"CQL0440: fragments may not have an empty body 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0440-fragments-may-not-have-an-empty-body-procedure_name","content":"The indicated procedure is one of the fragment types but has an empty body. This is not valid for any fragment type. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin /* something has to go here */ end;   "},{"title":"CQL0441: shared fragments may only have IF, SELECT, or WITH...SELECT at the top level 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0441-shared-fragments-may-only-have-if-select-or--withselect-at-the-top-level-procedure_name","content":"A shared fragment may consist of just one SELECT statement (including WITH...SELECT) or it can be an IF/ELSE statement that has a series of compatible select statements. There are no other valid options. "},{"title":"CQL0442: shared fragments with conditionals must include an else clause 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0442-shared-fragments-with-conditionals-must-include-an-else-clause-procedure_name","content":"In shared fragment with conditionals (i.e. it has an IF statement at the top) the fragment must have an ELSE block so that it is guaranteed to create chunk of SQL text in its expansion. When no rows are required you can do so with something like: IF ... THEN ... ELSE select 1 x, '2' y WHERE 0; END IF;  If the ELSE condition indicates that some join should not happen you might generate default values or NULLs for the join result like so: IF ... THEN ... ELSE select input_stuff.*, NULL x, -1 y; END IF;  "},{"title":"CQL0443: shared fragments with conditionals must have exactly one SELECT or WITH...SELECT in each statement list 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0443-shared-fragments-with-conditionals-must-have-exactly-one-select-or-withselect-in-each-statement-list-procedure_name","content":"In a shared fragment with conditionals the top level statement is an &quot;IF&quot;. All of the statement lists in the IF must have exactly one valid select statement. This error indicates that a statement list has the wrong number or type of statement. "},{"title":"CQL0444: this use of the named shared fragment is not legal because of name conflict 'procedure_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0444-this-use-of-the-named-shared-fragment-is-not-legal-because-of-name-conflict-procedure_name","content":"This error will be followed by additional diagnostic information about the call chain that is problematic. For instance: Procedure innermost has a different CTE that is also named foo The above originated from CALL inner USING foo AS source The above originated from CALL middle USING foo AS source The above originated from CALL outer USING foo AS source  This indicates that you are trying to call outer which in turn calls middle which in turn called inner. The conflict happened when the foo parameter was passed in to inner because it already has a CTE named foo that means something else. The way to fix this problem is to rename the CTE in probably the outermost call as that is likely the one you control. Renaming it in the innermost procedure might also be wise if that procedure is using a common name likely to conflict. It is wise to name the CTEs in shared fragments such that they are unlikely to eclipse outer CTEs that will be needed as table parameters. "},{"title":"CQL0445: @attribute(cql:try_is_proc_body) accepts no values​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0445-attributecqltry_is_proc_body-accepts-no-values","content":"The attribute cql:try_is_proc_body cannot be used with any values (e.g.,cql:try_is_proc_body=(...)). "},{"title":"CQL0446: @attribute(cql:try_is_proc_body) cannot be used more than once per procedure​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0446-attributecqltry_is_proc_body-cannot-be-used-more-than-once-per-procedure","content":"The purpose of cql:try_is_proc_body is to indicate that a particular TRYblock contains what should be considered to be the true body of the procedure. As it makes no sense for a procedure to have multiple bodies,cql:try_is_proc_body must appear only once within any given procedure. "},{"title":"CQL0447: virtual table 'table' claims to be eponymous but its module name 'module' differs from its table name​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0447-virtual-table-table-claims-to-be-eponymous-but-its-module-name-module-differs-from-its-table-name","content":"By definition, an eponymous virtual table has the same name as its module. If you use the @eponymous notation on a virtual table, you must also make the module and table name match. "},{"title":"CQL0448: table was marked @delete but it needs to be marked @recreate @delete 'table'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0448-table-was-marked-delete-but-it-needs-to-be-marked-recreate-delete-table","content":"The indicated table was on the recreate plan and was then deleted by adding an @delete(version) attribute. However, the previous @recreate annotation was removed. This would make the table look like it was a baseline table that had been deleted, and it isn't. To correctly drop a table on the @recreate you leave the recreate directive as it was and simply add @delete. No version information is required because the table is on the recreate plan anyway. Example: create table dropping_this ( f1 integer, f2 text ) @recreate(optional_group) @delete;  This error indicates that the @recreate(optional_group) annotation was removed. You should put it back. "},{"title":"CQL0449: unsubscribe does not make sense on non-physical tables 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0449-unsubscribe-does-not-make-sense-on-non-physical-tables-table_name","content":"The indicated table was marked for blob storage or is a backed table. In both cases there is no physical schema associated with it so unsubscribe does not make any sense there. If it's a backed table perhaps the intent was to remove the backing table? "},{"title":"CQL0450: a shared fragment used like a function must be a simple SELECT with no FROM clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0450-a-shared-fragment-used-like-a-function-must-be-a-simple-select-with-no-from-clause","content":"When using a shared fragment like an expression, the shared fragment must consist of a simple SELECT without a FROM clause. That SELECT, however, may contain a nested SELECT expression which, itself, may have a FROM clause. Additional constraints: the target of the call is a shared fragment the target therefore a single select statementthe target therefore has no out-arguments the target has no select clauses other than the select list, e.g. no FROM, WHERE, LIMIT etc.the target returns exactly one column, i.e. it's just one SQL expression "},{"title":"CQL0451: procedure as function call is not compatible with DISTINCT or filter clauses​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0451-procedure-as-function-call-is-not-compatible-with-distinct-or-filter-clauses","content":"Certain built-in functions like COUNT can be used with DISTINCT or FILTER options like so: select count(distinct ...); select sum(...) filter(where ...) over (...)  These options are not valid when calling a procedure as a function and so they generate errors if used. "},{"title":"CQL0452: function may not be used in SQL because it is not supported on old versions of SQLite 'function'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0452-function-may-not-be-used-in-sql-because-it-is-not-supported-on-old-versions-of-sqlite-function","content":"Due to an enabled enforcement (e.g., @enforce_strict sign function;), the indicated function may not be used within SQL because it is not supported on old versions of SQLite. "},{"title":"CQL0453: blob type is not a valid table 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0453-blob-type-is-not-a-valid-table-table_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like so: DECLARE blob_var blob&lt;table_name&gt;  But the named table table_name is not a table. "},{"title":"CQL0454: cursor was not declared for storage 'cursor_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0454-cursor-was-not-declared-for-storage-cursor_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the cursor variable have storage associated with it. This means it must be a value cursor or else a cursor that was fetched using the fetch C form and not the fetch C into [variables] form. The indicated cursor was either not fetched at all, or else is using only the fetch into form so it does not have storage that could be used to create a blob. "},{"title":"CQL0455: blob variable must have a type kind for type safety, 'blob_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0455-blob-variable-must-have-a-type-kind-for-type-safety-blob_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like so: DECLARE blob_name blob;  But it must be: DECLARE blob_name blob&lt;table_name&gt;;  Where table_name is a suitable table. "},{"title":"CQL0456: blob type is a view, not a table 'view_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0456-blob-type-is-a-view-not-a-table-view_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like: DECLARE blob_var blob&lt;view_name&gt;  Where the named type view_name is a view, not a table. "},{"title":"CQL0457: the indicated table is not marked with @attribute(cql:blob_storage) 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0457-the-indicated-table-is-not-marked-with-attributecqlblob_storage-table_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like: DECLARE blob_var blob&lt;table_name&gt;  but the indicated table is missing the necessary attribute @attribute(cql:blob_storage). This attribute is necessary so that CQL can enforce additional rules on the table to ensure that it is viable for blob storage. For instance, the table can have no primary key, no foreign keys, and may not be used in normal SQL statements. "},{"title":"CQL0458: the indicated table may only be used for blob storage 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0458-the-indicated-table-may-only-be-used-for-blob-storage-table_name","content":"The indicated table has been marked with @attribute(cql:blob_storage). This means that it isn't a real table -- it will have no SQL schema. Since it's only a storage shape, it cannot be used in normal operations that use tables such as DROP TABLE,CREATE INDEX, or inside of SELECT statements. The CREATE TABLE construct is used to declare a blob storage type because it's the natural way to define a structure in SQL and also because the usual versioning rules are helpful for such tables. But otherwise, blob storage isn't really a table at all. "},{"title":"CQL0459: table is not suitable for use as blob storage: [reason] 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0459-table-is-not-suitable-for-use-as-blob-storage-reason-table_name","content":"The indicated table was marked with @attribute(cql:blob_storage). This indicates that the table is going to be used to define the shape of blobs that could be stored in the database. It isn't going to be a &quot;real&quot; table. There are a number of reasons why a table might not be a valid as blob storage. For instance: it has a primary keyit has foreign keysit has constraintsit is a virtual table This error indicates that one of these items is present. The specific cause is included in the text of the message. "},{"title":"CQL0460: field of a nonnull reference type accessed before verifying that the cursor has a row 'cursor.field'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0460-field-of-a-nonnull-reference-type-accessed-before-verifying-that-the-cursor-has-a-row-cursorfield","content":"If a cursor has a field of a nonnull reference type (e.g., TEXT NOT NULL), it is necessary to verify that the cursor has a row before accessing the field (unless the cursor has been fetched in such a way that it must have a row, e.g., via FETCH ... FROM VALUES or LOOP FETCH). The reason for this is that, should the cursor not have a row, the field will be NULL despite the nonnull type. Assume we have the following: create table t (x text not null); declare proc requires_text_notnull(x text not null);  The following code is illegal: declare c cursor for select * from t; fetch c; -- ILLEGAL because `c` may not have a row and thus -- `c.x` may be `NULL` call requires_text_notnull(c.x);  To fix it, the cursor must be verified to have a row before the field is accessed: declare c cursor for select * from t; fetch c; if c then -- legal due to the above check call requires_text_notnull(c.x); end if;  Alternatively, one can perform a &quot;negative&quot; check by returning (or using another control flow statement) when the cursor does not have a row: declare c cursor for select * from t; fetch c; if not c then call some_logging_function(&quot;no rows in t&quot;); return; end if; -- legal as we would have returned if `c` did not -- have a row call requires_text_notnull(c.x);  If you are sure that a row must be present, you can throw to make that explicit: declare c cursor for select * from t; fetch c; if not c throw; -- legal as we would have thrown if `c` did not -- have a row call requires_text_notnull(c.x);  "},{"title":"CQL0461: fetch from blob operand is not a blob​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0461-fetch-from-blob-operand-is-not-a-blob","content":"The blob operand in the form FETCH [cursor] FROM BLOB [blob]must be a blob. The given expression is of some other type. "},{"title":"CQL0462: group declared variables must be top level 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0462-group-declared-variables-must-be-top-level-name","content":"A DECLARE GROUP statement for the named enum is happening inside of a procedure. This is not legal. To correct this, move the declaration outside of the procedure. CQL0463: variable definitions do not match in group 'name'​ The two described DECLARE GROUP statements have the same name but they are not identical. The error output contains the full text of both declarations to compare. "},{"title":"CQL0464: group not found 'group_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0464-group-not-found-group_name","content":"The indicated name was used in a context where a variable group name was expected but there is no such group. Perhaps the group was not included (missing an #include) or else there is a typo. "},{"title":"CQL0465 avaiable for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0465-avaiable-for-re-use","content":""},{"title":"CQL0466: the table/view named in an @unsub directive does not exist 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0466-the-tableview-named-in-an-unsub-directive-does-not-exist-name","content":"The indicated name is not a valid table or view. "},{"title":"CQL0467 available for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0467-available-for-re-use","content":""},{"title":"CQL0468: @attribute(cql:shared_fragment) may only be placed on a CREATE PROC statement 'proc_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0468-attributecqlshared_fragment-may-only-be-placed-on-a-create-proc-statement-proc_name","content":"In order to use a shared fragment the compiler must see the full body of the fragment, this is because the fragment will be inlined into the SQL in which it appears. As a consequence it makes no sense to try to apply the attribute to a procedure declaration. Instead put the shared fragment you want to use somewhere where it can be #included in full. example error: @attribute(cql:shared_fragment) declare proc x() (x integer); create proc y() begin with (call x()) select * from x; end;  Instead, include the entire body like so (this example is ultra simple). @attribute(cql:shared_fragment) create proc x() begin select 1 x; -- the procedure body must be present end;  "},{"title":"CQL0469: table/view is already deleted 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0469-tableview-is-already-deleted-name","content":"In an @unsub directive, the indicated table/view has already been deleted. It can no longer be managed via subscriptions. "},{"title":"CQL0470 available for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0470-available-for-re-use","content":""},{"title":"CQL0471 available for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0471-available-for-re-use","content":""},{"title":"CQL0472: table/view is already unsubscribed 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0472-tableview-is-already-unsubscribed-name","content":"In an @unsub directive, the indicated table/view has already been unsubscribed. It doesn't need another unsubscription. "},{"title":"CQL0473: @unsub is invalid because the table/view is still used by 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0473-unsub-is-invalid-because-the-tableview-is-still-used-by-name","content":"This error indicates that you are attempting to @unsub a table/view while there are still other tables/views that refer to it (e.g. by FK). You must @unsub all of those as well in order to safely @unsub the present table/view. All such dependencies will be listed. Note that some of those might, in turn, have the same issue. In short, a whole subtree has to be removed in order to do this operation safely. "},{"title":"CQL0474 available for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0474-available-for-re-use","content":""},{"title":"CQL0475 available for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0475-available-for-re-use","content":""},{"title":"CQL0476 available for re-use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0476-available-for-re-use","content":""},{"title":"CQL0477: interface name conflicts with func name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0477-interface-name-conflicts-with-func-name-name","content":"In a DECLARE INTERFACE statement, the given name conflicts with an already declared function (DECLARE FUNCTION or DECLARE SELECT FUNCTION). You'll have to choose a different name. "},{"title":"CQL0478: interface name conflicts with procedure name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0478-interface-name-conflicts-with-procedure-name-name","content":"In a DECLARE INTERFACE statement, the indicated name already corresponds to a created or declared stored procedure. You'll have to choose a different name. "},{"title":"CQL0479: interface declarations do not match 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0479-interface-declarations-do-not-match-name","content":"The interface was previously declared with a DECLARE INTERFACE statement but when subsequent DECLARE INTERFACE was encountered, it did not match the previous declaration. "},{"title":"CQL0480: declared interface must be top level 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0480-declared-interface-must-be-top-level-name","content":"A DECLARE INTERFACE statement is happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure. "},{"title":"CQL0481: proc name conflicts with interface name 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0481-proc-name-conflicts-with-interface-name-name","content":"In a CREATE PROCEDURE / DECLARE PROCEDURE statement, the given name conflicts with an already declared interface (DECLARE INTERFACE). You'll have to choose a different name. "},{"title":"CQL0482: interface not found 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0482-interface-not-found-name","content":"Interface with the name provided in cql:implements attribute does not exist "},{"title":"CQL0483: table is not suitable for use as backing storage: [reason] 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0483-table-is-not-suitable-for-use-as-backing-storage-reason-table_name","content":"The indicated table was marked with @attribute(cql:backing_table). This indicates that the table is going to be used to as a generic storage location stored in the database. There are a number of reasons why a table might not be a valid as backing storage. For instance: it has foreign keysit has constraintsit is a virtual tableit has schema versioning This error indicates that one of these items is present. The specific cause is included in the text of the message. "},{"title":"CQL0484: procedure '%s' is missing column '%s' of interface '%s'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0484-procedure-s-is-missing-column-s-of-interface-s","content":"Procedure should return all columns defined by the interface (and possibly others). The columns may be returned in any order. "},{"title":"CQL0485: column types returned by proc need to be the same as defined on the interface​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0485-column-types-returned-by-proc-need-to-be-the-same-as-defined-on-the-interface","content":"Procedure should return at least all columns defined by the interface and column type should be the same. "},{"title":"CQL0486: function cannot be both a normal function and an unchecked function, 'function_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0486-function-cannot-be-both-a-normal-function-and-an-unchecked-function-function_name","content":"The same function cannot be declared as a function with unchecked parameters with the NO CHECK clause and then redeclared with typed parameters, or vice versa. --- Declaration of an external function foo with unchecked parameters. DECLARE SELECT FUNCTION foo NO CHECK t text; ... --- A redeclaration of foo with typed paramters. This would be invalid if the previous declaration exists. DECLARE SELECT FUNCTION foo() t text;  Make sure the redeclaration of the function is consistent with the original declaration, or remove the redeclaration. "},{"title":"CQL0487: table is not suitable as backed storage: [reason] 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0487-table-is-not-suitable-as-backed-storage-reason-table_name","content":"The indicated table was marked with @attribute(cql:backing_table). This indicates that the table is going to be used to as a generic storage location stored in the database. There are a number of reasons why a table might not be a valid as backing storage. For instance: it has foreign keysit has constraintsit is a virtual tableit has schema versioning This error indicates that one of these items is present. The specific cause is included in the text of the message. "},{"title":"CQL0488: the indicated table is not declared for backed storage 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0488-the-indicated-table-is-not-declared-for-backed-storage-table_name","content":"When declaring a backed table, you must specify the physical table that will hold its data. The backed table is marked with @attribute(cql:backed_by=table_name). The backing table is marked with @attribute(cql:backing). The backing and backed_by attributes applies extra checks to tables to ensure they are suitable candidates. This error indicates that the named table is not marked as a backed table. "},{"title":"CQL0489: the indicated column is not present in the named backed storage 'table_name.column_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0489-the-indicated-column-is-not-present-in-the-named-backed-storage-table_namecolumn_name","content":"The named table is a backed table, but it does not have the indicated column. "},{"title":"CQL0490: argument must be table.column where table is a backed table 'function\"​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0490-argument-must-be-tablecolumn-where-table-is-a-backed-table-function","content":"The database blob access functions cql_blob_get, cql_blob_create, cql_blob_update all allow you to specify the backed table name and column you are trying to read/create/update. The named function was called with a table.column combination where the table is not a backed table, hence the call is invalid. Note that normally this error doesn't happen because these functions are typically called by CQL itself as part of the rewriting process for backed tables. However it is possible to use them manually, hence they are error checked. "},{"title":"CQL0491: argument 1 must be a table name that is a backed table 'cql_blob_create'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0491-argument-1-must-be-a-table-name-that-is-a-backed-table-cql_blob_create","content":"When using the cql_blob_create helper function, the first argument must be a valid backed table (i.e. one that was marked with @attribute(cql:backed_by=some_backing_table)). The type signature of this table is used to create a hash valid for the type of the blob that is created. This error indicates that the first argument is not even an identifier, much less a table name that is backed. There are more specific errors if the table is not found or the table is not backed. Note that normally this error doesn't happen because this functions is typically called by CQL itself as part of the rewriting process for backed tables. However it is possible to cql_blob_create manually, hence it is error checked. "},{"title":"CQL0492 available for use​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0492-available-for-use","content":""},{"title":"CQL0493: backed storage tables may not be used in indexes/triggers/drop 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0493-backed-storage-tables-may-not-be-used-in-indexestriggersdrop-table_name","content":"The indicated table name was marked as backed storage. Therefore it does not have a physical manifestation, and therefore it cannot be used in an index or in a trigger. You may be able to get the index or trigger you want by creating an index on the backing storage and then using the blob access functions to index a column or check colulmns in a trigger. For instance this index is pretty normal: @attribute(cql:backing_table) create table backing ( k blob primary key, v blob not null ); create index backing_type_index on backing(cql_blob_get_type(k));  This gives you a useful index on the type field of the blob for all backed tables that use backing_table. But generally, physical operations like indices, triggers, and drop are not applicable to backed tables. "},{"title":"CQL0494: mixing adding and removing columns from a shape 'name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0494-mixing-adding-and-removing-columns-from-a-shape-name","content":"When selecting columns from a shape you can use this form LIKE some_shape(name1, name2)  to extract the named columns or this form LIKE some_shape(-name1, -name2)  to extract everything but the named columns. You can't mix the positive and negative forms "},{"title":"CQL0495: no columns were selected in the LIKE expression​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0495-no-columns-were-selected-in-the-like-expression","content":"An expression that is supposed to select some columns from a shape such as LIKE some_shape(-name1, -name2)  ended up removing all the columns from some_shape. "},{"title":"CQL0496: SELECT NOTHING may only appear in the else clause of a shared fragment​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0496-select-nothing-may-only-appear-in-the-else-clause-of-a-shared-fragment","content":"A common case for conditional shared fragments is that there are rows that should be optionally included. The normal way this is handled is to have a condition like this IF something THEN SELECT your_data; ELSE SELECT dummy data WHERE 0; END IF;  The problem here is that dummy_data could be complex and involve a lot of typing to get nothing. To make this less tedious CQL allows: IF something THEN SELECT your_data; ELSE SELECT NOTHING; END IF;  However this is the only place SELECT NOTHING is allowed. It must be: in a procedurewhich is a conditional shared fragmentin the else clause Any violation results in the present error. "},{"title":"CQL0497: FROM clause not supported when updating backed table, 'table_name'​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0497-from-clause-not-supported-when-updating-backed-table-table_name","content":"SQLite supports an extended format of the update statement with a FROM clause. At this time backed tables cannot be updated using this form. This is likely to change fairly soon. "},{"title":"CQL0498: strict UPDATE ... FROM validation requires that the UPDATE statement not include a FROM clause​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0498-strict-update--from-validation-requires-that-the-update-statement-not-include-a-from-clause","content":"@enforce_strict has been use to enable strict update enforcement. When enabled update statements may not include a FROM clause. This is done if the code expects to target SQLite version 3.33 or lower. "},{"title":"CQL0499: alias_of attribute may only be added to a declare function statement​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0499-alias_of-attribute-may-only-be-added-to-a-declare-function-statement","content":"cql:alias_of attributes may only be used in DECLARE FUNC statements or DECLARE PROC statements. "},{"title":"CQL0500: alias_of attribute must be a non-empty string argument​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql0500-alias_of-attribute-must-be-a-non-empty-string-argument","content":"cql:alias_of must have a string argument to indicate the underlying function name that the aliased function references. For example: @attribute(cql:alias_of=foo) declare function bar() int  All subsequent calls to bar() in CQL will call the foo() function. "},{"title":"Appendix 5: JSON Schema Grammar​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-5-json-schema-grammar","content":"What follows is taken from the JSON validation grammar with the tree building rules removed. Snapshot as of Fri Mar 3 21:02:20 PST 2023 "},{"title":"Rules​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#rules-1","content":" json_schema: '{' '&quot;tables&quot;' ':' '[' opt_tables ']' ',' '&quot;virtualTables&quot;' ':' '[' opt_virtual_tables ']' ',' '&quot;views&quot;' ':' '[' opt_views ']' ',' '&quot;indices&quot;' ':' '[' opt_indices ']' ',' '&quot;triggers&quot;' ':' '[' opt_triggers ']' ',' '&quot;attributes&quot;' ':' '[' opt_attribute_list ']' ',' '&quot;queries&quot;' ':' '[' opt_queries ']' ',' '&quot;inserts&quot;' ':' '[' opt_inserts ']' ',' '&quot;generalInserts&quot;' ':' '[' opt_inserts_general ']' ',' '&quot;updates&quot;' ':' '[' opt_updates ']' ',' '&quot;deletes&quot;' ':' '[' opt_deletes ']' ',' '&quot;general&quot;' ':' '[' opt_generals ']' ',' '&quot;declareProcs&quot;' ':' '[' opt_declare_procs']' ',' '&quot;declareFuncs&quot;' ':' '[' opt_declare_funcs']' ',' '&quot;interfaces&quot;' ':' '[' opt_interfaces ']' ',' '&quot;regions&quot;' ':' '[' opt_regions ']' ',' '&quot;adHocMigrationProcs&quot;' ':' '[' opt_ad_hoc_migrations ']' ',' '&quot;enums&quot;' ':' '[' opt_enums ']' ',' '&quot;constantGroups&quot;' ':' '[' opt_const_groups ']' ',' '&quot;subscriptions&quot;' ':' '[' opt_subscriptions ']' '}' ; BOOL_LITERAL: '0' | '1' ; opt_tables: | tables ; tables: table | table ',' tables ; opt_backing_details: | '&quot;isBacking&quot;' ':' '1' ',' | '&quot;isBacked&quot;' ':' '1' ',' '&quot;typeHash&quot;' ':' num_literal ',' ; opt_type_hash: | '&quot;typeHash&quot;' ':' num_literal ',' ; table: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;schema&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' BOOL_LITERAL ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;withoutRowid&quot;' ':' BOOL_LITERAL ',' '&quot;isAdded&quot;' ':' BOOL_LITERAL ',' opt_added_version '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version '&quot;isRecreated&quot;' ':' BOOL_LITERAL ',' opt_recreate_group_name opt_unsub_version opt_backing_details opt_region_info opt_table_indices opt_attributes '&quot;columns&quot;' ':' '[' columns ']' ',' '&quot;primaryKey&quot;' ':' '[' opt_column_names ']' ',' '&quot;primaryKeySortOrders&quot;' ':' '[' opt_sort_order_names ']' ',' opt_primary_key_name '&quot;foreignKeys&quot;' ':' '[' opt_foreign_keys ']' ',' '&quot;uniqueKeys&quot;' ':' '[' opt_unique_keys ']' ',' '&quot;checkExpressions&quot;' ':' '[' opt_check_expressions ']' '}' ; opt_primary_key_name: | '&quot;primaryKeyName&quot;' ':' STRING_LITERAL ',' ; opt_virtual_tables: | virtual_tables ; virtual_tables: virtual_table | virtual_table ',' virtual_tables ; virtual_table: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;schema&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' '0' ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;withoutRowid&quot;' ':' BOOL_LITERAL ',' '&quot;isAdded&quot;' ':' BOOL_LITERAL ',' opt_added_version '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version '&quot;isRecreated&quot;' ':' BOOL_LITERAL ',' opt_region_info '&quot;isVirtual&quot;' ':' '1' ',' '&quot;isEponymous&quot;' ':' BOOL_LITERAL ',' '&quot;module&quot;' ':' STRING_LITERAL ',' opt_module_args opt_attributes '&quot;columns&quot;' ':' '[' columns ']' ',' '&quot;primaryKey&quot;' ':' '[' opt_column_names ']' ',' '&quot;primaryKeySortOrders&quot;' ':' '[' opt_sort_order_names ']' ',' '&quot;foreignKeys&quot;' ':' '[' opt_foreign_keys ']' ',' '&quot;uniqueKeys&quot;' ':' '[' opt_unique_keys ']' ',' '&quot;checkExpressions&quot;' ':' '[' opt_check_expressions ']' '}' ; opt_module_args: | '&quot;moduleArgs&quot;' ':' STRING_LITERAL ',' ; opt_added_version: | '&quot;addedVersion&quot;' ':' any_integer ',' opt_added_migration_proc ; opt_added_migration_proc: | '&quot;addedMigrationProc&quot;' ':' STRING_LITERAL ',' ; opt_unsub_version: | '&quot;unsubscribedVersion&quot;' ':' any_integer ',' ; opt_deleted_version: | '&quot;deletedVersion&quot;' ':' any_integer ',' opt_deleted_migration_proc ; opt_deleted_migration_proc: | '&quot;deletedMigrationProc&quot;' ':' STRING_LITERAL ',' ; opt_recreate_group_name: | '&quot;recreateGroupName&quot;' ':' STRING_LITERAL ',' ; opt_index_names: | index_names ; index_names: STRING_LITERAL | STRING_LITERAL ',' index_names ; opt_arg_names: | arg_names ; arg_names: STRING_LITERAL | STRING_LITERAL ',' arg_names ; opt_column_names: | column_names ; column_names: STRING_LITERAL | STRING_LITERAL ',' column_names ; opt_table_names: | table_names ; table_names: STRING_LITERAL | STRING_LITERAL ',' table_names ; opt_view_names: | view_names ; view_names: STRING_LITERAL | STRING_LITERAL ',' view_names ; opt_procedure_names: | procedure_names ; procedure_names: STRING_LITERAL | STRING_LITERAL ',' procedure_names ; opt_sort_order_names: | sort_order_names ; sort_order_names: STRING_LITERAL | STRING_LITERAL ',' sort_order_names ; columns: column | column ',' columns ; column: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' opt_attributes '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL ',' '&quot;isAdded&quot;' ':' BOOL_LITERAL ',' opt_added_version '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version opt_default_value opt_collate opt_check_expr opt_type_hash '&quot;isPrimaryKey&quot;' ':' BOOL_LITERAL ',' '&quot;isUniqueKey&quot;' ':' BOOL_LITERAL ',' '&quot;isAutoIncrement&quot;' ':' BOOL_LITERAL '}' ; opt_collate : | '&quot;collate&quot;' ':' STRING_LITERAL ',' ; opt_check_expr: | '&quot;checkExpr&quot;' ':' STRING_LITERAL ',' '&quot;checkExprArgs&quot;' ':' '[' opt_arg_names ']' ',' ; opt_default_value: | '&quot;defaultValue&quot;' ':' any_literal ',' ; opt_foreign_keys : | foreign_keys ; opt_kind: | '&quot;kind&quot;' ':' STRING_LITERAL ',' ; opt_is_sensitive: | '&quot;isSensitive&quot;' ':' '1' ',' ; foreign_keys : foreign_key | foreign_key ',' foreign_keys ; foreign_key : '{' opt_name '&quot;columns&quot;' ':' '[' column_names ']' ',' '&quot;referenceTable&quot;' ':' STRING_LITERAL ',' '&quot;referenceColumns&quot;' ':' '[' column_names ']' ',' '&quot;onUpdate&quot;' ':' STRING_LITERAL ',' '&quot;onDelete&quot;' ':' STRING_LITERAL ',' '&quot;isDeferred&quot;' ':' BOOL_LITERAL '}' ; opt_unique_keys : | unique_keys ; unique_keys : unique_key | unique_key ',' unique_keys ; unique_key: '{' opt_name '&quot;columns&quot;' ':' '[' column_names ']' ',' '&quot;sortOrders&quot;' ':' '[' sort_order_names ']' '}' ; opt_check_expressions: | check_expressions ; check_expressions: check_expression | check_expression ',' check_expressions ; check_expression: '{' opt_name '&quot;checkExpr&quot;' ':' STRING_LITERAL ',' '&quot;checkExprArgs&quot;' ':' '[' ']' '}' ; opt_name: | '&quot;name&quot;' ':' STRING_LITERAL ',' ; opt_table_indices: | table_indices ; table_indices: '&quot;indices&quot;' ':' '[' opt_index_names ']' ',' ; opt_attributes: | attributes ; attributes: '&quot;attributes&quot;' ':' '[' attribute_list ']' ',' ; opt_attribute_list: | attribute_list ; attribute_list: attribute | attribute ',' attribute_list ; attribute: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;value&quot;' ':' attribute_value '}' ; attribute_array: '[' opt_attribute_value_list ']' ; opt_attribute_value_list: | attribute_value_list ; attribute_value_list: attribute_value | attribute_value ',' attribute_value_list ; attribute_value: any_literal | attribute_array ; any_integer: BOOL_LITERAL | INT_LITERAL ; any_literal: BOOL_LITERAL | INT_LITERAL | '-' INT_LITERAL | LONG_LITERAL | '-' LONG_LITERAL | REAL_LITERAL | '-' REAL_LITERAL | STRING_LITERAL | NULL_LITERAL ; num_literal: BOOL_LITERAL | INT_LITERAL | '-' INT_LITERAL | LONG_LITERAL | '-' LONG_LITERAL | REAL_LITERAL | '-' REAL_LITERAL ; opt_views: | views ; views: view | view ',' views ; view: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' BOOL_LITERAL ',' '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version opt_region_info opt_attributes projection '&quot;select&quot;' ':' STRING_LITERAL ',' '&quot;selectArgs&quot;' ':' '[' ']' ',' dependencies '}' ; opt_region_info: | '&quot;region&quot;' ':' STRING_LITERAL ',' | '&quot;region&quot;' ':' STRING_LITERAL ',' '&quot;deployedInRegion&quot;' ':' STRING_LITERAL ',' ; opt_projection: | projection ; projection: '&quot;projection&quot;' ':' '[' projected_columns ']' ',' ; projected_columns: projected_column | projected_column ',' projected_columns ; projected_column: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; opt_indices: | indices ; indices: index | index ',' indices ; index: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;isUnique&quot;' ':' BOOL_LITERAL ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version opt_region_info opt_partial_index_where opt_attributes '&quot;columns&quot;' ':' '[' column_names ']' ',' '&quot;sortOrders&quot;' ':' '[' sort_order_names ']' '}' ; opt_partial_index_where: | '&quot;where&quot;' ':' STRING_LITERAL ',' ; opt_triggers: | triggers ; triggers: trigger | trigger ',' triggers ; trigger: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;target&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' BOOL_LITERAL ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version before_after_instead ',' delete_insert_update ',' opt_for_each_row opt_when_expr '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' ',' opt_region_info opt_attributes dependencies '}' ; before_after_instead: '&quot;isBeforeTrigger&quot;' ':' '1' | '&quot;isAfterTrigger&quot;' ':' '1' | '&quot;isInsteadOfTrigger&quot;' ':' '1' ; delete_insert_update: '&quot;isDeleteTrigger&quot;' ':' '1' | '&quot;isInsertTrigger&quot;' ':' '1' | '&quot;isUpdateTrigger&quot;' ':' '1' ; opt_for_each_row: | '&quot;forEachRow&quot;' ':' BOOL_LITERAL ',' ; opt_when_expr: | '&quot;whenExpr&quot;' ':' STRING_LITERAL ',' '&quot;whenExprArgs&quot;' ':' '[' opt_arg_names ']' ',' ; dependencies: opt_insert_tables opt_update_tables opt_delete_tables opt_from_tables opt_uses_procedures opt_uses_views '&quot;usesTables&quot;' ':' '[' opt_table_names ']' ; opt_uses_views: | '&quot;usesViews&quot;' ':' '[' opt_view_names ']' ',' ; opt_insert_tables: | '&quot;insertTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_update_tables: | '&quot;updateTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_delete_tables: | '&quot;deleteTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_from_tables: | '&quot;fromTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_uses_procedures : | '&quot;usesProcedures&quot;' ':' '[' opt_procedure_names ']' ',' ; opt_queries: | queries ; queries: query | query ',' queries ; query: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes projection '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_args: | args ; args: arg | arg ',' args ; arg: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;argOrigin&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; opt_inserts: | inserts ; inserts: insert | insert ',' inserts ; insert : '{' insert_details ',' '&quot;values&quot;' ':' '[' opt_values ']' '}' ; opt_inserts_general: | inserts_general ; inserts_general: insert_general | insert_general ',' inserts_general ; insert_details: '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' ',' '&quot;statementType&quot;' ':' STRING_LITERAL ',' '&quot;columns&quot;' ':' '[' column_names ']' insert_general : '{' insert_details '}' ; opt_values: | values ; values: value | value ',' values ; value: '{' '&quot;value&quot;' ':' STRING_LITERAL ',' '&quot;valueArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_updates: | updates ; updates: update | update ',' updates ; update : '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_deletes: | deletes ; deletes: delete | delete ',' deletes ; delete : '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_generals: | generals ; generals: general | general ',' generals ; general: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_complex_args ']' ',' dependencies ',' opt_regions opt_attributes opt_projection opt_result_contract '&quot;usesDatabase&quot;' ':' BOOL_LITERAL '}' ; opt_result_contract: | '&quot;hasSelectResult&quot;' ':' '1' ',' | '&quot;hasOutResult&quot;' ':' '1' ',' | '&quot;hasOutUnionResult&quot;' ':''1' ',' ; opt_complex_args: | complex_args ; complex_args: complex_arg | complex_arg ',' complex_args ; complex_arg: '{' binding '&quot;name&quot;' ':' STRING_LITERAL ',' opt_arg_origin '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; binding: | '&quot;binding&quot;' ':' '&quot;inout&quot;' ',' | '&quot;binding&quot;' ':' '&quot;out&quot;' ',' ; opt_arg_origin: | arg_origin ; arg_origin: '&quot;argOrigin&quot;' ':' STRING_LITERAL ',' ; opt_enums: | enums ; enums: enum | enum ',' enums ; enum: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' '&quot;isNotNull&quot;' ':' '1' ',' '&quot;values&quot;' ':' '[' enum_values ']' '}' ; enum_values: enum_value | enum_value ',' enum_values ; enum_value: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;value&quot;' ':' num_literal '}' ; opt_declare_procs: | declare_procs ; declare_procs: declare_proc | declare_proc ',' declare_procs declare_proc: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;args&quot;' ':' '[' opt_complex_args ']' ',' opt_attributes opt_projection '&quot;usesDatabase&quot;' ':' BOOL_LITERAL '}' ; opt_declare_funcs: | declare_funcs ; declare_funcs: declare_func | declare_func ',' declare_funcs ; declare_func: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;args&quot;' ':' '[' opt_complex_args ']' ',' opt_attributes opt_return_type '&quot;createsObject&quot;' ':' BOOL_LITERAL '}' ; opt_return_type: | '&quot;returnType&quot;' ':' return_type ',' ; return_type: '{' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; opt_interfaces: | interfaces ; interfaces: interface | interface ',' interfaces ; interface: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' opt_attributes '&quot;projection&quot;' ':' '[' projected_columns ']' '}' ; opt_subscriptions: | subscriptions ; subscriptions: subscription | subscription ',' subscriptions ; subscription: '{' '&quot;type&quot;' ':' STRING_LITERAL ',' '&quot;table&quot;' ':' STRING_LITERAL ',' opt_region_info '&quot;version&quot;' ':' any_integer '}' ; opt_const_groups: | const_groups ; const_groups: const_group | const_group ',' const_groups ; const_group: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;values&quot;' ':' '[' const_values ']' '}' ; const_values: const_value | const_value ',' const_values ; const_value: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind '&quot;isNotNull&quot;' ':' BOOL_LITERAL ',' '&quot;value&quot;' ':' num_literal '}' | '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind '&quot;isNotNull&quot;' ':' BOOL_LITERAL ',' '&quot;value&quot;' ':' STRING_LITERAL '}' ; opt_regions: | regions ; regions: region | region ',' regions ; region: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;isDeployableRoot&quot;' ':' BOOL_LITERAL ',' '&quot;deployedInRegion&quot;' ':' STRING_LITERAL ',' '&quot;using&quot;' ':' '[' opt_region_names ']' ',' '&quot;usingPrivately&quot;' ':' '[' opt_bool_list ']' '}' ; opt_region_names: | region_names ; region_names: STRING_LITERAL | STRING_LITERAL ',' region_names ; opt_bool_list: | bool_list ; bool_list: BOOL_LITERAL | BOOL_LITERAL ',' bool_list ; opt_ad_hoc_migrations: | ad_hoc_migrations ; ad_hoc_migrations: ad_hoc_migration | ad_hoc_migration ',' ad_hoc_migrations ; ad_hoc_migration: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' opt_attributes '&quot;version&quot;' ':' any_integer '}' | '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' opt_attributes '&quot;onRecreateOf&quot;' ':' STRING_LITERAL '}' ;  "},{"title":"Appendix 6: CQL In 20 Minutes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-6-cql-in-20-minutes","content":"What follows is a series of examples intended to illustrate the most important features of the CQL language. This appendix was significantly influenced by a similar article on Python at https://learnxinyminutes.com/docs/python/ Also of interest: http://sqlite.orghttps://learnxinyminutes.com/docs/sql And with no further delay, CQL in 20 minutes... -- Single line comments start with two dashes /* C style comments also work * * C pre-processor features like #include and #define are generally available * CQL is typically run through the C pre-processor before it is compile. */ /********************************************************** * 1. Primitive Datatypes and Operators *********************************************************/ -- You have numbers 3 -- an integer 3L -- a long integer 3.5 -- a real literal 0x10 -- 16 in hex -- Math is what you would expect 1 + 1 --&gt; 2 8 - 1 --&gt; 7 10 * 2 --&gt; 20 35.0 / 5 --&gt; 7.0 -- Modulo operation, same as C and SQLite 7 % 3 --&gt; 1 -7 % 3 --&gt; -1 7 % -3 --&gt; 1 -7 % 3 --&gt; -1 -- Bitwise operators bind left to right like in SQLite 1 | 4 &amp; 3 --&gt; 1 (not 0) -- Enforce precedence with parentheses 1 + 3 * 2 --&gt; 7 (1 + 3) * 2 --&gt; 8 -- Use true and false for bools, nullable bool is possible true --&gt; how to true false --&gt; how to false null --&gt; null means &quot;unknown&quot; in CQL like SQLite -- Negate with not not true --&gt; false not false --&gt; true not null --&gt; null (not unknown is unknown) -- Logical Operators 1 and 0 --&gt; 0 0 or 1 --&gt; 1 0 and x --&gt; 0 and x not evaluated 1 or x --&gt; 1 and x not evaluated -- Remember null is &quot;unknown&quot; null or false --&gt; null null or true --&gt; true null and false --&gt; false null and true --&gt; null -- Non-zero values are truthy 0 --&gt; false 4 --&gt; true -6 --&gt; true 0 and 2 --&gt; 0 (false) -5 or 0 --&gt; 1 (true) -- Equality is == or = 1 == 1 --&gt; true 1 = 1 --&gt; true (= and == are the same thing) 2 == 1 --&gt; false -- Note that null is not equal to anything (like SQL) null == 1 --&gt; null (hence not true) null == null --&gt; null (hence not true) &quot;x&quot; == &quot;x&quot; --&gt; true -- IS lets you compare against null 1 IS 1 --&gt; true 2 IS 1 --&gt; false null IS 1 --&gt; false null IS null --&gt; true (Unknown is Unknown? Yes it is!) &quot;x&quot; IS &quot;x&quot; --&gt; true -- x IS NOT y is the same as NOT (x IS y) 1 IS NOT 1 --&gt; false 2 IS NOT 1 --&gt; true null IS NOT 1 --&gt; true null IS NOT null --&gt; false &quot;x&quot; IS NOT &quot;x&quot; --&gt; false -- Inequality is != or &lt;&gt; 1 != 1 --&gt; false 2 &lt;&gt; 1 --&gt; true null != 1 --&gt; null null &lt;&gt; null --&gt; null -- More comparisons 1 &lt; 10 --&gt; true 1 &gt; 10 --&gt; false 2 &lt;= 2 --&gt; true 2 &gt;= 2 --&gt; true 10 &lt; null --&gt; null -- To test if a value is in a range 1 &lt; 2 and 2 &lt; 3 --&gt; true 2 &lt; 3 and 3 &lt; 2 --&gt; false -- BETWEEN makes this look nicer 2 between 1 and 3 --&gt; true 3 between 2 and 2 --&gt; false -- Strings are created with &quot;x&quot; or 'x' &quot;This is a string.\\n&quot; -- can have C style escapes (no embedded nulls) &quot;Th\\x69s is a string.\\n&quot; -- even hex literals 'This isn''t a C style string' -- use '' to escape single quote ONLY /********************************************************** * 2. Simple Variables *********************************************************/ -- CQL can call simple libc methods with a no-check declaration -- we'll need this for later examples so we can do something -- with our expressions (i.e. print them) declare procedure printf no check; call printf(&quot;I'm CQL. Nice to meet you!\\n&quot;); -- Variables are declared with DECLARE. -- Keywords and identifiers are not case sensitive. declare x integer not null; -- You can call it X, it is the same thing. set X := 0; -- All variables begin with a null value if allowed, else a zero value. declare y integer not null; if y == 0 then call printf(&quot;Yes, this will run.\\n&quot;); end if; -- A nullable variable (i.e. not marked with not null) is initialized to null declare z real; if z is null then call printf(&quot;Yes, this will run.\\n&quot;); end if; -- The various types declare a_blob blob; declare a_string text; declare a_real real; declare an_int integer; declare a_long long; declare an_object object; -- There are some typical SQL synonyms declare an_int int; declare a_long long integer; declare a_long long int; declare a_long long_int; -- The basic types can be tagged to make them less miscible declare m real&lt;meters&gt;; declare kg real&lt;kilos&gt;; set m := kg; -- error! -- Object variables can also be tagged so that they are not mixed-up easily declare dict object&lt;dict&gt; not null; declare list object&lt;list&gt; not null; set dict := create_dict(); -- an external function that creates a dict set dict := create_list(); -- error set list := create_list(); -- ok set list := dict; -- error -- Implied type initialization LET i := 1; -- integer not null LET l := 1L; -- long not null LET t := &quot;x&quot;; -- text not null LET b := x IS y; -- bool not null LET b := x = y; -- bool (maybe not null depending on x/y) -- The psuedo function &quot;nullable&quot; converts the type of its arg to the nullable -- version of the same thing. LET n_i := nullable(1); -- nullable integer variable initialized to 1 LET l_i := nullable(1L); -- nullable long variable initialized to 1 /********************************************************** * 3. Control Flow *********************************************************/ -- Just make a variable declare some_var integer not null; set some_var := 5 -- Here is an IF statement if some_var &gt; 10 then call printf(&quot;some_var is totally bigger than 10.\\n&quot;) else if some_var &lt; 10 then -- else if is optional call printf(&quot;some_var is smaller than 10.\\n&quot;) else -- else is optional call printf(&quot;some_var is indeed 10.\\n&quot;) end if; -- WHILE loops iterate as usual declare i integer not null; set i := 0; while i &lt; 5 begin call printf(&quot;%d\\n&quot;, i); set i := i + 1; end; -- Use LEAVE to end a loop early declare i integer not null; set i := 0; while i &lt; 500 begin if i &gt;= 5 then -- we are not going to get anywhere near 500 leave; end if; call printf(&quot;%d\\n&quot;, i); set i := i + 1; end; -- Use CONTINUE to go back to the loop test declare i integer not null; set i := 0; while i &lt; 500 begin set i := i + 1; if i % 2 then -- Note: we to do this after &quot;i&quot; is incremented! -- to avoid an infinite loop continue; end if; -- odd numbers will not be printed because of continue above call printf(&quot;%d\\n&quot;, i); end; /********************************************************** * 4. Complex Expression Forms *********************************************************/ -- Case is an expression, so it is more like the C &quot;?:&quot; operator -- than a switch statement. It is like &quot;?:&quot; on steroids. case i -- a switch expression is optional when 1 then &quot;one&quot; -- one or more cases when 2 then &quot;two&quot; else &quot;other&quot; -- else is optional end; -- Case with no common expression is a series of independent tests case when i == 1 then &quot;i = one&quot; -- booleans could be completely unrelated when j == 2 then &quot;j = two&quot; -- first match wins else &quot;other&quot; end; -- If nothing matches the cases, the result is null. -- The following expression yields null because 7 is not 1. case 7 when 1 then &quot;one&quot; end -- Case is just an expression, so it can nest case X when 1 case y when 1 &quot;x:1 y:1&quot; else &quot;x:1 y:other&quot; end else case when z == 1 &quot;x:other z:1&quot; else &quot;x:other z:other&quot; end end; -- IN is used to test for membership 5 IN (1, 2, 3, 4, 5) --&gt; true 7 IN (1, 2) --&gt; false null in (1, 2, 3) --&gt; null null in (1, null, 3) --&gt; null (null == null is not true) 7 NOT IN (1, 2) --&gt; true null not in (null, 3) --&gt; null /********************************************************** * 4. Working with and &quot;getting rid of&quot; null *********************************************************/ -- Null can be annoying, you might need a not null value. -- In most operations null is radioactive: null + x --&gt; null null * x --&gt; null null == null --&gt; null -- IS and IS NOT always return 0 or 1 null is 1 -&gt; 0 1 is not null -&gt; 1 -- COALESCE returns the first non null arg, or the last arg if all were null. -- If the last arg is not null, you get a non null result for sure. -- The following is never null, but it's false if either x or y is null COALESCE(x==y, false) -&gt; thought excercise: how is this different than x IS y? -- IFNULL is coalesce with 2 args only (COALESCE is more general) IFNULL(x, -1) --&gt; use -1 if x is null -- The reverse, NULLIF, converts a sentinel value to unknown, more exotic NULLIF(x, -1) --&gt; if x is -1 then use null -- the else part of a case can get rid of nulls CASE when x == y then 1 else 0 end; --&gt; true iff x = y and neither is null -- CASE can be used to give you a default value after various tests -- The following expression is never null; &quot;other&quot; is returned if x is null. CASE when x &gt; 0 then &quot;pos&quot; when x &lt; 0 then &quot;neg&quot; else &quot;other&quot; end; -- You can &quot;throw&quot; out of the current procedure (see exceptions below) declare x integer not null; set x := ifnull_throw(nullable_int_expr); -- returns non null, throws if null -- If you have already tested the expression then control flow analysis -- improves its type to &quot;not null&quot;. Many common check patterns are recognized. if nullable_int_expr is not null then -- nullable_int_expression is known to be not null in this context set x := nullable_int_expr; end if; /********************************************************** * 5. Tables, Views, Indices, Triggers *********************************************************/ -- Most forms of data definition language DDL are supported. -- &quot;Loose&quot; DDL (outside of any procedure) simply declares -- schema, it does not actually create it; the schema is assumed to -- exist as you specified. create table T1( id integer primary key, t text, r real ); create table T2( id integer primary key references T1(id), l long, b blob ); -- CQL can take a series of schema declarations (DDL) and -- automatically create a procedure that will materialize -- that schema and even upgrade previous versions of the schema. -- This system is discussed in Chapter 10 of The Guide. -- To actually create tables and other schema you need -- procedures that look like the below: create proc make_tables() begin create table T1 if not exists ( id integer primary key, t text, r real ); end; -- Views are supported create view V1 as (select * from T1); -- Triggers are supported create trigger if not exists trigger1 before delete on T1 begin delete from T2 where id = old.id; end; -- Indices are supported create index I1 on T1(t); create index I2 on T1(r); -- The various drop forms are supported drop index I1; drop index I2; drop view V1; drop table T2; drop table T1; -- A complete discussion of DDL is out of scope, refer to sqlite.org /********************************************************** * 6. Selecting Data *********************************************************/ -- We will use this scratch variable in the following examples declare rr real; -- First observe CQL is a two-headed language set rr := 1+1; -- this is evaluated in generated C code set rr := (select 1+1); -- this expresion goes to SQLite; SQLite does the addition -- CQL tries to do most things the same as SQLite in the C context -- but some things are exceedingly hard to emulate correctly. -- Even simple looking things such as: set rr := (select cast(&quot;1.23&quot; as real)); --&gt; rr := 1.23 set rr := cast(&quot;1.23&quot; as real); --&gt; error (not safe to emulate SQLite) -- In general, numeric/text conversions have to happen in SQLite context -- because the specific library that does the conversion could be and usually -- is different than the one CQL would use. It would not do to give different answers -- in one context or another so those conversions are simply not supported. -- Loose concatenation is not supported because of the implied conversions. -- Loose means &quot;not in the context of a SQL statement&quot;. set r := 1.23; set r := (select cast(&quot;100&quot;||r as real)); --&gt; 1001.23 (a number) set r := cast(&quot;100&quot;||r as real); --&gt; error, concat not supported in loose expr -- A simple insertion insert into T1 values (1, &quot;foo&quot;, 3.14); -- Finally, reading from the database set r := (select r from T1 where id = 1); --&gt; r = 3.14 -- The (select ...) form requires the result to have at least one row. -- You can use IF NOTHING forms to handle other cases such as: set r := (select r from T1 where id = 2 if nothing -1); --&gt; r = -1 -- If the SELECT statement might return a null result you can handle that as well set r := (select r from T1 where id = 2 if nothing or null -1); --&gt; r = -1 -- With no IF NOTHING clause, lack of a row will cause the SELECT expression to throw -- an exception. IF NOTHING THROW merely makes this explicit. set r := (select r from T1 where id = 2 if nothing throw); --&gt; will throw /********************************************************** * 6. Procedures, Results, Exceptions *********************************************************/ -- Procedures are a list of statements that can be executed, with arguments. create proc hello() begin call printf(&quot;Hello, world\\n&quot;); end; -- IN, OUT, and INOUT parameters are possible create proc swizzle(x integer, inout y integer, out z real not null) begin set y := x + y; -- any computation you like -- bizarre way to compute an id but this is just an illustration set z := (select r from T1 where id = x if nothing or null -1); end; -- Procedures like &quot;hello&quot; (above) have a void signature -- they return nothing -- as nothing can go wrong. Procedures that use the database like &quot;swizzle&quot; (above) -- can return an error code if there is a problem. -- &quot;will_fail&quot; (below) will always return SQLITE_CONSTRAINT, the second insert -- is said to &quot;throw&quot;. In CQL exceptions are just result codes. create proc will_fail() begin insert into T1 values (1, &quot;x&quot;, 1); insert into T1 values (1, &quot;x&quot;, 1); --&gt; duplicate key end; -- DML that fails generates an exception and -- exceptions can be caught. Here is a example: create proc upsert_t1( id_ integer primary key, t_ text, r_ real ) begin begin try -- try to insert insert into T1(id, t, r) values (id_, t_, r_); end try; begin catch -- if the insert fails, try to update update T1 set t = t_, r = r_ where id = id_; end catch; end; -- Shapes can be very useful in avoiding boilerplate code -- the following is equivalent to the above. -- More on shapes later. create proc upsert_t1(LIKE t1) -- my args are the same as the columns of T1 begin begin try insert into T1 from arguments end try; begin catch update T1 set t = t_, r = r_ where id = id_; end catch; end; -- You can (re)throw an error explicitly. -- If there is no current error you get SQLITE_ERROR create proc upsert_wrapper(LIKE t1) -- my args are the same as the columns of T1 begin if r_ &gt; 10 then throw end if; -- throw if r_ is too big call upsert_t1(from arguments); end; -- Procedures can also produce a result set. -- The compiler generates the code to create this result set -- and helper functions to read rows out of it. create proc get_low_r(r_ real) begin -- optionally insert some rows or do other things select * from T1 where T1.r &lt;= r_; end; -- A procedure can choose between various results, the choices must be compatible. -- The last &quot;select&quot; to run controls the ultimate result. create proc get_hi_or_low(r_ real, hi_not_low bool not null) begin -- trying to do this with one query would result in a poor plan, so -- instead we use two economical queries. if hi_not_low then select * from T1 where T1.r &gt;= r_; else select * from T1 where T1.r &lt;= r_; end if; end; -- Using IF to create to nice selects above is a powerful thing. -- SQLite has no IF, if we tried to create a shared query we get -- something that does not use indices at all. As in the below. -- The two-headed CQL beast has its advantages! select * from T1 where case hi_not_low then T1.r &gt;= r_ else T1.r &lt;= r_ end; -- You can get the current return code and use it in your CATCH logic. -- This upsert is a bit better than the first: create proc upsert_t1(LIKE t1) -- my args are the same as the columns of T1 begin begin try insert into T1 from arguments end try; begin catch; if @rc == 19 /* SQLITE_CONSTRAINT */ then update T1 set t = t_, r = r_ where id = id_; else throw; -- rethrow, something bad happened. end if; end catch; end; -- By convention, you can call a procedure that has an OUT argument -- as its last argument using function notation. The out argument -- is used as the return value. If the called procedure uses the -- database then it could throw which causes the caller to throw -- as usual. create proc fib(n integer not null, out result integer not null) begin set result := case n &lt;= 2 then 1 else fib(n-1) + fib(n-2) end; end; /********************************************************** * 7. Statement Cursors *********************************************************/ -- Statement cursors let you iterate over a select result. -- Here we introduce cursors, LOOP and FETCH. create proc count_t1(r_ real, out rows_ integer not null) begin declare rows integer not null; -- starts at zero guaranteed declare C cursor for select * from T1 where r &lt; r_; loop fetch C -- iterate until fetch returns no row begin -- goofy code to illustrate you can process the cursor -- in whatever way you deem appropriate if C.r &lt; 5 then rows := rows + 1; -- count rows with C.r &lt; 5 end if; end; set rows_ := rows; end; -- Cursors can be tested for presence of a row -- and they can be closed before the enumeration is finished. -- As before the below is somewhat goofy example code. create proc peek_t1(r_ real, out rows_ integer not null) begin /* rows_ is set to zero for sure! */ declare C cursor for select * from T1 where r &lt; r_ limit 2; open C; -- this is no-op, present because other systems have it fetch C; -- fetch might find a row or not if C then -- cursor name as bool indicates presence of a row set rows_ = rows_ + (C.r &lt; 5); fetch C; set rows_ = rows_ + (C and C.r &lt; 5); end if; close C; -- cursors auto-closed at end of method but early close possible end; -- The FETCH...INTO form can be used to fetch directly into variables fetch C into id_, t_, r_; --&gt; loads named locals instead of C.id, C.t, C.r -- A procedure can be the source of a cursor declare C cursor for call get_low_r(3.2); -- valid cursor source -- OUT can be used to create a result set that is just one row create proc one_t1(r_ real) begin declare C cursor for select * from T1 where r &lt; r_ limit 1; fetch C; out C; -- emits a row if we have one, no row is ok too, empty result set. end; /********************************************************** * 8. Value Cursors, Out, and Out Union *********************************************************/ -- To consume a procedure that uses &quot;out&quot; you can declare a value cursor. -- By itself such as cursor does not imply use of the database, but often -- the source of the cursor uses the database. In this example -- consume_one_t1 uses the database because of the call to one_t1. create proc consume_one_t1() begin -- a cursor whose shape matches the one_t1 &quot;out&quot; statement declare C cursor like one_t1; -- load it from the call fetch C from call one_t1(7); if C.r &gt; 10 then -- use values as you see fit call printf(&quot;woohoo&quot;); end if; end; -- You can do the above in one step with the compound form: declare C cursor fetch from call one_t1(7); -- declare and fetch -- Value cursors can come from anywhere and can be a procedure result create proc use_t1_a_lot() begin -- T1 is the same shape as one_t1, this will work, too declare C cursor like T1; fetch C from call one_t1(7); -- load it from the call -- some arbitrary logic might be here -- load C again with different args fetch C from call one_t1(12); -- load it again -- some arbitrary logic might be here -- now load C yet again with explicit args fetch C using 1 id, &quot;foo&quot; t, 8.2 r; -- now return it out C; end; -- Make a complex result set one row at a time create proc out_union_example() begin -- T1 is the same shape as one_t1, this will work, too declare C cursor like T1; -- load it from the call fetch C from call one_t1(7); -- note out UNION rather than just out, indicating potentially many rows out union C; -- load it again with different args fetch C from call one_t1(12); out union C; -- do something, then maybe load it again with explicit args fetch C using 1 id, &quot;foo&quot; t, 8.2 r; out union C; -- we have generated a 3 row result set end; -- Consume the above create proc consume_result() begin declare C cursor for call out_union_example(); loop fetch C begin -- use builtin cql_cursor_format to make the cursor into a string call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); --&gt; prints every column and value end; end; /********************************************************** * 9. Named Types and Enumerations *********************************************************/ -- Create a simple named types declare my_type type integer not null; -- make an alias for integer not null declare i my_type; -- use it, &quot;i&quot; is an integer -- Mixing in type kinds is helpful declare distance type real&lt;meters&gt;; -- e.g., distances to be measured in meters declare time type real&lt;seconds&gt;; -- e.g., time to be measured in seconds declare job_id type long&lt;job_id&gt;; declare person_id type long&lt;person_id&gt;; -- With the above done -- * vars/cols of type &quot;distance&quot; are incompatible with those of type &quot;time&quot; -- * vars/cols of types job_id are incompatible with person_id -- This is true even though the underlying type is the same for both! -- ENUM declarations can have any numeric type as their base type declare enum implement integer ( pencil, -- values start at 1 unless you use = to choose something pen, -- the next enum gets previous + 1 as its value (2) brush = 7 -- with = expression you get the indicated value ); -- The above also implicitly does this declare implement type integer&lt;implement&gt; not null; -- Using the enum -- simply use dot notation declare impl implement; set impl := implement.pen; -- value &quot;2&quot; -- You can emit an emum into the current .h file we are going to generate. -- Do not put this directive in an include file, you want it to go to one place. -- Instead, pick one compiland that will &quot;own&quot; the emission of the enum. -- C code can then #include that one .h file. @emit_enums implement; /********************************************************** * 10. Shapes and Their Uses *********************************************************/ -- Shapes first appeared to help define value cursors like so: -- A table or view name defines a shape declare C cursor like T1; -- The result of a proc defines a shape declare D cursor like one_t1; -- A dummy select statement defines a shape (the select does not run) -- this one is the same as (x integer not null, y text not null) declare E cursor like select 1 x, &quot;2&quot; y; -- Another cursor defines a shape declare F cursor like C; -- The arguments of a procedure define a shape. If you have -- create proc count_t1(r_ real, out rows_ integer not null) ... -- the shape will be: -- (r_ real, rows_ integer not null) declare G cursor like count_t1 arguments; -- A loaded cursor can be used to make a call call count_t1(from G); -- the args become G.r_, G.rows_ -- A shape can be used to define a procedures args, or some of the args -- In the following &quot;p&quot; will have arguments:s id_, t_, and r_ with types -- matching table T1. -- Note: To avoid ambiguity, an _ was added to each name! create proc p(like T1) begin -- do whatever you like end; -- The arguments of the current procedure are a synthetic shape -- called &quot;arguments&quot; and can used where other shapes can appear. -- For instance, you can have &quot;q&quot; shim to &quot;p&quot; using this form: create proc q(like T1, print bool not null) begin -- maybe pre-process, silly example set id_ := id_ + 1; -- shim to p call p(from arguments); -- pass my args through, whatever they are -- maybe post-process, silly example set r_ := r_ - 1; if print then -- convert args to cursor declare C like q arguments; fetch C from arguments; call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); --&gt; prints every column and value end if; -- insert a row based on the args insert into T1 from arguments; end; -- You an use a given shape more than once if you name each use. -- This would be more exciting if T1 was like a &quot;person&quot; or something. create proc r(a like T1, b like T1) begin call p(from a); call p(from b); -- you can refer to a.id, b.id etc. declare C like a; fetch C from a; call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); fetch C from b; call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); end; -- Shapes can be subsetted, for instance in the following example -- only the arguments that match C are used in the FETCH. fetch C from arguments(like C); -- Fetch the columns of D into C using the cursor D for the data source. -- Other columns get default values. fetch C(like D) from D; -- Use the D shape to load C, dummy values for the others. -- In this example, dummy_seed means use the provided value, 11, for -- any numerics that are not specified (not in D) and and use -- &quot;col_name_11&quot; for any strings/blobs. This pattern is useful in test code -- to create dummy data, hence the name. fetch C(like D) from D @dummy_seed(11); -- Use the Z shape to control which fields are copied. -- Use the dummy value even if the field is nullable and null would have be ok. fetch C(like Z) from D(like Z) @dummy_seed(11) @dummy_nullables; -- The above patterns also work for insert statements -- The shape constraints are generally useful. The dummy data -- sources are useful for inserting test data. insert into T1(like Z) from D(like Z) @dummy_seed(11) @dummy_nullables; -- We'll need this dummy procedure some_shape so we can use its return -- value in the examples that follow. We will never actual create this -- proc, we only declare it to define the shape, so this is kind of like -- a typedef. declare proc some_shape() (x integer not null, y integer not null, z integer not null); -- You can make a helper procedure to create test args that are mostly constant -- or computable. create get_foo_args(X like some_shape, seed_ integer not null) begin declare C cursor like foo arguments; -- any way of loading C could work this is one fetch C(like X) from X @dummy_seed(seed_); out C; end; -- Now we can use the &quot;get_foo_args&quot; to get full set of arguments for &quot;foo&quot; and then -- call &quot;foo&quot; with those arguments. In this example we're providing -- some of the arguments explicitly, &quot;some_shape&quot; is the part of the args that -- needs to manually vary in each test iteration, the rest of the arguments will -- be dummy values. There could be zillions of args in either category. -- In the below &quot;some_shape&quot; is going to get the manual values 1, 2, 3 while 100 -- will be the seed for the dummy args. declare foo_args cursor fetch from call get_foo_args(1,2,3, 100); call foo(from foo_args); /********************************************************** * 11. INSERT USING and FETCH USING *********************************************************/ -- This kind of thing is a pain insert into foo(a, b, c, d, e, f, g) values(1, 2, 3, null, null, 5, null); -- Instead, write this form: insert into foo USING 1 a, 2 b, 3 c, null d, null e, 5 f, null g; -- The FETCH statement can also be &quot;fetch using&quot; declare C cursor like foo; fetch C USING 1 a, 2 b, 3 c, null d, null e, 5 f, null g;  If you've read this far you know more than most now. :) "},{"title":"Appendix 7: CQL Anti-patterns​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-7-cql-anti-patterns","content":"These are a few of the antipatterns I've seen while travelling through various CQL source files. They are in various categories. Refer also to Appendix 8: CQL Best Practices. "},{"title":"Common Schema​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#common-schema","content":"For these examples let's create a couple of tables we might need for examples CREATE TABLE foo ( id integer primary key, name text ); CREATE TABLE bar ( id integer primary key, rate real );  "},{"title":"Declarations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#declarations","content":"DECLARE v LONG NOT NULL; SET v := 1;  better LET v := 1L; -- long literals have the L suffix like in C  Similarly DECLARE v REAL NOT NULL; SET v := 1;  better LET v := 1.0; -- use scientific notation or add .0 to make a real literal  "},{"title":"Casts​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#casts","content":"Redundant casts fatten the code and don't really add anything to readability. Sometimems it's necessary to cast NULL to a particular type so that you can be sure that generated result set has the right data type, but most of the casts below are not necessary.  SELECT CAST(foo.id as INTEGER) as id, CAST(foo.name as TEXT) as name, CAST(NULL as REAL) as rate FROM foo UNION ALL SELECT CAST(bar.id as INTEGER) as id, CAST(NULL as TEXT) as name, CAST(bar.rate as REAL) as rate FROM bar  Better  SELECT foo.id, foo.name, CAST(NULL as REAL) as rate FROM foo UNION ALL SELECT bar.id, CAST(NULL as TEXT) as name, bar.rate FROM bar  It's possible to do the following to make this even cleaner: -- somewhere central #define NULL_TEXT CAST(NULL as TEXT) #define NULL_REAL CAST(NULL as REAL) #define NULL_INT CAST(NULL as INTEGER) #define NULL_LONG CAST(NULL as LONG)  Then you can write  SELECT foo.id, foo.name, NULL_REAL as rate FROM foo UNION ALL SELECT bar.id, NULL_TEXT as name, bar.rate FROM bar  Booleans​ TRUE and FALSE can be used as boolean literals. SQLite doesn't care about the type but CQL will get the type information it needs to make the columns of type BOOL  SELECT foo.id, foo.name, NULL_REAL as rate, TRUE as has_name, -- this is a bit artificial but you get the idea FALSE as has_rate FROM foo UNION ALL SELECT bar.id, NULL_TEXT as name, bar.rate, FALSE as has_name, TRUE as has_rate FROM bar  "},{"title":"Boolean expressions and CASE/WHEN​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#boolean-expressions-and-casewhen","content":"It's easy to get carried away with the power of CASE expressions, I've seen this kind of thing: CAST(CASE WHEN foo.name IS NULL THEN 0 ELSE 1 END AS BOOL)  But this is simply foo.name IS NOT NULL  In general, if your case alternates are booleans a direct boolean expression would have served you better. "},{"title":"CASE and CAST and NULL​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#case-and-cast-and-null","content":"Somtimes there's clamping or filtering going on in a case statement CAST(CASE WHEN foo.name &gt; 'm' THEN foo.name ELSE NULL END AS TEXT)  Here the CAST is not needed at all so we could go to CASE WHEN foo.name &gt; 'm' THEN foo.name ELSE NULL END  NULL is already the default value for the ELSE clause so you never need ELSE NULL So better: CASE WHEN foo.name &gt; 'm' THEN foo.name END  "},{"title":"Filtering out NULLs​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#filtering-out-nulls","content":"Consider SELECT * FROM foo WHERE foo.name IS NOT NULL AND foo.name &gt; 'm';  There's no need to test for NOT NULL here, the boolean will result in NULL if foo.name is null which is not true so the WHERE test will fail. Better: SELECT * FROM foo WHERE foo.name &gt; 'm';  "},{"title":"Not null boolean expressions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#not-null-boolean-expressions","content":"In this statement we do not want to have a null result for the boolean expression SELECT id, name, CAST(IFNULL(name &gt; 'm', 0) AS BOOL) AS name_bigger_than_m FROM FOO;  So now we've made several mistakes. We could have used the usual FALSE defintion to avoid the cast. But even that would have left us with an IFNULL that's harder to read. Here's a much simpler formulation: SELECT id, name, name &gt; 'm' IS TRUE AS name_bigger_than_m FROM FOO;  Even without the TRUE macro you could do IS 1 above and still get a result of type BOOL NOT NULL "},{"title":"Using IS when it makes sense to do so​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#using-is-when-it-makes-sense-to-do-so","content":"This kind of boolean expression is also verbose for no reason  rate IS NOT NULL AND rate = 20  In a WHERE clause probably rate = 20 suffices but even if you really need a NOT NULL BOOLresult the expression above is exactly what the IS operator is for. e.g.  rate IS 20  The IS operator is frequently avoided except for IS NULL and IS NOT NULL but it's a general equality operator with the added semantic that it never returns NULL. NULL IS NULL is true. NULL IS [anything not null] is false. "},{"title":"Left joins that are not left joins​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#left-joins-that-are-not-left-joins","content":"Consider  SELECT foo.id, foo.name, bar.rate FROM foo LEFT JOIN bar ON foo.id = bar.id WHERE bar.rate &gt; 5;  This is no longer a left join because the WHERE clause demands a value for at least one column from bar. Better:  SELECT foo.id, foo.name, bar.rate FROM foo INNER JOIN bar ON foo.id = bar.id WHERE bar.rate &gt; 5;  "},{"title":"Appendix 8: CQL Best Practices​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-8-cql-best-practices","content":"This is a brief discussion of every statement type and some general best practices for that statement. The statements are in mostly alphabetical order except related statements were moved up in the order to make logical groups. Refer also to Appendix 7: CQL Anti-patterns. "},{"title":"Data Definition Language (DDL)​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#data-definition-language-ddl","content":"ALTER TABLE ADD COLUMNCREATE INDEXCREATE PROCCREATE TABLECREATE TRIGGERCREATE VIEWCREATE VIRTUAL TABLEDROP INDEXDROP TABLEDROP TRIGGERDROP VIEW These statements almost never appear in normal procedures and generally should be avoided. The normal way of handling schema in CQL is to have one or more files declare all the schema you need and then let CQL create a schema upgrader for you. This means you'll never manually drop tables or indices etc. The create declarations with their annotations will totally drive the schema. Any ad hoc DDL is usually a very bad sign. Test code is an obvious exception to this as it often does setup and teardown of schema to set up things for the test. "},{"title":"Ad Hoc Migrations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#ad-hoc-migrations-1","content":"@SCHEMA_AD_HOC_MIGRATION This is a special upgrade step that should be taken at the version indicated in the statement. These can be quite complex and even super important but should not be used lightly. Any migration procedure has to be highly tolerant of a variety of incoming schema versions and previous partial successes. In any case this directive should not appear in normal code. It should be part of the schema DDL declarations. "},{"title":"Transactions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#transactions","content":"BEGIN TRANSACTIONCOMMIT TRANSACTIONROLLBACK TRANSACTION Transactions do not nest and most procedures do not know the context in which they will be called, so the vast majority of procedures will not and should not actually start transactions. You can only do this if you know, somehow, for sure, that the procedure in question is somehow a &quot;top level&quot; procedure. So generally, don't use these statements. "},{"title":"Savepoints​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#savepoints","content":"SAVEPOINTROLLBACK TO SAVEPOINTRELEASE SAVEPOINTPROC SAVEPOINTCOMMIT RETURNROLLBACK RETURN Savepoints are the preferred tool for having interim state that can be rolled back if needed. You can use ad hoc savepoints, just give your save point and name then use RELEASE SAVEPOINT to commit it, or else ROLLBACK TO SAVEPOINTfollowed by a RELEASE to abort it. Note that you always RELEASE savepoints in both the rollback and the commit case. Managing savepoints can be tricky, especially given the various error cases. They combine nicely with TRY CATCH to do this job. However, even that is a lot of boilerplate. The best way to use savepoints is with PROC SAVEPOINT BEGIN .. END; When you use PROC SAVEPOINT, a savepoint is created for you with the name of your procedure. When the block exits the savepoint is released (committed). However you also get an automatically generated try/catch block which will rollback the savepoint if anything inside the block were to invoke THROW. Also, you may not use a regular RETURNinside this block, you must use either ROLLBACK RETURN or COMMIT RETURN. Both of these directly indicate the fate of the automatically generated statement when they run. This gives you useful options to early-out (with no error) while keeping or abandoning any work in progress. Of course you can use THROW to return an error and abandon the work in progress. "},{"title":"Compilation options​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#compilation-options","content":"@ENFORCE_NORMAL@ENFORCE_POP@ENFORCE_PUSH@ENFORCE_RESET@ENFORCE_STRICT CQL allows you to specify a number of useful options such as &quot;do not allow Window Functions&quot; or &quot;all foreign keys must choose some update or delete strategy&quot;. These additional enforcements are designed to prevent errors. Because of this they should be established once, somewhere central and they should be rarely if ever overridden. For instance @ENFORCE_NORMAL WINDOW FUNCTION would allow you to use window functions again, but this is probably a bad idea. If strict mode is on, disallowing them, that probably means your project is expected to target versions of SQLite that do not have window functions. Overriding that setting is likely to lead to runtime errors. In general you don't want to see these options in most code. "},{"title":"Previous Schema​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#previous-schema","content":"@PREVIOUS_SCHEMA CQL can ensure that the current schema is compatible with the previous schema, meaning that an upgrade script could reasonably be generated to go from the previous to the current. This directive demarks the start of the previous schema section when that validation happens. This directive is useless except for creating that schema validation so it should never appear in normal procedures. "},{"title":"Schema Regions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#schema-regions-1","content":"@BEGIN_SCHEMA_REGION@DECLARE_DEPLOYABLE_REGION@DECLARE_SCHEMA_REGION@END_SCHEMA_REGION CQL allows you to declare arbitrary schema regions and limit what parts of the schema any given region may consume. This helps you to prevent schema from getting entangled. There is never a reason to use this directives inside normal procedures; They should appear only in your schema declaration files. "},{"title":"Schema Version​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#schema-version","content":"@SCHEMA_UPGRADE_SCRIPT@SCHEMA_UPGRADE_VERSION The @SCHEMA_UPGRADE_SCRIPT directive is only used by CQL itself to declare that the incoming file is an autogenerated schema upgrade script. These scripts have slightly different rules for schema declaration that are not useful outside of such scripts. So you should never use this. @SCHEMA_UPGRADE_VERSION on the other hand is used if you are creating a manual migration script. You need this script to run in the context of the schema version that it affects. Use this directive at the start of the file to do so. Generally manual migration scripts are to be avoided so hopefully this directive is rarely if ever used. "},{"title":"C Text Echo​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#c-text-echo","content":"@ECHO This directive emits plain text directly into the compiler's output stream. It can be invaluable for adding new runtime features and for ensuring that (e.g.) additional #include or #define directives are present in the output but you can really break things by over-using this feature. Most parts of the CQL output are subject to change so any use of this should be super clean. The intended use was, as mentioned, to allow an extra #include in your code so that CQL could call into some library. Most uses of this combine with DECLARE FUNCTION or DECLARE PROCEDURE to declare an external entity. "},{"title":"Enumerations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#enumerations","content":"DECLARE ENUM@EMIT_ENUMS Avoid embedded constants whenever possible. Instead declare a suitable enumeration. Use @EMIT_ENUMS Some_Enum to get the enumeration constants into the generated .h file for C. But be sure to do this only from one compiland. You do not want the enumerations in every .h file. Choose a single .sql file (not included by lots of other things) to place the @EMIT_ENUMS directive. You can make a file specifically for this purpose if nothing else is serviceable. "},{"title":"Cursor Lifetime​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cursor-lifetime","content":"CLOSEOPEN The OPEN statement is a no-op, SQLite has no such notion. It was included because it is present in MYSQL and other variants and its inclusion can ease readability sometimes. But it does nothing. The CLOSE statement is normally not necessary because all cursors are closed at the end of the procedure they are declared in (unless they are boxed, see below). You only need CLOSE if you want to close a global cursor (which has no scope) or if you want to close a local cursor &quot;sooner&quot; because waiting to the end of the procedure might be a very long time. Using close more than once is safe, the second and later close operations do nothing. "},{"title":"Procedure Calls and Exceptions​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#procedure-calls-and-exceptions","content":"CALLTHROWTRY CATCH Remember that if you call a procedure and it uses THROW or else uses some SQL that failed, this return code will cause your code to THROW when the procedure returns. Normally that's exactly what you want, the error will ripple out and some top-levelCATCH will cause a ROLLBACK and the top level callers sees the error. If you have your own rollback needs be sure to install your own TRY/CATCH block or else use PROC SAVEPOINT as above to do it for you. Inside of a CATCH block you can use the special variable @RC to see the most recent return code from SQLite. "},{"title":"Control Flow with \"Big Moves\"​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#control-flow-with-big-moves","content":"CONTINUELEAVERETURN These work as usual but beware, you can easily use any of these to accidentally leave a block with a savepoint or transaction and you might skip over the ROLLBACK or COMMIT portions of the logic. Avoid this problem by using PROC SAVEPOINT. "},{"title":"Getting access to external code​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#getting-access-to-external-code","content":"DECLARE FUNCTIONDECLARE SELECT FUNCTIONDECLARE PROCEDURE The best practice is to put any declarations into a shared header file which you can #include in all the places it is needed. This is especially important should you have to forward declare a procedure. CQL normally provides exports for all procedures so you basically get an automatically generated and certain-to-be-correct #include file. But, if the procedures are being compiled together then an export file won't have been generated yet at the time you need it; To work around this you use the DECLARE PROCEDUREform. However, procedure declarations are tricky; they include not just the type of the arguments but the types of any/all of the columns in any result set the procedure might have. This must not be wrong or callers will get unpredictable failures. The easiest way to ensure it is correct is to use the same trick as you would in C -- make sure that you #include the declaration the in the translation unit with the definition. If they don't match there will be an error. A very useful trick: the error will include the exact text of the correct declaration. So if you don't know it, or are too lazy to figure it out; simply put ANY declaration in the shared header file and then paste in the correct declaration from the error. should the definition ever change you will get a compilation error which you can again harvest to get the correct declaration. In this way you can be sure the declarations are correct. Functions have no CQL equivalent, but they generally don't change very often. Use DECLARE FUNCTION to allow access to some C code that returns a result of some kind. Be sure to add the CREATE option if the function returns a reference that the caller owns. Use DECLARE SELECT FUNCTION to tell CQL about any User Defined Functions you have added to SQLite so that it knows how to call them. Note that CQL does not register those UDFs, it couldn't make that call lacking the essential C information required to do so. If you find that you are getting errors when calling a UDF the most likely reason for the failure is that the UDF was declared but never registered with SQLite at runtime. This happens in test code a lot -- product code tends to have some central place to register the UDFs and it normally runs at startup, e.g. right after the schema is upgraded. "},{"title":"Regular Data Manipulation Language (DML)​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#regular-data-manipulation-language-dml","content":"DELETEINSERTSELECTUPDATEUPSERT These statements are the most essential and they'll appear in almost every procedure. There are a few general best practices we can go over. Try to do as much as you can in one batch rather than iterating; e.g. don't write a loop with a DELETE statement that deletes one row if you can avoid it, write a delete statement that deletes all you need to deletedon't write a loop with of SELECT statement that fetches one row, try to fetch all the rows you need with one select Make sure UPSERT is supported on the SQLite system you are using, older versions do not support it Don't put unnecessary casts in your SELECT statements, they just add fat Don't use CASE/WHEN to compute a boolean, the boolean operations are more economical (e.g. use IS) Don't use COUNT if all you need to know is whether a row exists or not, use EXISTS Don't use GROUP BY, ORDER BY, or DISTINCT on large rowsets, the sort is expensive and it will make your SELECT statements write to disk rather than just read Always use the INSERT INTO FOO USING form of the INSERT statement, it's much easier to read than the standard form and compiles to the same thing "},{"title":"Variable and Cursor declarations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#variable-and-cursor-declarations","content":"DECLARE OUT CALLDECLARELETSET These are likely to appear all over as well. If you can avoid a variable declaration by using LET then do so; The code will be more concise and you'll get the exact variable type you need. This is the same as var x = foo(); in other languages. Once the variable is declared use SET. You can save yourself a lot of declarations of OUT variables with DECLARE OUT CALL. That declaration form automatically declares the OUT variables used in the call you are about to make with the correct type. If the number of arguments changes you just have to add the args you don't have to also add new declarations. The LIKE construct can be used to let you declare things whose type is the same as another thing. Patterns like DECLARE ARGS CURSOR LIKE FOO ARGUMENTSsave you a lot of typing and also enhance correctness. There's a whole chapter dedicated to &quot;shapes&quot; defined by LIKE. "},{"title":"Query Plans​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#query-plans","content":"EXPLAIN Explain can be used in front of other queries to generate a plan. The way SQLite handles this is that you fetch the rows of the plan as usual. So basicallyEXPLAIN is kind of like SELECT QUERY PLAN OF. This hardly ever comes up in normal coding. CQL has an output option where it will generate code that gives you the query plan for a procedures queries rather than the normal body of the procedure. "},{"title":"Fetching Data from a Cursor or from Loose Data​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#fetching-data-from-a-cursor-or-from-loose-data","content":"FETCHUPDATE CURSOR The FETCH statement has many variations, all are useful at some time or another. There are a few helpful guidelines. If fetching from loose values into a cursor use the FETCH USING form (as you would with INSERT INTO USING) because it is less error proneFETCH INTO is generally a bad idea, you'll have to declare a lot of variables, instead just rely on automatic storage in the cursor e.g.fetch my_cursor rather than fetch my_cursor into a, b, cIf you have data already in a cursor you can mutate some of the columns using UPDATE CURSOR, this can let you adjust values or apply defaults "},{"title":"Control Flow​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#control-flow","content":"IFLOOPSWITCHWHILE These are your bread and butter and they will appear all over. One tip: Use the ALL VALUES variant of switch whenever possible to ensure that you haven't missed any cases. "},{"title":"Manual Control of Results​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#manual-control-of-results","content":"OUT OUT UNION If you know you are producing exactly one row OUT is more economical than SELECT If you need complete flexibility on what rows to produce (e.g. skip some, add extras, mutate some) then OUT UNION will give you that, use it only when needed, it's more expensive than just SELECT "},{"title":"CTEs and Shared Fragments​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#ctes-and-shared-fragments","content":"To understand what kinds of things you can reasonably do with fragments, really you just have to understand the things that you can do with common table expressions or CTEs. For those who don't know, CTEs are the things you declare in the WITH clause of a SELECT statement. They're kind of like local views. Well, actually, they are exactly like local views. Query fragments help you to define useful CTEs so basically what you can do economically in a CTE directly determines what you can do economically in a fragment. To demonstrate some things that happen with CTEs we're going to use these three boring tables. create table A ( id integer primary key, this text not null ); create table B ( id integer primary key, that text not null ); create table C ( id integer primary key, other text not null );  Let's start with a very simple example, the first few examples are like control cases. explain query plan select * from A inner join B on B.id = A.id; QUERY PLAN |--SCAN TABLE A \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  OK as we can see A is not constrained so it has to be scanned but B isn't scanned, we use its primary key for the join. This is the most common kind of join: a search based on a key of the table you are joining to. Let's make it a bit more realistic. explain query plan select * from A inner join B on B.id = A.id where A.id = 5; QUERY PLAN |--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?)  Now A is constrained by the WHERE clause so we can use its index and then use the B index. So we get a nice economical join from A to B and no scans at all. Now suppose we try this with some CTE replacements for A and B. Does this make it worse? explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select * from B) select * from AA left join BB on BB.id = AA.id where AA.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  The answer is a resounding no. The CTE AA was not materialized it was expanded in place, as was the CTE BB. We get exactly the same query plan. Now this means that the inner expressions like select * from A could have been fragments such as: @attribute(cql:shared_fragment) create proc A_() begin select * from A; end; @attribute(cql:shared_fragment) create proc B_() begin select * from B; end; explain query plan with (call A_()), -- short for A_(*) AS (call A_()) (call B_()) -- short for B_(*) AS (call B_()) select * from A_ left join B_ on B_.id = A_.id where A_.id = 5;  Note: I'll use the convention that A_ is the fragment proc that could have generated the CTE AA, likewise with B_ and so forth. The above will expand into exactly what we had before and hence will have the exactly same good query plan. Of course this is totally goofy, why make a fragment like that -- it's just more typing. Well now lets generalize the fragments just a bit. @attribute(cql:shared_fragment) create proc A_(experiment bool not null) begin -- data source might come from somewhere else due to an experiment if not experiment then select * from A; else select id, this from somewhere_else; end if; end; @attribute(cql:shared_fragment) create proc B_() begin -- we don't actually refer to &quot;B&quot; if the filter is null if b_filter is not null then -- applies b_filter if specified select * from B where B.other like b_filter; else -- generates the correct shape but zero rows of it select null as id, null as that where false; end if; end; create proc getAB( id_ integer not null, experiment bool not null, b_filter text) begin with (call A_(experiment)), (call B_(b_filter)) select * from A_ left join B_ on B_.id = A_.id where A_.id = id_; end;  The above now has 4 combos economically encoded and all of them have a good plan. Importantly though, if b_filter is not specified then we don't actually join to B. The B_ CTE will have no reference to B, it just has zero rows. Now lets look at some things you don't want to do. Consider this form: explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select A.id, B.that from A left join B on B.id = A.id) select * from AA left join BB on BB.id = AA.id where AA.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  Note that here we get 3 joins. Now a pretty cool thing happened here -- even though the expression for BB does not include a WHERE clause SQLite has figured out the AA.id being 5 forces A.id to be 5 which in turn gives a constraint on BB. Nice job SQLite. If it hadn't been able to figure that out then the expansion of BB would have resulted in a table scan. Still, 3 joins is bad when we only need 2 joins to do the job. What happened? Well, when we did the original fragments with extensions and stuff we saw this same pattern in fragment code. Basically the fragment for BB isn't just doing the B things it's restarting from A and doing its own join to get B. This results in a wasted join. And it might result in a lot of work on the A table as well if the filtering was more complex and couldn't be perfectly inferred. You might think, &quot;oh, no problem, I can save this, I'll just refer to AA instead of A in the second query.&quot; This does not help (but it's going in the right direction): explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select AA.id, B.that from AA left join B on B.id = AA.id) select * from AA left join BB on BB.id = AA.id where AA.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  In terms of fragments the anti-pattern is this. @attribute(cql:shared_fragment) create proc B_() begin select B.* from A left join B on B.id = A.id; end;  The above starts the query for B again from the root. You can save this, the trick is to not try to generate just the B columns and then join them later. You can get a nice data flow going with chain of CTEs. explain query plan with AA(id, this) as (select * from A), AB(id, this, that) as (select AA.*, B.that from AA left join B on B.id = AA.id) select * from AB where AB.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  And we're right back to the perfect plan. The good form creates a CTE chain where we only need the result of the final CTE. A straight line of CTEs each depending on the previous one results in a excellent data flow. In terms of fragments this is now: @attribute(cql:shared_fragment) create proc A_() begin select * from A; end; @attribute(cql:shared_fragment) create proc AB_() begin with (call A_) select A_.*, B.that from A_ left join B on B.id = A_.id end; with (call AB_()) select * from AB_ where AB_.id = 5;  For brevity I didn't include the possibility of using IF and such. Another option that makes the same good query plan. We can generalize AB_ so that it doesn't know where the base data is coming from and can be used in more cases. @attribute(cql:shared_fragment) create proc A_() begin select * from A; end; @attribute(cql:shared_fragment) create proc AB_() begin with source(*) like A -- you must provide some source that is the same shape as A select source.*, B.that from source left join B on B.id = source.id end; with (call A_()) (call AB_() using A_ as source) select * from AB_ where AB_.id = 5;  Again this results in a nice straight chain of CTEs and even though the where clause is last the A table is constrained properly. It's important not to fork the chain... if you do that then whatever came before the fork must be materialized for use in both branches. That can be quite bad because then the filtering might come after the materialization. This is an example that is quite bad. explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select AA.id, B.that from AA left join B on B.id = AA.id), CC(id, other) as (select AA.id, C.other from AA left join C on C.id = AA.id) select * from AA left join BB on BB.id = AA.id left join CC on CC.id = AA.id where AA.id = 5; QUERY PLAN |--MATERIALIZE 2 | |--SCAN TABLE A | \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?) |--MATERIALIZE 3 | |--SCAN TABLE A | \\--SEARCH TABLE C USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SCAN SUBQUERY 2 \\--SEARCH SUBQUERY 3 USING AUTOMATIC COVERING INDEX (id=?)  Things have gone way of the rails here. As you can see A is now scanned twice. and there are many more joins. We could make this a lot better by moving the A condition all the way up into the first CTE. With fragments that would just mean creating something like @attribute(cql:shared_fragment) create proc A_(id_) begin select * from A where A.id = id_; end;  At least then if we have to materialize we'll get only one row. This could be a good thing to do universally, but it's especially important if you know that forking in the query shape is mandatory for some reason. A better pattern might be this: explain query plan with AA(id, this) as (select * from A), AB(id, this, that) as (select AA.*, B.that from AA left join B on B.id = AA.id), ABC(id, this, that, other) as (select AB.*, C.other from AB left join C on C.id = AB.id) select * from ABC where ABC.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE C USING INTEGER PRIMARY KEY (rowid=?)  Here we've just extended the chain. With shared fragments you could easily build anAB_ proc as before and then build an ABC_ proc either by calling AB_ directly or by having a table parameter that is LIKE AB_. Both cases will give you a great plan. So the most important things are: Avoid forking the chain of CTEs/fragments, a straight chain works great.Avoid re-joining to tables, even unconstrained CTEs result in great plans if they don't have to be materialized.If you do need to fork in your CTE chain, because of your desired shape, be sure to move as many filters as you can further upstream so that by the time you materialize only a very small number of rows need to be materialiized. These few rules will go far in helping you to create shapes. One last thing, without shared fragments, if you wanted to create a large 10 way join or something you had to type that join into your file and it would be very much in your face. Now that join might be hidden from you in a nice easy-to-use fragment. Which you might then decide you want to use 3 times... And now with a tiny amount of code you have 30 joins. The thing is shared fragments make it easy to generate a lot of SQL. It's not bad that shared fragments make things easy, but with great power comes great responsibility, so give a care as to what it is you are assembling. Understanding your fragments, especially any big ones, will help you to create great code. "},{"title":"Appendix 9: Using the CQL Amalgam​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-9-using-the-cql-amalgam","content":"This is a brief discussion of the CQL Amalgam and its normal usage patterns. "},{"title":"Building the Amalgam​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#building-the-amalgam","content":"The amalgam has to include the results of bison and flex, so a normal build must run first. The simplest way to build it starting from the sources directory is: make ./make_amalgam.sh  The result goes in out/cql_amalgam.c. It can then be built using cc with whatever flags you might desire. With a few -D directives it can readily be compiled with Microsoft C and it also works with Emscripten (emcc) basically unchanged. Clang and Gcc of course also work. The standard test script test.sh builds the amalgam and attempts to compile it as well, which ensures that the amalgam can at least compile at all times. "},{"title":"Testing the Amalgam​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#testing-the-amalgam","content":"Of course you can do whatever tests you might like by simply compiling the amalgam as is and then using it to compile things. But importantly the test script test.sh can test the amalgam build like so: test.sh --use_amalgam  This runs all the normal tests using the binary built from the amalgam rather than the normal binary. Normal CQL development practices result in this happening pretty often so the amalgam tends to stay in good shape. The code largely works in either form with very few affordances for the amalgam build needed. Most developers don't even think about the amalgam build flavor; to a first approximation &quot;it just works&quot;. "},{"title":"Using the Amalgam​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#using-the-amalgam","content":"To use the amalgam you'll want to do something like this: #define CQL_IS_NOT_MAIN 1 // Suppresses a bunch of warnings because the code // is in an #include context // PR's to remove these are welcome :D #pragma clang diagnostic ignored &quot;-Wnullability-completeness&quot; #include &quot;cql_amalgam.c&quot; void go_for_it(const char *your_buffer) { YY_BUFFER_STATE my_string_buffer = yy_scan_string(your_buffer); // Note: &quot;--in&quot; is irrelevant because the scanner is // going to read from the buffer above. // // If you don't use yy_scan_string, you could use &quot;--in&quot; // to get data from a file. int argc = 4; char *argv[] = { &quot;cql&quot;, &quot;--cg&quot;, &quot;foo.h&quot;, &quot;foo.c&quot; }; cql_main(argc, argv); yy_delete_buffer(my_string_buffer); }  So the general pattern is: predefine the options you want to use (see below)include the amalgamadd any functions you want that will call the amalgam Most amalgam functions are static to avoid name conflicts. You will want to create your own public functions such as go_for_it above that use the amalgam in all the ways you desire. You'll want to avoid calling any internal functions other than cql_main because they are liable to change. NOTE: The amalgam is C code not C++ code. Do not attempt to use it inside of an extern &quot;C&quot; block in a C++ file. It won't build. If you want a C++ API, expose the C functions you need and write a wrapper class. "},{"title":"CQL Amalgam Options​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#cql-amalgam-options","content":"The amalgam includes the following useful #ifdef options to allow you to customize it. CQL_IS_NOT_MAINCQL_NO_SYSTEM_HEADERSCQL_NO_DIAGNOSTIC_BLOCKcql_emit_errorcql_emit_outputcql_open_file_for_writecql_write_file CQL_IS_NOT_MAIN​ If this symbol is defined then cql_main will not be redefined to be main. As the comments in the source say: #ifndef CQL_IS_NOT_MAIN // Normally CQL is the main entry point. If you are using CQL // in an embedded fashion then you want to invoke its main at // some other time. If you define CQL_IS_NOT_MAIN then cql_main // is not renamed to main. You call cql_main when you want. #define cql_main main #endif  Set this symbol so that you own main and cql_main is called at your pleasure. CQL_NO_SYSTEM_HEADERS​ The amalgam includes the normal #include directives needed to make it compile, things like stdio and such. In your situation these headers may not be appropriate. If CQL_NO_SYSTEM_HEADERS is defined then the amalgam will not include anything; you can then add whatever headers you need before you include the amalgam. CQL_NO_DIAGNOSTIC_BLOCK​ The amalgam includes a set of recommended directives for warnings to suppress and include. If you want to make other choices for these you can suppress the defaults by defining CQL_NO_DIAGNOSTIC_BLOCK; you can then add whatever diagnostic pragmas you want/need. cql_emit_error​ The amalgam uses cql_emit_error to write its messages to stderr. The documentation is included in the code which is attached here. If you want the error messages to go somewhere else, define cql_emit_erroras the name of your error handling function. It should accept a const char * and record that string however you deem appropriate. #ifndef cql_emit_error // CQL &quot;stderr&quot; outputs are emitted with this API. // // You can define it to be a method of your choice with // &quot;#define cql_emit_error your_method&quot; and then your method // will get the data instead. This will be whatever output the // compiler would have emitted to stderr. This includes // semantic errors or invalid argument combinations. Note that // CQL never emits error fragments with this API; you always // get all the text of one error. This is important if you // are filtering or looking for particular errors in a test // harness or some such. // // You must copy the memory if you intend to keep it. &quot;data&quot; will // be freed. // // Note: you may use cql_cleanup_and_exit to force a failure from // within this API but doing so might result in unexpected cleanup // paths that have not been tested. void cql_emit_error(const char *err) { fprintf(stderr, &quot;%s&quot;, err); if (error_capture) { bprintf(error_capture, &quot;%s&quot;, err); } } #endif  Typically you would #define cql_emit_error your_error_function before you include the amalgam and then define your_error_function elsewhere in that file (before or after the amalgam is included are both fine). cql_emit_output​ The amalgam uses cql_emit_output to write its messages to stdout. The documentation is included in the code which is attached here. If you want the standard output to go somewhere else, define cql_emit_outputas the name of your output handling function. It should accept a const char * and record that string however you deem appropriate. #ifndef cql_emit_output // CQL &quot;stdout&quot; outputs are emitted (in arbitrarily small pieces) // with this API. // // You can define it to be a method of your choice with // &quot;#define cql_emit_output your_method&quot; and then your method will // get the data instead. This will be whatever output the // compiler would have emitted to stdout. This is usually // reformated CQL or semantic trees and such -- not the normal // compiler output. // // You must copy the memory if you intend to keep it. &quot;data&quot; will // be freed. // // Note: you may use cql_cleanup_and_exit to force a failure from // within this API but doing so might result in unexpected cleanup // paths that have not been tested. void cql_emit_output(const char *msg) { printf(&quot;%s&quot;, msg); } #endif  Typically you would #define cql_emit_output your_output_function before you include the amalgam and then define your_error_function elsewhere in that file (before or after the amalgam is included are both fine). cql_open_file_for_write​ If you still want normal file i/o for your output but you simply want to control the placement of the output (such as forcing it to be on some virtual drive) you can replace this function by defining cql_open_file_for_write. If all you need to do is control the origin of the FILE * that is written to, you can replace just this function. #ifndef cql_open_file_for_write // Not a normal integration point, the normal thing to do is // replace cql_write_file but if all you need to do is adjust // the path or something like that you could replace // this method instead. This presumes that a FILE * is still ok // for your scenario. FILE *_Nonnull cql_open_file_for_write( const char *_Nonnull file_name) { FILE *file; if (!(file = fopen(file_name, &quot;w&quot;))) { cql_error(&quot;unable to open %s for write\\n&quot;, file_name); cql_cleanup_and_exit(1); } return file; } #endif  Typically you would #define cql_open_file_for_write your_open_function before you include the amalgam and then define your_open_function elsewhere in that file (before or after the amalgam is included are both fine). cql_write_file​ The amalgam uses cql_write_file to write its compilation outputs to the file system. The documentation is included in the code which is attached here. If you want the compilation output to go somewhere else, define cql_write_fileas the name of your output handling function. It should accept a const char * for the file name and another for the data to be written. You can then store those compilation results however you deem appropriate. #ifndef cql_write_file // CQL code generation outputs are emitted in one &quot;gulp&quot; with this // API. You can define it to be a method of your choice with // &quot;#define cql_write_file your_method&quot; and then your method will // get the filename and the data. This will be whatever output the // compiler would have emitted to one of it's --cg arguments. // You can then write it to a location of your choice. // You must copy the memory if you intend to keep it. &quot;data&quot; will // be freed. // Note: you *may* use cql_cleanup_and_exit to force a failure // from within this API. That's a normal failure mode that is // well-tested. void cql_write_file( const char *_Nonnull file_name, const char *_Nonnull data) { FILE *file = cql_open_file_for_write(file_name); fprintf(file, &quot;%s&quot;, data); fclose(file); } #endif  Typically you would #define cql_write_file your_write_function before you include the amalgam and then define your_write_function elsewhere in that file (before or after the amalgam is included are both fine). "},{"title":"Amalgam LEAN choices​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#amalgam-lean-choices","content":"When you include the amalgam, you get everything by default. You may, however, only want some limited subset of the compiler's functions in your build. To customize the amalgam, there are a set of configuration pre-processor options. To opt-in to configuration, first define CQL_AMALGAM_LEAN. You then have to opt-in to the various pieces you might want. The system is useless without the parser, so you can't remove that; but you can choose from the list below. The options are: CQL_AMALGAM_LEAN` : enable lean mode; this must be set or you get everythingCQL_AMALGAM_CG_C : C codegenCQL_AMALGAM_CG_COMMON : common code generator piecesCQL_AMALGAM_GEN_SQL : the echoing featuresCQL_AMALGAM_JSON : JSON schema outputCQL_AMALGAM_OBJC : Objective-C code genCQL_AMALGAM_QUERY_PLAN : the query plan creatorCQL_AMALGAM_SCHEMA : the assorted schema output typesCQL_AMALGAM_SEM : semantic analysis (needed by most things)CQL_AMALGAM_TEST_HELPERS : test helper outputCQL_AMALGAM_UDF : the UDF stubs used by the query plan outputCQL_AMALGAM_UNIT_TESTS : some internal unit tests, which are pretty much needed by nobody Note that CQL_AMALGAM_SEM is necessary for any of the code generation features to work. Likewise, several generators require CQL_AMALGAM_CG_COMMON (e.g., C does). Pick what you want; stubs are created for what you omit to avoid linkage errors. "},{"title":"Other Notes​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#other-notes","content":"The amalgam will use malloc/calloc for its allocations and it is designed to release all memory it has allocated when cql_main returns control to you, even in the face of error. Internal compilation errors result in an assert failure leading to an abort. This is not supposed to ever happen but there can always be bugs. Normal errors just prevent later phases of the compiler from running so you might not see file output, but rather just error output. In all cases things should be cleaned up. The compiler can be called repeatedly with no troubles; it re-initializes on each use. The compiler is not multi-threaded so if there is threading you should use some mutex arrangement to keep it safe. A thread-safe version would require extensive modifications. "},{"title":"Appendix 10: CQL Working Example​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-10-cql-working-example","content":"This is a working example that shows all of the basic DML statements and the call patterns to access them. The code also includes the various helpers you can use to convert C types to CQL types. todo.sql​ -- This is a simple schema for keeping track of tasks and whether or not they have been completed -- this serves to both declare the table and create the schema create proc todo_create_tables() begin create table if not exists tasks( description text not null, done bool default false not null ); end; -- adds a new not-done task create proc todo_add(task TEXT NOT null) begin insert into tasks values(task, false); end; -- gets the tasks in inserted order create proc todo_tasks() begin select rowid, description, done from tasks order by rowid; end; -- updates a given task by rowid create proc todo_setdone_(rowid_ integer not null, done_ bool not null) begin update tasks set done = done_ where rowid == rowid_; end; -- deletes a given task by rowid create proc todo_delete(rowid_ integer not null) begin delete from tasks where rowid == rowid_; end;  main.c​ #include &lt;stdlib.h&gt; #include &lt;sqlite3.h&gt; #include &quot;todo.h&quot; int main(int argc, char **argv) { /* Note: not exactly world class error handling but that isn't the point */ // create a db sqlite3 *db; int rc = sqlite3_open(&quot;:memory:&quot;, &amp;db); if (rc != SQLITE_OK) { exit(1); } // make schema if needed (in memory databases always begin empty) rc = todo_create_tables(db); if (rc != SQLITE_OK) { exit(2); } // add some tasks const char * const default_tasks[] = { &quot;Buy milk&quot;, &quot;Walk dog&quot;, &quot;Write code&quot; }; for (int i = 0; i &lt; 3; i++) { // note we make a string reference from a c string here cql_string_ref dtask = cql_string_ref_new(default_tasks[i]); rc = todo_add(db, dtask); cql_string_release(dtask); // and then dispose of the reference if (rc != SQLITE_OK) { exit(3); } } // mark a task as done rc = todo_setdone_(db, 1, true); if (rc != SQLITE_OK) { exit(4); } // delete a row in the middle, rowid = 2 rc = todo_delete(db, 2); if (rc != SQLITE_OK) { exit(5); } // select out some results todo_tasks_result_set_ref result_set; rc = todo_tasks_fetch_results(db, &amp;result_set); if (rc != SQLITE_OK) { printf(&quot;error: %d\\n&quot;, rc); exit(6); } // get result count cql_int32 result_count = todo_tasks_result_count(result_set); // loop to print for (cql_int32 row = 0; row &lt; result_count; row++) { // note &quot;get&quot; semantics mean that a ref count is not added // if you want to keep the string you must &quot;retain&quot; it cql_string_ref text = todo_tasks_get_description(result_set, row); cql_bool done = todo_tasks_get_done(result_set, row); cql_int32 rowid = todo_tasks_get_rowid(result_set, row); // convert to c string format cql_alloc_cstr(ctext, text); printf(&quot;%d: rowid:%d %s (%s)\\n&quot;, row, rowid, ctext, done ? &quot;done&quot; : &quot;not done&quot;); cql_free_cstr(ctext, text); } // done with results, free the lot cql_result_set_release(result_set); // and close the database sqlite3_close(db); }  "},{"title":"Build Steps​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#build-steps","content":"# ${cgsql} refers to the root of the CG/SQL repo % cql --in todo.sql --cg todo.h todo.c % cc -o todo -I${cqsql}/sources main.c todo.c ${cgsql}/sources/cqlrt.c -lsqlite3  "},{"title":"Results​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#results","content":"Note that rowid 2 has been deleted, the leading number is the index in the result set. The rowid is of course the database rowid. % ./todo 0: rowid:1 Buy milk (done) 1: rowid:3 Write code (not done)  "},{"title":"Appendix 11: Production Considerations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#appendix-11-production-considerations","content":""},{"title":"Production Considerations​","type":1,"pageTitle":"guide","url":"/cql-guide/generated/guide#production-considerations","content":"This system as it appears in the sources here is designed to get some basic SQLite scenarios working but the runtime systems that are packaged here are basic, if only for clarity. There are some important things you should think about improving or customizing for your production environment. Here's a brief list. Concurrency​ The reference counting solution in the stock CQLRT implementation is single threaded. This might be ok, in many environments only one thread is doing all the data access. But if you plan to share objects between threads this is something you'll want to address. CQLRT is designed to be replacable. In fact there is another version included in the distribution cqlrt_cf that is more friendly to iOS and CoreFoundation. This alternate version is an excellent demonstration of what is possible. There are more details in Internals Part 5: CQL Runtime. Statement Caching​ SQLite statement management includes the ability to reset and re-prepare statements. This is an important performance optimization but the stock CQLRT does not take advantage of this. This is for two reasons: first, simplicity, and secondly (though more importantly), any kind of statement cache would require a caching policy and this simple CQLRT cannot possibly know what might consitute a good policy for your application. The following three macros can be defined in your cqlrt.h and they can be directed at a version that keeps a cache of your choice. #ifndef cql_sqlite3_exec #define cql_sqlite3_exec(db, sql) sqlite3_exec((db), (sql), NULL, NULL, NULL) #endif #ifndef cql_sqlite3_prepare_v2 #define cql_sqlite3_prepare_v2(db, sql, len, stmt, tail) sqlite3_prepare_v2((db), (sql), (len), (stmt), (tail)) #endif #ifndef cql_sqlite3_finalize #define cql_sqlite3_finalize(stmt) sqlite3_finalize((stmt)) #endif  As you might expect, prepare creates a statement or else returns one from the cache. When the finalize API is called the indicated statement can be returned to the cache or discarded. The exec API does both of these operations, but also, recall that exec can get a semicolon separated list of statements. Your exec implementation will have to use SQLite's prepare functions to split the list and get prepared statements for part of the string. Alternately, you could choose not to cache in the exec case. Your Underlying Runtime​ As you can see in cqlrt_cf, there is considerable ability to define what the basic data types mean. Importantly, the reference types text, blob, and object can become something different (e.g., something already supported by your environment). For instance, on Windows you could use COM or .NET types for your objects. All object references are substantially opaque to CQLRT; they have comparatively few APIs that are defined in the runtime: things like getting the text out of the string reference and so forth. In addition to the basic types and operations you can also define a few helper functions that allow you to create some more complex object types. For instance, list, set, and dictionary creation and management functions can be readily created and then you can declare them using the DECLARE FUNCTION language features. These objects will then be whatever list, set, or dictionary they need to be in order to interoperate with the rest of your environment. You can define all the data types you might need in your CQLRT and you can employ whatever threading model and locking primitives you need for correctness. Debugging and Tracing​ The CQLRT interface includes some helper macros for logging. These are defined as no-ops by default but, of course, they can be changed. #define cql_contract assert #define cql_invariant assert #define cql_tripwire assert #define cql_log_database_error(...) #define cql_error_trace()  cql_contract and cql_invariant are for fatal errors. They both assert something that is expected to always be true (like assert) with the only difference being that the former is conventionally used to validate preconditions of functions. cql_tripwire is a slightly softer form of assert that should crash in debug builds but only log an error in production builds. It is generally used to enforce a new condition that may not always hold with the goal of eventually transitioning over to cql_contract or cql_invariant once logging has demonstrated that the tripwire is never hit. When a fetch_results method is called, a failure results in a call to cql_log_database_error. Presently the log format is very simple. The invocation looks like this:  cql_log_database_error(info-&gt;db, &quot;cql&quot;, &quot;database error&quot;);  The logging facility is expected to send the message to wherever is appropriate for your environment. Additionally it will typically get the failing result code and error message from SQLite, however these are likely to be stale. Failed queries usually still require cleanup and so the SQLite error codes be lost because (e.g.) a finalize has happened, clearing the code. You can do better if, for instance, your runtime caches the results of recent failed prepare calls. In any case, what you log and where you log it is entirely up to you. The cql_error_trace macro is described in Internals Chapter 3. It will typically invoke printf or fprintf or something like that to trace the origin of thrown exceptions and to get the error text from SQLite as soon as possible. An example might be: #define cql_error_trace() fprintf(stderr, &quot;error %d in %s %s:%d\\n&quot;, _rc_, _PROC_, __FILE__, __LINE_)  Typically the cost of all these diagnostics is too high to include in production code so this is turned on when debugging failures. But you can make that choice for yourself. Customizing Code Generation​ The file rt_common.c defines the common result types, but the skeleton file rt.cincludes affordances to add your own types without having to worry about conflicts with the common types. These macros define #define RT_EXTRAS #define RT_EXTRA_CLEANUP  Simply define these two to create whatever rt_ data structures you want and add any cleanup function that might be needed to release resources. The other cleanup functions should provide a good template for you to make your own. The C data type rtdata includes many text fragments that directly control the code generation. If you want to make your generated code look more like say CoreFoundation you can define an rtdata that will do the job. This will mean a lot of your generated code won't require the #defines for the CQL types, it can use your runtime directly. You can also enable things like Pascal casing for procedure names and a common prefix on procedure names if those are useful in your environment. However, the system is designed so that such changes aren't necessary. The data types in cqlrt.h are enough for any remapping, additional changes with rtdata are merely cosmetic. Summary​ The CQLRT macros are very powerful, they allow you to target almost any runtime with a C API. The cqlrt_cf version is a good example of the sorts of changes you can make. Concurrency and Statement Caching are not supported in the basic version for cqlrt.h. If this is important to you you might want to customize for that. Helper functions for additional data types can be added, and they can be unique to your runtime. There are tracing macros to help with debugability. Providing some useful versions of those can be of great help in production environments. "},{"title":"Part 3: C Code Generation","type":0,"sectionRef":"#","url":"/cql-guide/int03","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#preface","content":"Part 3 continues with a discussion of the essentials of the C code generation pass of the CQL compiler. As in the previous sections, the goal here is not to go over every detail of code generation but rather to give a sense of how codegen happens in general -- the core strategies and implementation choices -- so that when reading the code you will have an idea how smaller pieces would fit into the whole. To accomplish this, various key data structures will be explained in detail as well as selected examples of their use. "},{"title":"C Code Generation​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#c-code-generation","content":"There are several key pieces of C code that we have to generate to make working CQL procedures using C functions. This all happens in cg_c.c. From a big picture perspective, these are the essential problems: we have to compile SQL expressions into C including expressions with variables that are nullableincluding SQL expressions that are highly complex like CASE..WHEN..THEN..END and IN (..) we have to generate control flow for things like IF, WHILE and, SWITCHwe have to make result sets including the code to slurp up all the rows from a SQL statement into an array of valueswe want to do this very economically we have to be able to create the text for every SQLite statement and bind any variables to itwe have to check every SQLite API for errors and throw exceptions consistently and deal with them including constructs that allow users to handle exceptions, such as TRY/CATCH we have to track any reference types carefully so that retain/release pairs are done consistently even in the presence of SQLite errors or other exceptions we have to produce a .h and a .c file for the C compiler contributions to these files could come from various places, not necessarily in orderthe .c file will itself have various sections and we might need to contribute to them at various points in the compilation we want to do this all in one pass over the ASTwe get to assume that the program is error-free -- codegen never runs unless semantic analysis reports zero errors so nothing can be wrong by the time the codegen pass runs, we never detect errors heresometimes we add Contract and Invariant statements to cg.c to make our assumptions clear and to prevent regressions There are some very important building blocks used to solve these problems: we will start with those, then move to a discussion of each of the essential kinds of code generation that we have to do to get working programs. "},{"title":"Launching the Code Generator​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#launching-the-code-generator","content":"Once semantic analysis is done, all of the code generators have the same contract: they have a main function like cg_c_main for the C code generator. It gets the root of the AST and it can use the public interface of the semantic analyzer to get additional information. See Part 2 for those details. // Main entry point for code-gen. This will set up the buffers for the global // variables and any loose calls or DML. Any code that needs to run in the // global scope will be added to the global_proc. This is the only codegen // error that is possible. If you need global code and you don't have a global // proc then you can't proceed. Semantic analysis doesn't want to know that stuff. // Otherwise all we do is set up the most general buffers for the global case and // spit out a function with the correct name. cql_noexport void cg_c_main(ast_node *head) { ... }  In addition to initializing its scratch storage, the main entry point also sets up a symbol table for AST dispatch just like the gen_ and sem_ functions do. Here are some samples from that table with the most common options:  DDL_STMT_INIT(drop_table_stmt); DDL_STMT_INIT(drop_view_stmt); DDL_STMT_INIT(create_table_stmt); DDL_STMT_INIT(create_view_stmt);  The DDL (Data Definition Language) statements all get the same handling: The text of the statement is generated from the AST. Any variables are bound and then the statement is executed. The work is done with cg_bound_sql_statement which will be discussed later. // Straight up DDL invocation. The ast has the statement, execute it! // We don't minify the aliases because DDL can have views and the view column names // can be referred to in users of the view. Loose select statements can have // no external references to column aliases. static void cg_any_ddl_stmt(ast_node *ast) { cg_bound_sql_statement(NULL, ast, CG_EXEC|CG_NO_MINIFY_ALIASES); }  DML (Data Manipulation Language) statements are declared similarly:  STD_DML_STMT_INIT(begin_trans_stmt); STD_DML_STMT_INIT(commit_trans_stmt); STD_DML_STMT_INIT(rollback_trans_stmt); STD_DML_STMT_INIT(savepoint_stmt); STD_DML_STMT_INIT(delete_stmt);  The DML statements are handled by cg_std_dml_exec_stmt; the processing is identical to DDL except CG_MINIFY_ALIASES is specified. This allows the code generator to remove unused column aliases in SELECT statements to save space. // Straight up DML invocation. The ast has the statement, execute it! static void cg_std_dml_exec_stmt(ast_node *ast) { cg_bound_sql_statement(NULL, ast, CG_EXEC|CG_MINIFY_ALIASES); }  Note that this flag difference only matters for the CREATE VIEW statement but for symmetry all the DDL is handled with one macro and all the DML with the second macro. Next, the easiest case... there are a bunch of statements that create no code-gen at all. These statements are type definitions that are interesting only to the semantic analyzer, or other control statements. Some examples:  NO_OP_STMT_INIT(declare_enum_stmt); NO_OP_STMT_INIT(declare_named_type);  Next, the general purpose statement handler. STMT_INIT creates mappings such as the if_stmt AST node mapping to cg_if_stmt.  STMT_INIT(if_stmt); STMT_INIT(switch_stmt); STMT_INIT(while_stmt); STMT_INIT(assign);  The next group of declarations are the expressions, with precedence and operator specified. There is a lot of code sharing between AST types as you can see from this sample:  EXPR_INIT(num, cg_expr_num, &quot;num&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(str, cg_expr_str, &quot;STR&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(null, cg_expr_null, &quot;NULL&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(dot, cg_expr_dot, &quot;DOT&quot;, C_EXPR_PRI_ROOT); EXPR_INIT(mul, cg_binary, &quot;*&quot;, C_EXPR_PRI_MUL); EXPR_INIT(div, cg_binary, &quot;/&quot;, C_EXPR_PRI_MUL); EXPR_INIT(mod, cg_binary, &quot;%&quot;, C_EXPR_PRI_MUL); EXPR_INIT(add, cg_binary, &quot;+&quot;, C_EXPR_PRI_ADD); EXPR_INIT(sub, cg_binary, &quot;-&quot;, C_EXPR_PRI_ADD); EXPR_INIT(not, cg_unary, &quot;!&quot;, C_EXPR_PRI_UNARY); EXPR_INIT(tilde, cg_unary, &quot;~&quot;, C_EXPR_PRI_UNARY); EXPR_INIT(uminus, cg_unary, &quot;-&quot;, C_EXPR_PRI_UNARY);  Most (not all) of the binary operators are handled with one function cg_binary and likewise most unary operators are handled with cg_unary. Note: the precedence constants are the C_EXPR_PRI_* flavor because, naturally, parentheses will be generated based on the C rules during C codegen. Importantly, the AST still, and always, authoritatively encodes the user-specified order of operations -- there's no change there. The only thing that changes is where parentheses are needed to get the desired result. Some parens may need to be added, and some that were present in the original text might no longer be needed. Here are some helpful examples: CREATE PROC p () BEGIN /* NOT is weaker than + */ LET x := (NOT 1) + (NOT 2); SET x := NOT 1 + 2; END;  void p(void) { cql_bool x = 0; /* ! is stronger than + */ x = ! 1 + ! 2; x = ! (1 + 2); }  Finally, many built-in functions need special codegen, such as:  FUNC_INIT(coalesce); FUNC_INIT(printf);  FUNC_INIT(coalesce) creates a mapping between the function name coalesce and the generator cg_func_coalesce. "},{"title":"Character Buffers and Byte Buffers​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#character-buffers-and-byte-buffers","content":"The first kind of text output that CQL could produce was the AST echoing. This was originally done directly with fprintf but that was never going to be flexible enough -- we have to be able to emit that output into other places like comments, or the text of SQL statements. This need forces that pass to use character buffers, which we touched on in Part 1. C Code generation has a more profound dependency on character buffers -- they are literally all over cg_c.c and we need to go over how they are used if we're going to understand the codegen passes. The public interface for charbuf is in charbuf.h and it's really quite simple. You allocate a charbuf and then you canbprintf into it. Let's be a bit more specific: #define CHARBUF_INTERNAL_SIZE 1024 #define CHARBUF_GROWTH_SIZE 1024 typedef struct charbuf { char *ptr; // pointer to stored data, if any uint32_t used; // bytes used in current buffer uint32_t max; // max bytes in current buffer // builtin buffer storage char internal[CHARBUF_INTERNAL_SIZE]; } charbuf; cql_data_decl( int32_t charbuf_open_count ); cql_noexport void bopen(charbuf* b); cql_noexport void bclose(charbuf *b); cql_noexport void bprintf(charbuf *b, const char *format, ...);  The typical pattern goes something like this:  charbuf foo; bopen(&amp;foo); bprintf(&amp;foo, &quot;Hello %s\\n&quot;, &quot;World&quot;); // do something with foo.ptr bclose(&amp;foo);  Note that charbuf includes CHARBUF_INTERNAL_SIZE of storage that does not have to be allocated with malloc and it doesn't grow very aggressively. This economy reflects that fact that most charbuf instances are very small. Of course a charbuf could go on the heap if it needs to outlive the function it appears in, but this is exceedingly rare. To make sure buffers are consistently closed -- and this is a problem because there are often a lot of them -- they are allocated with these simple helper macros: #define CHARBUF_OPEN(x) \\ int32_t __saved_charbuf_count##x = charbuf_open_count; \\ charbuf x; \\ bopen(&amp;x) #define CHARBUF_CLOSE(x) \\ bclose(&amp;x); \\ Invariant(__saved_charbuf_count##x == charbuf_open_count)  The earlier example would be written more properly:  CHARBUF_OPEN(foo); bprintf(&amp;foo, &quot;Hello %s\\n&quot;, &quot;World&quot;); // do something with foo.ptr CHARBUF_CLOSE(foo);  If you forget to close a buffer the count will get messed up and the next close will trigger an assertion failure. It's normal to create several buffers in the course of doing code generation. In fact some of these buffers become &quot;globally&quot; visible and get swapped out as needed. For instance, the kind of chaining we see inside of cg_create_proc_stmt is normal, here is the sequence: Make new buffers...  CHARBUF_OPEN(proc_fwd_ref); CHARBUF_OPEN(proc_body); CHARBUF_OPEN(proc_locals); CHARBUF_OPEN(proc_cleanup);  Save the current buffer pointers...  charbuf *saved_main = cg_main_output; charbuf *saved_decls = cg_declarations_output; charbuf *saved_scratch = cg_scratch_vars_output; charbuf *saved_cleanup = cg_cleanup_output; charbuf *saved_fwd_ref = cg_fwd_ref_output;  Switch to the new buffers...  cg_fwd_ref_output = &amp;proc_fwd_ref; cg_main_output = &amp;proc_body; cg_declarations_output = &amp;proc_locals; cg_scratch_vars_output = &amp;proc_locals; cg_cleanup_output = &amp;proc_cleanup;  And of course the code puts the original values back when it's done and then closes what it opened. This means that while processing a procedure the codegen that declares say scratch variables, which would go to cg_scratch_vars_output, is going to target the proc_locals buffer which will be emitted before the proc_body. By the time cg_stmt_list is invoked thecg_main_output variable will be pointing to the procedure body, thus any statements will go into there rather than being accumulated at the global level. Note: it's possible to have code that is not in a procedure (see --global_proc). In general, it's very useful to have different buffers open at the same time. New local variables or scratch variables can be added to their own buffer. New cleanup steps that are necessary can be added tocg_cleanup_output which will appear at the end of a procedure. The final steps of procedure codegen combines all of these pieces plus a little glue to make a working procedure. All codegen works like this -- statements, expressions, all of it. One interesting but unexpected feature of charbuf is that it provides helper methods for indenting a buffer by whatever amount you like. This turns out to be invaluable in creating well formatted C code because of course we want (e.g.) the body of an if statement to be indented. CQL tries to create well formatted code that is readable by humans as much as possible. Byte Buffers​ The byte buffers type, creatively called bytebuf is less commonly used. It is a peer to charbufand provides a growable binary buffer. bytebuf is often used to hold arrays of structures. Interestingly, cg_c.c doesn't currently consume byte buffers, the presence of bytebuf.c actually came late to the CQL compiler. However the CQL runtime cqlrt.c (and cqlrt_common.c) providecql_bytebuf_open, cql_bytebuf_alloc and, cql_bytebuf_close which are akin to the charbuf methods. These functions are used in the generated code to create result sets at runtime. bytebuf was so useful that it found its way back from the runtime into the compiler itself, and is used by other code-generators like the schema upgraded. The semantic analyzer also uses it to help with query fragments and to track the various upgrade annotations. Both charbuf and bytebuf are simple enough that they don't need special discussion. Surveying their code and comments is an excellent exercise for the reader. "},{"title":"Expressions​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#expressions","content":"Many of the output needs of CQL stemmed from the base case of creating the code for CQL expressions. A simple CQL expression like:  SET x := x + y;  seems innocuous enough, and we'd like that expression to compile to this code:  x = x + y;  And indeed, it does. Here's some actual output from the compiler: /* CREATE PROC p () BEGIN DECLARE x, y INTEGER NOT NULL; SET x := x + y; END; */ #define _PROC_ &quot;p&quot; void p(void) { cql_int32 x = 0; cql_int32 y = 0; x = x + y; } #undef _PROC_  (*) the output above was created by using out/cql --in x --cg x.h x.c --nolines to avoid all the # directives That expression looks easy enough. And indeed if all expressions were like this, we could do expression compilation pretty simply -- every binary operator would look something like this: recurse leftemit infix operatorrecurse right This would sort of build up your expressions inside out and your final buffer after all the recursion was done would have the whole expression. This doesn't work at all. To illustrate what goes wrong, we only have to change the test case a tiny bit. The result is telling: /* CREATE PROC p () BEGIN DECLARE x, y INTEGER; SET x := x + y; END; */ #define _PROC_ &quot;p&quot; void p(void) { cql_nullable_int32 x; cql_set_null(x); cql_nullable_int32 y; cql_set_null(y); cql_combine_nullables(x, x.is_null, y.is_null, x.value + y.value); } #undef _PROC_  In this new example above, x and y became nullable variables i.e. the NOT NULL was removed from their declarations -- this makes all the difference in the world. Let's take a quick look at cql_nullable_int32 and we'll see the crux of the problem immediately: typedef struct cql_nullable_int32 { cql_bool is_null; cql_int32 value; } cql_nullable_int32;  The problem is that nullable value types like cql_nullable_int32 have both their value field and a boolean is_null and these don't flow into expressions that use operators like +, -, / and so forth. This means that even simple expressions involving nullable types actually expand into several statements. And, in general, these statements need a place to put their temporary results to accumulate the correct answer, so scratch variables are required to make all this work. Here's a more realistic example: /* CREATE PROC combine (x INTEGER, y INTEGER, OUT result INTEGER) BEGIN SET result := 5 * x + 3 * y; END; */ #define _PROC_ &quot;combine&quot; void combine(cql_nullable_int32 x, cql_nullable_int32 y, cql_nullable_int32 *_Nonnull result) { cql_contract_argument_notnull((void *)result, 3); cql_nullable_int32 _tmp_n_int_1; cql_set_null(_tmp_n_int_1); cql_nullable_int32 _tmp_n_int_2; cql_set_null(_tmp_n_int_2); cql_set_null(*result); // set out arg to non-garbage cql_set_nullable(_tmp_n_int_1, x.is_null, 5 * x.value); cql_set_nullable(_tmp_n_int_2, y.is_null, 3 * y.value); cql_combine_nullables(*result, _tmp_n_int_1.is_null, _tmp_n_int_2.is_null, _tmp_n_int_1.value + _tmp_n_int_2.value); } #undef _PROC_ #pragma clang diagnostic pop  _tmp_n_int_1 : holds the product of x and 5, it's null if x.is_null is true_tmp_n_int_2 : holds the product of y and 3, it's null if y.is_null is true*result : holds the answer, it's null if either of _tmp_n_int_1.is_null, _tmp_n_int_2.is_null is true otherwise it's _tmp_n_int_1.value + _tmp_n_int_2.value So, in general, we need to emit arbitrarily many statements in the course of evaluating even simple looking expressions and we need good mechanisms to manage that. This is what we'll talk about in the coming sections. Managing Scratch Variables​ The function that actually assigns scratch variables is cg_scratch_var // The scratch variable helper uses the given sem_type and the current // stack level to create a temporary variable name for that type at that level. // If the variable does not already have a declaration (as determined by the masks) // then a declaration is added to the scratch_vars section. This is one of the root // ways of getting an .is_null and .value back. Note that not null variables always // have a .is_null of &quot;0&quot; which becomes important when deciding how to assign // one result to another. Everything stays uniform. static void cg_scratch_var(ast_node *ast, sem_t sem_type, charbuf *var, charbuf *is_null, charbuf *value)  The signature is a bit unexpected so we'll go over it, some of below will make more sense as we learn about expressions generally, but this is as good an introduction as any. ast : holds a reference to a variable we want to assign to this argument is normally NULL for scratch variablesast is not null for the RESULT macros which we'll study laterfor now, we can basically ignore this argument sem_type : holds the type of the variable we need it must be a unitary type, optionally with SEM_TYPE_NOTNULL set var : a character buffer that will get the name of the variableis_null : a character buffer that will get the is_null expression for this variable (more below)value : a character buffer that will get the value expression for this variable (more below) And this is a good time to talk about is_null and value because they will be everywhere. The codegen for expressions in the C code generator produces two results: the text that corresponds to the current value so far (e.g. &quot;(1+2)*3&quot;), and,the text that will tell you if the current value is null this could be as simple as &quot;0&quot; for an expression that is known to be not null Let's make this a little more concrete: Suppose we ask for a scratch &quot;not null integer&quot;, we get results like this: var: &quot;_tmp_n_int_1&quot;is_null: &quot;0&quot;value: &quot;_tmp_n_int_1&quot; Meaning: if we want the value, use the text &quot;_tmp_n_int_1&quot; if we want to know if the variable is null, we use the text &quot;0&quot; Note: many parts of cg_c.c special case an is_null value of &quot;0&quot; to make better code because such a thing is known to be not null at compile time. Now let's suppose we ask for a scratch nullable integer, we get results like this: var: &quot;_tmp_int_1&quot;is_null: &quot;_tmp_int_1.is_null&quot;value: &quot;_tmp_int_1.value&quot; So again, we have exactly the text we need to test for null, and the test we need to get the value. Additional notes: scratch variables can be re-used, they are on a &quot;stack&quot;a bitmask is used to track which scratch variables have already had a declaration emitted, so they are only declared oncethe variable name is based on the current value of the stack_level variable which is increased in a push/pop fashion as temporaries come in and out of scope this strategy isn't perfect, but the C compiler can consolidate locals even if the CQL codegen is not perfect so it ends up being not so badimportantly, there is one stack_level variable for all temporaries not one stack_level for every type of temporary, this seemed like a reasonable simplification Allocating Scratch Variables​ The most common reason to create a &quot;scratch&quot; variable is that a temporary variable is needed for some part of the computation. The most common reason for a temporary variable is to hold an intermediate result of a computation involving nullable arithmetic. These temporaries are created with CG_PUSH_TEMP which simply creates the three charbuf variables needed and then asks for a scratch variable of the required type. The variables follow a simple naming convention. The stack level is increased after each temporary is allocated. // Create buffers for a temporary variable. Use cg_scratch_var to fill in the buffers // with the text needed to refer to the variable. cg_scratch_var picks the name // based on stack level-and type. #define CG_PUSH_TEMP(name, sem_type) \\ CHARBUF_OPEN(name); \\ CHARBUF_OPEN(name##_is_null); \\ CHARBUF_OPEN(name##_value); \\ cg_scratch_var(NULL, sem_type, &amp;name, &amp;name##_is_null, &amp;name##_value); \\ stack_level++;  Symmetrically, CG_POP_TEMP closes the charbuf variables and restores the stack level. // Release the buffers for the temporary, restore the stack level. #define CG_POP_TEMP(name) \\ CHARBUF_CLOSE(name##_value); \\ CHARBUF_CLOSE(name##_is_null); \\ CHARBUF_CLOSE(name); \\ stack_level--;  As with the other PUSH/POP OPEN/CLOSE macro types, these macros are designed to make it impossible to forget to free the buffers, or to get the stack level wrong. The stack level can be (and is) checked at strategic places to ensure it's back to baseline -- this is easy because the code can always just snapshot stack_level, do some work that should be clean, and then check that stack_level is back to where it's supposed to be with an Invariant. Recursing Sub-expressions​ Now that we understand that we can create scratch variables as needed, it's time to take a look at the typical evaluation patterns and how the evaluation works within that pattern. This is everywhere in cg_c.c. So let's look at an actual evaluator, the simplest of them all, this one does code generation for the NULL literal. static void cg_expr_null( ast_node *expr, CSTR op, charbuf *is_null, charbuf *value, int32_t pri, int32_t pri_new) { Contract(is_ast_null(expr)); // null literal bprintf(value, &quot;NULL&quot;); bprintf(is_null, &quot;1&quot;); }  Now this may be looking familiar: the signature of the code generator is something very much like the signature of the the gen_ functions in the echoing code. That's really because in some sense the echoing code is a very simple code generator itself. expr : the AST we are generating code forop : the relevant operator if any (operators share code)is_null : a charbuf into which we can write the is_null expression textvalue : a charbuf into which we can write the value expression textpri : the binding strength of the node above this onepri_new : the binding strength of this node This particular generator is going to produce &quot;NULL&quot; for the value and &quot;1&quot; for the is_null expression. is_null and value are the chief outputs, and the caller will use these to create its own expression results with recursive logic. But the expression logic can also write into the statement stream, the cleanup stream, even into the header file stream, and as we'll see, it does. pri and pri_new work exactly like they did in the echoing code (see Part 1), they are used to allow the code generator to decide if it needs to emit parentheses. But recall that the binding strengths now will be the C binding strengths NOT the SQL binding strengths (discussed above). Let's look at one of the simplest operators: the IS NULL operator handled by cg_expr_is_null Note: this code has a simpler signature because it's actually part of codegen for cg_expr_is which has the general contract. // The code-gen for is_null is one of the easiest. The recursive call // produces is_null as one of the outputs. Use that. Our is_null result // is always zero because IS NULL is never, itself, null. static void cg_expr_is_null(ast_node *expr, charbuf *is_null, charbuf *value) { sem_t sem_type_expr = expr-&gt;sem-&gt;sem_type; // expr IS NULL bprintf(is_null, &quot;0&quot;); // the result of is null is never null // The fact that this is not constant not null for not null reference types reflects // the weird state of affairs with uninitialized reference variables which // must be null even if they are typed not null. if (is_not_nullable(sem_type_expr) &amp;&amp; !is_ref_type(sem_type_expr)) { // Note, sql has no side-effects so we can fold this away. bprintf(value, &quot;0&quot;); } else { CG_PUSH_EVAL(expr, C_EXPR_PRI_ROOT); bprintf(value, &quot;%s&quot;, expr_is_null.ptr); CG_POP_EVAL(expr); } }  So walking through the above: the result of IS NULL is never null, so we can immediately put &quot;0&quot; into the is_null bufferif the operand is a not-null numeric type then the result of IS NULL is 0if the operand might actually be null then use CG_PUSH_EVAL to recursively do codegen for itcopy its expr_is_null text into our value text Note: the code reveals one of the big CQL secrets -- that not null reference variables can be null... C has the same issue with _Nonnull globals. Now let's look at those helper macros, they are pretty simple: // Make a temporary buffer for the evaluation results using the canonical // naming convention. This might exit having burned some stack slots // for its result variables, that's normal. #define CG_PUSH_EVAL(expr, pri) \\ CHARBUF_OPEN(expr##_is_null); \\ CHARBUF_OPEN(expr##_value); \\ cg_expr(expr, &amp;expr##_is_null, &amp;expr##_value, pri);  The push macro simply creates buffers to hold the is_null and value results, then it calls cg_expr to dispatch the indicated expression. The pri value provided to this macro represents the binding strength that the callee should assume its parent has. Usually this is the pri_new of the caller. but often C_EXPR_PRI_ROOT can be used if the current context implies that the callee will never need parentheses. How do we know that parens are not needed here? It seems like the operand of IS NULL could be anything, surely it might need parentheses? Let's consider: if the operand is of not null numeric type then we aren't even going to evaluate it, we're on the easy &quot;no it's not null&quot; path no parens there if the operand is nullable then the only place the answer can be stored is in a scratch variable and its is_null expression will be exactly like var.is_null no parens there if the operand is a reference type, there are no operators that combine reference types to get more reference types, so again the result must be in a variable, and is is_null expression will be like !var no parens there So, none of these require further wrapping regardless of what is above the IS NULL node in the tree because of the high strength of the . and ! operators. Other cases are usually simpler, such as &quot;no parentheses need to be added by the child node because it will be used as the argument to a helper function so there will always be parens hard-coded anyway&quot;. However these things need to be carefully tested hence the huge variety of codegen tests. Note that after calling cg_expr the temporary stack level might be increased. We'll get to that in the next section. For now, looking at POP_EVAL we can see it's very straightforward: // Close the buffers used for the above. // The scratch stack is not restored so that any temporaries used in // the evaluation of expr will not be re-used prematurely. They // can't be used again until either the expression is finished, // or they have been captured in a less-nested result variable. #define CG_POP_EVAL(expr) \\ CHARBUF_CLOSE(expr##_value); \\ CHARBUF_CLOSE(expr##_is_null);  CG_POP_EVAL simply closes the buffers, leaving the stack level unchanged. More on this in the coming section. Result Variables​ When recursion happens in the codegen, a common place that the result will be found is in a temporary variable i.e. the generated code will use one or more statements to arrange for the correct answer to be in a variable. To do this, the codegen needs to first get the name of a result variable of a suitable type. This is the &quot;other&quot; reason for making scratch variables. There are three macros that make this pretty simple. The first is CG_RESERVE_RESULT_VAR // Make a scratch variable to hold the final result of an evaluation. // It may or may not be used. It should be the first thing you put // so that it is on the top of your stack. This only saves the slot. // If you use this variable you can reclaim other temporaries that come // from deeper in the tree since they will no longer be needed. #define CG_RESERVE_RESULT_VAR(ast, sem_type) \\ int32_t stack_level_reserved = stack_level; \\ sem_t sem_type_reserved = sem_type; \\ ast_node *ast_reserved = ast; \\ CHARBUF_OPEN(result_var); \\ CHARBUF_OPEN(result_var_is_null); \\ CHARBUF_OPEN(result_var_value); \\ stack_level++;  If this looks a lot like PUSH_TEMP that shouldn't be surprising. The name of the variable and the expression parts always go into charbuf variables named result_var, result_var_is_null, and result_var_valuebut the scratch variable isn't actually allocated! However -- we burn the stack_level as though it had been allocated. The name of the macro provides a clue: this macro reserves a slot for the result variable, it's used if the codegen might need a result variable, but it might not. If/when the result variable is needed, it we can artificially move the stack level back to the reserved spot, allocate the scratch variable, and then put the stack level back. When the name is set we know that the scratch variable was actually used. The CG_USE_RESULT_VAR macro does exactly this operation. // If the result variable is going to be used, this writes its name // and .value and .is_null into the is_null and value fields. #define CG_USE_RESULT_VAR() \\ int32_t stack_level_now = stack_level; \\ stack_level = stack_level_reserved; \\ cg_scratch_var(ast_reserved, sem_type_reserved, &amp;result_var, &amp;result_var_is_null, &amp;result_var_value); \\ stack_level = stack_level_now; \\ Invariant(result_var.used &gt; 1); \\ bprintf(is_null, &quot;%s&quot;, result_var_is_null.ptr); \\ bprintf(value, &quot;%s&quot;, result_var_value.ptr)  Once the code generator decides that it will in fact be using a result variable to represent the answer, then the is_null and value buffers can be immediately populated to whatever the values were for the result variable. That text will be correct regardless of what codegen is used to populate the variable. The variable is the result. There is a simpler macro that reserves and uses the result variable in one step, it's used frequently. The &quot;reserve&quot; pattern is only necessary when there are some paths that need a result variable and some that don't. // This does reserve and use in one step #define CG_SETUP_RESULT_VAR(ast, sem_type) \\ CG_RESERVE_RESULT_VAR(ast, sem_type); \\ CG_USE_RESULT_VAR();  And now armed with this knowledge we can look at the rest of the scratch stack management. // Release the buffer holding the name of the variable. // If the result variable was used, we can re-use any temporaries // with a bigger number. They're no longer needed since they // are captured in this result. We know it was used if it // has .used &gt; 1 (there is always a trailing null so empty is 1). #define CG_CLEANUP_RESULT_VAR() \\ if (result_var.used &gt; 1) stack_level = stack_level_reserved + 1; \\ CHARBUF_CLOSE(result_var_value); \\ CHARBUF_CLOSE(result_var_is_null); \\ CHARBUF_CLOSE(result_var);  As it happens when you use CG_PUSH_EVAL it is entirely possible, even likely, that the result of cg_expr is in a result variable. The convention is that if the codegen requires a result variable it is allocated first, before any other temporaries. This is why there is a way to reserve a variable that you might need. Now if it turns out that you used the result variable at your level it means that any temporary result variables from deeper levels have been used and their values plus whatever math was needed is now in your result variable. This means that the stack_levelvariable can be decreased to one more than the level of the present result. This is in the fact the only time it is safe to start re-using result variables because you otherwise never know how many references to result variables that were &quot;deep in the tree&quot; are left in the contents of expr_value or expr_is_null. Now, armed with the knowledge that there are result variables and temporary variables and both come from the scratch variables we can resolve the last mystery we left hanging. Why does the scratch variable API accept an AST pointer? The only place that AST pointer can be not null is in the CG_USE_RESULT_VAR macro, it was this line: cg_scratch_var(ast_reserved, sem_type_reserved, &amp;result_var, &amp;result_var_is_null, &amp;result_var_value);  And ast_reserved refers to the AST that we are trying to evaluate. There's an important special case that we want to optimize that saves a lot of scratch variables. That case is handled by this code in cg_scratch_var:  // try to avoid creating a scratch variable if we can use the target of an assignment in flight. if (is_assignment_target_reusable(ast, sem_type)) { Invariant(ast &amp;&amp; ast-&gt;parent &amp;&amp; ast-&gt;parent-&gt;left); EXTRACT_ANY_NOTNULL(name_ast, ast-&gt;parent-&gt;left); EXTRACT_STRING(name, name_ast); if (is_out_parameter(name_ast-&gt;sem-&gt;sem_type)) { bprintf(var, &quot;*%s&quot;, name); } else { bprintf(var, &quot;%s&quot;, name); } }  The idea is that if the generator is doing an assignment like:  SET x := a + b;  Then the code generator doesn't need a scratch variable to hold the result of the expression a + b like it would in many other contexts. It can use x as the result variable! The SET codegen will discover that the value it's supposed to set is already in x so it does nothing and everything just works out. The price of this is a call to is_assignment_target_reusable and then some logic to handle the case where x is an out argument (hence call by reference, hence needs to be used as *x). "},{"title":"Basic Control Flow Patterns​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#basic-control-flow-patterns","content":"To get a sense of how the compiler generates code for statements, we can look at some of the easiest cases. // &quot;While&quot; suffers from the same problem as IF and as a consequence // generating while (expression) would not generalize. // The overall pattern for while has to look like this: // // for (;;) { // prep statements; // condition = final expression; // if (!condition) break; // // statements; // } // // Note that while can have leave and continue substatements which have to map // to break and continue. That means other top level statements that aren't loops // must not create a C loop construct or break/continue would have the wrong target. static void cg_while_stmt(ast_node *ast) { Contract(is_ast_while_stmt(ast)); EXTRACT_ANY_NOTNULL(expr, ast-&gt;left); EXTRACT(stmt_list, ast-&gt;right); sem_t sem_type = expr-&gt;sem-&gt;sem_type; // WHILE [expr] BEGIN [stmt_list] END bprintf(cg_main_output, &quot;for (;;) {\\n&quot;); CG_PUSH_EVAL(expr, C_EXPR_PRI_ROOT); if (is_nullable(sem_type)) { bprintf(cg_main_output, &quot;if (!cql_is_nullable_true(%s, %s)) break;\\n&quot;, expr_is_null.ptr, expr_value.ptr); } else { bprintf(cg_main_output, &quot;if (!(%s)) break;\\n&quot;, expr_value.ptr); } bool_t loop_saved = cg_in_loop; cg_in_loop = true; CG_POP_EVAL(expr); cg_stmt_list(stmt_list); bprintf(cg_main_output, &quot;}\\n&quot;); cg_in_loop = loop_saved; }  The comment before the cg_while_stmt actually describes the situation pretty clearly; the issue with this codegen is that the expression in the while statement might actually require many C statements to evaluate. There are many cases of this sort of thing, but the simplest is probably when any nullable types are in that expression. A particular example illustrates this pretty clearly: CREATE PROC p () BEGIN DECLARE x INTEGER NOT NULL; SET x := 1; WHILE x &lt; 5 BEGIN SET x := x + 1; END; END;  which generates: void p(void) { cql_int32 x = 0; x = 1; for (;;) { /* in trickier cases there would be code right here */ if (!(x &lt; 5)) break; x = x + 1; } }  In this case, a while statement could have been used because the condition is simply x &lt; 5so this more general pattern is overkill. But consider this program, just a tiny bit different: CREATE PROC p () BEGIN DECLARE x INTEGER; -- x is nullable SET x := 1; WHILE x &lt; 5 BEGIN SET x := x + 1; END; END;  which produces: void p(void) { cql_nullable_int32 x; cql_set_null(x); cql_nullable_bool _tmp_n_bool_0; cql_set_null(_tmp_n_bool_0); cql_set_notnull(x, 1); for (;;) { cql_set_nullable(_tmp_n_bool_0, x.is_null, x.value &lt; 5); if (!cql_is_nullable_true(_tmp_n_bool_0.is_null, _tmp_n_bool_0.value)) break; cql_set_nullable(x, x.is_null, x.value + 1); } }  Even for this small little case, the nullable arithmetic macros have to be used to keep x up to date. The result of x &lt; 5 is of type BOOL rather than BOOL NOT NULL so a temporary variable captures the result of the expression. This is an easy case, but similar things happen if the expression includes e.g. CASE...WHEN... or IN constructs. There are many other cases. So with this in mind, let's reconsider what cg_while_stmt is doing: we start the for statement in the output there's a bprintf for that we evaluate the while expression, the details will be in is_null and value we use CG_PUSH_EVAL for that if the result is nullable there is a helper macro cql_is_nullable_true that tells us if the value is not null and trueif the result is not nullable we can use expr_value.ptr directlywe make a note that we're in a loop (this matters for statement cleanup, more on that later)we recurse to do more statements with cg_stmt_listfinally we end the for that we began This kind of structure is common to all the control flow cases. Generally, we have to deal with the fact that CQL expressions often become C statements so we use a more general flow control strategy. But with this in mind, it's easy to imagine how the IF, LOOP, and SWITCH switch statements are handled. "},{"title":"Cleanup and Errors​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#cleanup-and-errors","content":"There are a number of places where things can go wrong when running a CQL procedure. The most common sources are: (1) SQLite APIs, almost all of which can fail, and, (2) calling other procedures which also might fail. Here's a very simple example: DECLARE PROC something_that_might_fail (arg TEXT) USING TRANSACTION; CREATE PROC p () BEGIN LET arg := &quot;test&quot;; CALL something_that_might_fail(arg); END;  Which generates: cql_string_literal(_literal_1_test_p, &quot;test&quot;); CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; cql_string_ref arg = NULL; cql_set_string_ref(&amp;arg, _literal_1_test_p); _rc_ = something_that_might_fail(_db_, arg); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_string_release(arg); return _rc_; }  Let's look at those fragments carefully: first, we had to declare something_that_might_fail the declaration included USING TRANSACTION indicating the procedure uses the databasewe didn't provide the procedure definition, this is like an extern ... foo(...); declaration there is a string literal named _literal_1_test_p that is auto-created cql_string_literal can expand into a variety of things, whatever you want &quot;make a string literal&quot; to meanit's defined in cqlrt.h and it's designed to be replaced cql_set_string_ref(&amp;arg, _literal_1_test_p); is expected to &quot;retain&quot; the string (+1 ref count)cql_cleanup is the exit label, this cleanup code will run on all exit paths cleanup statements are accumulated by writing to cg_cleanup_output which usually writes to the proc_cleanup bufferbecause cleanup is in its own buffer you can add to it freely whenever a new declaration that requires cleanup arisesin this case the declaration of the string variable caused the C variable arg to be created and also the cleanup code now we call something_that_might_fail passing it our database pointer and the argumentthe hidden _db_ pointer is passed to all procedures that use the databasethese procedures are also the ones that can failany failed return code (not SQLITE_OK) causes two things: the cql_error_trace() macro is invoked (this macro typically expands to nothing)the code is redirected to the cleanup block via goto cql_cleanup; The essential sequence is this one:  if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; }  The C code generator consistently uses this pattern to check if anything went wrong and to exit with an error code. Extensive logging can be very expensive, but in debug builds it's quite normal for cql_error_trace to expand into something like fprintf(stderr, &quot;error %d in %s %s:%d\\n&quot;, _rc_, _PROC_, __FILE__, __LINE_) which is probably a lot more logging than you want in a production build but great if you're debugging. Recall that CQL generates something like #define _PROC_ &quot;p&quot; before every procedure. This error pattern generalizes well and indeed if we use the exception handling pattern, we get a lot of control. Let's generalize this example a tiny bit: CREATE PROC p (OUT success BOOL NOT NULL) BEGIN LET arg := &quot;test&quot;; BEGIN TRY CALL something_that_might_fail(arg); SET success := 1; END TRY; BEGIN CATCH SET success := 0; END CATCH; END;  CQL doesn't have complicated exception objects or anything like that, exceptions are just simple control flow. Here's the code for the above: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, cql_bool *_Nonnull success) { cql_contract_argument_notnull((void *)success, 1); cql_code _rc_ = SQLITE_OK; cql_string_ref arg = NULL; *success = 0; // set out arg to non-garbage cql_set_string_ref(&amp;arg, _literal_1_test_p); // try { _rc_ = something_that_might_fail(_db_, arg); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto catch_start_1; } *success = 1; goto catch_end_1; } catch_start_1: { *success = 0; } catch_end_1:; _rc_ = SQLITE_OK; cql_string_release(arg); return _rc_; }  The code in this case is nearly the same as the previous example. Let's look at the essential differences: If there is an error, goto catch_start_1 will runIf the try block succeeds, goto catch_end_1 will runboth the TRY and CATCH branches set the success out parametersince an out argument was added, CQL generated an error check to ensure that success is not null cql_contract_argument_notnull((void *)success, 1), the 1 means &quot;argument 1&quot; and will appear in the error message if this test failsthe hidden _db_ argument doesn't count for error message purposes, so success is still the first argument How does this happen? Let's look at cg_trycatch_helper which does this work: // Very little magic is needed to do try/catch in our context. The error // handlers for all the sqlite calls check _rc_ and if it's an error they // &quot;goto&quot; the current error target. That target is usually CQL_CLEANUP_DEFAULT_LABEL. // Inside the try block, the cleanup handler is changed to the catch block. // The catch block puts it back. Otherwise, generate nested statements as usual. static void cg_trycatch_helper(ast_node *try_list, ast_node *try_extras, ast_node *catch_list) { CHARBUF_OPEN(catch_start); CHARBUF_OPEN(catch_end); // We need unique labels for this block ++catch_block_count; bprintf(&amp;catch_start, &quot;catch_start_%d&quot;, catch_block_count); bprintf(&amp;catch_end, &quot;catch_end_%d&quot;, catch_block_count); // Divert the error target. CSTR saved_error_target = error_target; bool_t saved_error_target_used = error_target_used; error_target = catch_start.ptr; error_target_used = 0; ...  The secret is the error_target global variable. All of the error handling will emit a goto error_target statement. The try/catch pattern simply changes the current error target. The rest of the code in the helper is just to save the current error target and to create unique labels for the try/catch block. The important notion is that, if anything goes wrong, whatever it is, the generator simply does a goto error_target and that will either hit the catch block or else go to cleanup. The THROW operation illustrates this well: // Convert _rc_ into an error code. If it already is one keep it. // Then go to the current error target. static void cg_throw_stmt(ast_node *ast) { Contract(is_ast_throw_stmt(ast)); bprintf(cg_main_output, &quot;_rc_ = cql_best_error(%s);\\n&quot;, rcthrown_current); bprintf(cg_main_output, &quot;goto %s;\\n&quot;, error_target); error_target_used = 1; rcthrown_used = 1; }  first we make sure _rc_ has some kind of error in it, either rcthrown_current or else SQLITE_ERRORthen we go to the current error targeterror_target_used tracks whether if the error label was used, this is just to avoid C compiler errors about unused labels. if the label is not used it won't be emittedthe code never jumps back to an error label, so we'll always know if the label was used before we need to emit it Note: every catch block captures the value of _rc_ in a local variable whose name is in rcthrown_current. This captured value is the current failing result code accessible by @RC in CQL. A catch block can therefore do stuff like: IF @RC = 1 THEN THROW; ELSE call attempt_retry(); END IF;  This entire mechanism is built with basically just a few state variables that nest. There is no complicated stack walking or anything like that. All the code has to do is chain the error labels together and let users create new catch blocks with new error labels. All that together gives you very flexible try/catch behaviour with very little overhead. "},{"title":"String Literals​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#string-literals","content":"Before we move on to more complex statements we have to discuss string literals a little bit. We've mentioned before that the compiler is going to generate something like this: cql_string_literal(_literal_1_test_p, &quot;test&quot;);  To create a reference counted object _literal_1_test_p that it can use. Now we're going to talk about how the text &quot;test&quot; was created and how that gets more complicated. The first thing to remember is that the generator creates C programs. That means no matter what kind of literal we might be processing it's ending up encoded as a C string for the C compiler. The C compiler will be the first thing the decodes the text the generator produces and puts the byte we need into the final programs data segment or wherever. That means if we have SQL format strings that need to go to SQLite they will be twice-encoded, the SQL string is escaped as needed for SQLite and that is escaped again for the C compiler. An example might make this clearer consider the following SQL:  SELECT '&quot;x''y&quot;' AS a, &quot;'y'\\n&quot; AS b;  The generated text for this statement will be:  &quot;SELECT '\\&quot;x''y\\&quot;', '''y''\\n'&quot;  Let's review that in some detail: the first string &quot;a&quot; is a standard SQL string it is represented unchanged in the AST, it is not unescapedeven the outer single quotes are preserved, CQL has no need to change it at allwhen we emit it into our output it will be read by the C compiler, soat that time it is escaped again into C format the double quotes which required no escaping in SQL become \\&quot; the single quote character requires no escape but there are still two of them because SQLite will also process this string the second string &quot;b&quot; is a C formatted string literal SQLite doesn't support this format or its escapes, thereforeas discussed in Part 1, it is decoded to plain text, then re-encoded as a SQL escaped stringinternal newlines do not require escaping in SQL, they are in the string as the newline character not '\\n' or anything like that to be completely precise the byte value 0x0a is in the string unescaped internal single quotes don't require escaping in C, these have to be doubled in a SQL stringthe outer double quotes are removed and replaced by single quotes during this processthe AST now has a valid SQL formatted string possibly with weird characters in itas before, this string has to be formatted for the C compiler so now it has to be escaped againthe single quotes require no further processing, though now there are quite a few of themthe embedded newline is converted to the escape sequence &quot;\\n&quot; so we're back to sort of where we started the C compiler will convert this back to the byte 0x0a which is what ends up in the data segment In the above example we were making one overall string for the SELECT statement so the outer double quotes are around the whole statement. That was just for the convenience of this example. If the literals had been in some other loose context then individual strings would be produced the same way. Except, not so fast, not every string literal is heading for SQLite. Some are just making regular strings. In that case even if they are destined for SQLite they will go as bound arguments to a statement not in the text of the SQL. That means those strings do not need SQL escaping. Consider:  LET a := '&quot;x''y&quot;'; LET b := &quot;'y'\\n&quot;;  To do those assignments we need: cql_string_literal(_literal_1_x_y_p, &quot;\\&quot;x'y\\&quot;&quot;); cql_string_literal(_literal_2_y_p, &quot;'y'\\n&quot;);  In both of these cases the steps are: unescape the escaped SQL string in the AST to plain text removing the outer single quotes of course re-escape the plain text (which might include newlines and such) as a C string emit that text, including its outer double quotes Trivia: the name of the string literal variables include a fragment of the string to make them a little easier to spot. encoders.h has the encoding functions cg_decode_string_literalcg_encode_string_literalcg_encode_c_string_literalcg_decode_c_string_literal As well as similar functions for single characters to make all this possible. Pretty much every combination of encoding and re-encoding happens in some path through the code generator. "},{"title":"Executing SQLite Statements​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#executing-sqlite-statements","content":"By way of example let's consider a pretty simple piece of SQL we might want to run. CREATE TABLE foo(id INTEGER, t TEXT); CREATE PROC p (id_ INTEGER, t_ TEXT) BEGIN UPDATE foo SET t = t_ WHERE id = id_; END;  To make this happen we're going to have to do the following things: create a string literal with the statement we needthe references to id_ and t_ have to be replaced with ?we prepare that statementwe bind the values of id_ and t_we step the statementwe finalize the statementsuitable error checks have to be done at each stage That's quite a bit of code and it's easy to forget a step, this is an area where CQL shines. The code we had to write in CQL was very clear and all the error checking is implicit. This is the generated code. We'll walk through it and discuss how it is created. CQL_WARN_UNUSED cql_code p( sqlite3 *_Nonnull _db_, cql_nullable_int32 id_, cql_string_ref _Nullable t_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *_temp_stmt = NULL; _rc_ = cql_prepare(_db_, &amp;_temp_stmt, &quot;UPDATE foo &quot; &quot;SET t = ? &quot; &quot;WHERE id = ?&quot;); cql_multibind(&amp;_rc_, _db_, &amp;_temp_stmt, 2, CQL_DATA_TYPE_INT32, &amp;id_, CQL_DATA_TYPE_STRING, t_); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(_temp_stmt); if (_rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } cql_finalize_stmt(&amp;_temp_stmt); _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;_temp_stmt); return _rc_; }  the functions signature includes the hidden _db_ parameter plus the two argumentswe need a hidden _rc_ variable to hold the result codes from SQLitewe need a scratch sqlite3_stmt * named _temp_stmt to talk to SQLite when this is created, the cleanup section gets cql_finalize_stmt(&amp;_temp_stmt);cql_finalize_stmt sets the statement to null and does nothing if it's already null the string &quot;INSERT INTO foo(id, t) VALUES(?, ?)&quot; is created from the AST recall that we have variables_callback as an option, it's used here to track the variables and replace them with ?more on this shortly cql_multibind is used to bind the values of id_ and t_ this is just a varargs version of the normal SQLite binding functions, it's only done this way to save spaceonly one error check is needed for any binding failurethe type of binding is encoded very economicallythe &quot;2&quot; here refers to two arguments the usual error processing happens with cql_error_trace and goto cql_cleanupthe statement is executed with sqlite3_steptemporary statements are finalized immediately with cql_finalize_stmt in this case its redundant because the code is going to fall through to cleanup anywayin general there could be many statements and we want to finalize immediatelythis is an optimization opportunity, procedures with just one statement are very common Most of these steps are actually hard coded. There is no variability in the sequence after the multibind call, so that's just boiler-plate the compiler can inject. We don't want to declare _temp_stmt over and over so there's a flag that records whether it has already been declared in the current procedure. // Emit a declaration for the temporary statement _temp_stmt_ if we haven't // already done so. Also emit the cleanup once. static void ensure_temp_statement() { if (!temp_statement_emitted) { bprintf(cg_declarations_output, &quot;sqlite3_stmt *_temp_stmt = NULL;\\n&quot;); bprintf(cg_cleanup_output, &quot; cql_finalize_stmt(&amp;_temp_stmt);\\n&quot;); temp_statement_emitted = 1; } }  This is a great example of how, no matter where the processing happens to be, the generator can emit things into the various sections. Here it adds a declaration and an cleanup with no concern about what else might be going on. So most of the above is just boiler-plate, the tricky part is: getting the text of the SQLbinding the variables All of this is the business of this function: // This is the most important function for sqlite access; it does the heavy // lifting of generating the C code to prepare and bind a SQL statement. // If cg_exec is true (CG_EXEC) then the statement is executed immediately // and finalized. No results are expected. To accomplish this we do the following: // * figure out the name of the statement, either it's given to us // or we're using the temp statement // * call get_statement_with_callback to get the text of the SQL from the AST // * the callback will give us all the variables to bind // * count the variables so we know what column numbers to use (the list is backwards!) // * if CG_EXEC and no variables we can use the simpler sqlite3_exec form // * bind any variables // * if there are variables CG_EXEC will step and finalize static void cg_bound_sql_statement(CSTR stmt_name, ast_node *stmt, int32_t cg_flags) { ... }  The core of this function looks like this:  gen_sql_callbacks callbacks; init_gen_sql_callbacks(&amp;callbacks); callbacks.variables_callback = cg_capture_variables; callbacks.variables_context = &amp;vars; // ... more flags CHARBUF_OPEN(temp); gen_set_output_buffer(&amp;temp); gen_statement_with_callbacks(stmt, &amp;callbacks);  It's set up the callbacks for variables and it calls the echoing function on the buffer. We've talked about gen_statement_with_callbacks in Part 1. Let's take a look at that callback function: // This is the callback method handed to the gen_ method that creates SQL for us // it will call us every time it finds a variable that needs to be bound. That // variable is replaced by ? in the SQL output. We end up with a list of variables // to bind on a silver platter (but in reverse order). static bool_t cg_capture_variables(ast_node *ast, void *context, charbuf *buffer) { list_item **head = (list_item**)context; add_item_to_list(head, ast); bprintf(buffer, &quot;?&quot;); return true; }  The context variable was set to be vars, we convert it back to the correct type and add the current ast to that list. add_item_to_list always puts things at the head so the list will be in reverse order. With this done, we're pretty much set. We'll produce the statement with a sequence like this one (there are a couple of variations, but this is the most general)  bprintf(cg_main_output, &quot;_rc_ = cql_prepare(_db_, %s%s_stmt,\\n &quot;, amp, stmt_name); cg_pretty_quote_plaintext(temp.ptr, cg_main_output, PRETTY_QUOTE_C | PRETTY_QUOTE_MULTI_LINE); bprintf(cg_main_output, &quot;);\\n&quot;);  cg_pretty_quote_plaintext is one of the C string encoding formats, it could have been just the regular C string encoding but that would have been a bit wasteful and it wouldn't have looked as nice. This function does a little transform. The normal echo of the update statement in question looks like this:  UPDATE foo SET t = ? WHERE id = ?;  Note that it has indenting and newlines embedded in it. The standard encoding of that would look like this: &quot; UPDATE foo\\n SET t = ?\\n WHERE id = ?;&quot;  That surely works, but it's wasteful and ugly. The pretty format instead produces:  &quot;UPDATE foo &quot; &quot;SET t = ? &quot; &quot;WHERE id = ?&quot;  So, the newlines are gone from the string (they aren't needed), instead the string literal was broken into lines for readability. The indenting is gone from the string, instead the string fragments are indented. So what you get is a string literal that reads nicely but doesn't have unnecessary whitespace for SQLite. Obviously you can't use pretty-quoted literals in all cases, it's exclusively for SQLite formatting. All that's left to do is bind the arguments. Remember that arg list is in reverse order:  uint32_t count = 0; for (list_item *item = vars; item; item = item-&gt;next, count++) ; // ... reverse_list(&amp;vars); if (count) { bprintf(cg_main_output, &quot;cql_multibind(&amp;_rc_, _db_, %s%s_stmt, %d&quot;, amp, stmt_name, count); // Now emit the binding args for each variable for (list_item *item = vars; item; item = item-&gt;next) { Contract(item-&gt;ast-&gt;sem-&gt;name); bprintf(cg_main_output, &quot;,\\n &quot;); cg_bind_column(item-&gt;ast-&gt;sem-&gt;sem_type, item-&gt;ast-&gt;sem-&gt;name); } bprintf(cg_main_output, &quot;);\\n&quot;); }  first compute the count, we don't need to bind if there are no variablesreverse_list does exactly what is sounds like (finally a real-world use-case for reverse-list-in-place)cg_bind_column creates one line of the var-args output: column type and variable name the type and name information is right there on the AST in the sem_node And that's it. With those few helpers we can bind any SQLite statement the same way. All of theDDL_STMT_INIT and DML_STMT_INIT statements are completely implemented by this path. "},{"title":"Reading Single Values​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#reading-single-values","content":"In many cases you need just one value CREATE PROC p (id_ INTEGER NOT NULL, OUT t_ TEXT) BEGIN SET t_ := ( SELECT t FROM foo WHERE id = id_ ); END;  This is going to be very similar to the examples we've seen so far: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, cql_int32 id_, cql_string_ref _Nullable *_Nonnull t_) { cql_contract_argument_notnull((void *)t_, 2); cql_code _rc_ = SQLITE_OK; cql_string_ref _tmp_text_0 = NULL; sqlite3_stmt *_temp_stmt = NULL; *(void **)t_ = NULL; // set out arg to non-garbage _rc_ = cql_prepare(_db_, &amp;_temp_stmt, &quot;SELECT t &quot; &quot;FROM foo &quot; &quot;WHERE id = ?&quot;); cql_multibind(&amp;_rc_, _db_, &amp;_temp_stmt, 1, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, id_); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(_temp_stmt); if (_rc_ != SQLITE_ROW) { cql_error_trace(); goto cql_cleanup; } cql_column_string_ref(_temp_stmt, 0, &amp;_tmp_text_0); cql_finalize_stmt(&amp;_temp_stmt); cql_set_string_ref(&amp;*t_, _tmp_text_0); _rc_ = SQLITE_OK; cql_cleanup: cql_string_release(_tmp_text_0); cql_finalize_stmt(&amp;_temp_stmt); return _rc_; }  _db_ : incoming arg for a procedure that uses the database same as always, check*(void **)t_ = NULL; : out args are always set to NULL on entry, note, there is no release here argument is assumed to be garbage, that's the ABIif argument is non-garbage caller must release it first, that's the ABI _rc_ : same as always, check_tmp_text_0 : new temporary text, including cleanup (this could have been avoided)_temp_stmt : as before, including cleanupcql_prepare : same as always, checkcql_multibind : just one integer bound this timesqlite3_step : as before, we're stepping once, this time we want the dataif (_rc_ != SQLITE_ROW) new error check and goto cleanup if no row this is the same as the IF NOTHING THROW variant of construct, that's the default cql_column_string_ref : reads one string from _temp_stmtcql_finalize_stmt : as beforecql_set_string_ref(&amp;*t_, _tmp_text_0) : copy the temporary string to the out arg includes retain, out arg is NULL so cql_set_string_ref will do no releaseif this were (e.g.) running in a loop, the out arg would not be null and there would be a release, as expectedif something else had previously set the out arg, again, there would be a release as expected There are variations of this form such as: CREATE PROC p (id_ INTEGER NOT NULL, OUT t_ TEXT) BEGIN SET t_ := ( SELECT t FROM foo WHERE id = id_ IF NOTHING ''); END;  This simply changes the handling of the case where there is no row. The that part of the code ends up looking like this:  if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (_rc_ == SQLITE_ROW) { cql_column_string_ref(_temp_stmt, 0, &amp;_tmp_text_1); cql_set_string_ref(&amp;_tmp_text_0, _tmp_text_1); } else { cql_set_string_ref(&amp;_tmp_text_0, _literal_1_p); }  any error code leads to cleanupSQLITE_ROW : leads to the same fetch as beforeSQLITE_DONE : leads to the no row case which sets _tmp_text_0 to the empty string cql_string_literal(_literal_1_p, &quot;&quot;); is included as a data declaration There is also the IF NOTHING OR NULL variant which is left as an exercise to the reader. You can find all the flavors in cg_c.c in the this function: // This is a nested select expression. To evaluate we will // * prepare a temporary to hold the result // * generate the bound SQL statement // * extract the exactly one argument into the result variable // which is of exactly the right type // * use that variable as the result. // The helper methods take care of sqlite error management. static void cg_expr_select(...  This handles all of the (select ...) expressions and it has the usual expression handler syntax. Another great example of a CQL expressions that might require many C statements to implement. "},{"title":"Reading Rows With Cursors​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#reading-rows-with-cursors","content":"This section is about the cases where we are expecting results back from SQLite. By results here I mean the results of some kind of query, not like a return code. SQLite does this by giving you a sqlite3_stmt * which you can then use like a cursor to read out a bunch of rows. So it should be no surprise that CQL cursors map directly to SQLite statements. Most of the code to get a statement we've already seen before, we only saw the _temp_stmtcase and we did very little with it. Let's look at the code for something a little bit more general and we'll see how little it takes to generalize. First, let's look at how a CQL cursor is initialized: CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; END;  Now in this case there can only be one row in the result, but it would be no different if there were more. Here's the C code: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; cql_bool _C_has_row_ = 0; _rc_ = cql_prepare(_db_, &amp;C_stmt, &quot;SELECT 1, 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); return _rc_; }  Let's look over that code very carefully and see what is necessary to make it happen. _db_ : incoming arg for a procedure that uses the database same as always, check_rc_ : same as always, checkC_stmt : we need to generate this instead of using _temp_stmt cql_finalize_stmt(&amp;C_stmt) in cleanup, just like _temp_stmt cql_prepare : same as always, checkcql_multibind : could have been binding, not none needed here, but same as always anyway, checkno step, no finalize (until cleanup) : that boiler-plate is removed And that's it, we now have a statement in C_stmt ready to go. We'll see later that _C_has_row_will track whether or not the cursor has any data in it. How do we make this happen? Well you could look at cg_declare_cursor and your eye might hurt at first. The truth is there are many kinds of cursors in CQL and this method handles all of them. We're going to go over the various flavors but for now we're only discussing the so-called &quot;statement cursor&quot;, so named because it simply holds a SQLite statement. This was the first, and for a while only, type of cursor added to the CQL language. OK so how do we make a statement cursor. It's once again cg_bound_sql_statement just like so: cg_bound_sql_statement(cursor_name, select_stmt, CG_PREPARE|CG_MINIFY_ALIASES);  The entire difference is that the first argument is the cursor name rather than NULL. If you pass NULL it means use the temporary statement. And you'll notice that even in this simple example the SQLite text was altered a bit: the text that went to SQLite was &quot;SELECT 1, 2&quot; -- that's CG_MINIFY_ALIASES at work. SQLite didn't need to see those column aliases, it makes no difference in the result. Column aliases are often long and numerous. Even in this simple example we saved 4 bytes. But the entire query was only 12 bytes long (including trailing null) so that's 25%. It's not a huge savings in general but it's something. The other flag CG_PREPARE tells the binder that it should not step or finalize the query. The alternative is CG_EXEC (which was used in the previous section for the UPDATE example). "},{"title":"Fetching Data From Cursors​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#fetching-data-from-cursors","content":"The first cursor reading primitive that was implemented as FETCH [cursor] INTO [variables] and it's the simplest to understand so let's start there. We change the example just a bit: CREATE PROC p () BEGIN DECLARE x INTEGER NOT NULL; DECLARE y INTEGER NOT NULL; DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; FETCH C INTO x, y; END;  For simplicity I will only include the code that is added. The rest is the same.  cql_int32 x = 0; cql_int32 y = 0; // same as before _rc_ = sqlite3_step(C_stmt); _C_has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; }  Do to the FETCH we do the following: step the cursorset the _C_has_row_ variable so to indicate if we got a row or notuse the varargs cql_multifetch to read 2 columns from the cursor this helper simply uses the usual sqlite3_*_column functions to read the data outagain, we do it this way so that there is less error checking needed in the generated codealso, there are fewer function calls so the code is overall smallertrivia: multibind and multifetch are totally references to The Fifth Element hence, they should be pronounced like Leeloo saying &quot;multipass&quot; multifetch uses the varargs to clobber the contents of the target variables if there is no row according to _rc_multifetch uses the CQL_DATA_TYPE_NOT_NULL to decide if it should ask SQLite first if the column is null So now this begs the question, in the CQL, how do you know if a row was fetched or not? The answer is, you can use the cursor name like a boolean. Let's complicate this up a little more. DECLARE PROCEDURE printf NO CHECK; CREATE PROC p () BEGIN DECLARE x INTEGER NOT NULL; DECLARE y INTEGER NOT NULL; DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; FETCH C INTO x, y; WHILE C BEGIN CALL printf(&quot;%d, %d\\n&quot;, x, y); FETCH C INTO x, y; END; END;  Again here is what is now added, we've seen the WHILE pattern before:  for (;;) { if (!(_C_has_row_)) break; printf(&quot;%d, %d\\n&quot;, x, y); _rc_ = sqlite3_step(C_stmt); _C_has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } }  So, if you use the cursors name in an ordinary expression that is converted to a reference to the boolean _C_has_row_. Within the loop we're going to print some data and then fetch the next row. The internal fetch is of course the same as the first. The next improvement that was added to the language was the LOOP statement. Let's take a look: BEGIN DECLARE x INTEGER NOT NULL; DECLARE y INTEGER NOT NULL; DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; LOOP FETCH C INTO x, y BEGIN CALL printf(&quot;%d, %d\\n&quot;, x, y); END; END;  The generated code is very similar:  for (;;) { _rc_ = sqlite3_step(C_stmt); _C_has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (!_C_has_row_) break; printf(&quot;%d, %d\\n&quot;, x, y); }  This is done by: emit the for (;;) { to start the loopgenerate the FETCH just as if it was standaloneemit if (!_C_has_row_) break; (with the correct cursor name)use cg_stmt_list to emit the internal statement list (CALL printf in this case)close the loop with } and we're done "},{"title":"Cursors With Storage​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#cursors-with-storage","content":"We now come to the big motivating reasons for having the notion of shapes in the CQL language. This particular case was the first such example in the language and it's very commonly used and saves you a lot of typing. Like the other examples it's only sugar in that it doesn't give you any new language powers you didn't have, but it does give clarity and maintenance advantages. And it's just a lot less to type. Let's go back to one of the earlier examples, but write it the modern way: CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; FETCH C; END;  And the generated C code: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 x; cql_int32 y; } p_C_row; CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; p_C_row C = { 0 }; _rc_ = cql_prepare(_db_, &amp;C_stmt, &quot;SELECT 1, 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); return _rc_; }  Let's look at what's different here: struct p_C_row has been created, it contains: _has_row_ for the cursorx and y the data fields_refs_count the number of reference fields in the cursor (0 in this case)_refs_offset the offset of the references fields (they always go at the end)because the references are together a cursor with lots of reference fields can be cleaned up easily in the generated code the variable C refers to the current data that has been fetched convenient for debugging p C in lldb shows you the row references to x and y became C.x and C.yreferences to _C_has_row_ became C._has_row_ That's pretty much it. The beauty of this is that you can't get the declarations of your locals wrong and you don't have to list them all no matter how big the data is. If the data shape changes the cursor change automatically changes to accommodate it. Everything is still statically typed. Now lets look at the loop pattern: CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, 2 AS y; LOOP FETCH C BEGIN CALL printf(&quot;%d, %d\\n&quot;, C.x, C.y); END; END;  Note that the columns of the cursor were defined by the column aliases of the SELECT.  for (;;) { _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (!C._has_row_) break; printf(&quot;%d, %d\\n&quot;, C.x, C.y); }  The loop is basically the same except x and y have been replaced with C.x and C.yand again _C_has_row_ is now C._has_row_. The code generator knows that it should allocate storage for the C cursor if it has the flag SEM_TYPE_HAS_SHAPE_STORAGE on it. The semantic analyzer adds that flag if it ever finds FETCH C with no INTO part. Finally let's look at an example with cleanup required. We'll just change the test case a tiny bit. CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT 1 AS x, &quot;2&quot; AS y; LOOP FETCH C BEGIN CALL printf(&quot;%d, %s\\n&quot;, C.x, C.y); END; END;  The x column is now text. We'll get this code which will be studied below: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_C_row; #define p_C_refs_offset cql_offsetof(p_C_row, x) // count = 1 CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset }; _rc_ = cql_prepare(_db_, &amp;C_stmt, &quot;SELECT '1', 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } for (;;) { _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_STRING, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } if (!C._has_row_) break; cql_alloc_cstr(_cstr_1, C.x); printf(&quot;%s, %d\\n&quot;, _cstr_1, C.y); cql_free_cstr(_cstr_1, C.x); } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); cql_teardown_row(C); return _rc_; }  It's very similar to what we had before, let's quickly review the differences. typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_C_row; #define p_C_refs_offset cql_offsetof(p_C_row, x) // count = 1  x is now cql_string_ref _Nonnull x; rather than cql_int32x has moved to the end (because it's a reference type)the offset of the first ref is computed in a constant Recall the reference types are always at the end and together.  p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset };  p_C_row is now initialized to to ref count 1 and refs offset p_C_refs_offset defined above  cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_STRING, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y);  C.x is now of type string  cql_alloc_cstr(_cstr_1, C.x); printf(&quot;%s, %d\\n&quot;, _cstr_1, C.y); cql_free_cstr(_cstr_1, C.x);  C.x has to be converted to a C style string before it can be used with printf as a %s argument  cql_teardown_row(C);  the cleanup section has to include code to teardown the cursor, this will release all of its reference variables in bulk remember we know the count, and the offset of the first one -- that's all we need to do them all With these primitives we can easily create cursors of any shape and load them up with data. We don't have to redundantly declare locals that match the shape of our select statements which is both error prone and a hassle. All of this is actually very easy for the code-generator. The semantic analysis phase knows if the cursor needs shape storage. And it also recognizes when a variable reference like C.x happens, the variable references are re-written in the AST so that the code-generator doesn't even have to know there was a cursor reference, from its perspective the variable IS C.x (which it sort of is). The code generator does have to create the storage for the cursor but it knows it should do so because the cursor variable is marked with SEM_TYPE_HAS_SHAPE_STORAGE. A cursor without this marking only gets its statement (but not always as we'll see later) and its _cursor_has_row_hidden variable. "},{"title":"Flowing SQLite Statements Between Procedures​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#flowing-sqlite-statements-between-procedures","content":"Earlier we saw that we can get a cursor from a SQLite SELECT statement. The cursor is used to iterate over the sqlite3_stmt * that SQLite provides to us. This process can be done between procedures. Here's a simple example: @ATTRIBUTE(cql:private) CREATE PROC q () BEGIN SELECT &quot;1&quot; AS x, 2 AS y; END;  This is the first example of a procedure that return a result set that we've seen. The wiring for this is very simple. static CQL_WARN_UNUSED cql_code q( sqlite3 *_Nonnull _db_, sqlite3_stmt *_Nullable *_Nonnull _result_stmt) { cql_code _rc_ = SQLITE_OK; *_result_stmt = NULL; _rc_ = cql_prepare(_db_, _result_stmt, &quot;SELECT '1', 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: if (_rc_ == SQLITE_OK &amp;&amp; !*_result_stmt) _rc_ = cql_no_rows_stmt(_db_, _result_stmt); return _rc_; }  First note that there are now two hidden parameters to q: _db_ : the database pointer as usual,_result_stmt : the statement produced by this procedure The rest of the code is just like any other bound SQL statement. Note that if_result_stmt isn't otherwise set by the code it will be initialized to a statement that will return zero rows. All of this is pretty much old news except for the new hidden variable. Note let's look how we might use this. We can write a procedure that calls q, like so: CREATE PROC p () BEGIN DECLARE C CURSOR FOR CALL q(); FETCH C; END;  This generates: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_) { cql_code _rc_ = SQLITE_OK; sqlite3_stmt *C_stmt = NULL; p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset }; _rc_ = q(_db_, &amp;C_stmt); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = sqlite3_step(C_stmt); C._has_row_ = _rc_ == SQLITE_ROW; cql_multifetch(_rc_, C_stmt, 2, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_STRING, &amp;C.x, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.y); if (_rc_ != SQLITE_ROW &amp;&amp; _rc_ != SQLITE_DONE) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: cql_finalize_stmt(&amp;C_stmt); cql_teardown_row(C); return _rc_; }  All of the above is exactly the same as the previous cases where we got data from the database except that instead of using cql_prepare the compiler produced _rc_ = q(_db_, &amp;C_stmt); That function call gives us, of course, a ready-to-use sqlite3_stmt * which we can then step, and use to fetch values. The shape of the cursor C is determined by the result type of procedure q -- hence they always match. If q was in some other module, it could be declared with: DECLARE PROC q () (x TEXT NOT NULL, y INTEGER NOT NULL);  This is a procedure that takes no arguments and returns a result with the indicated shape. CQL can generate this declaration for you if you add --generate_exports to the command line. Note that in this case q was marked with @attribute(cql:private) which caused q to be staticin the output. Hence it can't be called outside this translation unit and --generate_exportswon't provide the declaration. If the private annotation were removed, the full exports for this file would be: DECLARE PROC q () (x TEXT NOT NULL, y INTEGER NOT NULL); DECLARE PROC p () USING TRANSACTION;  And these would allow calling both procedures from elsewhere. Simply #include the exports file. There is a special function in the echoing code that can emit a procedure that was created in the form that is needed to declare it, this is gen_declare_proc_from_create_proc. "},{"title":"Value Cursors​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#value-cursors","content":"Once CQL had the ability to fetch rows into a cursor with no need to declare all the locals it was clear that it could benefit from the ability to save a copy of any given row. That is basic cursor operations seemed like they should be part of the calculus of CQL. Here's a simple sample program that illustrates this. CREATE PROC p () BEGIN DECLARE C CURSOR FOR SELECT &quot;1&quot; AS x, 2 AS y; FETCH C; DECLARE D CURSOR LIKE C; FETCH D from C; END;  We already have a good idea what is going to happen with C in this program. Let's look at the generated code focusing just on the parts that involve D. First there is a row defintion for D. Unsurprisingly it is exactly the samea as the one for C. This must be the case since we specified D CURSOR LIKE C. typedef struct p_D_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_D_row; #define p_D_refs_offset cql_offsetof(p_D_row, x) // count = 1  Then the D cursor variables will be needed: p_D_row D = { ._refs_count_ = 1, ._refs_offset_ = p_D_refs_offset };  The above also implies the cleanup code:  cql_teardown_row(D);  finally, we fetch D from C. That's just some assignments:  D._has_row_ = 1; cql_set_string_ref(&amp;D.x, C.x); D.y = C.y;  Importantly, there is no D_stmt variable. D is not a statement cursor like C, it's a so-called &quot;value&quot; cursor. In that it can only hold values. A value cursor can actually be loaded from anywhere, it just holds data. You don't loop over it (attempts to do so will result in errors). The general syntax for loading such a cursor is something like this:  FETCH D(x, y) FROM VALUES(C.x, C.y);  And indeed the form FETCH D FROM C was rewritten automatically into the general form. The short form is just sugar. Once loaded, D.x and D.y can be used as always. The data type of D is similar to C. The AST would report: {declare_cursor_like_name}: D: select: { x: text notnull, y: integer notnull } variable shape_storage value_cursor  meaning D has the flags SEM_TYPE_STRUCT, SEM_TYPE_VARIABLE, SEM_TYPE_HAS_SHAPE_STORAGE, and SEM_TYPE_VALUE_CURSOR. That last flag indicates that there is no statement for this cursor, it's just values. And all such cursors must haveSEM_TYPE_HAS_SHAPE_STORAGE -- if they had no statement and no storage they would be -- nothing. Value cursors are enormously helpful and there is sugar for loading them from all kinds of sources with a shape. These forms are described more properly in Chapter 5 of the Guide but they all end up going through the general form, making the codegen considerably simpler. There are many examples where the semantic analyzer rewrites a sugar form to a canonical form to keep the codegen from forking into dozens of special cases and most of them have to do with shapes and cursors. "},{"title":"Returning Value Cursors​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#returning-value-cursors","content":"Let's look at an example that is similar to the previous one: @ATTRIBUTE(cql:private) CREATE PROC q () BEGIN DECLARE C CURSOR LIKE SELECT &quot;1&quot; AS x, 2 AS y; FETCH C USING &quot;foo&quot; x, 3 y; OUT C; END; CREATE PROC p () BEGIN DECLARE C CURSOR FETCH FROM CALL q(); -- do something with C END;  Let's discuss some of what is above, first looking at q: DECLARE C CURSOR LIKE SELECT &quot;1&quot; AS x, 2 AS y; : this makes an empty value cursor note the shape is LIKE the indicated SELECT, the SELECT does not actually run FETCH ... USING : this form is sugar, it lets you put the column names x and y adjacent to the values but is otherwise equivalent to the canonical form FETCH C(x, y) FROM VALUES(&quot;foo&quot;, 3); is the canonical formcodegen only ever sees the canonical form OUT C is new, we'll cover this shortly Now let's look at the C for q typedef struct q_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } q_row; #define q_refs_offset cql_offsetof(q_row, x) // count = 1 cql_string_literal(_literal_1_foo_q, &quot;foo&quot;); typedef struct q_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } q_C_row; #define q_C_refs_offset cql_offsetof(q_C_row, x) // count = 1 static void q(q_row *_Nonnull _result_) { memset(_result_, 0, sizeof(*_result_)); q_C_row C = { ._refs_count_ = 1, ._refs_offset_ = q_C_refs_offset }; C._has_row_ = 1; cql_set_string_ref(&amp;C.x, _literal_1_foo_q); C.y = 3; _result_-&gt;_has_row_ = C._has_row_; _result_-&gt;_refs_count_ = 1; _result_-&gt;_refs_offset_ = q_refs_offset; cql_set_string_ref(&amp;_result_-&gt;x, C.x); _result_-&gt;y = C.y; cql_teardown_row(C); }  the _result_ variable is clobbed with zeros, it is assumed to be junk coming in if it had valid data, the caller is expected to use cql_teardown_row to clean it up before calling q_row : this is new, this is the structure type for the result of q it's exactly the same shape as Cit has its own q_refs_offset like other shapes q_C_row : this is the same old same old row structure for cursor Cstatic void q(q_row *_Nonnull _result_) : q now accepts a q_row to fill in! note that q does not have the _db_ parameter, it doesn't use the database!it is entirely possible to fill value cursors from non-database sources, e.g. constants, math, whatever C : the value cursor is declared as usualC.x and C.y are loaded, this resolves the FETCH statementthe _result_ fields are copied from C, this resolves the OUT statementC can be torn downthere is no cleanup label, there are no error cases, nothing can go wrong! The net of all this is that we have loaded a value cursor that was passed in to the procedure via a hidden argument and it has retained references as appropriate. Now let's look at p: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 y; cql_string_ref _Nonnull x; } p_C_row; #define p_C_refs_offset cql_offsetof(p_C_row, x) // count = 1 void p(void) { p_C_row C = { ._refs_count_ = 1, ._refs_offset_ = p_C_refs_offset }; cql_teardown_row(C); q((q_row *)&amp;C); // q_row identical to cursor type // usually you do something with C at this point cql_teardown_row(C); }  p_C_row : the cursor type for C in the procedure p is definedp_C_refs_offset : the refs offset for C in p as usualC = {...} : the usual initialization for a cursor with shape note that C is a value cursor, so it has no C_stmt cql_teardown_row(C) : releases any references in C, there are none this pattern is general purpose, the call to q might be in a loop or somethingin this instance the teardown here is totally redundant, but harmless q((q_row *)&amp;C) : fetch C by calling q p_C_row has been constructed to be exactly the same as q_row so this cast is safethere are no error checks because q can't fail! some code that would use C is absent for this sample, it would go where the comment isthe cleanup label is missing because there are no error cases, emitting the label would just cause warnings such warnings are often escalated to errors in production builds... cql_teardown_row(C) is needed as always, even though there is no cleanup label the teardown is in the cleanup sectionthe teardown was added as usual when C was declared So with just normal value cursor codegen we can pretty easily create a situation where procedures can move structures from one to another. As we saw, the source of value cursors may or may not be the database. Value cursors are frequently invaluable in test code as they can easily hold mock rows based on any kind of computation. "},{"title":"Result Sets​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#result-sets","content":"In addition to returning a single row into a value cursor, or returning a statement to consume with a statement cursor, it's possible to generate a result set. So far the samples have included @attribute(cql:private) to suppress that code. This pattern is intended to let regular C code access the data so private suppresses it. Let's consider a simple example, this example returns only one row but the mechanism works for any number of rows, we're just using this form because it's what we've used so far and its simple. Let's begin: CREATE PROC p () BEGIN SELECT &quot;1&quot; AS x, 2 AS y; END;  The core generated function is this one: CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, sqlite3_stmt *_Nullable *_Nonnull _result_stmt) { cql_code _rc_ = SQLITE_OK; *_result_stmt = NULL; _rc_ = cql_prepare(_db_, _result_stmt, &quot;SELECT '1', 2&quot;); if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; } _rc_ = SQLITE_OK; cql_cleanup: if (_rc_ == SQLITE_OK &amp;&amp; !*_result_stmt) _rc_ = cql_no_rows_stmt(_db_, _result_stmt); return _rc_; }  We've seen this before, it creates the SQLite statement. But that isn't all the code that was generated, let's have a look at what else we got in our outputs: CQL_WARN_UNUSED cql_code p_fetch_results( sqlite3 *_Nonnull _db_, p_result_set_ref _Nullable *_Nonnull result_set) { sqlite3_stmt *stmt = NULL; cql_profile_start(CRC_p, &amp;p_perf_index); cql_code rc = p(_db_, &amp;stmt); cql_fetch_info info = { .rc = rc, .db = _db_, .stmt = stmt, .data_types = p_data_types, .col_offsets = p_col_offsets, .refs_count = 1, .refs_offset = p_refs_offset, .encode_context_index = -1, .rowsize = sizeof(p_row), .crc = CRC_p, .perf_index = &amp;p_perf_index, }; return cql_fetch_all_results(&amp;info, (cql_result_set_ref *)result_set); }  p_fetch_results does two main things: cql_code rc = p(_db_, &amp;stmt) : it calls the underlying function to get the statementcql_fetch_all_results : it calls a standard helper to read all the results from the statement and put them into result_set to do the fetch, it sets up a cql_fetch_info for this result set, this has all the information needed to do the fetchthe intent here is that even a complex fetch with lots of columns can be done economically, andthe code that does the fetching is shared Let's look at the things that are needed to load up that info structure. typedef struct p_row { cql_int32 y; cql_string_ref _Nonnull x; } p_row; uint8_t p_data_types[p_data_types_count] = { CQL_DATA_TYPE_STRING | CQL_DATA_TYPE_NOT_NULL, // x CQL_DATA_TYPE_INT32 | CQL_DATA_TYPE_NOT_NULL, // y }; #define p_refs_offset cql_offsetof(p_row, x) // count = 1 static cql_uint16 p_col_offsets[] = { 2, cql_offsetof(p_row, x), cql_offsetof(p_row, y) };  p_row : the row structure for this result set, same as always, reference types lastp_data_types : an array with the data types encoded as bytesp_refs_offset : the offset of the first reference typep_col_offsets : this is the offset of each column within the row structure these are in column order, not offset order Code generation creates a .c file and a .h file, we haven't talked much about the .hbecause it's mostly prototypes for the functions in the .c file. But in this case we have a few more interesting things. We need just two of them: #define CRC_p -6643602732498616851L #define p_data_types_count 2  Now we're armed to discuss loading the info structure: .rc : the fetcher needs to know if p was successful, it won't read from the statement if it wasn't.db : the database handle, the fetcher needs this to call SQLite APIs.stmt : the statement that came from p that is to be enumerated.data_types : types of the columns, this tells the fetcher what columns to read to the statement in what order.col_offsets : the column offsets, this tells the fetcher were to store the column data within each row.refs_count : the number of references in the row, this is needed to tear down the rows in the result set when it is released.refs_offset : the first reference offset, as usual this tells the fetcher where the references that need to be released are.encode_context_index : it's possible to have sensitive fields encoded, this identifies an optional column that will be combined with the sensitive data . .rowsize : the size of p_row, this is needed to allocate rows in a growable buffer.crc : this is a CRC of the code of p, it's used to uniquely identify p economically, performance logging APIs typically use this CRC in a begin/end logging pair.perf_index : performance data for stored procedures is typically stored in an array of stats, CQL provides storage for the index for each procedure With this data (which is in the end pretty small) the cql_fetch_all_results can do all the things it needs to do: cql_profile_start has already been called, it can call cql_profile_end once the data is fetched cql_profile_start and _end do nothing by default, but those macros can be defined to log performance data however you like it can allocate a bytebuf with cql_bytebuf_open and then grow it with cql_bytebuf_alloc in the end all the rows are in one contiguous block of storage cql_multifetch_meta is used to read each row from the result set, it's similar to cql_multifetch the meta version uses data_types and column_offsets instead of varargs but is otherwise the samethe first member of the col_offsets array is the count of columns With this background, cql_fetch_all_results should be very approachable. There's a good bit of work but it's all very simple. // By the time we get here, a CQL stored proc has completed execution and there is // now a statement (or an error result). This function iterates the rows that // come out of the statement using the fetch info to describe the shape of the // expected results. All of this code is shared so that the cost of any given // stored procedure is minimized. Even the error handling is consolidated. cql_code cql_fetch_all_results( cql_fetch_info *_Nonnull info, cql_result_set_ref _Nullable *_Nonnull result_set) {...}  The core of that function looks like this:  ... cql_bytebuf_open(&amp;b); ... for (;;) { rc = sqlite3_step(stmt); if (rc == SQLITE_DONE) break; if (rc != SQLITE_ROW) goto cql_error; count++; row = cql_bytebuf_alloc(&amp;b, rowsize); memset(row, 0, rowsize); cql_multifetch_meta((char *)row, info); } ... cql_profile_stop(info-&gt;crc, info-&gt;perf_index); ...  cql_bytebuf_open : open the buffer, get ready to start appending rowssqlite3_step : keep reading while we get SQLITE_ROW, stop on SQLITE_DONEcql_bytebuf_alloc : allocate a new row in the buffermemset : zero the rowcql_multifetch_meta : read the data from the the statement into the rowcql_profile_stop : signals that processing is done and profiling can stopif all goes well, SQLITE_OK is returned as usual The remaining logic is largely about checking for errors and tearing down the result set if anything goes wrong. There is not very much to it, and it's worth a read. Now recall that the way cql_fetch_all_results was used, was as follows:  return cql_fetch_all_results(&amp;info, (cql_result_set_ref *)result_set)  And result_set was the out-argument for the the p_fetch_results method. So p_fetch_results is used to get that result set. But what can you do with it? Well, the result set contains copy of all the selected data, ready to use in with a C-friendly API. The interface is in the generated .h file. Let's look at that now, it's the final piece of the puzzle. #ifndef result_set_type_decl_p_result_set #define result_set_type_decl_p_result_set 1 cql_result_set_type_decl(p_result_set, p_result_set_ref); #endif extern cql_string_ref _Nonnull p_get_x(p_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 p_get_y(p_result_set_ref _Nonnull result_set, cql_int32 row); extern cql_int32 p_result_count(p_result_set_ref _Nonnull result_set); extern CQL_WARN_UNUSED cql_code p_fetch_results(sqlite3 *_Nonnull _db_, p_result_set_ref _Nullable *_Nonnull result_set); #define p_row_hash(result_set, row) cql_result_set_get_meta( \\ (cql_result_set_ref)(result_set))-&gt;rowHash((cql_result_set_ref)(result_set), row) #define p_row_equal(rs1, row1, rs2, row2) \\ cql_result_set_get_meta((cql_result_set_ref)(rs1))-&gt;rowsEqual( \\ (cql_result_set_ref)(rs1), \\ row1, \\ (cql_result_set_ref)(rs2), \\  cql_result_set_type_decl : declares p_result_set_ref to avoid being defined more than once, the declaration is protected by #ifndef result_set_type_decl_p_result_set p_get_x, p_get_y : allow access to the named fields of the result set at any given rowp_result_count : provides the count of rows in the result setp_fetch_results : the declaration of the fetcher (previously discussed)p_row_hash : provides a hash of any given row, useful for detecting changes between result setsp_row_equal : tests two rows in two results sets of the same shape for equality The getters are defined very simply: cql_string_ref _Nonnull p_get_x(p_result_set_ref _Nonnull result_set, cql_int32 row) { p_row *data = (p_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data[row].x; } cql_int32 p_get_y(p_result_set_ref _Nonnull result_set, cql_int32 row) { p_row *data = (p_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data[row].y; }  The p_row is exactly the right size, and of course the right shape, the final access looks something like data[row].x. Result Sets from the OUT statement​ Recalling this earlier example: CREATE PROC q () BEGIN DECLARE C CURSOR LIKE SELECT &quot;1&quot; AS x, 2 AS y; FETCH C USING &quot;foo&quot; x, 3 y; OUT C; END;  The original example had @attribute(cql:private) to suppress the result set, but normally a one-row result is is generated from such a method. The C API is almost identical. However, there count is always 0 or 1. The getters do not have the row number: extern cql_string_ref _Nonnull q_get_x(q_result_set_ref _Nonnull result_set); extern cql_int32 q_get_y(q_result_set_ref _Nonnull result_set);  The actual getters are nearly the same as well cql_string_ref _Nonnull q_get_x(q_result_set_ref _Nonnull result_set) { q_row *data = (q_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data-&gt;x; } cql_int32 q_get_y(q_result_set_ref _Nonnull result_set) { q_row *data = (q_row *)cql_result_set_get_data((cql_result_set_ref)result_set); return data-&gt;y; }  Basically data[row].x just became data-&gt;x and the rest is nearly the same. Virtually all the code for this is shared. You can find all this and more in cg_c.c by looking here: // If a stored procedure generates a result set then we need to do some extra work // to create the C friendly rowset creating and accessing helpers. If stored // proc &quot;foo&quot; creates a row set then we need to: // * emit a struct &quot;foo_row&quot; that has the shape of each row // * this isn't used by the client code but we use it in our code-gen // * emit a function &quot;foo_fetch_results&quot; that will call &quot;foo&quot; and read the rows // from the statement created by &quot;foo&quot;. // * this method will construct a result set object via cql_result_create and store the data // * the remaining functions use cql_result_set_get_data and _get_count to get the data back out // * for each named column emit a function &quot;foo_get_[column-name]&quot; which // gets that column out of the rowset for the indicated row number. // * prototypes for the above go into the main output header file static void cg_proc_result_set(ast_node *ast)  There are many variations in that function to handle the cases mentioned so far, but they are substantially similar to each other with a lot of shared code. There is one last variation we should talk about and that is the OUT UNION form. It is the most flexible of them all. Result Sets from the OUT UNION statement​ The OUT statement, allows the programmer to produce a result set that has exactly one row,OUT UNION instead accumulates rows. This is very much like writing your own fetcher procedure with your own logic. The data could come from the database, by, for instance, enumerating a cursor. Or it can come from some computation or a mix of both. Here's a very simple example: CREATE PROC q () BEGIN DECLARE C CURSOR LIKE SELECT 1 AS x; LET i := 0; WHILE i &lt; 5 BEGIN FETCH C(x) FROM VALUES(i); OUT UNION C; SET i := i + 1; END; END;  Let's look at the code for the above, it will be very similar to other examples we've seen so far: typedef struct q_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 x; } q_C_row; void q_fetch_results(q_result_set_ref _Nullable *_Nonnull _result_set_) { cql_bytebuf _rows_; cql_bytebuf_open(&amp;_rows_); *_result_set_ = NULL; q_C_row C = { 0 }; cql_int32 i = 0; cql_profile_start(CRC_q, &amp;q_perf_index); i = 0; for (;;) { if (!(i &lt; 5)) break; C._has_row_ = 1; C.x = i; cql_retain_row(C); if (C._has_row_) cql_bytebuf_append(&amp;_rows_, (const void *)&amp;C, sizeof(C)); i = i + 1; } cql_results_from_data(SQLITE_OK, &amp;_rows_, &amp;q_info, (cql_result_set_ref *)_result_set_); }  q_C_row : the shape of the cursor, as always_rows_ : the bytebuf that will hold our datacql_bytebuf_open(&amp;_rows_); : initializes the buffercql_profile_start(CRC_q, &amp;q_perf_index); : start profiling as beforefor (;;) : the while pattern as beforeC.x = i; : loads the cursorcql_retain_row(C); : retains any references in the cursor (there are none) we're about to copy the cursor into the buffer so all refs need to be +1 cql_bytebuf_append : append the the cursor's bytes into the bufferthe loop does its repetitions until finallycql_results_from_data : used instead of cql_fetch_all_results because all the data is already prepared in this particular example there is nothing to go wrong so it always gets SQLITE_OKin a more complicated example, cql_results_from_data frees any partly created result set in case of errorcql_results_from_data also performs any encoding of sensitive data that might be needed q_info : created as before, but it can be static as it's always the same now Importantly, when using OUT UNION the codegen only produces q_fetch_results, there is no q. If you try to call q from CQL you will instead call q_fetch_results. But since many results as possible, a cursor is needed to make the call. Here's an example, here p calls the q method above: CREATE PROC p (OUT s INTEGER NOT NULL) BEGIN DECLARE C CURSOR FOR CALL q(); LOOP FETCH C BEGIN SET s := s + C.x; END; END;  And the relevant code for this is as follows: typedef struct p_C_row { cql_bool _has_row_; cql_uint16 _refs_count_; cql_uint16 _refs_offset_; cql_int32 x; } p_C_row; CQL_WARN_UNUSED cql_code p(sqlite3 *_Nonnull _db_, cql_int32 *_Nonnull s) { cql_contract_argument_notnull((void *)s, 1); cql_code _rc_ = SQLITE_OK; q_result_set_ref C_result_set_ = NULL; cql_int32 C_row_num_ = 0; cql_int32 C_row_count_ = 0; p_C_row C = { 0 }; *s = 0; // set out arg to non-garbage q_fetch_results(&amp;C_result_set_); C_row_num_ = C_row_count_ = -1; C_row_count_ = cql_result_set_get_count((cql_result_set_ref)C_result_set_); for (;;) { C_row_num_++; C._has_row_ = C_row_num_ &lt; C_row_count_; cql_copyoutrow(NULL, (cql_result_set_ref)C_result_set_, C_row_num_, 1, CQL_DATA_TYPE_NOT_NULL | CQL_DATA_TYPE_INT32, &amp;C.x); if (!C._has_row_) break; *s = (*s) + C.x; } _rc_ = SQLITE_OK; cql_object_release(C_result_set_); return _rc_; }  p_C_row : the cursor row as alwayscql_contract_argument_notnull((void *)s, 1) : verify that the out arg pointer is not nullC_result_set_ : this will hold the result set from q_fetch_resultsC_row_num_ : the current row number being processed in the result setC_row_count_ : the total number of rows in the result setother locals are intialized as usual*s = 0; : set the out arg to non-garbage as usualq_fetch_results : get the result set from q_fetch_results in this case no database access was required so this API can't failC_row_num : set to -1C_row_count : set to the row count cql_copyoutrow : copies one row from the result set into the cursor*s = (*s) + C.x; : computes the sumcql_object_release : the result set is torn downif there are any reference fields in the cursor there would have been a cql_teardown_row(C) In short, this is another form of cursor, it's a value cursor, so it has no statement but it also needs a result set, a count and an index to go with it so that it can enumerate the result set. In the AST it looks like this: {name C}: C: select: { x: integer notnull } variable shape_storage uses_out_union  This implies we have the semantic flags: SEM_TYPE_STRUCT, SEM_TYPE_VARIABLE, SEM_TYPE_HAS_SHAPE_STORAGE, and SEM_TYPE_USES_OUT_UNION. The difference is of course the presence of SEM_TYPE_USES_OUT_UNION. This is the last of the cursor forms and the final complication of cg_proc_result_set. "},{"title":"Recap​","type":1,"pageTitle":"Part 3: C Code Generation","url":"/cql-guide/int03#recap","content":"At present cg_c.c is a little over 7400 lines of code, maybe 1500 of those lines are comments. So cg_c.c is actually quite a bit smaller and simpler than sem.c (roughly 1/3 the size). It is, however, the most complex of the code generators by far. Part 3 of the internals guide has come out a lot larger than Part 2 but that's mainly because there are a few more cases worth discussing in detail and the code examples of Part 3 are bigger than the AST examples of Part 2. Topics covered included: compiling expressions into C, including nullable typestechniques used to generate control flowcreation of result sets, including: various helpers to do the reading economicallythe use of cql_bytebuf to manage the memory create the text for SQLite statements and binding variables to that texterror management, and how it relates to TRY and CATCH blocksuse of cleanup sections to ensure that references and SQLite statement lifetime is always correctthe contents of the .c and .h files and the key sections in themthe use of charbuf to create and assemble fragments As with the other parts, this is not a complete discussion of the code but a useful survey that should give readers enough context to understand cg_c.c and the runtime helpers in cqlrt.cand cqlrt_common.c. Good luck in your personal exploration! "},{"title":"Part 4: Testing","type":0,"sectionRef":"#","url":"/cql-guide/int04","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#preface","content":"Part 4 continues with a discussion of the essentials testing frameworks for the CQL compiler. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how testing happens in general -- the core strategies and implementation choices -- so that when reading the tests you will have an idea how it all hangs together. To accomplish this, various key tools will be explained in detail as well as selected examples of their use. "},{"title":"Testing​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#testing","content":"There are several types of tests in the system, all of which are launched by the test.shscript which builds the compiler and does a full test pass, there are well over 3000 tests as of this writing. Broadly these are in these few categories: parse tests : these are in test.sql the test script verifies that the compiler can parse this file with no errorsthe parse pass echoes what it read in normalized form, this is compared against a reference copy and any differences are notedeach difference can be accepted or rejected; rejecting a difference stops the scriptverification here is very light and in fact much of parsing is actually tested in the next pass semantic tests : these are in sem_test.sql the file has no parse errors but it has MANY semantic errors, nearly every such error in factsemantic analysis is run with the --test flag which produces AST fragments and echoed CQLthe test file includes patterns which either must appear, or must not appear, in the output to pass the testthe AST includes full type information, so virtually anything about the semantic results can be, and is, verifiedmany tests are designed to exercise the parser as well, ensuring that the correct AST was built and then analyzed e.g. operator precedence can be verified herethe AST echoing logic can also be verified here, e.g. placement of parenthesis in the echoed output any semantic rewrites can be verified here because the rewritten form is emitted in the test output, not the original inputall other operations that happen during the semantic pass (e.g. constant evaluation) are also tested herethe full semantic output is also normalized (e.g. removing line numbers) and is compared against a reference copy, any differences are notedeach difference can be accepted or rejected; rejecting a difference stops the scriptthere are additional files to test different modes like &quot;previous schema&quot; validation (q.v.) as well as dev mode and the schema migrator, the files in this family are: sem_test.sql, sem_test_dev.sql, sem_test_migrate.sql, sem_test_prev.sql code gen tests : the basic test in this family is cg_test.sql which has the C codegen tests these test files do pattern matching just like the semantic case except the codegen output is checked rather than the ASTthe test output is normalized and checked against a reference, just like the semantic teststhere is generally no need to check for errors in test output because all errors are detected during semantic analysisthere are MANY tests in this family, at least one for each of the various generators: cg_test.sql, cg_test_assembly_query.sql, cg_test_base_fragment.sql, cg_test_c_type_getters.sql, cg_test_extension_fragment.sql, cg_test_generate_copy.sql, cg_test_generated_from.sql, cg_test_json_schema.sql, cg_test_no_result_set.sql, cg_test_out_object.sql, cg_test_out_union.sql, cg_test_prev_invalid.sql, cg_test_query_plan.sql, cg_test_schema_upgrade.sql, cg_test_single_proc_not_nullable.sql, cg_test_single_proc_nullable.sql, cg_test_suppressed.sql, cg_test_test_helpers.sql, cg_test_with_object.sql, run tests : the main run test creatively named run_test.sql this test code is compiled and excutedthe test contains expectations like any other unit testit has CQL parts and C parts, the C parts test the C API to the procedures, plus do initial setupthese test include uses of all CQL features and all of the CQL runtime featuresthe schema upgrader tests are arguably &quot;run tests&quot; as well in that they run the code but they have a much different verification strategy unit test : the compiler supports the --run_unit_tests flag this causes the compile to self-test certain of its helper functions that are otherwise difficult to testmostly this is buffers that need to be growable to but in practice only grow with huge input filesother exotic cases that would be hard to reliability hit in some other fashion are covered by this code Test coverage is maintained at 100% line coverage (sometimes there are a few hours when it drops to 99.9% or something like that but this never lasts). Branch coverage is not especially targetted but is nonethless quite high. To see the true branch coverage you have to build the compiler with the asserts (Contract and Invariant) off. Last time it was measured, it was well over 80%. To start the tests you should run test.sh, this launches common/test_common.sh to do the work. This structure allows anyone to make their own harness that launches the common test passes and adds their own extra tests, or passes in additional flags. test.sh itself uses make to build the compiler. As mentioned above, test.sh normally allows the user to accept or reject differences in output, but this is automatically disabled in non-terminal environments, and manually disabled if the script is run with --non_interactive. ok.sh can be run to copy all of the outputs from the most recent test run over the previous references. To get the coverage report, use cov.sh which in turn launches test.sh with suitable flags and then assembles the coverage report using gcovr. "},{"title":"Parse Tests​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#parse-tests","content":"Looking at test/test_common.sh we find the source for the most basic test. This is entirely unremarkable stuff. basic_test() { echo '--------------------------------- STAGE 2 -- BASIC PARSING TEST' echo running &quot;${TEST_DIR}/test.sql&quot; if ! ${CQL} --dev --in &quot;${TEST_DIR}/test.sql&quot; &gt;&quot;${OUT_DIR}/test.out&quot; then echo basic parsing test failed failed fi echo &quot; computing diffs (empty if none)&quot; on_diff_exit test.out }  it's &quot;STAGE 2&quot; because &quot;STAGE 1&quot; was the buildall it tries to do is run the compiler over test/test.sqlif there are errors the test failsif there are any differences between test.out and test.out.ref the test fails That's it. "},{"title":"Sematic Tests​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#sematic-tests","content":"The semantic tests are not much different but this is where the pattern matching comes in. First let's look at the shell script: semantic_test() { echo '--------------------------------- STAGE 4 -- SEMANTIC ANALYSIS TEST' echo running semantic analysis test if ! sem_check --sem --ast --dev --in &quot;${TEST_DIR}/sem_test.sql&quot; &gt;&quot;${OUT_DIR}/sem_test.out&quot; 2&gt;&quot;${OUT_DIR}/sem_test.err&quot; then echo &quot;CQL semantic analysis returned unexpected error code&quot; cat &quot;${OUT_DIR}/sem_test.err&quot; failed fi echo validating output trees if ! &quot;${OUT_DIR}/cql-verify&quot; &quot;${TEST_DIR}/sem_test.sql&quot; &quot;${OUT_DIR}/sem_test.out&quot; then echo failed verification failed fi echo running dev semantic analysis test ... same thing again for sem_test_dev.sql echo &quot; computing diffs (empty if none)&quot; on_diff_exit sem_test.out on_diff_exit sem_test.err ... same thing again for sem_test_dev.out and .err }  There are basically 3 steps: run the compiler over test/sem_test.sql fail if this generates no errors (yes you read that right, see below) do the pattern matching on the output using cql-verify to ensure the patterns match (discussed below) fail if the output is not consistent with the patterns compare the reference output for the AST and the errors fail if there are any differences In the first step the compiler MUST produce an error code, let's look at sem_check to see why: sem_check() { ${CQL} &quot;$@&quot; if [ &quot;$?&quot; -ne &quot;1&quot; ] then echo 'All semantic analysis checks have errors in the test' echo 'the normal return code is &quot;1&quot; -- any other return code is bad news' echo 'A return code of zero indicates we reported success in the face of errors' echo 'A return code other than 1 indicates an unexpected fatal error of some type' return 1 fi }  In short sem_test.sql is FULL of semantic errors, that's part of the test. If the compiler reports success something is seriously wrong. In the next phase we're going to do some pattern matching, let's look at a couple of examples to illustrate how this works. The program cql-verify actually does all this matching and that program is itself written in (mostly) CQL which is cute. It can be found in the tester directory. Here's a very simple example: -- TEST: we'll be using printf in lots of places in the tests as an external proc -- + {declare_proc_no_check_stmt}: ok -- - Error DECLARE PROCEDURE printf NO CHECK;  The code under test is of course DECLARE PROCEDURE printf NO CHECK. The patterns happen immediately before this code. Let's look at each line: -- TEST: etc. : this is just a comment, it means nothing and serves no purpose other than documentation-- + {declare_proc_no_check_stmt}: ok : the comment stats with &quot; + &quot;, this is a trigger the test output from the statement under test must include indicated textthis happens to be the text for the AST of declare_proc_no_check_stmt after semantic successthere is no type info hence the ok designation (recall SEM_TYPE_OK) -- Error : the comment starts with &quot; - &quot;, this is a trigger the test output from the statement under test must NOT include indicated textin this case that means no reported erros Easy enough. Now does this happen? The test output includes: text like &quot;The statement ending at line XXXX&quot; where XXXX is appropriate line numberan echo of the statement that was analyzed (after any rewrites)the AST of that statement including semantic type info that was computed Using the value of XXXX the tester searches the test file in this case sem_test.sql, it extracts the test patterns that happen AFTER the previous XXXX value for the previous statement and up to the indicated line number. This is The Price Is Right algorithm where you read up to the designated lines without going over. Each pattern is matched, or not matched, using the SQL LIKE or NOT LIKE operator. In case of errors the tester writes out the actual output and the expected patterns having all this information handy. The line numbers are all changed to literally &quot;XXXX&quot; after this pass so that the difference in later passes is not a cascade of of trivial line number changes in otherwise identical output. Let's look at another example: -- TEST: create a table using type discrimation: kinds -- + {create_table_stmt}: with_kind: { id: integer&lt;some_key&gt;, cost: real&lt;dollars&gt;, value: real&lt;dollars&gt; } -- + {col_def}: id: integer&lt;some_key&gt; -- + {col_def}: cost: real&lt;dollars&gt; -- + {col_def}: value: real&lt;dollars&gt; -- - Error create table with_kind( id integer&lt;some_key&gt;, cost real&lt;dollars&gt;, value real&lt;dollars&gt; );  This reads pretty easily now: {create_table_stmt} : the struct type of the table must be an exact match for what is expected{col_def} : there are 3 different {col_def} nodes, one for each column- Error : there are no reported errors So there are no errors reported nor are there any in the AST. At least the part of the AST that was checked. The AST actually had other stuff too but it's normal to just test the &quot;essential&quot; stuff. There are many tests that try many variations and we don't want to check every fact in every case of every test. If you want to see the whole AST output for this, it's easy enough. It's sitting in sem_test.out.ref The statement ending at line XXXX CREATE TABLE with_kind( id INTEGER&lt;some_key&gt;, cost REAL&lt;dollars&gt;, value REAL&lt;dollars&gt; ); {create_table_stmt}: with_kind: { id: integer&lt;some_key&gt;, cost: real&lt;dollars&gt;, value: real&lt;dollars&gt; } | {create_table_name_flags} | | {table_flags_attrs} | | | {int 0} | | {name with_kind} | {col_key_list} | {col_def}: id: integer&lt;some_key&gt; | | {col_def_type_attrs}: ok | | {col_def_name_type} | | {name id} | | {type_int}: integer&lt;some_key&gt; | | {name some_key} | {col_key_list} | {col_def}: cost: real&lt;dollars&gt; | | {col_def_type_attrs}: ok | | {col_def_name_type} | | {name cost} | | {type_real}: real&lt;dollars&gt; | | {name dollars} | {col_key_list} | {col_def}: value: real&lt;dollars&gt; | {col_def_type_attrs}: ok | {col_def_name_type} | {name value} | {type_real}: real&lt;dollars&gt; | {name dollars}  As you can see there was potentially a lot more than could have been verified but those view key lines were selected because their correctness really implies the rest. In fact just the {create_table_stmt} line really was enough to know that everthing was fine. Let's look at one more example, this time on that is checking for errors. Many tests check for errors because correctly reporting errors is the primary job of sem.c. It's fair to say that there are more tests for error cases than there are for correct cases because there are a lot more ways to write code incorrectly than correctly. Here's the test: -- TEST: join with bogus ON expression type -- + Error % expected numeric expression 'ON' -- +1 Error -- + {select_stmt}: err -- + {on}: err select * from foo inner join bar as T2 on 'v' where 'w' having 'x' limit 'y';  + Error % expected numeric expression 'ON' : there must be a reported Error message with the indicated error text+1 Error : this indicates that there must be exactly 1 match for the pattern &quot;Error&quot; (i.e. exactly one error) note that there are several problems with the test statement but error processing is supposed to stop after the first -- + {on}: err : verifies that the ON clause was marked as being in error-- + {select_stmt}: err : verifies that the error correctly propogated up to the top level statement Note that the patterns can be in any order and every pattern is matched against the whole input so for instance: -- + {on}: err -- + {on}: err  The above does not imply that there are two such {on} nodes. The second line will match the same text as the first. To to enforce that there were exactly two matches you use: -- +2 {on}: err  There is no syntax for &quot;at least two matches&quot; though one could easily be added. So far it hasn't been especially necessary. As we'll see this simple pattern is used in many other tests. All that is required for it work is output with lines of the form &quot;The statement ending at line XXXX&quot; The sem_test_dev.sql test file is a set of tests that are run with the --dev flag passed to CQL. This is the mode where certain statements that are prohibited in production code are verified. This file is very small indeed and the exact prohibitions are left as an exercise to the reader. "},{"title":"Code Generation Tests​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#code-generation-tests","content":"The test logic for the &quot;codegen&quot; family of tests (cg_test*.sql) is virtually identical to the semantic test family. The same testing utililty is used, and it works the same way, looking for the same marker. The only difference in this stage is that the test output is generated code, not an AST. The codegen tests are a great way to lock down important code fragments in the output. Note that the codegen tests do not actually execute any generated code. That's the next category. Here's an sample test: -- TEST: unused temp in unary not emitted -- - cql_int32 _tmp_int_0 = 0; -- - cql_int32 _tmp_int_1 = 0; -- + o = i.value; -- + o = - 1; create proc unused_temp(i integer, out o integer not null) begin set o := coalesce(i, -1); end;  This test is verifying one of the optimizations that we talked about inPart 3. In many cases temporary variables for results (such as function calls) can be elided. - cql_int32 _tmp_int_0 = 0; : verifies that this temporary is NOT created- cql_int32 _tmp_int_1 = 0; : likewise+ o = i.value; : the first alternative in coalesce directly assigns to o+ o = - 1; : as does the second It might be helpful to look at the full output, which as always is in a .ref file. In this case cg_test.c.ref. Here is the full output with the line number normalized: // The statement ending at line XXXX /* CREATE PROC unused_temp (i INTEGER, OUT o INTEGER NOT NULL) BEGIN SET o := coalesce(i, -1); END; */ #define _PROC_ &quot;unused_temp&quot; // export: DECLARE PROC unused_temp (i INTEGER, OUT o INTEGER NOT NULL); void unused_temp(cql_nullable_int32 i, cql_int32 *_Nonnull o) { cql_contract_argument_notnull((void *)o, 2); *o = 0; // set out arg to non-garbage do { if (!i.is_null) { *o = i.value; break; } *o = - 1; } while (0); } #undef _PROC_  As we can see, the test has picked out the bits that it wanted to verify. The coalescefunction is verified elsewhere -- in this test we're making sure that this pattern doesn't cause extra temporaries. Let's take a quick look at the part of test_common.sh that runs this: code_gen_c_test() { echo '--------------------------------- STAGE 5 -- C CODE GEN TEST' echo running codegen test if ! ${CQL} --test --cg &quot;${OUT_DIR}/cg_test_c.h&quot; &quot;${OUT_DIR}/cg_test_c.c&quot; \\ &quot;${OUT_DIR}/cg_test_exports.out&quot; --in &quot;${TEST_DIR}/cg_test.sql&quot; \\ --global_proc cql_startup --generate_exports 2&gt;&quot;${OUT_DIR}/cg_test_c.err&quot; then echo &quot;ERROR:&quot; cat &quot;${OUT_DIR}/cg_test_c.err&quot; failed fi echo validating codegen if ! &quot;${OUT_DIR}/cql-verify&quot; &quot;${TEST_DIR}/cg_test.sql&quot; &quot;${OUT_DIR}/cg_test_c.c&quot; then echo &quot;ERROR: failed verification&quot; failed fi echo testing for successful compilation of generated C rm -f out/cg_test_c.o if ! do_make out/cg_test_c.o then echo &quot;ERROR: failed to compile the C code from the code gen test&quot; failed fi ... echo &quot; computing diffs (empty if none)&quot; on_diff_exit cg_test_c.c on_diff_exit cg_test_c.h ... other tests }  Briefly reviewing this, we see the following important steps: {CQL} --test --cg etc. : run the compiler on the test input the test fails if there are any errors cql-verify : performs the pattern matching the output has the same statement markers as in the semantic case do_make : use make to build the generated code ensuring it compiles cleanly if the C compiler returns any failure, the test fails on_diff_exit : compares the test output to the reference output any difference fails the test This is all remarkably similar to the semantic tests. All the code generators are tested in the same way. "},{"title":"Run Tests​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#run-tests","content":"The last category of tests actually does execution. The main &quot;run test&quot; happens at &quot;stage 13&quot;, because there are many codegen tests for the various output formats and these all pass before before we try to execute anything. This is not so bad because the tests are quite quick with a full test pass taking less than 90s on my laptop. run_test() { echo '--------------------------------- STAGE 13 -- RUN CODE TEST' echo running codegen test with execution if ! cc -E -x c -w &quot;${TEST_DIR}/run_test.sql&quot; \\ &gt;&quot;${OUT_DIR}/run_test_cpp.out&quot; then echo preprocessing failed. failed elif ! ${CQL} --nolines \\ --cg &quot;${OUT_DIR}/run_test.h&quot; &quot;${OUT_DIR}/run_test.c&quot; \\ --in &quot;${OUT_DIR}/run_test_cpp.out&quot; \\ --global_proc cql_startup --rt c then echo codegen failed. failed elif ! (echo &quot; compiling code&quot;; do_make run_test ) then echo build failed failed elif ! (echo &quot; executing tests&quot;; &quot;./${OUT_DIR}/a.out&quot;) then echo tests failed failed fi ...  The main structure is mostly what one would expect: cc -E -x c : this is used to pre-process the run test file so that we can use C pre-processor features to define tests there are quite a few helpful macros as we'll seeif pre-processing fails, the test fails {CQL} --nolines --cg ... : this is used to create the .h and .c file for the compiland --nolines is used to suppress the # directives that would associate the generated code with the .sql filecompilation failures cause the test to fail do_make : as before this causes make to build the compiland (run_test) this build target includes the necessary bootstrap code to open a database and start the testsany failures cause the test to fail a.out : the tests execute the tests return a failure status code if anything goes wrongany failure causes the test to fail The test file run_test.sql includes test macros from cqltest.h -- all of these are very simple. The main ones are BEGIN_SUITE, END_SUITE, BEGIN_TEST and END_TEST for structure; and EXPECT to verify a boolean expression. Here's a simple test case with several expectations: BEGIN_TEST(arithmetic) EXPECT_SQL_TOO((1 + 2) * 3 == 9); EXPECT_SQL_TOO(1 + 2 * 3 == 7); EXPECT_SQL_TOO(6 / 3 == 2); EXPECT_SQL_TOO(7 - 5 == 2); EXPECT_SQL_TOO(6 % 5 == 1); EXPECT_SQL_TOO(5 / 2.5 == 2); EXPECT_SQL_TOO(-(1+3) == -4); EXPECT_SQL_TOO(-1+3 == 2); EXPECT_SQL_TOO(1+-3 == -2); EXPECT_SQL_TOO(longs.neg == -1); EXPECT_SQL_TOO(-longs.neg == 1); EXPECT_SQL_TOO(- -longs.neg == -1); END_TEST(arithmetic)  We should also reveal EXPECT_SQL_TOO, discussed below: -- use this for both normal eval and SQLite eval #define EXPECT_SQL_TOO(x) EXPECT(x); EXPECT((select x))  Now back to the test: EXPECT(x) : verifies that x is true (i.e. a non-zero numeric) not used directly in this example EXPECT_SQL_TOO : as the definition shows, x must be true (as above)(select x) must also be true, i.e. when SQLite is asked to evaluate the expression the result is also a &quot;pass&quot; this is used to verify consistency of order of operations and other evaluations that must be the same in both formsnote that when (select ...) is used, CQL plays no part in evaluating the expression, the text of the expression goes to SQLite and any variables are bound as described in Part 3. The run test exercises many features, but the testing strategy is always the same: exercise some code patternuse EXPECT to validate the results are correctthe expressions in the EXPECT are usually crafted carefully to show that a certain mistake is not being made e.g. expressions where the result would be different if there are bugs in order of operationse.g. expressions that would crash with divide by zero if code that isn't supposed to run actually ran "},{"title":"Schema Upgrade Testing​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#schema-upgrade-testing","content":"The schema upgrade tester is quite a bit different than the others and relies heavily on execution of the upgraders. Before we get into that there is a preliminary topic: &quot;Previous Schema&quot; Validation​ In order to ensure that it is possible to create an upgrader, CQL provides features to validate the current schema against the previous schema ensuring that nothing has been done that would make an upgrader impossible. This is more fully discussed inChapter 11 of the Guide. &quot;Previous Schema&quot; validation is a form of semantic check and so its testing happens as described above. Importantly, as with the other back-end passes the schema upgrader does not have to concern itself with error cases as they are already ruled out. The upgrader itself will be the subject of Part 5. Packing List​ The test assets for upgrade tests are found in the upgrade directory and consist of SchemaPersistentV0.sql : baseline version of the test schemaSchemaPersistentV1.sql : v1 of the test schemaSchemaPersistentV2.sql : v2 of the test schemaSchemaPersistentV3.sql : v3 of the test schemadowngrade_test.c : a test that simulates attemping to go backwards in schema versionsupgrade_test.c : the C harness that launches the upgraders and fires the testsupgrade_test.sh : the shell script that makes all this happenupgrade_validate.sql : some simple code that sanity checks the recorded schema version against tables in it used to ensure that the schema we are on is the schema we think we are on, not to validate all facets of italso renders the contents of sqlite_master in a canonical form We haven't yet discussed the internals of schema upgrade, so for purposes of this part we're only going to discuss how the testing proceeds. The upgrade will be considered &quot;magic&quot; for now. In addition to these assets, we also have reference files: upgrade_schema_v0.out.ref : expected content of v0upgrade_schema_v1.out.ref : expected content of v1upgrade_schema_v2.out.ref : expected content of v2upgrade_schema_v3.out.ref : expected content of v3 upgrade_validate.sql​ This file has a single procedure validate_transition which does the two jobs: emits the canonicalized version of sqlite_master to the output this is needed because sqlite_master text can vary between Sqlite versions checks for basic things that should be present in a given version The output of the validator looks like this: reference results for version 0 ----- g1 ----- type: table tbl_name: g1 CREATE TABLE g1( id INTEGER PRIMARY KEY, name TEXT) ----- sqlite_autoindex_test_cql_schema_facets_1 ----- type: index tbl_name: test_cql_schema_facets ----- test_cql_schema_facets ----- type: table tbl_name: test_cql_schema_facets CREATE TABLE test_cql_schema_facets( facet TEXT NOT NULL PRIMARY KEY, version LONG_INT NOT NULL)  The formatting rules are very simple and so the output is pretty readable. The verifications are very simple. First this happens: let version := cast(test_cql_get_facet_version(&quot;cql_schema_version&quot;) as integer);  The printing happens, then this simple validation:  let recreate_sql := ( select sql from sqlite_master where name = 'test_this_table_will_become_create' if nothing null); ... switch version when 0 then if recreate_sql is null or recreate_sql not like '%xyzzy INTEGER%' then call printf(&quot;ERROR! test_this_table_will_become_create should have a column named xyzzy in v%d\\n&quot;, version); throw; end if; ... else call printf(&quot;ERROR! expected schema version v%d\\n&quot;, version); throw; end;  In short, the version number must be one of the valid versions and each version is expecting that particular table to be in some condition it can recognize. The real validation is done by noting any changes in the reference output plus a series of invariants. Prosecution of the Upgrade Test​  Launch  We kick things off as follows: test.sh calls upgrade/upgrade_test.sh this test doesn't usually run standalone (but it can)  Build Stage  This creates the various binaries we will need: upgrade_validate.sql is compiled down to C this code works for all schema versions, it's generic SchemaPersistentV[0-3].sql are compiled into C (this takes two steps) first, the CQL upgrader is generated from the schemasecond, the CQL upgrader is compiled to C make is used to lower all of the C into executables upgrade[0-3] plus downgrade_test the shared validation code is linked into all 4 upgradersdowngrade_test.c is linked with the code for upgrade1  Basic Upgrades  Here we test going from scratch to each of the 4 target versions: upgrade[0-3] are each run in turn with no initial database i.e. their target database is deleted before each run the validation output is compared against the reference output any differences fail the test  Previous Schema Validation  This sanity checks that the chain of schema we have built should work when upgrading from one version to the next: try each schema with this predecessor: SchemaPersistentV1.sql with SchemaPersistentV0.sql as the previousSchemaPersistentV2.sql with SchemaPersistentV1.sql as the previousSchemaPersistentV3.sql with SchemaPersistentV2.sql as the previous if any of these produce errors something is structurally wrong with the test or else previous schema validation is broken  Two-Step Upgrades  Now we verify that we can go from any version to any other version with a stop in between to persist. An example should make this clearer: We start from scratch and go to v2 this should produce the v2 reference schema output as before We run the v4 upgrader on this v2 schema this should produce the v4 reference schema output as beforei.e. if we go from nothing to v2 to v4 we get the same as if we just go to v4 directly There are quite a few combinations like this, the test output lists them all: Upgrade from nothing to v0, then to v0 -- must match direct update to v0 Upgrade from nothing to v0, then to v1 -- must match direct update to v1 Upgrade from nothing to v1, then to v1 -- must match direct update to v1 Upgrade from nothing to v0, then to v2 -- must match direct update to v2 Upgrade from nothing to v1, then to v2 -- must match direct update to v2 Upgrade from nothing to v2, then to v2 -- must match direct update to v2 Upgrade from nothing to v0, then to v3 -- must match direct update to v3 Upgrade from nothing to v1, then to v3 -- must match direct update to v3 Upgrade from nothing to v2, then to v3 -- must match direct update to v3 Upgrade from nothing to v3, then to v3 -- must match direct update to v3  Note that one of the combinations tested is starting on Vn and &quot;upgrading&quot; from there to Vn. This should do nothing.  Testing downgrade  Here we make sure that any attempt to &quot;go backwards&quot; results in an error. the v3 schema created by the previous test is used as input to the downgrade testthe downgrade test was linked with the v2 upgraderwhen executed the v2 upgrader should report the error this test's verifier checks for a correct error report the test test fails if the error is no correctly reported The combination of testing reference outputs plus testing these many invariants at various stages results in a powerful integration test. The actual schema for the varios versions includes all the supported transitions such as creating and deleting tables and columns, and recreating views, indicies, and triggers. All of the possible transitions are more fully discussed inChapter 10 of the Guide which pairs nicely with the previous schema validions discussed inChapter 11. "},{"title":"Testing the #line directives produced by CQL​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#testing-the-line-directives-produced-by-cql","content":"[An additional section should be added for the code that verifies the source line number mappings even though this is a pretty exotic case.] "},{"title":"Summary​","type":1,"pageTitle":"Part 4: Testing","url":"/cql-guide/int04#summary","content":"While there are a few more isolated verifications that happen in test.sh and of course there is the plumbing necessary to let cov.sh use the test script to create coverage reports, the above forms make up the vast majority of the test patterns. Generally, the test files are designed to hold as many tests as can reasonably fit with the gating factor being cases where different flags are necessary. There are two different stages were many different tiny input files are used to create trivial failures like missing command line arguments and such. But those cases are all just looking for simple error text and a failure code, so they should be self-evident. With so many options, many such baby tests are needed. "},{"title":"Part 5: CQL Runtime","type":0,"sectionRef":"#","url":"/cql-guide/int05","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#preface","content":"Part 5 continues with a discussion of the essentials of the CQL Runtime. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how the runtime works in general -- the core strategies and implementation choices -- so that when reading the source you will have an idea how it all hangs together. To accomplish this, we'll illustrate the key pieces that can be customized and we'll discuss some interesting cases. "},{"title":"CQL Runtime​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#cql-runtime","content":"The parts of the runtime that you can change are in cqlrt.h, that file invariably ends by includingcqlrt_common.h which are the runtime parts that you shouldn't change. Of course this is open source so you can change anything, but the common things usually don't need to change -- cqlrt.h should provide you with everything you need to target new environments. The compiler itself can be customized see rt.c to emit different strings to work with your runtime. This is pretty easy to do without creating a merge hell for yourself. Meta Platforms, for instance, has its own CQL runtime customized for use on phones that is not open source (and really I don't think anyone would want it anyway). But the point is that you can make your own. In fact I know of two just within Meta Platforms. We'll go over cqlrt.h bit by bit. Keeping in mind it might change but this is essentially what's going on. And the essentials don't change very often. "},{"title":"Standard headers​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#standard-headers","content":"The rest of the system will use these, cqlrt.h is responsible for bringing in what you need later, or what cqlrt_common.h needs on your system. #pragma once #include &lt;assert.h&gt; #include &lt;stddef.h&gt; #include &lt;stdint.h&gt; #include &lt;math.h&gt; #include &lt;sqlite3.h&gt; #ifndef __clang__ #ifndef _Nonnull /* Hide Clang-only nullability specifiers if not Clang */ #define _Nonnull #define _Nullable #endif #endif  "},{"title":"Contract and Error Macros​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#contract-and-error-macros","content":"CQL has a few different macros it uses for errors. contract, invariant, and tripwireusually all map to assert. Note that tripwire doesn't have to be fatal, it can log in production and continue. This is a &quot;softer&quot; assertion. Something that you're trying out that you'd like to be a contract but maybe there are lingering cases that have to be fixed first. #define cql_contract assert #define cql_invariant assert #define cql_tripwire assert #define cql_log_database_error(...) #define cql_error_trace()  "},{"title":"The Value Types​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#the-value-types","content":"You can define these types to be whatever is appropriate on your system. Usually the mapping is pretty obvious. // value types typedef unsigned char cql_bool; #define cql_true (cql_bool)1 #define cql_false (cql_bool)0 typedef unsigned long cql_hash_code; typedef int32_t cql_int32; typedef uint32_t cql_uint32; typedef uint16_t cql_uint16; typedef sqlite3_int64 cql_int64; typedef double cql_double; typedef int cql_code;  "},{"title":"The Reference Types​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#the-reference-types","content":"The default runtime first defines 4 types of reference objects. These are the only reference types that CQL creates itself. In fact CQL doesn't actually create CQL_C_TYPE_OBJECT but the tests do. CQL never creates raw object things, only external functions can do that. // metatypes for the straight C implementation #define CQL_C_TYPE_STRING 0 #define CQL_C_TYPE_BLOB 1 #define CQL_C_TYPE_RESULTS 2 #define CQL_C_TYPE_BOXED_STMT 3 #define CQL_C_TYPE_OBJECT 4  All the reference types are reference counted. So they need a simple shape that allows them to know their own type and have a count. They also have a finalize method to clean up their memory when the count goes to zero. You get to define cql_type_ref to be whatever you want. // base ref counting struct typedef struct cql_type *cql_type_ref; typedef struct cql_type { int type; int ref_count; void (*_Nullable finalize)(cql_type_ref _Nonnull ref); } cql_type;  Whatever you do with the types you'll need to define a retain and release method that uses them as the signature. Normal references should have a generic value comparison and a hash. void cql_retain(cql_type_ref _Nullable ref); void cql_release(cql_type_ref _Nullable ref); cql_hash_code cql_ref_hash(cql_type_ref _Nonnull typeref); cql_bool cql_ref_equal(cql_type_ref _Nullable typeref1, cql_type_ref _Nullable typeref2);  Now each of the various kinds of reference types needs an object which probably includes the base type above. It doesn't have to. You can arrange for some other universal way to do these. On iOS these can be easily mapped to CF types. The retain and release macros should all map to the same thing. The compiler emits different variations for readability only. It doesn't really work if they don't have common retain/release semantics. // builtin object typedef struct cql_object *cql_object_ref; typedef struct cql_object { cql_type base; const void *_Nonnull ptr; } cql_object; #define cql_object_retain(object) cql_retain((cql_type_ref)object); #define cql_object_release(object) cql_release((cql_type_ref)object);  Boxed statement gets its own implementation, same as object. // builtin statement box typedef struct cql_boxed_stmt *cql_boxed_stmt_ref; typedef struct cql_boxed_stmt { cql_type base; sqlite3_stmt *_Nullable stmt; } cql_boxed_stmt;  Same for blob, and blob has a couple of additional helper macros that are used to get information. Blobs also have hash and equality functions. // builtin blob typedef struct cql_blob *cql_blob_ref; typedef struct cql_blob { cql_type base; const void *_Nonnull ptr; cql_uint32 size; } cql_blob; #define cql_blob_retain(object) cql_retain((cql_type_ref)object); #define cql_blob_release(object) cql_release((cql_type_ref)object); cql_blob_ref _Nonnull cql_blob_ref_new(const void *_Nonnull data, cql_uint32 size); #define cql_get_blob_bytes(data) (data-&gt;ptr) #define cql_get_blob_size(data) (data-&gt;size) cql_hash_code cql_blob_hash(cql_blob_ref _Nullable str); cql_bool cql_blob_equal(cql_blob_ref _Nullable blob1, cql_blob_ref _Nullable blob2);  Strings are the same as the others but they have many more functions associated with them. // builtin string typedef struct cql_string *cql_string_ref; typedef struct cql_string { cql_type base; const char *_Nullable ptr; } cql_string; cql_string_ref _Nonnull cql_string_ref_new(const char *_Nonnull cstr); #define cql_string_retain(string) cql_retain((cql_type_ref)string); #define cql_string_release(string) cql_release((cql_type_ref)string);  The compiler uses this macro to create a named string literal. You decide how those will be implemented right here. #define cql_string_literal(name, text) \\ cql_string name##_ = { \\ .base = { \\ .type = CQL_C_TYPE_STRING, \\ .ref_count = 1, \\ .finalize = NULL, \\ }, \\ .ptr = text, \\ }; \\ cql_string_ref name = &amp;name##_  Strings get assorted comparison and hashing functions. Note blob also had a hash. int cql_string_compare(cql_string_ref _Nonnull s1, cql_string_ref _Nonnull s2); cql_hash_code cql_string_hash(cql_string_ref _Nullable str); cql_bool cql_string_equal(cql_string_ref _Nullable s1, cql_string_ref _Nullable s2); int cql_string_like(cql_string_ref _Nonnull s1, cql_string_ref _Nonnull s2);  Strings can be converted from their reference form to standard C form. These macros define how this is done. Note that temporary allocations are possible here but the standard implementation does not actually need to do an alloc. It stores UTF8 in the string pointer so it's ready to go. #define cql_alloc_cstr(cstr, str) const char *_Nonnull cstr = (str)-&gt;ptr #define cql_free_cstr(cstr, str) 0  The macros for result sets have somewhat less flexibility. The main thing that you can do here is add additional fields to the &quot;meta&quot; structure. It needs those key fields because it is created by the compiler. However the API is used to create a result set so that can be any object you like. It only has to respond to the get_meta, get_data, and get_count apis. Those can be mapped as you desire. In principle there could have been a macro to create the &quot;meta&quot; as well (a PR for this is welcome) but it's really a pain for not much benefit. The advantage of defining your own &quot;meta&quot; is that you can use it to add additional custom APIs to your result set that might need some storage. The additional API cql_result_set_note_ownership_transferred(result_set)is used in the event that you are moving ownership of the buffers from out of CQL's universe. So like maybe JNI is absorbing the result, or Objective C is absorbing the result. The default implementation is a no-op. // builtin result set typedef struct cql_result_set *cql_result_set_ref; typedef struct cql_result_set_meta { ... } typedef struct cql_result_set { cql_type base; cql_result_set_meta meta; cql_int32 count; void *_Nonnull data; } cql_result_set; #define cql_result_set_type_decl(result_set_type, result_set_ref) \\ typedef struct _##result_set_type *result_set_ref; cql_result_set_ref _Nonnull cql_result_set_create( void *_Nonnull data, cql_int32 count, cql_result_set_meta meta); #define cql_result_set_retain(result_set) cql_retain((cql_type_ref)result_set); #define cql_result_set_release(result_set) cql_release((cql_type_ref)result_set); #define cql_result_set_note_ownership_transferred(result_set) #define cql_result_set_get_meta(result_set) (&amp;((cql_result_set_ref)result_set)-&gt;meta) #define cql_result_set_get_data(result_set) ((cql_result_set_ref)result_set)-&gt;data #define cql_result_set_get_count(result_set) ((cql_result_set_ref)result_set)-&gt;count  "},{"title":"Mocking​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#mocking","content":"The CQL run test needs to do some mocking. This bit is here for that test. If you want to use the run test with your version of cqlrt you'll need to define a shim for sqlite3_step that can be intercepted. This probably isn't going to come up. #ifdef CQL_RUN_TEST #define sqlite3_step mockable_sqlite3_step SQLITE_API cql_code mockable_sqlite3_step(sqlite3_stmt *_Nonnull); #endif  "},{"title":"Profiling​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#profiling","content":"If you want to support profiling you can implement cql_profile_start and cql_profile_stopto do whatever you want. The CRC uniquely identifies a procedure (you can log that). Theindex provides you with a place to store something that you can use as a handle in your logging system. Typically an integer. This lets you assign indices to the procedures you actually saw in any given run and then log them or something like that. No data about parameters is provided, this is deliberate. // No-op implementation of profiling // * Note: we emit the crc as an expression just to be sure that there are no compiler // errors caused by names being incorrect. This improves the quality of the CQL // code gen tests significantly. If these were empty macros (as they once were) // you could emit any junk in the call and it would still compile. #define cql_profile_start(crc, index) (void)crc; (void)index; #define cql_profile_stop(crc, index) (void)crc; (void)index;  The definitions in cqlrt_common.c can provide codegen than either has generic &quot;getters&quot; for each column type (useful for JNI) or produces a unique getter that isn't shared. The rowset metadata will include the values for getBoolean, getDouble etc. if CQL_NO_GETTERS is 0. Getters are a little slower for C but give you a small number of functions that need to have JNI if you are targeting Java. // the basic version doesn't use column getters #define CQL_NO_GETTERS 1  "},{"title":"Encoding of Sensitive Columns​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#encoding-of-sensitive-columns","content":"By setting an attribute on any procedure that produces a result set you can have the selected sensitive values encoded. If this happens CQL first asks for the encoder and then calls the encode methods passing in the encoder. These aren't meant to be cryptograhically secure but rather to provide some ability to prevent mistakes. If you opt in, sensitive values have to be deliberately decoded and that provides an audit trail. The default implementation of all this is a no-op. // implementation of encoding values. All sensitive values read from sqlite db will // be encoded at the source. CQL never decode encoded sensitive string unless the // user call explicitly decode function from code. cql_object_ref _Nullable cql_copy_encoder(sqlite3 *_Nonnull db); cql_bool cql_encode_bool(...) cql_int32 cql_encode_int32(...) cql_int64 cql_encode_int64(...) cql_double cql_encode_double(...) cql_string_ref _Nonnull cql_encode_string_ref_new(...); cql_blob_ref _Nonnull cql_encode_blob_ref_new(..); cql_bool cql_decode_bool(...); cql_int32 cql_decode_int32(...); cql_int64 cql_decode_int64(...); cql_double cql_decode_double(...); cql_string_ref _Nonnull cql_decode_string_ref_new(...); cql_blob_ref _Nonnull cql_decode_blob_ref_new(...);  "},{"title":"The Common Headers​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#the-common-headers","content":"The standard APIs all build on the above, so they should be included last. Now in some cases the signature of the things you provide in cqlrt.h is basically fixed, so it seems like it would be easier to move the prototpyes into cqlrt_common.h. However, in many cases additional things are needed like declspec or export or other system specific things. The result is that cqlrt.h is maybe a bit more verbose that it strictly needs to be. Also some versions of cqlrt.h choose to implement some of the APIs as macros... // NOTE: This must be included *after* all of the above symbols/macros. #include &quot;cqlrt_common.h&quot;  "},{"title":"The cqlrt_cf Runtime​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#the-cqlrt_cf-runtime","content":"In order to use the Objective-C code-gen (--rt objc) you need a runtime that has reference types that are friendly to Objective-C. For this purpose we created an open-source version of such a runtime: it can be found in the sources/cqlrt_cf directory. This runtime is also a decent example of how much customization you can do with just a little code. Some brief notes: This runtime really only makes sense on macOS, iOS, or maybe some other place that Core Foundation (CF) exists As such its build process is considerably less portable than other parts of the system The CQL reference types have been redefined so that they map to: CFStringRef (strings)CFTypeRef (objects)CFDataRef (blobs) The key worker functions use CF, e.g. cql_ref_hash maps to CFHashcql_ref_equal maps to CFEqualcql_retain uses CFRetain (with a null guard)cql_release uses CFRelease (with a null guard) Strings use CF idioms, e.g. string literals are created with CFSTRC strings are created by using CFStringGetCStringPtr or CFStringGetCString when needed Of course, since the meaning of some primitive types has changed, the contract to the CQL generated code has changed accordingly. For instance: procedures compiled against this runtime expect string arguments to be CFStringRefresult sets provide CFStringRef values for string columns The consequence of this is that the Objective-C code generation --rt objc finds friendly contracts that it can freely convert to types like NSString * which results in seamless integration with the rest of an Objective-C application. Of course the downside of all this is that the cqlrt_cf runtime is less portable. It can only go where CF exists. Still, it is an interesting demonstration of the flexablity of the system. The system could be further improved by creating a custom result type (e.g. --rt c_cf) and using some of the result type options for the C code generation. For instance, the compiler could do these things: generate CFStringRef foo; instead of cql_string_ref foo; for declarationsgenerate SInt32 an_integer instead of cql_int32 an_integer Even though cqlrt_cf is already mapping cql_int32 to something compatible with CF, making such changes would make the C output a little bit more CF idiomatic. This educational exercise could probably be completed in just a few minutes by interested readers. The make.sh file in the sources/cqlrt_cf directory illustrates how to get CQL to use this new runtime. The demo itself is a simple port of the code in Appendix 10. "},{"title":"Recap​","type":1,"pageTitle":"Part 5: CQL Runtime","url":"/cql-guide/int05#recap","content":"The CQL runtime, cqlrt.c, is intended to be replaced. The version that ships with the distribution is a simple, portable implementation that is single threaded. Serious users of CQL will likely want to replace the default version of the runtime with something more tuned to their use case. Topics covered included: contract, error, and tracing macroshow value types are definedhow reference types are definedmocking (for use in a test suite)profilingencoding of sensitive columnsboxing statementsthe cqlrt_cf runtime As with the other parts, no attempt was made to cover every detail. That is best done by reading the source code. "},{"title":"Part 6: Schema Management","type":0,"sectionRef":"#","url":"/cql-guide/int06","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#preface","content":"Part 6 continues with a discussion of the essentials of schema management in the CQL compiler. As in the previous parts, the goal here is not to go over every detail of the system but rather to give a sense of how schema management happens in general -- the core strategies and implementation choices -- so that when reading the management code you will have an idea how it all hangs together. To accomplish this, various key data structures will be explained in detail and accompanied by examples of their use. "},{"title":"Schema Management​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#schema-management","content":"The primary goal of the schema management features of the CQL compiler is to provide the ability to create a &quot;schema upgrader&quot; that can move a given user's database from a previous version of the schema to the current version. Because of the limitations of SQL in general, and SQLite in particular, not all transforms are possible; so additionally the system must correctly detect and prevent upgrades that cannot be safely performed. The full set of schema attributes and their meaning is described in Chapter 10and the full set of validations is described in Chapter 11. Briefly the directives are: @create(n): indicates a table/column is to be created at version n.@delete(n): indicates a table/column is to be deleted at version n.@recreate: indicates the table contents are not precious the table can be dropped and created when its schema changesthis does not combine with @createit applies only to tablesviews, triggers, and indices are always on the @recreate plan and do not have to be marked so Now the various annotations can occur substantially in any order as there are no rules that require that tables that are created later in time appear later in the input. This means the appearance order of tables is in general very inconvenient for any upgrading logic. However, the semantic validation pass gathers all the annotations into two large bytebuf objects which can be readily sorted -- one for things on the @create plan and one for the @recreate plan. These will be discussed below. At this point it's probably best to start looking at some of the code fragments. We're going to be looking at all the steps in the top level function: // Main entry point for schema upgrade code-gen. cql_noexport void cg_schema_upgrade_main(ast_node *head) { Contract(options.file_names_count == 1); ... }  Note that the schema upgrader code generator in CQL does not produce C but rather it produces more CQL which then has to be compiled down to C. This choice means that the codegen is a lot more readable and gets the benefit of the usual CQL error checking and exception management. "},{"title":"Check for errors, check for --global_proc​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#check-for-errors-check-for---global_proc","content":"We start with some simple error checks: Any semantic errors abort the code-generation. The --global_proc names the procedure that will do the upgrade. It is also used as a prefix on all of the tables that the upgrader requires. This makes it possible, if desired, to have separate upgraders for different parts of your schema, or to combine upgraders from two different unrelated subsystems in the same database.  cql_exit_on_semantic_errors(head); exit_on_no_global_proc();  "},{"title":"Preparing the Attributes​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#preparing-the-attributes","content":"The two arrays schema_annotations and recreate_annotations are sorted. The item count can be easily computed using the allocated size of these items, both of which are of type bytebuf. The comparators provided to qsortput these arrays in exactly the order needed.  // first sort the schema annotations according to version, type etc. // we want to process these in an orderly fashion and the upgrade rules // are nothing like the declared order. void *base = schema_annotations-&gt;ptr; size_t schema_items_size = sizeof(schema_annotation); size_t schema_items_count = schema_annotations-&gt;used / schema_items_size; schema_annotation *notes = (schema_annotation*)base; int32_t max_schema_version = 0; if (schema_items_count) { qsort(base, schema_items_count, schema_items_size, annotation_comparator); max_schema_version = notes[schema_items_count - 1].version; } // likewise, @recreate annotations, in the correct upgrade order (see comparator) base = recreate_annotations-&gt;ptr; size_t recreate_items_size = sizeof(recreate_annotation); size_t recreate_items_count = recreate_annotations-&gt;used / recreate_items_size; if (recreate_items_count) { qsort(base, recreate_items_count, recreate_items_size, recreate_comparator); } recreate_annotation *recreates = (recreate_annotation *)base;  "},{"title":"Creating the Global CRC​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#creating-the-global-crc","content":"Schema upgrade is expensive, so we want to be able to quickly detect if the schema installed is already the latest version. To do this we compute a single global 64-bit CRC for the current version of the schema. This can be compared against a stored schema CRC from the last run. If the CRCs match, no work needs to be done.  CHARBUF_OPEN(all_schema); // emit canonicalized schema for everything we will upgrade // this will include the schema declarations for the ad hoc migrations, too; cg_generate_schema_by_mode(&amp;all_schema, SCHEMA_TO_UPGRADE); // compute the master CRC using schema and migration scripts llint_t schema_crc = (llint_t)crc_charbuf(&amp;all_schema); CHARBUF_CLOSE(all_schema);  The schema generator is used to emit the full schema, including annotations, into a buffer. A raw CRC of the buffer gives us the &quot;global&quot; or &quot;overall&quot; CRC for the whole schema. "},{"title":"Output Fragments​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#output-fragments","content":"A number of buffers will hold the various pieces of output.  CHARBUF_OPEN(preamble); CHARBUF_OPEN(main); CHARBUF_OPEN(decls); CHARBUF_OPEN(pending); CHARBUF_OPEN(upgrade); CHARBUF_OPEN(baseline);  These will be assembled as follows:  CHARBUF_OPEN(output_file); bprintf(&amp;output_file, &quot;%s\\n&quot;, decls.ptr); bprintf(&amp;output_file, &quot;%s&quot;, preamble.ptr); bprintf(&amp;output_file, &quot;%s&quot;, main.ptr); cql_write_file(options.file_names[0], output_file.ptr); CHARBUF_CLOSE(output_file);  In short: first decls, this declares the schema among other thingsthen, preamble, this contains helper proceduresthen, main, the primary upgrader steps go here We'll go over all of these in subsequent sections. "},{"title":"Declarations Section​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#declarations-section","content":"The result type includes a customizable prefix string. This is the first thing to go out. Typically this is the appropriate copyright notice. rt.c has this information and that file is replaceable.  bprintf(&amp;decls, &quot;%s&quot;, rt-&gt;source_prefix);  The schema upgrade script is in the business of creating tables from old versions and then altering them. The table declarations will be for the final shape. We need to emit @SCHEMA_UPGRADE_SCRIPT so that the CQL compiler knows that there will be multiple declarations of the same table and they might not be identical. The upgrade script is in the business of getting things to the end state. Likewise it is normal for the schema upgrade script to refer to columns that have been deleted, this is because a column might be created in say version 5 and then deleted in version 10. The upgrade code goes through the columns lifecycle, so even though the declarations already say the column is doomed to die in version 10, the creation code in version 5 is legal -- and necessary. Schema migration steps that run in version 6, 7, 8, or 9 might use the contents of the column as part of essential data migration. We can never know what version we might find in a database that is being upgraded, it could be very far in the past, at a time where a deleted column still existed.  bprintf(&amp;decls, &quot;-- no columns will be considered hidden in this script\\n&quot;); bprintf(&amp;decls, &quot;-- DDL in procs will not count as declarations\\n&quot;); bprintf(&amp;decls, &quot;@SCHEMA_UPGRADE_SCRIPT;\\n\\n&quot;);  A convenience comment goes in the decls section with the CRC.  bprintf(&amp;decls, &quot;-- schema crc %lld\\n\\n&quot;, schema_crc);  There are a set of functions that allow the creation of, and access to, an in-memory cache of the facet state. These functions are all defined in cqlrt_common.c. But they have to be declared to CQL to use them.  cg_schema_emit_facet_functions(&amp;decls);  The table sqlite_master is used to read schema state. That table has to be declared.  cg_schema_emit_sqlite_master(&amp;decls);  The full schema may be used by the upgraders, we need a declaration of all of that.  bprintf(&amp;decls, &quot;-- declare full schema of tables and views to be upgraded and their dependencies -- \\n&quot;); cg_generate_schema_by_mode(&amp;decls, SCHEMA_TO_DECLARE);  At this point a quick side-step to the output modes and region arguments is appropriate. Schema Region Arguments​ The upgrader honors the arguments --include_regions and --exclude_regions. If they are absent that is the same as &quot;include everything&quot; and &quot;exclude nothing&quot;. Recall that schema regions allow you to group schema as you wish. A typical use might be to define some &quot;core&quot; schema in a set of regions (maybe just one) and then a set of &quot;optional&quot; schema in some additional regions. An upgrader for just &quot;core&quot; could be created by adding --include_regions core. When creating upgraders for the optional parts, there are two choices: --include-regions optional1 : makes an upgrader for optional1 and core (the assumption being that optional1 was declared to depend on core)--include-regions optional1 --exclude-regions core : makes an upgrader for optional1 which should run after the standalone core upgrader has already run this allows you to share the &quot;core&quot; parts between any number of &quot;optional&quot; partsand of course this can nest; there can be several &quot;core&quot; parts; and so forth Schema Output Modes​ The flag bits are these: // We declare all schema we might depend on in this upgrade (this is the include list) // e.g. we need all our dependent tables so that we can legally use them in an FK #define SCHEMA_TO_DECLARE 1 // We only emit schema that we are actually updating (this is include - exclude) // e.g. a table on the exclude list is assumed to be upgraded by its own script // in a different run. #define SCHEMA_TO_UPGRADE 2 // We get TEMP items IF and ONLY IF this bit is set #define SCHEMA_TEMP_ITEMS 4  As we saw before, the schema we CRC is SCHEMA_TO_UPGRADE. This is all the regions that were selected but not their dependencies. The point of this is that you might make an upgrader for say a &quot;core&quot; part of your schema which can be shared and then make additional upgraders for various parts that use the &quot;core&quot; but are otherwise &quot;optional&quot;. Each of those &quot;optional&quot; upgraders needs its own CRC that includes its schema but not the &quot;core&quot; schema. However the &quot;optional&quot; schema can refer to &quot;core&quot; schema (e.g. in foreign keys) so all of the tables are declared. This is SCHEMA_TO_DECLARE mode. declare all schema you are allowed to refer toCRC, and upgrade, only the parts selected by the region arguments "},{"title":"The Schema Helpers​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#the-schema-helpers","content":"This bit generates the facets table, the full name is your_global_proc_cql_schema_facets whereyour_global_proc is the --global_proc argument. This is referred to simply as the facets table. There is an identical temporary table that is used to store the contents of the facets table upon startup. This allows the upgrader to produce a complete difference. The facets table is nothing more than a mapping between the name of some facet of the schema (like a table, a view, a column) and its last known verison info -- usually its CRC. NOTE: this temp table predates the in-memory facets data structure so it could probably be removed the diff would have to work against the in-memory datastructure which is immutable hence just as good as a temp tablelook for a change like this soon The remaining procedures are for testing facet state or sqlite_master state. All of them get the usual global prefix. For ease of discussion I will elide the prefix for the rest of this document. check_column_exists : checks if the indicated column is present in sqlite_master necessary because there is no ALTER TABLE ADD COLUMN IF NOT EXISTS command create_cql_schema_facets_if_needed : actually creates the facets table if it does not existsave_cql_schema_facets : creates the cql_schema_facets_saved temp table and populates itcql_set_facet_version : sets one facet to the indicated value this writes to the database, not the in-memory version of the table cql_get_facet_version : reads a facet value from the facet table this is only used to check the master schema value, after that the in-memory version is used cql_get_version_crc : gets the CRC for a given schema version each schema version has its own CRC in addition to the global CRCthis information is stored in the facets table with a simple naming convention for the facet namethe in memory version of the table is always used here cql_set_version_crc : sets the CRC for a given schema version in the facet table this writes to the database, not the in-memory version of the table cql_drop_legacy_triggers : drops any triggers of the from tr__* for historical reasons the original triggers did not include tombstones when deletedthis kludge is here to clean up legacy triggers and its peculiar to Messenger onlythis should really be removed from the OSS version but it's never been a prioritysorry...  cg_schema_helpers(&amp;decls);  "},{"title":"Declared Upgrade Procedures​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#declared-upgrade-procedures","content":"The annotations can include an upgrade procedure. The term &quot;migration&quot; procedure is sometimes used as well and is synonymous. This is some code that should run after the schema alteration has been made to create/delete/move some data around in the new schema. Each of these must be declared before it is used and the declarations will be here, at the end of the decls section after this introductory comment.  bprintf(&amp;decls, &quot;-- declared upgrade procedures if any\\n&quot;);  "},{"title":"The Upgrading Workers​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#the-upgrading-workers","content":"The main upgrader will invoke these key workers to do its job. This is where the preamblesection starts. It contains the meat of the upgrade steps wrapped in procedures that do the job.  cg_schema_emit_baseline_tables_proc(&amp;preamble, &amp;baseline); int32_t view_creates = 0, view_drops = 0; cg_schema_manage_views(&amp;preamble, &amp;view_drops, &amp;view_creates); int32_t index_creates = 0, index_drops = 0; cg_schema_manage_indices(&amp;preamble, &amp;index_drops, &amp;index_creates); int32_t trigger_creates = 0, trigger_drops = 0; cg_schema_manage_triggers(&amp;preamble, &amp;trigger_drops, &amp;trigger_creates); if (recreate_items_count) { cg_schema_manage_recreate_tables(&amp;preamble, recreates, recreate_items_count); } bool_t has_temp_schema = cg_schema_emit_temp_schema_proc(&amp;preamble); bool_t one_time_drop_needed = false;  These are the last of the worker methods: cg_schema_emit_baseline_tables_proc : emits a procedure that will create the schema at its baseline version this means whatever &quot;v0&quot; of the schema was, no creates or deletes have yet happened cg_schema_manage_views : creates the view management procedures cql_drop_all_views : drops all viewscql_create_all_views : creates all viewsboth of these run unless the global CRC matches cg_schema_manage_indices : creates the index management procedures cql_drop_all_indices : drops any index that exists and whose CRC changedcql_create_all_indices : creates any index whose CRC changedrecreating indices can be costly so it is only done if the index actually changed cg_schema_manage_triggers : creates the trigger management procedures cql_drop_all_triggers : drops all triggerscql_create_all_triggers : creates all triggersboth of these run unless the global CRC matchesadditionally any legacy triggers will be deleted (see cql_drop_legacy_triggers) cg_schema_manage_recreate_tables : creates the cql_recreate_tables worker the recreate_annotations array is used to find all the recreate tablesthe entries are sorted by group, then name, so that annotations within a group are togetherthe procedure contains code to delete the procedure or group and recreate it if the CRC does not matchthe CRC is computed using the code for create instructions and is stored in a facet with a suitable namethe easiest way to think of this code is that it always emits a chunk of recreates for a group ungrouped tables are a group of 1group delete/create instructions accumulate until the next entry is in a different group cg_schema_emit_temp_schema_proc : emits a procedure to create any temporary schema temp tables are always created in full at the latest versionthis code is run regardless of whether the global CRC matches or not All of these functions semantic outputs like all_indices_list, all_views_list, etc. to do their job (exceptcg_schema_manage_recreate_tables as noted). Generally they have all the data they need handed to them on a silver platter by the semantic pass. This is not an accident. Reading the Facets into Memory​ The setup_facets procedure simply selects out the entire facets table with a cursor and uses cql_facet_add to get them into a hash table. This is the primary source of facets information during the run. This is a good example of what the codegen looks like so we'll include this one in full.  // code to read the facets into the hash table bprintf(&amp;preamble, &quot;@attribute(cql:private)\\n&quot;); bprintf(&amp;preamble, &quot;CREATE PROCEDURE %s_setup_facets()\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot;BEGIN\\n&quot;); bprintf(&amp;preamble, &quot; BEGIN TRY\\n&quot;); bprintf(&amp;preamble, &quot; SET %s_facets := cql_facets_new();\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot; DECLARE C CURSOR FOR SELECT * from %s_cql_schema_facets;\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot; LOOP FETCH C\\n&quot;); bprintf(&amp;preamble, &quot; BEGIN\\n&quot;); bprintf(&amp;preamble, &quot; LET added := cql_facet_add(%s_facets, C.facet, C.version);\\n&quot;, global_proc_name); bprintf(&amp;preamble, &quot; END;\\n&quot;); bprintf(&amp;preamble, &quot; END TRY;\\n&quot;); bprintf(&amp;preamble, &quot; BEGIN CATCH\\n&quot;); bprintf(&amp;preamble, &quot; -- if table doesn't exist we just have empty facets, that's ok\\n&quot;); bprintf(&amp;preamble, &quot; END CATCH;\\n&quot;); bprintf(&amp;preamble, &quot;END;\\n\\n&quot;); ### The Main Upgrader And now we come to the main upgrading procedure `perform_upgrade_steps`. We'll go over this section by section. #### Standard Steps ```c // the main upgrade worker bprintf(&amp;main, &quot;\\n@attribute(cql:private)\\n&quot;); bprintf(&amp;main, &quot;CREATE PROCEDURE %s_perform_upgrade_steps()\\n&quot;, global_proc_name); bprintf(&amp;main, &quot;BEGIN\\n&quot;); bprintf(&amp;main, &quot; DECLARE schema_version LONG INTEGER NOT NULL;\\n&quot;); if (view_drops) { bprintf(&amp;main, &quot; -- dropping all views --\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_drop_all_views();\\n\\n&quot;, global_proc_name); } if (index_drops) { bprintf(&amp;main, &quot; -- dropping condemned or changing indices --\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_drop_all_indices();\\n\\n&quot;, global_proc_name); } if (trigger_drops) { bprintf(&amp;main, &quot; -- dropping condemned or changing triggers --\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_drop_all_triggers();\\n\\n&quot;, global_proc_name); } if (baseline.used &gt; 1) { llint_t baseline_crc = (llint_t)crc_charbuf(&amp;baseline); bprintf(&amp;main, &quot; ---- install baseline schema if needed ----\\n\\n&quot;); bprintf(&amp;main, &quot; CALL %s_cql_get_version_crc(0, schema_version);\\n&quot;, global_proc_name); bprintf(&amp;main, &quot; IF schema_version != %lld THEN\\n&quot;, baseline_crc); bprintf(&amp;main, &quot; CALL %s_cql_install_baseline_schema();\\n&quot;, global_proc_name); bprintf(&amp;main, &quot; CALL %s_cql_set_version_crc(0, %lld);\\n&quot;, global_proc_name, baseline_crc); bprintf(&amp;main, &quot; END IF;\\n\\n&quot;); }  First we deal with the preliminaries: drop the views if there are anydrop the indices that need droppingdrop the triggers if there are anyinstall the baseline schema if there is any Process Standard Annotations​ In this phase we walk the annotations from schema_annotations which are now stored in notes. They have been sorted in exactly the right order to process them (by version, then type, then target). We'll create one set of instructions per version number as we simply accumulate instructions for any version while we're still on the same version then spit them all out. Adding target to the sort order ensures that the results have a total ordering (there are no ties that might yield an ambiguous order). We set up a loop to walk over the annotations and we flush if we ever encounter an annotation for a different version number. We'll have to force a flush at the end as well. cg_schema_end_versiondoes the flush.  int32_t prev_version = 0; for (int32_t i = 0; i &lt; schema_items_count; i++) { schema_annotation *note = &amp;notes[i]; ast_node *version_annotation = note-&gt;annotation_ast; uint32_t type = note-&gt;annotation_type; Contract(type &gt;= SCHEMA_ANNOTATION_FIRST &amp;&amp; type &lt;= SCHEMA_ANNOTATION_LAST); Contract(is_ast_version_annotation(version_annotation)); EXTRACT_OPTION(vers, version_annotation-&gt;left); Invariant(note-&gt;version == vers); Invariant(vers &gt; 0); if (prev_version != vers) { cg_schema_end_version(&amp;main, &amp;upgrade, &amp;pending, prev_version); prev_version = vers; }  If we find any item that is in a region we are not upgrading, we skip it.  CSTR target_name = note-&gt;target_name; Invariant(type &gt;= SCHEMA_ANNOTATION_FIRST &amp;&amp; type &lt;= SCHEMA_ANNOTATION_LAST); if (!include_from_region(note-&gt;target_ast-&gt;sem-&gt;region, SCHEMA_TO_UPGRADE)) { continue; }  There are several annotation types. Each one requires appropriate commands  switch (type) { case SCHEMA_ANNOTATION_CREATE_COLUMN: { ... emit ALTER TABLE ADD COLUMN if the column does not already exist break; } case SCHEMA_ANNOTATION_DELETE_COLUMN: { ... it's not possible to delete columns in SQLite (this is changing) ... we simply emit a comment and move on break; } case SCHEMA_ANNOTATION_CREATE_TABLE: { ... if the table is moving from @recreate to @create we have to drop any stale version ... of it one time. We emit a call to `cql_one_time_drop` and record that we need ... to generate that procedure in `one_time_drop_needed`. ...in all cases emit a CREATE TABLE IF NOT EXISTS break; } case SCHEMA_ANNOTATION_DELETE_TABLE: { ... emit DROP TABLE IF EXISTS for the target break; } case SCHEMA_ANNOTATION_DELETE_INDEX: case SCHEMA_ANNOTATION_DELETE_VIEW: case SCHEMA_ANNOTATION_DELETE_TRIGGER: ... this annotation indicates there is a tombstone on the item ... this was handled in the appropriate `manage` worker above, nothing needs ... to be done here except run any migration procs (see below) break; case SCHEMA_ANNOTATION_AD_HOC: ... ad hoc migration procs allow for code to be run one time when we hit ... a particular schema version, this just allows the migration proc to run // no annotation based actions other than migration proc (handled below) Contract(version_annotation-&gt;right); bprintf(&amp;upgrade, &quot; -- ad hoc migration proc %s will run\\n\\n&quot;, target_name); break; }  The above constitutes the bulk of the upgrading logic which, as you can see, isn't that complicated. Any of the above might have a migration proc. If there is one in the node, then generate: emit a call to cql_facet_find to see if the migration proc has already runemit a declaration for the migration proc into the decls sectionemit a call to the procedure (it accept no arguments)emit a call to cql_set_facet_version to record that the migrator ran When the loop is done, any pending migration code is flushed using cg_schema_end_version again. At this point we can move on to the finalization steps. Finalization Steps​ With the standard upgrade finished, there is just some house keeping left:  if (recreate_items_count) { bprintf(&amp;main, &quot; CALL %s_cql_recreate_tables();\\n&quot;, global_proc_name); } if (view_creates) { bprintf(&amp;main, &quot; CALL %s_cql_create_all_views();\\n&quot;, global_proc_name); } if (index_creates) { bprintf(&amp;main, &quot; CALL %s_cql_create_all_indices();\\n&quot;, global_proc_name); } if (trigger_creates) { bprintf(&amp;main, &quot; CALL %s_cql_create_all_triggers();\\n&quot;, global_proc_name); } bprintf(&amp;main, &quot; CALL %s_cql_set_facet_version('cql_schema_version', %d);\\n&quot;, global_proc_name, prev_version); bprintf(&amp;main, &quot; CALL %s_cql_set_facet_version('cql_schema_crc', %lld);\\n&quot;, global_proc_name, schema_crc); bprintf(&amp;main, &quot;END;\\n\\n&quot;);  cql_recreate_tables : must run if there are any tables marked recreate this procedure will have code to drop and recreate any changed tablesthis procedure was created by cg_schema_manage_recreate_tables and that process is described above basically, it uses recreate_annotations to do the job any that were condemned by marking with @delete will not be created again here cql_create_all_views : must run if there are any views, they need to be put back any that were condemned by marking with @delete are not created again here cql_create_all_indices : must run if there are any indices, this will create any that are missing any that were changing were previously deleted, this is where they come backany that were condemned by marking with @delete are not created again here cql_create_all_triggers : must run if there are any triggers, they need to be put back any that were condemned by marking with @delete are not created again heretriggers might cause weird side-effects during upgrade hence they are always droppedstale triggers especially could be problematicany triggers that refer to views couldn't possibly run as the views are gonehence, triggers are always dropped and recreated The &quot;Main&quot; Steps​ We're getting very close to the top level now perform_needed_upgrades : this orchestrates the upgrade, if it is called there is one cql_facet_find : is used to check for a schema &quot;downgrade&quot; abort with an error if that happens save_cql_schema_facets : saves the facets as they exist so we can diff themperform_upgrade_steps : does the upgradea LEFT OUTER JOIN between cql_schema_facets and cql_schema_facets_saved reports differencesany errors will cause the normal CQL error flow the main entry point is named by global_proc_name create_cql_schema_facets_if_needed is used to create the facets table if it doesn't already existthe special facet cql_schema_crc is read from the facets tableif the CRC stored there matches our target then we return &quot;no differences&quot;, otherwisesetup_facets : loads the in-memory version of the facets tableperform_needed_upgrades : does the work and creates the diffcql_facets_delete is used to free the in-memory storage, even if there were errors in perform_needed_upgrades cql_install_temp_schema : installs temporary schema if there is any, regardless of the CRC the one_time_drop code is emitted if it was needed Writing the Buffer​ At this point the main buffers decls, preamble, and main are ready to go. We're back to where we started but we can quickly recap the overall flow.  CHARBUF_OPEN(output_file); bprintf(&amp;output_file, &quot;%s\\n&quot;, decls.ptr); bprintf(&amp;output_file, &quot;%s&quot;, preamble.ptr); bprintf(&amp;output_file, &quot;%s&quot;, main.ptr); cql_write_file(options.file_names[0], output_file.ptr); CHARBUF_CLOSE(output_file);  There is nothing left but to CHARBUF_CLOSE the interim buffers we created. "},{"title":"Recap​","type":1,"pageTitle":"Part 6: Schema Management","url":"/cql-guide/int06#recap","content":"At present cg_schema.c accomplishes a lot and is fairly light at only 1313 lines (at present). It is able to do so because it can leverage heavy lifting done in the semantic analysis phase and schema generation that can be done like all other SQL generation by the echoing code discussed in Part 1. Topics covered included: the essential sources of schema information from the semantic passthe state tables used in the database and helpers for read/write of the samethe interaction with schema regionsthe prosecution steps for tables, columns, views, triggers, indicesthe key annotation types and what code they createthe handling of recreate tables, temp tables, and the base schemahow all of these are wired together starting from the upgrader's &quot;main&quot; As with the other parts, no attempt was made to cover every function in detail. That is best done by reading the source code. But there is overall structure here and an understanding of the basic principles is helpful before diving into the source code. "},{"title":"Part 7: JSON Generation","type":0,"sectionRef":"#","url":"/cql-guide/int07","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#preface","content":"Part 7 continues with a discussion of the JSON generation code. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how JSON creation works in general -- the core strategies and implementation choices -- so that when reading the source you will have an idea how it all hangs together. To accomplish this, we'll illustrate the key strategies used to extract the data and format the JSON. "},{"title":"JSON Schema​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#json-schema","content":"The JSON schema is described in Chapter 13 of the Guide and there is a nice diagram of its grammar for reference. So, we won't be discussing all the details of the output. Instead we're going to go over the theory of how the JSON generator works. It is structured very much like the other code generators but it happens to produce a JSON file. It's call the &quot;JSON Schema&quot; because most of the content is a description of the database schema in JSON form. As such it's almost entirely just a simple walk of the AST in the correct order. The only really tricky bit is the extra dependency analysis on the AST. This allows us to emit usage information in the output for downstream tools to use as needed. We'll cover these topics: walking the ASTformattingcomputing the dependencies This should be a short chapter compared to the others, this output really is much simpler to create than the C or the schema upgrader. "},{"title":"Walking the AST​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#walking-the-ast","content":"If you run this command: $ cql --in x --rt json_schema --cg x.json  Where x is an empty file, you'll get the following skeletal JSON, lightly reformatted for brevity: { &quot;tables&quot; : [ ], &quot;virtualTables&quot; : [ ], &quot;views&quot; : [ ], &quot;indices&quot; : [ ], &quot;triggers&quot; : [ ], &quot;attributes&quot; : [ ], &quot;queries&quot; : [ ], &quot;inserts&quot; : [ ], &quot;generalInserts&quot; : [ ], &quot;updates&quot; : [ ], &quot;deletes&quot; : [ ], &quot;general&quot; : [ ], &quot;regions&quot; : [ ], &quot;adHocMigrationProcs&quot; : [ ], &quot;enums&quot; : [ ] }  From this we can deduce a great deal of the structure of the code: // Main entry point for json schema format cql_noexport void cg_json_schema_main(ast_node *head) { Contract(options.file_names_count == 1); cql_exit_on_semantic_errors(head); tables_to_procs = symtab_new(); CHARBUF_OPEN(main); charbuf *output = &amp;main; bprintf(output, &quot;%s&quot;, rt-&gt;source_prefix); // master dictionary begins bprintf(output, &quot;\\n{\\n&quot;); BEGIN_INDENT(defs, 2); cg_json_tables(output); bprintf(output, &quot;,\\n&quot;); cg_json_virtual_tables(output); bprintf(output, &quot;,\\n&quot;); cg_json_views(output); bprintf(output, &quot;,\\n&quot;); cg_json_indices(output); bprintf(output, &quot;,\\n&quot;); cg_json_triggers(output); bprintf(output, &quot;,\\n&quot;); cg_json_stmt_list(output, head); bprintf(output, &quot;,\\n&quot;); cg_json_regions(output); bprintf(output, &quot;,\\n&quot;); cg_json_ad_hoc_migration_procs(output); bprintf(output, &quot;,\\n&quot;); cg_json_enums(output); if (options.test) { bprintf(output, &quot;,\\n&quot;); cg_json_table_users(output); } END_INDENT(defs); bprintf(output, &quot;\\n}\\n&quot;); cql_write_file(options.file_names[0], output-&gt;ptr); CHARBUF_CLOSE(main); SYMTAB_CLEANUP(tables_to_procs); }  cg_json_schema_main is the main function and you can see that it mirrors that skeletal JSON output nearly exactly with some additional test output options. We'll cover the test output in a later section when we've had a chance to discuss the dependency analysis. Example JSON Writer: Views​ These are sufficiently easy that we can just walk through one of the procedures front to back. Let's look at the &quot;views&quot; section. // The set of views look rather like the query section in as much as // they are in fact nothing more than named select statements. However // the output here is somewhat simplified. We only emit the whole select // statement and any binding args, we don't also emit all the pieces of the select. static void cg_json_views(charbuf *output) { bprintf(output, &quot;\\&quot;views\\&quot; : [\\n&quot;); BEGIN_INDENT(views, 2); int32_t i = 0; for (list_item *item = all_views_list; item; item = item-&gt;next) { ast_node *ast = item-&gt;ast; Invariant(is_ast_create_view_stmt(ast)); ast_node *misc_attrs = NULL; ast_node *attr_target = ast-&gt;parent; if (is_ast_stmt_and_attr(attr_target)) { EXTRACT_STMT_AND_MISC_ATTRS(stmt, misc, attr_target-&gt;parent); misc_attrs = misc; } cg_json_test_details(output, ast, misc_attrs); EXTRACT_OPTION(flags, ast-&gt;left); EXTRACT(view_and_attrs, ast-&gt;right); EXTRACT(name_and_select, view_and_attrs-&gt;left); EXTRACT_ANY_NOTNULL(select_stmt, name_and_select-&gt;right); EXTRACT_ANY_NOTNULL(name_ast, name_and_select-&gt;left); EXTRACT_STRING(name, name_ast); if (i &gt; 0) { bprintf(output, &quot;,\\n&quot;); } bprintf(output, &quot;{\\n&quot;); bool_t is_deleted = ast-&gt;sem-&gt;delete_version &gt; 0; BEGIN_INDENT(view, 2); bprintf(output, &quot;\\&quot;name\\&quot; : \\&quot;%s\\&quot;&quot;, name); bprintf(output, &quot;,\\n\\&quot;CRC\\&quot; : \\&quot;%lld\\&quot;&quot;, crc_stmt(ast)); bprintf(output, &quot;,\\n\\&quot;isTemp\\&quot; : %d&quot;, !!(flags &amp; VIEW_IS_TEMP)); bprintf(output, &quot;,\\n\\&quot;isDeleted\\&quot; : %d&quot;, is_deleted); if (is_deleted) { bprintf(output, &quot;,\\n\\&quot;deletedVersion\\&quot; : %d&quot;, ast-&gt;sem-&gt;delete_version); cg_json_deleted_migration_proc(output, view_and_attrs); } if (ast-&gt;sem-&gt;region) { cg_json_emit_region_info(output, ast); } if (misc_attrs) { bprintf(output, &quot;,\\n&quot;); cg_json_misc_attrs(output, misc_attrs); } cg_json_projection(output, select_stmt); cg_fragment_with_params(output, &quot;select&quot;, select_stmt, gen_one_stmt); cg_json_dependencies(output, ast); END_INDENT(view); bprintf(output, &quot;\\n}\\n&quot;); i++; } END_INDENT(views); bprintf(output, &quot;]&quot;); }  View Loop​ Already we can see the structure emerging, and of course its nothing more than a bunch of bprintf. Let's do it section by section: bprintf(output, &quot;\\&quot;views\\&quot; : [\\n&quot;); BEGIN_INDENT(views, 2); for (list_item *item = all_views_list; item; item = item-&gt;next) { .. } END_INDENT(views); bprintf(output, &quot;]&quot;);  Unsurprisingly, this code will iterate the all_views_list which was created precisely for this kind of output. The semantic pass populates this list for use downstream. We'll deal with BEGIN_INDENT a bit later, but it should be clear what it does by the name for now. So we've made the &quot;views&quot; section and we'll put 0 or more views in it. View Extraction​ The next section extracts the necessary information and emits the test output:  ast_node *ast = item-&gt;ast; Invariant(is_ast_create_view_stmt(ast)); ast_node *misc_attrs = NULL; ast_node *attr_target = ast-&gt;parent; if (is_ast_stmt_and_attr(attr_target)) { EXTRACT_STMT_AND_MISC_ATTRS(stmt, misc, attr_target-&gt;parent); misc_attrs = misc; } cg_json_test_details(output, ast, misc_attrs); EXTRACT_OPTION(flags, ast-&gt;left); EXTRACT(view_and_attrs, ast-&gt;right); EXTRACT(name_and_select, view_and_attrs-&gt;left); EXTRACT_ANY_NOTNULL(select_stmt, name_and_select-&gt;right); EXTRACT_ANY_NOTNULL(name_ast, name_and_select-&gt;left); EXTRACT_STRING(name, name_ast);  The is_ast_stmt_and_attr node tell us if there were any misc attributes on the statement. Those attributes can be extracted and printed. We have to look up the tree a little bit from where we are because this is the &quot;all views&quot; list, if there were attributes on this view they were attached two levels up. In any case misc_attrs ends with attributes if there are any. After the test output, the necessary view attributes are extracted the usual way with EXTRACT macros for the view shape. Test Output​ static void cg_json_test_details(charbuf *output, ast_node *ast, ast_node *misc_attrs) { if (options.test) { bprintf(output, &quot;\\nThe statement ending at line %d\\n&quot;, ast-&gt;lineno); bprintf(output, &quot;\\n&quot;); gen_set_output_buffer(output); if (misc_attrs) { gen_with_callbacks(misc_attrs, gen_misc_attrs, NULL); } gen_with_callbacks(ast, gen_one_stmt, NULL); bprintf(output, &quot;\\n\\n&quot;); } }  All of the JSON fragments have the usual test pattern &quot;The statement ending at line nnn&quot;. This means that the normal validator will be able to find comments in the test file and associate them with json parts. The testing strategies are discussed in[Part 4]((https://cgsql.dev/cql-guide/int04). In addition, while in test mode, we also emit the original statement that caused this JSON fragment to be created. This allows the test patterns to cross check the input and output and also makes the test output more readable for humans. Note that in test mode the JSON is effectively corrupted by the test output as it is not well-formed JSON in any way. So use of --test is strictly for validation only. View Basics​ All of the things that go into the JSON have some attributes that are universally present and generally come directly from the AST.  if (i &gt; 0) { bprintf(output, &quot;,\\n&quot;); } bprintf(output, &quot;{\\n&quot;); bool_t is_deleted = ast-&gt;sem-&gt;delete_version &gt; 0; BEGIN_INDENT(view, 2); bprintf(output, &quot;\\&quot;name\\&quot; : \\&quot;%s\\&quot;&quot;, name); bprintf(output, &quot;,\\n\\&quot;CRC\\&quot; : \\&quot;%lld\\&quot;&quot;, crc_stmt(ast)); bprintf(output, &quot;,\\n\\&quot;isTemp\\&quot; : %d&quot;, !!(flags &amp; VIEW_IS_TEMP)); bprintf(output, &quot;,\\n\\&quot;isDeleted\\&quot; : %d&quot;, is_deleted); if (is_deleted) { bprintf(output, &quot;,\\n\\&quot;deletedVersion\\&quot; : %d&quot;, ast-&gt;sem-&gt;delete_version); cg_json_deleted_migration_proc(output, view_and_attrs); } ... END_INDENT(view); bprintf(output, &quot;\\n}\\n&quot;); i++; }  This part of the output is the simplest we emit a comma if we need one (only the first entry doesn't)we start the view object '{'more indenting for the interior of the viewemit the view nameemit the CRC of the view (this makes it easy to see if the view changed) crc_stmt computes the CRC by echoing the statement into a scratch buffer and then running the CRC algorithm on that buffer note the &quot;,\\n&quot; pattern, this pattern is used because sometimes there are optional parts and using a leading &quot;,\\n&quot; makes it clear which part is supposed to emit the comma it turns out getting the commas right is one of the greater annoyances of JSON output emit &quot;isTemp&quot;emit &quot;isDeleted&quot;if the view is deleted, emit &quot;deletedVersion&quot;if there is a migration procedure on the @delete attribute emit that as well cg_json_deleted_migration_proc scans the attribute list for @delete attribute and emits the procedure name on that attribute if there is one Optional Info​ The next fragment emits two optional pieces that are present in many types of objects:  if (ast-&gt;sem-&gt;region) { cg_json_emit_region_info(output, ast); } if (misc_attrs) { bprintf(output, &quot;,\\n&quot;); cg_json_misc_attrs(output, misc_attrs); }  if there is a region assocatied with this view, we emit it here cg_json_emit_region_info emits two things: the view's regionthe &quot;deployment region&quot; of that region if any (regions are contained in deployable groups)see Chapter 10 for more info on regions and deployment regions if there are any miscellaneous attributes they are emitted we'll use cg_json_misc_attrs as our general formatting example when we get to that The View Details​ There is very little left in the view emitting code:  cg_json_projection(output, select_stmt); cg_fragment_with_params(output, &quot;select&quot;, select_stmt, gen_one_stmt); cg_json_dependencies(output, ast);  cg_json_projection emits the name and type of each column in the view select listcg_fragment_with_params emits the statement that creates the view in an attribute named &quot;select&quot; the normal echoing code emits the statementviews have no variables to bind but other statement forms inside of procedures can have variables in the statementthe variable names are replace with &quot;?&quot; in the text of the statementthe names of the variable appear in &quot;selectArgs&quot; (always empty for views) cg_json_dependencies emits the tables and views that were used by this view, it gets its own section Those few things produce all JSON for a view. All the other schema elements do basically the same things. Most of the helpers are shared so, for instance, regions, misc attributes, and dependencies appear in nearly every kind of object in the JSON. "},{"title":"Formatting the JSON​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#formatting-the-json","content":"To make the JSON pretty we want to indent it appropriately and put commas in the right places. There are some useful macros for this, and they all rely on the fact that the emitted text goes to a charbuf variable creatively called output. Here's a sample procedure that was mentioned earlier, it does the usual things: // Emit a list of attributes for the current entity, it could be any kind of entity. // Whatever it is we spit out the attributes here in array format. static void cg_json_misc_attrs(charbuf *output, ast_node *_Nonnull list) { Contract(is_ast_misc_attrs(list)); bprintf(output, &quot;\\&quot;attributes\\&quot; : [\\n&quot;); BEGIN_INDENT(attr, 2); BEGIN_LIST; for (ast_node *item = list; item; item = item-&gt;right) { COMMA; cg_json_misc_attr(output, item-&gt;left); } END_LIST; END_INDENT(attr); bprintf(output, &quot;]&quot;); }  The miscellaneous attributes are going to be emitted in a list, and since any one attribute can actually be a list of attributes, this ends up being recursive (cg_json_misc_attr can end up calling back to cg_json_misc_attrs). Attributes are actually quite flexible. Let's look at the helpers that will be used to do this formatting. From charbuf.h: // These helpers push a buffer and use it for the output temporarily. // When the buffer is finished (at END_INDENT) bindent is used to // indent it by the indicated amount. They assume the output buffer is called // &quot;output&quot;. #define BEGIN_INDENT(name, level) \\ charbuf *name##_saved = output; \\ int32_t name##_level = level; \\ CHARBUF_OPEN(name); \\ output = &amp;name; #define END_INDENT(name) \\ output = name##_saved; \\ bindent(output, &amp;name, name##_level); \\ CHARBUF_CLOSE(name); \\  BEGIN_INDENT : sets up the indenting save the current output bufferstash the desired indent level in a named localmake a new scratch buffer using the given nameset the output to be the scratch buffer END_INDENT : flushes the indented stuff restores the output buffer to what it waswrites the temporary buffer into the output buffer, indenting it by the desired abountclose the temporrary buffer bindent : a charbuf helper that reads the input line by line and writes it with indenting spaces to the output The rest of the helpers manage the commas in the (nested) lists: // These little helpers are for handling comma seperated lists where you may or may // not need a comma in various places. The local tracks if there is an item already // present and you either get &quot;,\\n&quot; or just &quot;\\n&quot; as needed. #define BEGIN_LIST bool_t list_start = 1 #define CONTINUE_LIST bool_t list_start = 0 #define COMMA if (!list_start) bprintf(output, &quot;,\\n&quot;); else list_start = 0 #define END_LIST if (!list_start) bprintf(output, &quot;\\n&quot;)  BEGIN_LIST : starts a list, records that we are at the beginning of the listCONTINUE_LIST : starts a list, but assumes things have already been put into itCOMMA : a new item is about to be emitted, add a comma if one is needed i.e. add a comma if we are not on the first item END_LIST : emits a blank line if anything went into the list this puts us in the write place to put an end marker such as ']' or '}' So reviewing this bit of code, emit the attribute name and start the array &quot;[&quot;we start indentingwe start a listwe emit a comma if neededwe emit the new misc attribute this will crack the AST, and get the attribute name and valuethis can recursecg_json_misc_attr is pretty simple and a good exercise for the reader repeat for all attributesend the listend the indentingemit the attribute end &quot;]&quot; Quoted Text​ Most quoted text in the JSON output is either hard-coded constants, or else is a CQL identifier and therefore has no special characters. Those two cases are very simple and no escaping or special formatting is needed. We just emit the text with quotes around it. However, there are cases where general text that might have special characters in it needs to be emitted. When that happens a call like this is used: cg_pretty_quote_plaintext( sql.ptr, output, PRETTY_QUOTE_JSON | PRETTY_QUOTE_SINGLE_LINE);  cg_pretty_quote_plaintext has been discussed before when it was used to create SQL strings for the C output. This usage is similar. Here we're using PRETTY_QUOTE_JSON to indicate that only escape sequences supported by JSON should appear in the output. The format for hexadecimal escape sequences for non-printable characters is different than C and some of the C short escapes are not supported (e.g. &quot;\\a&quot; is not legal JSON). We always use PRETTY_QUOTE_SINGLE_LINEin the JSON output so that multi-line SQL is rendered as one line. Remember here we are are JSON-escaping the SQL so the embedded newlines in the original SQL were already converted to '\\' 'n' (two characters) and therefore any newlines still in the string are those placed there by the line breaking of the SQL not by newlines in string literals. Hence those newlines are optional, any whitespace will do. In any case, cg_pretty_quote_plaintext is just the function to do what we need and this output is only slightly different than what would be emitted for the C codegen. "},{"title":"Dependency Analysis​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#dependency-analysis","content":"There are a number of places where dependencies have to be computed. To do this job, this function is used universally: // For procedures and triggers we want to walk the statement list and emit a set // of dependency entries that show what the code in question is using and how. // We track tables that are used and if they appear in say the FROM clause // (or some other read-context) or if they are the subject of an insert, update, // or delete. We also track the use of nested procedures and produce a list of // procs the subject might call. Of course no proc calls ever appear in triggers. static void cg_json_dependencies(charbuf *output, ast_node *ast) { ... }  In general this code walks any AST looking for a variety of patterns in the AST that correspond to use of tables, directly or indirectly. Actually more accurately,cg_json_dependencies uses find_table_refs to do the job, and it does so by: creating an output buffer for each kind of thing find_table_refs might findsetting up a simple callback to fill in the bufferinvoking find_table_refsformatting the buffers that have any resulting dependency data and emitting them as dependencies This works for any kind of AST really, though typically you do this for procedures or triggers because they have an interesting body. But the analysis also makes sense for views because views can refer to other views and to tables. The primary code looks like this:  table_callbacks callbacks = { .callback_any_table = cg_found_table, .callback_any_view = cg_found_view, .callback_inserts = cg_found_insert, .callback_updates = cg_found_update, .callback_deletes = cg_found_delete, .callback_from = cg_found_from, .callback_proc = cg_found_proc, .callback_context = &amp;context, }; find_table_refs(&amp;callbacks, ast);  And an example callback: // This is the callback function that tells us a view name was found in the body // of the stored proc we are currently examining. The void context information // is how we remember which proc we were processing. For each table we have // a character buffer. We look it up, create it if not present, and write into it. // We also write into the buffer for the current proc which came in with the context. static void cg_found_view( CSTR view_name, ast_node* table_ast, void* pvContext) { json_context *context = (json_context *)pvContext; Contract(context-&gt;cookie == cookie_str); // sanity check Contract(context-&gt;used_views); add_name_to_output(context-&gt;used_views, view_name); }  The callback gets the pvContext back, which is the context local variable from cg_json_dependencies. This has all the buffers in it. All we have to do is add the name to the buffer, which is done as follows: static void add_name_to_output(charbuf* output, CSTR table_name) { Contract(output); if (output-&gt;used &gt; 1) { bprintf(output, &quot;, &quot;); } bprintf(output, &quot;\\&quot;%s\\&quot;&quot;, table_name); }  add a comma if neededadd the namedone :D Note: The added name of course doesn't have to be a table name, but it usually is. So we can see that find_table_refs will tell us the kind of thing it found and the name of the thing. When all this is done each kind of dependency is emitted if it exists, like so:  if (used_views.used &gt; 1) { bprintf(output, &quot;,\\n\\&quot;usesViews\\&quot; : [ %s ]&quot;, used_views.ptr); }  This gives us a quoted list of the dependencies. Now, how do we find these? Walking the AST for Dependencies​ find_table_refs is a fairly simple tree walk that looks for certain key patterns actually the tree walk happens in find_table_node which looks for tables and procedure calls in the nested AST. find_table_refs records the callbacks that were specified, and it makes some symbol tables so that the same table/view/procedure is not reported twice. After that it starts walking the AST recursively looking for the patterns. Here's an example:  // Recursively finds table nodes, executing the callback for each that is found. The // callback will not be executed more than once for the same table name. static void find_table_node(table_callbacks *callbacks, ast_node *node) { // Check the type of node so that we can find the direct references to tables. We // can't know the difference between a table or view in the ast, so we will need to // later find the definition to see if it points to a create_table_stmt to distinguish // from views. find_ast_str_node_callback alt_callback = NULL; symtab *alt_visited = NULL; ast_node *table_or_view_name_ast = NULL; ... else if (is_ast_delete_stmt(node)) { EXTRACT_ANY_NOTNULL(name_ast, node-&gt;left); table_or_view_name_ast = name_ast; alt_callback = callbacks-&gt;callback_deletes; alt_visited = callbacks-&gt;visited_delete; } ... }  The code afterward will do these steps: notice that table_or_view_name_ast was set, hence something was founddetermine that it is in fact a tablecall the general callback for any table seen (but only once for this table)call the alternate callback that this is a table being deleted (but only once) Almost all the other operations work similarly: table_or_view_name_ast is setalt_callback is called but only ifalt_visited doesn't already have the symbol The exception to the above is the processing that's done for procedure calls. We've actually only talked about table dependencies so far but, additionally, any procedure includes dependencies on the procedures it calls. If a procedure call is found then callbacks-&gt;callback_proc is used andcallbacks-&gt;visited_proc verifies that there are no duplicates. So much the same except the names are procedure names. Note that the code does not do transitive closure of procedure calls because in general the called procedure is likely in a different translation unit. However with the direct calls in place it is easy enough to do transitive closure from the JSON if you do have all the procedures in one unit or if you have several JSON results from different compilations. However, when a view is encountered, the code does follow into the view body and recursively reports what the view uses. This means that the reported tables do include any tables that were used indirectly via views. Finally, any CTEs that are used will not be reported becausefind_table_or_view_even_deleted will fail for a CTE. However the body of the CTE is processed so while the CTE name does not appear, what the CTE uses does appear, just like any other table usage. "},{"title":"Additional Test Output​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#additional-test-output","content":"The extra test output is simply a reverse index: a mapping that goes from any table to the procedures that depend on that table. The mapping can easily be created by processing the JSON for procedures, each such procedure includes its dependency information. As a result it's only used for additional validation. "},{"title":"Recap​","type":1,"pageTitle":"Part 7: JSON Generation","url":"/cql-guide/int07#recap","content":"The JSON output produced by cg_json_schema.c is similar to other codegen output but lacks most of the complexities. It deals largely with the declared schema and the declared procedures and their parameters. Most of the output it needs to produce is well supported by the normal text emission features in the compiler and so we end up with a very straightforward walk of the AST, visiting each of the relevant kinds of nodes in order. Topics covered included: the types of output that will be producedthe general structure of the main JSON emitteran example emittertypical formatting features necessary to produce good quality JSONa tour of the dependency emitter As with the other parts, no attempt was made to cover every function in detail. That is best done by reading the source code. But there is overall structure here and an understanding of the basic principles is helpful before diving into the source code. "},{"title":"Appendix 1: Command Line Options","type":0,"sectionRef":"#","url":"/cql-guide/x1","content":"","keywords":""},{"title":"With No Options​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#with-no-options","content":"emits a usage message "},{"title":"--in file​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--in-file","content":"reads the given file for the input instead of stdinthe input should probably have already been run through the C pre-processor as abovereturns non-zero if the file fails to parse Example: cql --in test.sql  "},{"title":"--sem​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--sem","content":"performs semantic analysis on the input file ONLYthe return code is zero if there are no errors Example: cql --in sem_test.sql --sem  "},{"title":"--ast​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--ast","content":"walks the AST and prints it to stdout in human readable text formmay be combined with --sem (semantic info will be included) Example cql --in sem_test.sql --sem --ast &gt;sem_ast.out  "},{"title":"--echo​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--echo","content":"walks the AST and emits the text of a program that would create itthis has the effect of &quot;beautifying&quot; badly formatted input or &quot;canonicalizing&quot; it some sensible indenting is added but it might not be the original indentingextraneous whitespace, parens, etc. are removed may be combined with --sem (in which case you see the source after any rewrites for sugar)this also validates that the input can be parsed Example cql --in test.sql --echo &gt;test.out # test.out is &quot;equivalent&quot; to test.sql  "},{"title":"--dot​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--dot","content":"prints the internal AST to stdout in DOT format for graph visualizationthis is really only interesting for small graphs for discussion as it rapidly gets insane Example: cql --dot --in dottest.sql  "},{"title":"--cg output1 output2 ...​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--cg-output1-output2-","content":"any number of output files may be needed for a particular result type, two is commonthe return code is zero if there are no errorsany --cg option implies --sem Example: cql --in foo.sql --cg foo.h foo.c  "},{"title":"--nolines​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--nolines","content":"Suppress the # directives for lines. Useful if you need to debug the C code. Example: cql --in test.sql --nolines --cg foo.h foo.c  "},{"title":"--global_proc name​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--global_proc-name","content":"any loose SQL statements not in a stored proc are gathered and put into a procedure of the given namewhen generating a schema migrate script the global proc name is used as a prefix on all of the artifacts so that there can be several independent migrations linked into a single executable "},{"title":"--compress​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--compress","content":"for use with the C result type, (or any similar types added to the runtime array in your compiler)string literals for the SQL are broken into &quot;fragments&quot; the DML is then represented by an array of fragmentssince DML is often very similar there is a lot of token sharing possiblethe original string is recreated at runtime from the fragments and then executedcomments show the original string inline for easier debugging and searching NOTE: different result types require a different number of output files with different meanings "},{"title":"--test​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--test","content":"some of the output types can include extra diagnostics if --test is includedthe test output often makes the outputs badly formed so this is generally good for humans only "},{"title":"--dev​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--dev","content":"some codegen features only make sense during development, this enables dev mode to turn those one ** example: explain query plan "},{"title":"--c_include_namespace​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--c_include_namespace","content":"for the C codegen runtimes, it determines the header namespace (as in #include &quot;namespace/file.h&quot;) that goes into the output C fileif this option is used, it is prefixed to the first argment to --cg to form the include path in the C fileif absent there is no &quot;namespace/&quot; prefix "},{"title":"--c_include_path​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--c_include_path","content":"for the C codegen runtimes, it determines the full header path (as in #include &quot;your_arg&quot;) that goes into the output C fileif this option is used, the first argment to --cg controls only the output path and does not appear in include path at allthis form overrides --c_include_namespace if both are specified "},{"title":"--objc_c_include_path​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#--objc_c_include_path","content":"for ObjC codegen runtimes that need to refer to the generated C code, this represents the header of the C generated code that will be used during inclusion from the ObjC file "},{"title":"Result Types (--rt *)​","type":1,"pageTitle":"Appendix 1: Command Line Options","url":"/cql-guide/x1#result-types---rt-","content":"These are the various outputs the compiler can produce. --rt c​ requires two output files (foo.h and foo.c)this is the standard C compilation of the sql file --cqlrt foo.h​ emits #include &quot;foo.h&quot; into the C output instead of #include &quot;cqlrt.h&quot; --generate_type_getters​ changes C output for CQL result sets so that the field readers used shared functions to get fields of a certain typethis style of codegen makes result-sets more interoperable with each other if they have similar shape so it can be useful --generate_exports​ adds an additional output fileexample: `--in foo.sql --generate_exports --rt c --cg foo.h foo.c foo_exports.sqlthe output file foo_exports.sql includes procedure declarations for the contents of foo.sqlbasically automatically generates the CQL header file you need to access the procedures in the input from some other fileif it were C it would be like auto-generating foo.h from foo.c --rt objc​ objective C wrappers for result sets produced by the stored procedures in the inputthese depend on the output of a standard codegen run so this is additiverequires one output file (foo.h) --rt schema​ produces the canonical schema for the given input filesstored procedures etc. are removedwhitespace etc. is removedsuitable for use to create the next or first &quot;previous&quot; schema for schema validationrequires one output file --rt schema_upgrade​ produces a CQL schema upgrade script which can then be compiled with CQL itselfsee the chapter on schema upgrade/migration: Chapter 10requires one output file (foo.sql) --include_regions a b c​ the indicated regions will be declaredused with --rt schema_upgrade or --rt schemain the upgrade case excluded regions will not be themselves upgraded, but may be referred, to by things that are being upgraded --exclude_regions x y z​ the indicated regions will still be declared but the upgrade code will be suppressed, the presumption being that a different script already upgrades x y zused with --rt schema_upgrade --min_schema_version n​ the schema upgrade script will not include upgrade steps for schema older than the version specified --schema_exclusive​ the schema upgrade script assumes it owns all the schema in the database, it aggressively removes other things --rt json_schema​ produces JSON output suitable for consumption by downstream codegenthe JSON includes a definition of the various entities in the inputsee the section on JSON output for details --rt query_plan​ produces CQL output which can be re-compiled by CQL as normal inputthe output consists of a set of procedures that will emit all query plans for the DML that was in the inputsee also --rt udf and Chapter 15 --rt stats​ produces a simple .csv file with node count information for AST nodes per procedure in the inputrequires one output file (foo.csv) --rt udf​ produces stub UDF implementations for all UDFS that were seen in the inputthis output is suitable for use with --rt query_plan so that SQL with UDFs will run in a simple contextrequires two output files (e.g. udfs.h and udfs.c)See also Chapter 15 "},{"title":"Part 8: Test Helpers","type":0,"sectionRef":"#","url":"/cql-guide/int08","content":"","keywords":""},{"title":"Preface​","type":1,"pageTitle":"Part 8: Test Helpers","url":"/cql-guide/int08#preface","content":"Part 8 continues with a discussion of the Test Helper generation code. As in the previous sections, the goal here is not to go over every detail but rather to give a sense of how helpers are created in general -- the core strategies and implementation choices -- so that when reading the source you will have an idea how it all hangs together. "},{"title":"Test Helpers​","type":1,"pageTitle":"Part 8: Test Helpers","url":"/cql-guide/int08#test-helpers","content":"The testability features are described in Chapter 12 of the Guide So, we won't be discussing all the details of what can be created. Instead we're going to go over the theory of how the generator works. This generator is somewhat different than others in that it only concerns itself with procedures and only those that have been suitably annotated -- there are large parts of the tree that are of no interest to the test helper logic, including, importantly the body of procedures. Only the signature matters. As we'll see there is a fairly large family of generators that are like this. We'll have one section for every kind of output, but really only the dummy_test helper is worthy of detailed discussion the others, as we'll see, are very simple. "},{"title":"Initialization​","type":1,"pageTitle":"Part 8: Test Helpers","url":"/cql-guide/int08#initialization","content":"The generator is wired like the others with a suitable main, this one is pretty simple: // Main entry point for test_helpers cql_noexport void cg_test_helpers_main(ast_node *head) { Contract(options.file_names_count == 1); cql_exit_on_semantic_errors(head); exit_on_validating_schema(); cg_test_helpers_reset_globals(); CHARBUF_OPEN(output_buf); cg_th_output = &amp;output_buf; bprintf(cg_th_output, &quot;%s&quot;, rt-&gt;source_prefix); cg_test_helpers_stmt_list(head); cql_write_file(options.file_names[0], cg_th_output-&gt;ptr); CHARBUF_CLOSE(output_buf); cg_test_helpers_reset_globals(); }  The text output will be ultimately put into output_buf defined here and helper_flags will track which kinds of helpers we saw. This helps us to emit the right sections of output as we'll see. The code iterates the AST looking at the top level statement list only and in particular looking for CREATE PROCstatements. // Iterate through statement list static void cg_test_helpers_stmt_list(ast_node *head) { Contract(is_ast_stmt_list(head)); init_all_trigger_per_table(); init_all_indexes_per_table(); CHARBUF_OPEN(procs_buf); CHARBUF_OPEN(decls_buf); cg_th_procs = &amp;procs_buf; cg_th_decls = &amp;decls_buf; for (ast_node *ast = head; ast; ast = ast-&gt;right) { EXTRACT_STMT_AND_MISC_ATTRS(stmt, misc_attrs, ast); if (is_ast_create_proc_stmt(stmt)) { EXTRACT_STRING(proc_name, stmt-&gt;left); cg_test_helpers_create_proc_stmt(stmt, misc_attrs); } } bprintf(cg_th_output, &quot;%s&quot;, decls_buf.ptr); bprintf(cg_th_output, &quot;\\n&quot;); bprintf(cg_th_output, &quot;%s&quot;, procs_buf.ptr); CHARBUF_CLOSE(decls_buf); CHARBUF_CLOSE(procs_buf); symtab_delete(all_tables_with_triggers); all_tables_with_triggers = NULL; symtab_delete(all_tables_with_indexes); all_tables_with_indexes = NULL; }  There are some preliminaries: we make a symbol table that maps from tables names to the list of triggers on that table by walking all the triggerswe make a symbol table that maps from tables names to the list of indices on that table by walking all the indiceswe'll need two buffers one for declarations (that must go first) and one for procedure bodieseach CREATE PROC statement potentially contributes to both sectionscg_test_helpers_create_proc_stmt checks for the helper attributes and sets up the dispatch to emit the test helpers To do this we have to walk any misc attributes on the procedure we're looking for things of the form @attribute(cql:autotest=xxx) static void cg_test_helpers_create_proc_stmt(ast_node *stmt, ast_node *misc_attrs) { Contract(is_ast_create_proc_stmt(stmt)); if (misc_attrs) { helper_flags = 0; dummy_test_infos = symtab_new(); find_misc_attrs(misc_attrs, test_helpers_find_ast_misc_attr_callback, stmt); symtab_delete(dummy_test_infos); dummy_test_infos = NULL; } }  find_misc_attrs calls test_helpers_find_ast_misc_attr_callback. We're going to keep track of which kinds of helpers we have found to help us with the output. This is where helper_flagscomes in. The flags are: #define DUMMY_TABLE 1 // dummy_table attribute flag #define DUMMY_INSERT 2 // dummy_insert attribute flag #define DUMMY_SELECT 4 // dummy_select attribute flag #define DUMMY_RESULT_SET 8 // dummy_result_set attribute flag #define DUMMY_TEST 0x10 // dummy_test attribute flag  And now we're ready for actual dispatch: // This is invoked for every misc attribute on every create proc statement // in this translation unit. We're looking for attributes of the form cql:autotest=(...) // and we ignore anything else. static void test_helpers_find_ast_misc_attr_callback( CSTR _Nullable misc_attr_prefix, CSTR _Nonnull misc_attr_name, ast_node *_Nullable ast_misc_attr_value_list, void *_Nullable context) { ast_node *stmt = (ast_node *)context; Contract(is_ast_create_proc_stmt(stmt)); if (misc_attr_prefix &amp;&amp; misc_attr_name &amp;&amp; !Strcasecmp(misc_attr_prefix, &quot;cql&quot;) &amp;&amp; !Strcasecmp(misc_attr_name, &quot;autotest&quot;)) { ... } }  The main dispatch looks like this: // In principle, any option can be combined with any other but some only make sense for procs with // a result. EXTRACT_STRING(autotest_attr_name, misc_attr_value); if (is_autotest_dummy_test(autotest_attr_name)) { cg_test_helpers_dummy_test(stmt); } // these options are only for procs that return a result set if (has_result_set(stmt) || has_out_stmt_result(stmt) || has_out_union_stmt_result(stmt)) { if (is_autotest_dummy_table(autotest_attr_name)) { helper_flags |= DUMMY_TABLE; cg_test_helpers_dummy_table(proc_name); } else if (is_autotest_dummy_insert(autotest_attr_name)) { helper_flags |= DUMMY_INSERT; cg_test_helpers_dummy_insert(proc_name); } else if (is_autotest_dummy_select(autotest_attr_name)) { helper_flags |= DUMMY_SELECT; cg_test_helpers_dummy_select(proc_name); } else if (is_autotest_dummy_result_set(autotest_attr_name)) { helper_flags |= DUMMY_RESULT_SET; cg_test_helpers_dummy_result_set(proc_name); } }  Most of these options are very simple indeed. cg_test_helpers_dummy_test is the trickiest by far and we'll save it for last, let's dispense with the easy stuff. "},{"title":"Dummy Table, Dummy Insert, Dummy Select, Dummy Result Set​","type":1,"pageTitle":"Part 8: Test Helpers","url":"/cql-guide/int08#dummy-table-dummy-insert-dummy-select-dummy-result-set","content":"All of these are a very simple template. The language includes just the right features to emit these procedures as nearly constant strings. The LIKE construct was literally designed to make these patterns super simple. You can see all the patterns in Chapter 12 but let's look at the code for the first one. This is &quot;dummy table&quot;. // Emit an open proc which creates a temp table in the form of the original proc // Emit a close proc which drops the temp table static void cg_test_helpers_dummy_table(CSTR name) { bprintf(cg_th_procs, &quot;\\n&quot;); bprintf(cg_th_procs, &quot;CREATE PROC open_%s()\\n&quot;, name); bprintf(cg_th_procs, &quot;BEGIN\\n&quot;); bprintf(cg_th_procs, &quot; CREATE TEMP TABLE test_%s(LIKE %s);\\n&quot;, name, name); bprintf(cg_th_procs, &quot;END;\\n&quot;); bprintf(cg_th_procs, &quot;\\n&quot;); bprintf(cg_th_procs, &quot;CREATE PROC close_%s()\\n&quot;, name); bprintf(cg_th_procs, &quot;BEGIN\\n&quot;); bprintf(cg_th_procs, &quot; DROP TABLE test_%s;\\n&quot;, name); bprintf(cg_th_procs, &quot;END;\\n&quot;); }  The purpose of this is to create helper functions that can create a temporary table with the same columns in it as the procedure you are trying to mock. You can then select rows out of that table (with dummy_select) or insert rows into the table (with dummy_insert). Or you can make a single row result set (often enough) with dummy_result_set. As we can see we simply prepend open_ to the procedure name and use that to create a test helper that make the temporary table. The table's columns are defined to be LIKE the result shape of the procedure under test. Recall this helper is only available to procedures that return a result set. The temporary table gets a test_ prefix. Assuming the procedure with the annotation is foo then this code is universal: CREATE TEMP TABLE test_foo(LIKE foo);  Is universal, no matter the result shape of foo you get a table with those columns. For this to work we need to emit a declaration of foo before this code. However, since we have the full definition of foo handy that is no problem. We remember that we'll need it by setting a flag in helper_flags. The code for close_foo is even simpler if that's possible. The great thing is all need to know the columns of foo has been removed from the test helper. The CQL compiler handles this as a matter of course and it is generally useful. See Chapter 5for more examples. All the others are equally simple and use similar tricks. These were the first test helpers. They're actually not that popular because they are so easy to create yourself anyway. "},{"title":"Dummy Test​","type":1,"pageTitle":"Part 8: Test Helpers","url":"/cql-guide/int08#dummy-test","content":"The dummy test code emitter is non-trivial. Let's quickly review the things it has to do and then we can go over how each of these is accomplished. Assuming we have an procedureyour_proc that has been annotated like this: @attribute(cql:autotest=(dummy_test)) create proc your_proc(..args...) begin -- assorted references to tables and views end;  Dummy test will produce the following: test_your_proc_create_tables a procedure that creates all the tables and views that your_proc needs test_your_proc_drop_tables a procedure that drops those same tables and views test_your_proc_create_indexes a procedure that creates your indices, in a test you may or may not want to create the indices test_your_proc_drop_indexes a procedure the drops those same indices test_your_proc_create_triggers a procedure that creates your trigger, in a test you may or may not want to create the triggers test_your_proc_drop_triggers a procedure the drops those same triggers test_your_proc_read_table1 for each table or view in the create_tables a procedure that selects all the data out of that object is created in case you need it test_your_proc_populate_tables a procedure that loads all the tables from create_tables with sample dataFK relationships are obeyeduser data may be specified in an attribute and that data will be used in preference to auto-generated data These are more fully discussed in Chapter 12. Building the Trigger and Index mappings​ In order to know which indices and triggers we might need we have to be able to map from the tables/views in your_proc to the indices. To set up for this a general purpose reverse mapping is created. We'll look at the triggers version. The indices version is nearly identical. // Walk through all triggers and create a dictionnary of triggers per tables. static void init_all_trigger_per_table() { Contract(all_tables_with_triggers == NULL); all_tables_with_triggers = symtab_new(); for (list_item *item = all_triggers_list; item; item = item-&gt;next) { EXTRACT_NOTNULL(create_trigger_stmt, item-&gt;ast); EXTRACT_NOTNULL(trigger_body_vers, create_trigger_stmt-&gt;right); EXTRACT_NOTNULL(trigger_def, trigger_body_vers-&gt;left); EXTRACT_NOTNULL(trigger_condition, trigger_def-&gt;right); EXTRACT_NOTNULL(trigger_op_target, trigger_condition-&gt;right); EXTRACT_NOTNULL(trigger_target_action, trigger_op_target-&gt;right); EXTRACT_ANY_NOTNULL(table_name_ast, trigger_target_action-&gt;left); EXTRACT_STRING(table_name, table_name_ast); if (create_trigger_stmt-&gt;sem-&gt;delete_version &gt; 0) { // dummy_test should not emit deleted trigger continue; } symtab_append_bytes(all_tables_with_triggers, table_name, &amp;create_trigger_stmt, sizeof(create_trigger_stmt)); } }  The steps are pretty simple: we make a symbol table that will map from the table name to an array of statementsthere is a convenient all_triggers list that has all the triggersfrom each trigger we EXTRACT the table or view name (named table_name even if it's a view)we append the trigger statement pointer to the end of such statements for the tableany triggers marked with @delete are not included for obvious reasons At the end of this looking up the table name will give you a list of trigger statement AST pointers. From there of course you can get everything you need. The index version is basically the same, the details of the EXTRACT ops to go from index to table name are different and of course we start from the all_indices_list Computing The Dependencies of a Procedure​ Sticking with our particular example, in order to determine that tables/views that your_proc might need, the generator has to walk its entire body looking for things that are tables. This is handled by thefind_all_table_nodes function. static void find_all_table_nodes(dummy_test_info *info, ast_node *node) { table_callbacks callbacks = { .callback_any_table = found_table_or_view, .callback_any_view = found_table_or_view, .callback_context = info, .notify_table_or_view_drops = true, .notify_fk = true, .notify_triggers = true, }; info-&gt;callbacks = &amp;callbacks; find_table_refs(&amp;callbacks, node); // stitch the views to the tables to make one list, views first for (list_item *item = info-&gt;found_views; item; item = item-&gt;next) { if (!item-&gt;next) { item-&gt;next = info-&gt;found_tables; info-&gt;found_tables = info-&gt;found_views; break; } } // this shouldn't be used after it's been linked in info-&gt;found_views = NULL; }  This code uses the general dependency walker in cg_common.c to visit all tables and views. It is a recursive walk and the general steps for prosecution go something like this: starting from your_proc the entire body of the procedure is visitedreferences to tables or views in update, delete, insert, select etc. statements are identifiedeach such table/view is added to the found tables list (at most once)for views, the recursion proceeds to the body of the view as though the body had been inline in the procedurefor tables, the recursion proceeds to the body of the table to discover any FK relationships that need to be followedif any found item has triggers, the trigger body is walked, any tables/views mentioned there become additional found itemsany given table/view and hence trigger is only visited once The net of all this, the &quot;found items&quot;, is a list of all the tables and views that the procedure uses, directly or indirectly. As discussed in Chapter 12this walk does not include tables and views used by procedures that your_proc calls. To get the dependencies in the correct order, the tables have been walked following the foreign key chain and all views go after all tables. The views are stitched together. The business of diving into views/tables/triggers and maintainence of the found items is done by the callback function found_table_or_view. The actual source is more descriptive comments than code but the code is included here as it is brief. // comments elided for brevity, the why of all this is described in the code static void found_table_or_view( CSTR _Nonnull table_or_view_name, ast_node *_Nonnull table_or_view, void *_Nullable context) { Contract(table_or_view); dummy_test_info *info = (dummy_test_info *)context; bool deleted = table_or_view-&gt;sem-&gt;delete_version &gt; 0; if (!deleted) { continue_find_table_node(info-&gt;callbacks, table_or_view); if (is_ast_create_view_stmt(table_or_view)) { add_item_to_list(&amp;info-&gt;found_views, table_or_view); } else { add_item_to_list(&amp;info-&gt;found_tables, table_or_view); } find_all_triggers_node(info, table_or_view_name); } }  The general purpose walker notifies exactly once on each visited table/view and continue_find_table_node is used to dive into the bodies of views/tables that would otherwise not be searched. Likewise find_all_triggers_nodedives into the body of any triggers that are on the found item. Emitting Indices and Triggers​ With the &quot;found tables&quot; computed (creatively stored in a field called found_tables) it's very easy to loop over these and generate the necessary indices for each found table (keeping in mind the &quot;found table&quot; can be a view). The create index statement is emitted by the usual gen_statement_with_callbacks form that echos the AST. The drop index can be trivially created by name. // Emit create and drop index statement for all indexes on a table. static void cg_emit_index_stmt( CSTR table_name, charbuf *gen_create_indexes, charbuf *gen_drop_indexes, gen_sql_callbacks *callback) { symtab_entry *indexes_entry = symtab_find(all_tables_with_indexes, table_name); bytebuf *buf = indexes_entry ? (bytebuf *)indexes_entry-&gt;val : NULL; ast_node **indexes_ast = buf ? (ast_node **)buf-&gt;ptr : NULL; int32_t count = buf ? buf-&gt;used / sizeof(*indexes_ast) : 0; gen_set_output_buffer(gen_create_indexes); for (int32_t i = 0; i &lt; count; i++) { ast_node *index_ast = indexes_ast[i]; EXTRACT_NOTNULL(create_index_stmt, index_ast); EXTRACT_NOTNULL(create_index_on_list, create_index_stmt-&gt;left); EXTRACT_ANY_NOTNULL(index_name_ast, create_index_on_list-&gt;left); EXTRACT_STRING(index_name, index_name_ast); gen_statement_with_callbacks(index_ast, callback); bprintf(gen_create_indexes, &quot;;\\n&quot;); bprintf(gen_drop_indexes, &quot;DROP INDEX IF EXISTS %s;\\n&quot;, index_name); } }  Triggers are done in exactly the same way except that instead of looping over found tables we can actually generate them as they are discovered inside of find_all_triggers_node. Recal that we had to visit the triggers when computing the found tables anyway. We did not have to visit the indices hence the difference. These walks allow us to produce: test_your_proc_create_indexes, test_your_proc_drop_indexes, test_your_proc_create_triggers, test_your_proc_drop_triggers Emitting Tables and Views​ Starting from the found tables, again it is very easy to generate the code to create and drop the tables and views. The only trick here is that the tables depend on one another so order is important. The tables are discovered with the deepest dependency first, new found items are added to the head of the found tables but it's a post-order walk so that means that the deepest tables/views are at the front of the list. This means the list is naturally in the order that it needs to be to delete the tables (parent tables at the end). So the algorithm goes like this: emit the drop tables/views in the found orderreverse the listemit the create tables/views in the reverse orderfor each table/view emit the reader `testyour_proc_read[item]for tables we emit an insertion fragment into test_your_proc_populate_tables using cg_dummy_test_populate population is discussed in the following sections As in the other cases gen_statement_with_callbacks is used to create the DDL statements: CREATE TABLECREATE VIEWCREATE VIRTUAL TABLE The delete side is easily created with ad hoc DROP TABLE or DROP VIEW statements. The reading procedure is always of the form SELECT * FROM foo so that too is trivial to generate with a fixed template. The &quot;echoing&quot; system once again is doing a lot of the heavy lifting. These walks give us test_your_proc_create_tables, test_your_proc_drop_tables, and test_your_proc_read_[item] and drive the population process Gathering Ad Hoc Data To Be Inserted​ Before we get into the mechanics of the population code, we have to visit one more area. It's possible to include data in the thedummy_test annotaiton itself. This is data that you want to have populated. This data will be included in the overall data populator. If there is enough of it (at least 2 rows per candidate table) then it might be all the data you get. Now the data format here is not designed to be fully general, after all it's not that hard to just write INSERT ... VALUES for all your tables anyway. The goal is to provide something that will help you not have to remember all the FK relationships and maybe let you economically specify some leaf data you need and get the rest for free. It's also possible to manually create dummy data that just won't work, again, scrubbing all this is way beyond the ability of a simple test helper. When the code runs you'll get SQLite errors which can be readily addressed. So keeping in mind this sort of &quot;entry level data support&quot; as the goal, we can take a look at how the system works -- it's all in the function collect_dummy_test_info which includes this helpful comment on structure. // the data attribute looks kind of like this: // @attribute(cql:autotest = ( // .. other auto test attributes // (dummy_test, // (table_name1, (col1, col2), (col1_val1, col2_val1), (col1_val2, col2_val2) ), // (table_name2, (col1, col2), (col1_val1, col2_val1), (col1_val2, col2_val2) ), // ... // ) // .. other auto test attributes // )) // // we're concerned with the dummy_test entries here, they have a very specific format // i.e. first the table then the column names, and then a list of matching columns and values  So we're going to walk a list of attributes each one begins with a table name, then a list of columns, and then a list of values. All of the data is in the symbol table dummy_test_infos which is indexed by table name. For each table name we find we ensure there is a symbol table at that slot. So dummy_test_infos is a symbol table of symbol tables. It's actually going to be something like value_list = dummy_test_infos['table']['column']  // collect table name from dummy_test info ast_node *table_list = dummy_attr-&gt;left; EXTRACT_STRING(table_name, table_list-&gt;left); symtab *col_syms = symtab_ensure_symtab(dummy_test_infos, table_name);  Next we're going to find the column names, they are the next entry in the list so we go right to get the column_name_list // collect column names from dummy_test info ast_node *column_name_list = table_list-&gt;right; for (ast_node *list = column_name_list-&gt;left; list; list = list-&gt;right) { EXTRACT_STRING(column_name, list-&gt;left); sem_t col_type = find_column_type(table_name, column_name); bytebuf *column_values = symtab_ensure_bytebuf(col_syms, column_name); // store the column meta data, create space to hold values in databuf bytebuf_append_var(&amp;col_data_buf, column_values); bytebuf_append_var(&amp;col_type_buf, col_type); bytebuf_append_var(&amp;col_name_buf, column_name); }  The primary purpose of this part of the loop is then to add the column names to col_syms so that they are linked to the dummy info for this table. The line bytebuf *column_values = symtab_ensure_bytebuf(col_syms, column_name); does this. And this also creates the byte buffer that will hold the eventual values. We also keep a side set of buffers that has the column name, type, and the values in the col_name, col_type, and col_data buffers respectively. These are used to handle the foreign key work shortly and they allow us to not have to look up all the names over and over. // collect column value from dummy_test info. We can have multiple rows of column value for (ast_node *values_ast = column_name_list-&gt;right; values_ast; values_ast = values_ast-&gt;right) { int32_t column_index = 0; // collect one row of column value for (ast_node *list = values_ast-&gt;left; list; list = list-&gt;right) { ast_node *misc_attr_value = list-&gt;left; Contract(col_data_buf.used); bytebuf *column_values = ((bytebuf **) col_data_buf.ptr)[column_index]; sem_t column_type = ((sem_t *) col_type_buf.ptr)[column_index]; CSTR column_name = ((CSTR *) col_name_buf.ptr)[column_index]; bytebuf_append_var(column_values, misc_attr_value); column_index++; ...foreign key stuff goes here... } .. some cleanup }  The most important part is bytebuf_append_var(column_values, misc_attr_value); this is where the attribute value is added to the list of values that are on the column. Finally, the &quot;foreign key stuff&quot;. What we need to do here is check the column name in the table to see if it's part of a foreign key and if it is we recursively add the current data value to the referenced column in the reference table. That way if you add an initalizer to a leaf table you don't also have to add it to all the parent tables. If it wasn't for this feature the manual data wouldn't be very useful at all, hand written INSERT statements would be just as good. // If a column value is added to dummy_test info for a foreign key column then // we need to make sure that same column value is also added as a value in the // the referenced table's dummy_test info. // e.g. // create table A(id integer primary key); // create table B(id integer primary key references A(id)); // // If there is sample data provided for B.id then we must also ensure that // the value provided for B.id is also add as a sample row in A with the same // value for id. if (is_foreign_key(column_type)) { add_value_to_referenced_table(table_name, column_name, column_type, misc_attr_value); }  When this is a done all of the initializers will have been added to the appropriate column of the appropriate table. Again the overall structure is something like: value_list = dummy_test_infos['table']['column'] Emitting the Table Population Fragments​ With any custom initalizers in the dummy_test_infos structure we can do the population fragment for any given table. The general algorithm here goes like this: the total number of rows we will generate will be the number of column values in the initializers or else DUMMY_TEST_INSERT_ROWS, whichever is largerthe insert statement generated will include dummy_seed([value_seed]) where value_seed starts at 123 and goes up 1 for every row generated dummy_seed will create values for any missing columns using the seed so any combination of included columns is ok, we'll always get a complete insert foreign key columns use a provided intializer from the parent table if there is one, or else they use 1, 2, 3 etc. likewise if a column is referenceable by some other table it uses the known sequence 1, 2, 3 etc. for its value rather than the varying seedin this way child tables can know that partent tables will have a value they can use since both tables will have at least DUMMY_TEST_INSERT_ROWS and any rows that were not manually initialized will matchnote that foreign key columns always get this treatment, whether they were mentioned or not to mix things up the dummy_nullables and dummy_defaults are added on every other row which makes missing values be NULL and/or the default value if one is present This is enough to generate a set of insert statements for the table in question and since the fragments are generated in the table creation order the resulting insert statements will have the parent tables first so the foreign keys of later tables will be correct. This can go wrong if the manual initializations use keys that conflict with the default generation or if the manual intializations have PK conflicts or other such things. No attempt is made to sort that out. The run time errors should be clear and these are, after all, only test helpers. It's very easy to avoid these hazards and you get a pretty clear error message if you don't so that seems good enough. These fragments are ultimately combined to make the body of the procedure test_your_proc_populate_tables. "},{"title":"Recap​","type":1,"pageTitle":"Part 8: Test Helpers","url":"/cql-guide/int08#recap","content":"The test helpers in cg_test_helpers.c are very simple nearly-constant templates with the exception of dummy_test which includes: table and view creationindex creationtrigger creationdata population Topics covered included: how the candidate procedures are discoveredhow the attributes are scanned for test directiveshow each dummy test type is dispatchedhow dummy_test handles data initializationhow dummy_test does its dependency analysis As with the other parts, no attempt was made to cover every function in detail. That is best done by reading the source code. But there is overall structure here and an understanding of the basic principles is helpful before diving into the source code. "},{"title":"Appendix 10: CQL Working Example","type":0,"sectionRef":"#","url":"/cql-guide/x10","content":"","keywords":""},{"title":"Build Steps​","type":1,"pageTitle":"Appendix 10: CQL Working Example","url":"/cql-guide/x10#build-steps","content":"# ${cgsql} refers to the root of the CG/SQL repo % cql --in todo.sql --cg todo.h todo.c % cc -o todo -I${cqsql}/sources main.c todo.c ${cgsql}/sources/cqlrt.c -lsqlite3  "},{"title":"Results​","type":1,"pageTitle":"Appendix 10: CQL Working Example","url":"/cql-guide/x10#results","content":"Note that rowid 2 has been deleted, the leading number is the index in the result set. The rowid is of course the database rowid. % ./todo 0: rowid:1 Buy milk (done) 1: rowid:3 Write code (not done)  "},{"title":"Appendix 11: Production Considerations","type":0,"sectionRef":"#","url":"/cql-guide/x11","content":"","keywords":""},{"title":"Production Considerations​","type":1,"pageTitle":"Appendix 11: Production Considerations","url":"/cql-guide/x11#production-considerations","content":"This system as it appears in the sources here is designed to get some basic SQLite scenarios working but the runtime systems that are packaged here are basic, if only for clarity. There are some important things you should think about improving or customizing for your production environment. Here's a brief list. Concurrency​ The reference counting solution in the stock CQLRT implementation is single threaded. This might be ok, in many environments only one thread is doing all the data access. But if you plan to share objects between threads this is something you'll want to address. CQLRT is designed to be replacable. In fact there is another version included in the distribution cqlrt_cf that is more friendly to iOS and CoreFoundation. This alternate version is an excellent demonstration of what is possible. There are more details in Internals Part 5: CQL Runtime. Statement Caching​ SQLite statement management includes the ability to reset and re-prepare statements. This is an important performance optimization but the stock CQLRT does not take advantage of this. This is for two reasons: first, simplicity, and secondly (though more importantly), any kind of statement cache would require a caching policy and this simple CQLRT cannot possibly know what might consitute a good policy for your application. The following three macros can be defined in your cqlrt.h and they can be directed at a version that keeps a cache of your choice. #ifndef cql_sqlite3_exec #define cql_sqlite3_exec(db, sql) sqlite3_exec((db), (sql), NULL, NULL, NULL) #endif #ifndef cql_sqlite3_prepare_v2 #define cql_sqlite3_prepare_v2(db, sql, len, stmt, tail) sqlite3_prepare_v2((db), (sql), (len), (stmt), (tail)) #endif #ifndef cql_sqlite3_finalize #define cql_sqlite3_finalize(stmt) sqlite3_finalize((stmt)) #endif  As you might expect, prepare creates a statement or else returns one from the cache. When the finalize API is called the indicated statement can be returned to the cache or discarded. The exec API does both of these operations, but also, recall that exec can get a semicolon separated list of statements. Your exec implementation will have to use SQLite's prepare functions to split the list and get prepared statements for part of the string. Alternately, you could choose not to cache in the exec case. Your Underlying Runtime​ As you can see in cqlrt_cf, there is considerable ability to define what the basic data types mean. Importantly, the reference types text, blob, and object can become something different (e.g., something already supported by your environment). For instance, on Windows you could use COM or .NET types for your objects. All object references are substantially opaque to CQLRT; they have comparatively few APIs that are defined in the runtime: things like getting the text out of the string reference and so forth. In addition to the basic types and operations you can also define a few helper functions that allow you to create some more complex object types. For instance, list, set, and dictionary creation and management functions can be readily created and then you can declare them using the DECLARE FUNCTION language features. These objects will then be whatever list, set, or dictionary they need to be in order to interoperate with the rest of your environment. You can define all the data types you might need in your CQLRT and you can employ whatever threading model and locking primitives you need for correctness. Debugging and Tracing​ The CQLRT interface includes some helper macros for logging. These are defined as no-ops by default but, of course, they can be changed. #define cql_contract assert #define cql_invariant assert #define cql_tripwire assert #define cql_log_database_error(...) #define cql_error_trace()  cql_contract and cql_invariant are for fatal errors. They both assert something that is expected to always be true (like assert) with the only difference being that the former is conventionally used to validate preconditions of functions. cql_tripwire is a slightly softer form of assert that should crash in debug builds but only log an error in production builds. It is generally used to enforce a new condition that may not always hold with the goal of eventually transitioning over to cql_contract or cql_invariant once logging has demonstrated that the tripwire is never hit. When a fetch_results method is called, a failure results in a call to cql_log_database_error. Presently the log format is very simple. The invocation looks like this:  cql_log_database_error(info-&gt;db, &quot;cql&quot;, &quot;database error&quot;);  The logging facility is expected to send the message to wherever is appropriate for your environment. Additionally it will typically get the failing result code and error message from SQLite, however these are likely to be stale. Failed queries usually still require cleanup and so the SQLite error codes be lost because (e.g.) a finalize has happened, clearing the code. You can do better if, for instance, your runtime caches the results of recent failed prepare calls. In any case, what you log and where you log it is entirely up to you. The cql_error_trace macro is described in Internals Chapter 3. It will typically invoke printf or fprintf or something like that to trace the origin of thrown exceptions and to get the error text from SQLite as soon as possible. An example might be: #define cql_error_trace() fprintf(stderr, &quot;error %d in %s %s:%d\\n&quot;, _rc_, _PROC_, __FILE__, __LINE_)  Typically the cost of all these diagnostics is too high to include in production code so this is turned on when debugging failures. But you can make that choice for yourself. Customizing Code Generation​ The file rt_common.c defines the common result types, but the skeleton file rt.cincludes affordances to add your own types without having to worry about conflicts with the common types. These macros define #define RT_EXTRAS #define RT_EXTRA_CLEANUP  Simply define these two to create whatever rt_ data structures you want and add any cleanup function that might be needed to release resources. The other cleanup functions should provide a good template for you to make your own. The C data type rtdata includes many text fragments that directly control the code generation. If you want to make your generated code look more like say CoreFoundation you can define an rtdata that will do the job. This will mean a lot of your generated code won't require the #defines for the CQL types, it can use your runtime directly. You can also enable things like Pascal casing for procedure names and a common prefix on procedure names if those are useful in your environment. However, the system is designed so that such changes aren't necessary. The data types in cqlrt.h are enough for any remapping, additional changes with rtdata are merely cosmetic. Summary​ The CQLRT macros are very powerful, they allow you to target almost any runtime with a C API. The cqlrt_cf version is a good example of the sorts of changes you can make. Concurrency and Statement Caching are not supported in the basic version for cqlrt.h. If this is important to you you might want to customize for that. Helper functions for additional data types can be added, and they can be unique to your runtime. There are tracing macros to help with debugability. Providing some useful versions of those can be of great help in production environments. "},{"title":"Appendix 3: Control Directives","type":0,"sectionRef":"#","url":"/cql-guide/x3","content":"Appendix 3: Control Directives The control directives are those statements that begin with @ and they are distinguished from other statements because they influence the compiler rather than the program logic. Some of these are of great importance and discussed elsewhere. The complete list (as of this writing) is: @ENFORCE_STRICT@ENFORCE_NORMAL These enable or disable more strict semanic checking the sub options are FOREIGN KEY ON UPDATE: all FK's must choose some ON UPDATE strategyFOREIGN KEY ON DELETE: all FK's must choose some ON DELETE strategyPROCEDURE: all procedures must be declared before they are called (eliminating the vanilla C call option)JOIN: all joins must be ANSI style, the form FROM A,B is not allowed (replace with A INNER JOIN BWINDOW FUNC: window functions are disallowed (useful if targeting old versions of SQLite)UPSERT STATEMENT: the upsert form is disallowed (useful if targeting old versions of SQLite) @SENSITIVE marks a column or variable as 'sensitive' for privacy purposes, this behaves somewhat like nullability (See Chapter 3) in that it is radioactive, contaminating anything it touchesthe intent of this annotation is to make it clear where sensitive data is being returned or consumed in your proceduresthis information appears in the JSON output for further codegen or for analysis (See Chapter 13) @DECLARE_SCHEMA_REGION@DECLARE_DEPLOYABLE_REGION@BEGIN_SCHEMA_REGION@END_SCHEMA_REGION These directives control the declaration of schema regions and allow you to place things into those regions -- see Chapter 10 @SCHEMA_AD_HOC_MIGRATION Allows for the creation of a ad hoc migration step at a given schema version, (See Chapter 10) @ECHO Emits text into the C output stream, useful for emiting things like function prototypes or preprocessor directivese.g. `echo C, '#define foo bar' @RECREATE@CREATE@DELETE used to mark the schema version where an object is created or deleted, or alternatively indicate the the object is always dropped and recreated when it changes (See Chapter 10) @SCHEMA_UPGRADE_VERSION used to indicate that the code that follows is part of a migration script for the indicated schema versionthis has the effect of making the schema appear to be how it existed at the indicated versionthe idea here is that migration procedures operate on previous versions of the schema where (e.g.) some columns/tables hadn't been deleted yet @PREVIOUS_SCHEMA indicates the start of the previous version of the schema for comparison (See Chapter 11) @SCHEMA_UPGRADE_SCRIPT CQL emits a schema upgrade script as part of its upgrade features, this script declares tables in their final form but also creates the same tables as they existed when they were first createdthis directive instructs CQL to ignore the incompatible creations, the first declaration controlsthe idea here is that the upgrade script is in the business of getting you to the finish line in an orderly fashion and some of the interim steps are just not all the way there yetnote that the upgrade script recapitulates the version history, it does not take you directly to the finish line, this is so that all instances get to the same place the same way (and this fleshes out any bugs in migration) @DUMMY_NULLABLES@DUMMY_DEFAULTS@DUMMY_SEED these control the creation of dummy data for insert and fetch statements (See Chapters 5 and 12) @FILE a string literal that corresponds to the current file name with a prefix stripped (to remove build lab junk in the path) @ATTRIBUTE the main purpose of @attribute is to appear in the JSON output so that it can control later codegen stages in whatever way you deem appropriate the nested nature of attribute values is sufficiently flexible than you could encode an arbitrary LISP program in an attribute, so really anything you might need to express is possible there are a number of attributes known to the compiler which I list below (complete as of this writing) cql:autodrop=(table1, table2, ...) when present the indicated tables, which must be temp tables, are dropped when the results of the procedure have been fetched into a rowset cql:identity=(column1, column2, ...) the indicated columns are used to create a row comparator for the rowset corresponding to the procedure, this appears in a C macro of the form procedure_name_row_same(rowset1, row1, rowset2, row2) cql:suppress_getters the annotated procedure should not emit its related column getter functions. Useful if you only indend to call the procedure from CQL.Saves code generation and removes the possibility of C code using the getters. cql:suppress_result_set the annotated procedure should not emit its related &quot;fetch results&quot; function. Useful if you only indend to call the procedure from CQL.Saves code generation and removes the possibility of C code using the result set or getters.Implies cql:suppress_getters; since there is no result set, getters would be redundant.Note: an OUT UNION procedure cannot have a suppressed result set since all such a procedure does is produce a result set. This attribute is ignored for out union procedures. cql:private the annotated procedure will be static in the generated C Because the generated function is static it cannot be called from other modules and therefore will not go in any CQL exports file (that would be moot since you couldn't call it).This attribute also implies cql:suppress_result_set since only CQL code in the same translation unit could possibly call it and hence the result set procedure is useless to other C code. cql:generate_copy the code generation for the annotated procedure will produce a [procedure_name]_copy function that can make complete or partial copies of its result set. cql:base_fragment=frag_name used for base fragments (See Chapter 14) cql:extension_fragment=frag_name used for extension fragments (See Chapter 14) cql:assembly_fragment=frag_name used for assembly fragments (See Chapter 14) cql:shared_fragment is used to create shared fragments (See Chapter 14) cql:no_table_scan for query plan processing, indicates that the table in question should never be table scanned in any plan (for better diagnostics) cql:autotest=([many forms]) declares various autotest features (See Chapter 12) cql:query_plan_branch=[integer] is used by the query plan generator to determine which conditional branch to use in query plan analysis when a shared fragment that contains an IF statement is used. (See Chapter 15) cql:alias_of=[c_function_name] are used on function declarations to declare a function or procedure in CQL that calls a function of a different name. This is intended to used for aliasing native (C) functions. Both the aliased function name and the original function name may be declared in CQL at the same time. Note that the compiler does not enforce any consistency in typing between the original and aliased functions.","keywords":""},{"title":"Appendix 2: CQL Grammar","type":0,"sectionRef":"#","url":"/cql-guide/x2","content":"","keywords":""},{"title":"Operators and Literals​","type":1,"pageTitle":"Appendix 2: CQL Grammar","url":"/cql-guide/x2#operators-and-literals","content":"These are in order of priority lowest to highest &quot;UNION ALL&quot; &quot;UNION&quot; &quot;INTERSECT&quot; &quot;EXCEPT&quot; &quot;:=&quot; &quot;OR&quot; &quot;AND&quot; &quot;NOT&quot; &quot;BETWEEN&quot; &quot;NOT BETWEEN&quot; &quot;&lt;&gt;&quot; &quot;!=&quot; '=' &quot;==&quot; &quot;LIKE&quot; &quot;NOT LIKE&quot; &quot;GLOB&quot; &quot;NOT GLOB&quot; &quot;MATCH&quot; &quot;NOT MATCH&quot; &quot;REGEXP&quot; &quot;NOT REGEXP&quot; &quot;IN&quot; &quot;NOT IN&quot; &quot;IS NOT&quot; &quot;IS&quot; &quot;IS TRUE&quot; &quot;IS FALSE&quot; &quot;IS NOT TRUE&quot; &quot;IS NOT FALSE&quot; &quot;ISNULL&quot; &quot;NOTNULL&quot; '&lt;' '&gt;' &quot;&gt;=&quot; &quot;&lt;=&quot; &quot;&lt;&lt;&quot; &quot;&gt;&gt;&quot; '&amp;' '|' '+' '-' '*' '/' '%' &quot;||&quot; &quot;COLLATE&quot; &quot;UMINUS&quot; '~'  NOTE: The above varies considerably from the C binding order!!! Literals: ID /* a name */ STRLIT /* a string literal in SQL format e.g. 'it''s sql' */ CSTRLIT /* a string literal in C format e.g. &quot;hello, world\\n&quot; */ BLOBLIT /* a blob literal in SQL format e.g. x'12ab' */ INTLIT /* integer literal */ LONGLIT /* long integer literal */ REALLIT /* floating point literal */  "},{"title":"Statement/Type Keywords​","type":1,"pageTitle":"Appendix 2: CQL Grammar","url":"/cql-guide/x2#statementtype-keywords","content":"&quot;@ATTRIBUTE&quot; &quot;@BEGIN_SCHEMA_REGION&quot; &quot;@BLOB_CREATE_KEY&quot; &quot;@BLOB_CREATE_VAL&quot; &quot;@BLOB_GET_KEY&quot; &quot;@BLOB_GET_KEY_TYPE&quot; &quot;@BLOB_GET_VAL&quot; &quot;@BLOB_GET_VAL_TYPE&quot; &quot;@BLOB_UPDATE_KEY&quot; &quot;@BLOB_UPDATE_VAL&quot; &quot;@CREATE&quot; &quot;@DECLARE_DEPLOYABLE_REGION&quot; &quot;@DECLARE_SCHEMA_REGION&quot; &quot;@DELETE&quot; &quot;@DUMMY_SEED&quot; &quot;@ECHO&quot; &quot;@EMIT_CONSTANTS&quot; &quot;@EMIT_ENUMS&quot; &quot;@EMIT_GROUP&quot; &quot;@END_SCHEMA_REGION&quot; &quot;@ENFORCE_NORMAL&quot; &quot;@ENFORCE_POP&quot; &quot;@ENFORCE_PUSH&quot; &quot;@ENFORCE_RESET&quot; &quot;@ENFORCE_STRICT&quot; &quot;@EPONYMOUS&quot; &quot;@FILE&quot; &quot;@PREVIOUS_SCHEMA&quot; &quot;@PROC&quot; &quot;@RC&quot; &quot;@RECREATE&quot; &quot;@SCHEMA_AD_HOC_MIGRATION&quot; &quot;@SCHEMA_UPGRADE_SCRIPT&quot; &quot;@SCHEMA_UPGRADE_VERSION&quot; &quot;@SENSITIVE&quot; &quot;@UNSUB&quot; &quot;ABORT&quot; &quot;ACTION&quot; &quot;ADD&quot; &quot;AFTER&quot; &quot;ALL&quot; &quot;ALTER&quot; &quot;ARGUMENTS&quot; &quot;AS&quot; &quot;ASC&quot; &quot;AUTOINCREMENT&quot; &quot;BEFORE&quot; &quot;BEGIN&quot; &quot;BLOB&quot; &quot;BY&quot; &quot;CALL&quot; &quot;CASCADE&quot; &quot;CASE&quot; &quot;CAST&quot; &quot;CATCH&quot; &quot;CHECK&quot; &quot;CLOSE&quot; &quot;COLUMN&quot; &quot;COLUMNS&quot; &quot;COMMIT&quot; &quot;CONST&quot; &quot;CONSTRAINT&quot; &quot;CONTEXT COLUMN&quot; &quot;CONTEXT TYPE&quot; &quot;CONTINUE&quot; &quot;CREATE&quot; &quot;CROSS&quot; &quot;CURRENT ROW&quot; &quot;CURSOR HAS ROW&quot; &quot;CURSOR&quot; &quot;DECLARE&quot; &quot;DEFAULT&quot; &quot;DEFERRABLE&quot; &quot;DEFERRED&quot; &quot;DELETE&quot; &quot;DESC&quot; &quot;DISTINCT&quot; &quot;DISTINCTROW&quot; &quot;DO&quot; &quot;DROP&quot; &quot;ELSE IF&quot; &quot;ELSE&quot; &quot;ENCODE&quot; &quot;END&quot; &quot;ENUM&quot; &quot;EXCLUDE CURRENT ROW&quot; &quot;EXCLUDE GROUP&quot; &quot;EXCLUDE NO OTHERS&quot; &quot;EXCLUDE TIES&quot; &quot;EXCLUSIVE&quot; &quot;EXISTS&quot; &quot;EXPLAIN&quot; &quot;FAIL&quot; &quot;FETCH&quot; &quot;FILTER&quot; &quot;FIRST&quot; &quot;FOLLOWING&quot; &quot;FOR EACH ROW&quot; &quot;FOR&quot; &quot;FOREIGN&quot; &quot;FROM BLOB&quot; &quot;FROM&quot; &quot;FUNC&quot; &quot;FUNCTION&quot; &quot;GROUP&quot; &quot;GROUPS&quot; &quot;HAVING&quot; &quot;HIDDEN&quot; &quot;IF&quot; &quot;IGNORE&quot; &quot;IMMEDIATE&quot; &quot;INDEX&quot; &quot;INITIALLY&quot; &quot;INNER&quot; &quot;INOUT&quot; &quot;INSERT&quot; &quot;INSTEAD&quot; &quot;INT&quot; &quot;INTEGER&quot; &quot;INTERFACE&quot; &quot;INTO&quot; &quot;JOIN&quot; &quot;KEY&quot; &quot;LAST&quot; &quot;LEAVE&quot; &quot;LEFT&quot; &quot;LET&quot; &quot;LIMIT&quot; &quot;LONG&quot; &quot;LONG_INT&quot; &quot;LONG_INTEGER&quot; &quot;LOOP&quot; &quot;NO&quot; &quot;NOT DEFERRABLE&quot; &quot;NOTHING&quot; &quot;NULL&quot; &quot;NULLS&quot; &quot;OBJECT&quot; &quot;OF&quot; &quot;OFFSET&quot; &quot;ON CONFLICT&quot; &quot;ON&quot; &quot;OPEN&quot; &quot;ORDER&quot; &quot;OUT&quot; &quot;OUTER&quot; &quot;OVER&quot; &quot;PARTITION&quot; &quot;PRECEDING&quot; &quot;PRIMARY&quot; &quot;PRIVATE&quot; &quot;PROC&quot; &quot;PROCEDURE&quot; &quot;QUERY PLAN&quot; &quot;RAISE&quot; &quot;RANGE&quot; &quot;REAL&quot; &quot;RECURSIVE&quot; &quot;REFERENCES&quot; &quot;RELEASE&quot; &quot;RENAME&quot; &quot;REPLACE&quot; &quot;RESTRICT&quot; &quot;RETURN&quot; &quot;RIGHT&quot; &quot;ROLLBACK&quot; &quot;ROWID&quot; &quot;ROWS&quot; &quot;SAVEPOINT&quot; &quot;SELECT&quot; &quot;SET&quot; &quot;SIGN FUNCTION&quot; &quot;STATEMENT&quot; &quot;SWITCH&quot; &quot;TABLE&quot; &quot;TEMP&quot; &quot;TEXT&quot; &quot;THEN&quot; &quot;THROW&quot; &quot;TO&quot; &quot;TRANSACTION&quot; &quot;TRIGGER&quot; &quot;TRY&quot; &quot;TYPE&quot; &quot;TYPE_CHECK&quot; &quot;UNBOUNDED&quot; &quot;UNIQUE&quot; &quot;UPDATE&quot; &quot;UPSERT&quot; &quot;USING&quot; &quot;VALUES&quot; &quot;VAR&quot; &quot;VIEW&quot; &quot;VIRTUAL&quot; &quot;WHEN&quot; &quot;WHERE&quot; &quot;WHILE&quot; &quot;WINDOW&quot; &quot;WITH&quot; &quot;WITHOUT&quot;  "},{"title":"Rules​","type":1,"pageTitle":"Appendix 2: CQL Grammar","url":"/cql-guide/x2#rules","content":"Note that in many cases the grammar is more generous than the overall language and errors have to be checked on top of this, often this is done on purpose because even when it's possible it might be very inconvenient to do checks with syntax. For example the grammar cannot enforce non-duplicate ids in id lists, but it could enforce non-duplicate attributes in attribute lists. It chooses to do neither as they are easily done with semantic validation. Thus the grammar is not the final authority on what constitutes a valid program but it's a good start.  program: opt_stmt_list ; opt_stmt_list: /*nil*/ | stmt_list ; stmt_list: stmt ';' | stmt_list stmt ';' ; stmt: misc_attrs any_stmt ; any_stmt: alter_table_add_column_stmt | begin_schema_region_stmt | begin_trans_stmt | blob_get_key_type_stmt | blob_get_val_type_stmt | blob_get_key_stmt | blob_get_val_stmt | blob_create_key_stmt | blob_create_val_stmt | blob_update_key_stmt | blob_update_val_stmt | call_stmt | close_stmt | commit_return_stmt | commit_trans_stmt | continue_stmt | create_index_stmt | create_proc_stmt | create_table_stmt | create_trigger_stmt | create_view_stmt | create_virtual_table_stmt | declare_deployable_region_stmt | declare_enum_stmt | declare_const_stmt | declare_group_stmt | declare_select_func_no_check_stmt | declare_func_stmt | declare_out_call_stmt | declare_proc_no_check_stmt | declare_proc_stmt | declare_interface_stmt | declare_schema_region_stmt | declare_vars_stmt | declare_forward_read_cursor_stmt | declare_fetched_value_cursor_stmt | declare_type_stmt | delete_stmt | drop_index_stmt | drop_table_stmt | drop_trigger_stmt | drop_view_stmt | echo_stmt | emit_enums_stmt | emit_group_stmt | emit_constants_stmt | end_schema_region_stmt | enforce_normal_stmt | enforce_pop_stmt | enforce_push_stmt | enforce_reset_stmt | enforce_strict_stmt | explain_stmt | select_nothing_stmt | fetch_call_stmt | fetch_stmt | fetch_values_stmt | fetch_cursor_from_blob_stmt | guard_stmt | if_stmt | insert_stmt | leave_stmt | let_stmt | loop_stmt | out_stmt | out_union_stmt | out_union_parent_child_stmt | previous_schema_stmt | proc_savepoint_stmt | release_savepoint_stmt | return_stmt | rollback_return_stmt | rollback_trans_stmt | savepoint_stmt | select_stmt | schema_ad_hoc_migration_stmt | schema_unsub_stmt | schema_upgrade_script_stmt | schema_upgrade_version_stmt | set_stmt | switch_stmt | throw_stmt | trycatch_stmt | update_cursor_stmt | update_stmt | upsert_stmt | while_stmt | with_delete_stmt | with_insert_stmt | with_update_stmt | with_upsert_stmt ; explain_stmt: &quot;EXPLAIN&quot; opt_query_plan explain_target ; opt_query_plan: /* nil */ | &quot;QUERY PLAN&quot; ; explain_target: select_stmt | update_stmt | delete_stmt | with_delete_stmt | with_insert_stmt | insert_stmt | upsert_stmt | drop_table_stmt | drop_view_stmt | drop_index_stmt | drop_trigger_stmt | begin_trans_stmt | commit_trans_stmt ; previous_schema_stmt: &quot;@PREVIOUS_SCHEMA&quot; ; schema_upgrade_script_stmt: &quot;@SCHEMA_UPGRADE_SCRIPT&quot; ; schema_upgrade_version_stmt: &quot;@SCHEMA_UPGRADE_VERSION&quot; '(' &quot;integer-literal&quot; ')' ; set_stmt: &quot;SET&quot; name &quot;:=&quot; expr | &quot;SET&quot; name &quot;FROM&quot; &quot;CURSOR&quot; name ; let_stmt: &quot;LET&quot; name &quot;:=&quot; expr ; version_attrs_opt_recreate: /* nil */ | &quot;@RECREATE&quot; opt_delete_plain_attr | &quot;@RECREATE&quot; '(' name ')' opt_delete_plain_attr | version_attrs ; opt_delete_plain_attr: /* nil */ | &quot;@DELETE&quot; ; opt_version_attrs: /* nil */ | version_attrs ; version_attrs: &quot;@CREATE&quot; version_annotation opt_version_attrs | &quot;@DELETE&quot; version_annotation opt_version_attrs ; opt_delete_version_attr: /* nil */ | &quot;@DELETE&quot; version_annotation ; drop_table_stmt: &quot;DROP&quot; &quot;TABLE&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;TABLE&quot; name ; drop_view_stmt: &quot;DROP&quot; &quot;VIEW&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;VIEW&quot; name ; drop_index_stmt: &quot;DROP&quot; &quot;INDEX&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;INDEX&quot; name ; drop_trigger_stmt: &quot;DROP&quot; &quot;TRIGGER&quot; &quot;IF&quot; &quot;EXISTS&quot; name | &quot;DROP&quot; &quot;TRIGGER&quot; name ; create_virtual_table_stmt: &quot;CREATE&quot; &quot;VIRTUAL&quot; &quot;TABLE&quot; opt_vtab_flags name &quot;USING&quot; name opt_module_args &quot;AS&quot; '(' col_key_list ')' opt_delete_version_attr ; opt_module_args: /* nil */ | '(' misc_attr_value_list ')' | '(' &quot;ARGUMENTS&quot; &quot;FOLLOWING&quot; ')' ; create_table_prefix_opt_temp: &quot;CREATE&quot; opt_temp &quot;TABLE&quot; ; create_table_stmt: create_table_prefix_opt_temp opt_if_not_exists name '(' col_key_list ')' opt_no_rowid version_attrs_opt_recreate ; opt_temp: /* nil */ | &quot;TEMP&quot; ; opt_if_not_exists: /* nil */ | &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; ; opt_no_rowid: /* nil */ | &quot;WITHOUT&quot; &quot;ROWID&quot; ; opt_vtab_flags: /* nil */ | &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; | &quot;@EPONYMOUS&quot; | &quot;@EPONYMOUS&quot; &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; | &quot;IF&quot; &quot;NOT&quot; &quot;EXISTS&quot; &quot;@EPONYMOUS&quot; ; col_key_list: col_key_def | col_key_def ',' col_key_list ; col_key_def: col_def | pk_def | fk_def | unq_def | check_def | shape_def ; check_def: &quot;CONSTRAINT&quot; name &quot;CHECK&quot; '(' expr ')' | &quot;CHECK&quot; '(' expr ')' ; shape_exprs : shape_expr ',' shape_exprs | shape_expr ; shape_expr: name | '-' name ; shape_def: shape_def_base | shape_def_base '(' shape_exprs ')' ; shape_def_base: &quot;LIKE&quot; name | &quot;LIKE&quot; name &quot;ARGUMENTS&quot; ; col_name: name ; misc_attr_key: name | name ':' name ; misc_attr_value_list: misc_attr_value | misc_attr_value ',' misc_attr_value_list ; misc_attr_value: name | any_literal | const_expr | '(' misc_attr_value_list ')' | '-' num_literal | '+' num_literal ; misc_attr: &quot;@ATTRIBUTE&quot; '(' misc_attr_key ')' | &quot;@ATTRIBUTE&quot; '(' misc_attr_key '=' misc_attr_value ')' ; misc_attrs: /* nil */ | misc_attr misc_attrs ; col_def: misc_attrs col_name data_type_any col_attrs ; pk_def: &quot;CONSTRAINT&quot; name &quot;PRIMARY&quot; &quot;KEY&quot; '(' indexed_columns ')' opt_conflict_clause | &quot;PRIMARY&quot; &quot;KEY&quot; '(' indexed_columns ')' opt_conflict_clause ; opt_conflict_clause: /* nil */ | conflict_clause ; conflict_clause: &quot;ON CONFLICT&quot; &quot;ROLLBACK&quot; | &quot;ON CONFLICT&quot; &quot;ABORT&quot; | &quot;ON CONFLICT&quot; &quot;FAIL&quot; | &quot;ON CONFLICT&quot; &quot;IGNORE&quot; | &quot;ON CONFLICT&quot; &quot;REPLACE&quot; ; opt_fk_options: /* nil */ | fk_options ; fk_options: fk_on_options | fk_deferred_options | fk_on_options fk_deferred_options ; fk_on_options: &quot;ON&quot; &quot;DELETE&quot; fk_action | &quot;ON&quot; &quot;UPDATE&quot; fk_action | &quot;ON&quot; &quot;UPDATE&quot; fk_action &quot;ON&quot; &quot;DELETE&quot; fk_action | &quot;ON&quot; &quot;DELETE&quot; fk_action &quot;ON&quot; &quot;UPDATE&quot; fk_action ; fk_action: &quot;SET&quot; &quot;NULL&quot; | &quot;SET&quot; &quot;DEFAULT&quot; | &quot;CASCADE&quot; | &quot;RESTRICT&quot; | &quot;NO&quot; &quot;ACTION&quot; ; fk_deferred_options: &quot;DEFERRABLE&quot; fk_initial_state | &quot;NOT DEFERRABLE&quot; fk_initial_state ; fk_initial_state: /* nil */ | &quot;INITIALLY&quot; &quot;DEFERRED&quot; | &quot;INITIALLY&quot; &quot;IMMEDIATE&quot; ; fk_def: &quot;CONSTRAINT&quot; name &quot;FOREIGN&quot; &quot;KEY&quot; '(' name_list ')' fk_target_options | &quot;FOREIGN&quot; &quot;KEY&quot; '(' name_list ')' fk_target_options ; fk_target_options: &quot;REFERENCES&quot; name '(' name_list ')' opt_fk_options ; unq_def: &quot;CONSTRAINT&quot; name &quot;UNIQUE&quot; '(' indexed_columns ')' opt_conflict_clause | &quot;UNIQUE&quot; '(' indexed_columns ')' opt_conflict_clause ; opt_unique: /* nil */ | &quot;UNIQUE&quot; ; indexed_column: expr opt_asc_desc ; indexed_columns: indexed_column | indexed_column ',' indexed_columns ; create_index_stmt: &quot;CREATE&quot; opt_unique &quot;INDEX&quot; opt_if_not_exists name &quot;ON&quot; name '(' indexed_columns ')' opt_where opt_delete_version_attr ; name: &quot;ID&quot; | &quot;TEXT&quot; | &quot;TRIGGER&quot; | &quot;ROWID&quot; | &quot;REPLACE&quot; | &quot;KEY&quot; | &quot;VIRTUAL&quot; | &quot;TYPE&quot; | &quot;HIDDEN&quot; | &quot;PRIVATE&quot; | &quot;FIRST&quot; | &quot;LAST&quot; ; opt_name: /* nil */ | name ; name_list: name | name ',' name_list ; opt_name_list: /* nil */ | name_list ; cte_binding_list: cte_binding | cte_binding ',' cte_binding_list ; cte_binding: name name | name &quot;AS&quot; name ; col_attrs: /* nil */ | &quot;NOT&quot; &quot;NULL&quot; opt_conflict_clause col_attrs | &quot;PRIMARY&quot; &quot;KEY&quot; opt_conflict_clause col_attrs | &quot;PRIMARY&quot; &quot;KEY&quot; opt_conflict_clause &quot;AUTOINCREMENT&quot; col_attrs | &quot;DEFAULT&quot; '-' num_literal col_attrs | &quot;DEFAULT&quot; '+' num_literal col_attrs | &quot;DEFAULT&quot; num_literal col_attrs | &quot;DEFAULT&quot; const_expr col_attrs | &quot;DEFAULT&quot; str_literal col_attrs | &quot;COLLATE&quot; name col_attrs | &quot;CHECK&quot; '(' expr ')' col_attrs | &quot;UNIQUE&quot; opt_conflict_clause col_attrs | &quot;HIDDEN&quot; col_attrs | &quot;@SENSITIVE&quot; col_attrs | &quot;@CREATE&quot; version_annotation col_attrs | &quot;@DELETE&quot; version_annotation col_attrs | fk_target_options col_attrs ; version_annotation: '(' &quot;integer-literal&quot; ',' name ')' | '(' &quot;integer-literal&quot; ',' name ':' name ')' | '(' &quot;integer-literal&quot; ')' ; opt_kind: /* nil */ | '&lt;' name '&gt;' ; data_type_numeric: &quot;INT&quot; opt_kind | &quot;INTEGER&quot; opt_kind | &quot;REAL&quot; opt_kind | &quot;LONG&quot; opt_kind | &quot;BOOL&quot; opt_kind | &quot;LONG&quot; &quot;INTEGER&quot; opt_kind | &quot;LONG&quot; &quot;INT&quot; opt_kind | &quot;LONG_INT&quot; opt_kind | &quot;LONG_INTEGER&quot; opt_kind ; data_type_any: data_type_numeric | &quot;TEXT&quot; opt_kind | &quot;BLOB&quot; opt_kind | &quot;OBJECT&quot; opt_kind | &quot;OBJECT&quot; '&lt;' name &quot;CURSOR&quot; '&gt;' | &quot;OBJECT&quot; '&lt;' name &quot;SET&quot; '&gt;' | &quot;ID&quot; ; data_type_with_options: data_type_any | data_type_any &quot;NOT&quot; &quot;NULL&quot; | data_type_any &quot;@SENSITIVE&quot; | data_type_any &quot;@SENSITIVE&quot; &quot;NOT&quot; &quot;NULL&quot; | data_type_any &quot;NOT&quot; &quot;NULL&quot; &quot;@SENSITIVE&quot; ; str_literal: str_chain ; str_chain: str_leaf | str_leaf str_chain ; str_leaf: &quot;sql-string-literal&quot; | &quot;c-string-literal&quot; ; num_literal: &quot;integer-literal&quot; | &quot;long-literal&quot; | &quot;real-literal&quot; | &quot;TRUE&quot; | &quot;FALSE&quot; ; const_expr: &quot;CONST&quot; '(' expr ')' ; any_literal: str_literal | num_literal | &quot;NULL&quot; | &quot;@FILE&quot; '(' str_literal ')' | &quot;@PROC&quot; | &quot;sql-blob-literal&quot; ; raise_expr: &quot;RAISE&quot; '(' &quot;IGNORE&quot; ')' | &quot;RAISE&quot; '(' &quot;ROLLBACK&quot; ',' expr ')' | &quot;RAISE&quot; '(' &quot;ABORT&quot; ',' expr ')' | &quot;RAISE&quot; '(' &quot;FAIL&quot; ',' expr ')' ; call: name '(' arg_list ')' opt_filter_clause | name '(' &quot;DISTINCT&quot; arg_list ')' opt_filter_clause | basic_expr ':' name '(' arg_list ')' ; basic_expr: name | &quot;@RC&quot; | name '.' name | any_literal | const_expr | '(' expr ')' | call | window_func_inv | raise_expr | '(' select_stmt ')' | '(' select_stmt &quot;IF&quot; &quot;NOTHING&quot; expr ')' | '(' select_stmt &quot;IF&quot; &quot;NOTHING&quot; &quot;OR&quot; &quot;NULL&quot; expr ')' | '(' select_stmt &quot;IF&quot; &quot;NOTHING&quot; &quot;THROW&quot;')' | &quot;EXISTS&quot; '(' select_stmt ')' | &quot;CASE&quot; expr case_list &quot;END&quot; | &quot;CASE&quot; expr case_list &quot;ELSE&quot; expr &quot;END&quot; | &quot;CASE&quot; case_list &quot;END&quot; | &quot;CASE&quot; case_list &quot;ELSE&quot; expr &quot;END&quot; | &quot;CAST&quot; '(' expr &quot;AS&quot; data_type_any ')' | &quot;TYPE_CHECK&quot; '(' expr &quot;AS&quot; data_type_with_options ')' math_expr: basic_expr | math_expr '&amp;' math_expr | math_expr '|' math_expr | math_expr &quot;&lt;&lt;&quot; math_expr | math_expr &quot;&gt;&gt;&quot; math_expr | math_expr '+' math_expr | math_expr '-' math_expr | math_expr '*' math_expr | math_expr '/' math_expr | math_expr '%' math_expr | math_expr &quot;IS NOT TRUE&quot; | math_expr &quot;IS NOT FALSE&quot; | math_expr &quot;ISNULL&quot; | math_expr &quot;NOTNULL&quot; | math_expr &quot;IS TRUE&quot; | math_expr &quot;IS FALSE&quot; | '-' math_expr | '+' math_expr | '~' math_expr | &quot;NOT&quot; math_expr | math_expr '=' math_expr | math_expr &quot;==&quot; math_expr | math_expr '&lt;' math_expr | math_expr '&gt;' math_expr | math_expr &quot;&lt;&gt;&quot; math_expr | math_expr &quot;!=&quot; math_expr | math_expr &quot;&gt;=&quot; math_expr | math_expr &quot;&lt;=&quot; math_expr | math_expr &quot;NOT IN&quot; '(' expr_list ')' | math_expr &quot;NOT IN&quot; '(' select_stmt ')' | math_expr &quot;IN&quot; '(' expr_list ')' | math_expr &quot;IN&quot; '(' select_stmt ')' | math_expr &quot;LIKE&quot; math_expr | math_expr &quot;NOT LIKE&quot; math_expr | math_expr &quot;MATCH&quot; math_expr | math_expr &quot;NOT MATCH&quot; math_expr | math_expr &quot;REGEXP&quot; math_expr | math_expr &quot;NOT REGEXP&quot; math_expr | math_expr &quot;GLOB&quot; math_expr | math_expr &quot;NOT GLOB&quot; math_expr | math_expr &quot;BETWEEN&quot; math_expr &quot;AND&quot; math_expr | math_expr &quot;NOT BETWEEN&quot; math_expr &quot;AND&quot; math_expr | math_expr &quot;IS NOT&quot; math_expr | math_expr &quot;IS&quot; math_expr | math_expr &quot;||&quot; math_expr | math_expr &quot;COLLATE&quot; name ; expr: math_expr | expr &quot;AND&quot; expr | expr &quot;OR&quot; expr ; case_list: &quot;WHEN&quot; expr &quot;THEN&quot; expr | &quot;WHEN&quot; expr &quot;THEN&quot; expr case_list ; arg_expr: '*' | expr | shape_arguments ; arg_list: /* nil */ | arg_expr | arg_expr ',' arg_list ; expr_list: expr | expr ',' expr_list ; shape_arguments: &quot;FROM&quot; name | &quot;FROM&quot; name shape_def | &quot;FROM&quot; &quot;ARGUMENTS&quot; | &quot;FROM&quot; &quot;ARGUMENTS&quot; shape_def ; column_calculation: &quot;COLUMNS&quot; '(' col_calcs ')' | &quot;COLUMNS&quot; '(' &quot;DISTINCT&quot; col_calcs ')' ; col_calcs: col_calc | col_calc ',' col_calcs ; col_calc: name | shape_def | name shape_def | name '.' name ; call_expr: expr | shape_arguments ; call_expr_list: call_expr | call_expr ',' call_expr_list ; cte_tables: cte_table | cte_table ',' cte_tables ; cte_decl: name '(' name_list ')' | name '(' '*' ')' ; shared_cte: call_stmt | call_stmt &quot;USING&quot; cte_binding_list ; cte_table: cte_decl &quot;AS&quot; '(' select_stmt ')' | cte_decl &quot;AS&quot; '(' shared_cte')' | '(' call_stmt ')' | '(' call_stmt &quot;USING&quot; cte_binding_list ')' | cte_decl &quot;LIKE&quot; '(' select_stmt ')' | cte_decl &quot;LIKE&quot; name ; with_prefix: &quot;WITH&quot; cte_tables | &quot;WITH&quot; &quot;RECURSIVE&quot; cte_tables ; with_select_stmt: with_prefix select_stmt_no_with ; select_nothing_stmt: &quot;SELECT&quot; &quot;NOTHING&quot; ; select_stmt: with_select_stmt | select_stmt_no_with ; select_stmt_no_with: select_core_list opt_orderby opt_limit opt_offset ; select_core_list: select_core | select_core compound_operator select_core_list ; values: '(' insert_list ')' | '(' insert_list ')' ',' values ; select_core: &quot;SELECT&quot; select_opts select_expr_list opt_from_query_parts opt_where opt_groupby opt_having opt_select_window | &quot;VALUES&quot; values ; compound_operator: &quot;UNION&quot; | &quot;UNION ALL&quot; | &quot;INTERSECT&quot; | &quot;EXCEPT&quot; ; window_func_inv: name '(' arg_list ')' opt_filter_clause &quot;OVER&quot; window_name_or_defn ; opt_filter_clause: /* nil */ | &quot;FILTER&quot; '(' opt_where ')' ; window_name_or_defn: window_defn | name ; window_defn: '(' opt_partition_by opt_orderby opt_frame_spec ')' ; opt_frame_spec: /* nil */ | frame_type frame_boundary_opts frame_exclude ; frame_type: &quot;RANGE&quot; | &quot;ROWS&quot; | &quot;GROUPS&quot; ; frame_exclude: /* nil */ | &quot;EXCLUDE NO OTHERS&quot; | &quot;EXCLUDE CURRENT ROW&quot; | &quot;EXCLUDE GROUP&quot; | &quot;EXCLUDE TIES&quot; ; frame_boundary_opts: frame_boundary | &quot;BETWEEN&quot; frame_boundary_start &quot;AND&quot; frame_boundary_end ; frame_boundary_start: &quot;UNBOUNDED&quot; &quot;PRECEDING&quot; | expr &quot;PRECEDING&quot; | &quot;CURRENT ROW&quot; | expr &quot;FOLLOWING&quot; ; frame_boundary_end: expr &quot;PRECEDING&quot; | &quot;CURRENT ROW&quot; | expr &quot;FOLLOWING&quot; | &quot;UNBOUNDED&quot; &quot;FOLLOWING&quot; ; frame_boundary: &quot;UNBOUNDED&quot; &quot;PRECEDING&quot; | expr &quot;PRECEDING&quot; | &quot;CURRENT ROW&quot; ; opt_partition_by: /* nil */ | &quot;PARTITION&quot; &quot;BY&quot; expr_list ; opt_select_window: /* nil */ | window_clause ; window_clause: &quot;WINDOW&quot; window_name_defn_list ; window_name_defn_list: window_name_defn | window_name_defn ',' window_name_defn_list ; window_name_defn: name &quot;AS&quot; window_defn ; region_spec: name | name &quot;PRIVATE&quot; ; region_list: region_spec ',' region_list | region_spec ; declare_schema_region_stmt: &quot;@DECLARE_SCHEMA_REGION&quot; name | &quot;@DECLARE_SCHEMA_REGION&quot; name &quot;USING&quot; region_list ; declare_deployable_region_stmt: &quot;@DECLARE_DEPLOYABLE_REGION&quot; name | &quot;@DECLARE_DEPLOYABLE_REGION&quot; name &quot;USING&quot; region_list ; begin_schema_region_stmt: &quot;@BEGIN_SCHEMA_REGION&quot; name ; end_schema_region_stmt: &quot;@END_SCHEMA_REGION&quot; ; schema_unsub_stmt: &quot;@UNSUB&quot; '(' name ')' ; schema_ad_hoc_migration_stmt: &quot;@SCHEMA_AD_HOC_MIGRATION&quot; version_annotation | &quot;@SCHEMA_AD_HOC_MIGRATION&quot; &quot;FOR&quot; &quot;@RECREATE&quot; '(' name ',' name ')' ; emit_enums_stmt: &quot;@EMIT_ENUMS&quot; opt_name_list ; emit_group_stmt: &quot;@EMIT_GROUP&quot; opt_name_list ; emit_constants_stmt: &quot;@EMIT_CONSTANTS&quot; name_list ; opt_from_query_parts: /* nil */ | &quot;FROM&quot; query_parts ; opt_where: /* nil */ | &quot;WHERE&quot; expr ; opt_groupby: /* nil */ | &quot;GROUP&quot; &quot;BY&quot; groupby_list ; groupby_list: groupby_item | groupby_item ',' groupby_list ; groupby_item: expr ; opt_asc_desc: /* nil */ | &quot;ASC&quot; opt_nullsfirst_nullslast | &quot;DESC&quot; opt_nullsfirst_nullslast ; opt_nullsfirst_nullslast: /* nil */ | &quot;NULLS&quot; &quot;FIRST&quot; | &quot;NULLS&quot; &quot;LAST&quot; ; opt_having: /* nil */ | &quot;HAVING&quot; expr ; opt_orderby: /* nil */ | &quot;ORDER&quot; &quot;BY&quot; orderby_list ; orderby_list: orderby_item | orderby_item ',' orderby_list ; orderby_item: expr opt_asc_desc ; opt_limit: /* nil */ | &quot;LIMIT&quot; expr ; opt_offset: /* nil */ | &quot;OFFSET&quot; expr ; select_opts: /* nil */ | &quot;ALL&quot; | &quot;DISTINCT&quot; | &quot;DISTINCTROW&quot; ; select_expr_list: select_expr | select_expr ',' select_expr_list | '*' ; select_expr: expr opt_as_alias | name '.' '*' | column_calculation ; opt_as_alias: /* nil */ | as_alias ; as_alias: &quot;AS&quot; name | name ; query_parts: table_or_subquery_list | join_clause ; table_or_subquery_list: table_or_subquery | table_or_subquery ',' table_or_subquery_list ; join_clause: table_or_subquery join_target_list ; join_target_list: join_target | join_target join_target_list ; table_or_subquery: name opt_as_alias | '(' select_stmt ')' opt_as_alias | '(' shared_cte ')' opt_as_alias | table_function opt_as_alias | '(' query_parts ')' ; join_type: /*nil */ | &quot;LEFT&quot; | &quot;RIGHT&quot; | &quot;LEFT&quot; &quot;OUTER&quot; | &quot;RIGHT&quot; &quot;OUTER&quot; | &quot;INNER&quot; | &quot;CROSS&quot; ; join_target: join_type &quot;JOIN&quot; table_or_subquery opt_join_cond ; opt_join_cond: /* nil */ | join_cond ; join_cond: &quot;ON&quot; expr | &quot;USING&quot; '(' name_list ')' ; table_function: name '(' arg_list ')' ; create_view_stmt: &quot;CREATE&quot; opt_temp &quot;VIEW&quot; opt_if_not_exists name &quot;AS&quot; select_stmt opt_delete_version_attr ; with_delete_stmt: with_prefix delete_stmt ; delete_stmt: &quot;DELETE&quot; &quot;FROM&quot; name opt_where ; opt_insert_dummy_spec: /*nil*/ | &quot;@DUMMY_SEED&quot; '(' expr ')' dummy_modifier ; dummy_modifier: /* nil */ | &quot;@DUMMY_NULLABLES&quot; | &quot;@DUMMY_DEFAULTS&quot; | &quot;@DUMMY_NULLABLES&quot; &quot;@DUMMY_DEFAULTS&quot; | &quot;@DUMMY_DEFAULTS&quot; &quot;@DUMMY_NULLABLES&quot; ; insert_stmt_type: &quot;INSERT&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;REPLACE&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;IGNORE&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;ROLLBACK&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;ABORT&quot; &quot;INTO&quot; | &quot;INSERT&quot; &quot;OR&quot; &quot;FAIL&quot; &quot;INTO&quot; | &quot;REPLACE&quot; &quot;INTO&quot; ; with_insert_stmt: with_prefix insert_stmt ; opt_column_spec: /* nil */ | '(' opt_name_list ')' | '(' shape_def ')' ; from_shape: &quot;FROM&quot; &quot;CURSOR&quot; name opt_column_spec | &quot;FROM&quot; name opt_column_spec | &quot;FROM&quot; &quot;ARGUMENTS&quot; opt_column_spec ; insert_stmt: insert_stmt_type name opt_column_spec select_stmt opt_insert_dummy_spec | insert_stmt_type name opt_column_spec from_shape opt_insert_dummy_spec | insert_stmt_type name &quot;DEFAULT&quot; &quot;VALUES&quot; | insert_stmt_type name &quot;USING&quot; select_stmt | insert_stmt_type name &quot;USING&quot; expr_names opt_insert_dummy_spec ; insert_list_item: expr | shape_arguments ; insert_list: /* nil */ | insert_list_item | insert_list_item ',' insert_list ; basic_update_stmt: &quot;UPDATE&quot; opt_name &quot;SET&quot; update_list opt_from_query_parts opt_where ; with_update_stmt: with_prefix update_stmt ; update_stmt: &quot;UPDATE&quot; name &quot;SET&quot; update_list opt_from_query_parts opt_where opt_orderby opt_limit ; update_entry: name '=' expr ; update_list: update_entry | update_entry ',' update_list ; with_upsert_stmt: with_prefix upsert_stmt ; upsert_stmt: insert_stmt &quot;ON CONFLICT&quot; conflict_target &quot;DO&quot; &quot;NOTHING&quot; | insert_stmt &quot;ON CONFLICT&quot; conflict_target &quot;DO&quot; basic_update_stmt ; update_cursor_stmt: &quot;UPDATE&quot; &quot;CURSOR&quot; name opt_column_spec &quot;FROM&quot; &quot;VALUES&quot; '(' insert_list ')' | &quot;UPDATE&quot; &quot;CURSOR&quot; name opt_column_spec from_shape | &quot;UPDATE&quot; &quot;CURSOR&quot; name &quot;USING&quot; expr_names ; conflict_target: /* nil */ | '(' indexed_columns ')' opt_where ; function: &quot;FUNC&quot; | &quot;FUNCTION&quot; ; declare_out_call_stmt: &quot;DECLARE&quot; &quot;OUT&quot; call_stmt ; declare_enum_stmt: &quot;DECLARE&quot; &quot;ENUM&quot; name data_type_numeric '(' enum_values ')' ; enum_values: enum_value | enum_value ',' enum_values ; enum_value: name | name '=' expr ; declare_const_stmt: &quot;DECLARE&quot; &quot;CONST&quot; &quot;GROUP&quot; name '(' const_values ')' ; declare_group_stmt: &quot;DECLARE&quot; &quot;GROUP&quot; name &quot;BEGIN&quot; simple_variable_decls &quot;END&quot; ; simple_variable_decls: declare_vars_stmt ';' | declare_vars_stmt ';' simple_variable_decls ; const_values: const_value | const_value ',' const_values ; const_value: name '=' expr ; declare_select_func_no_check_stmt: &quot;DECLARE&quot; &quot;SELECT&quot; function name &quot;NO&quot; &quot;CHECK&quot; data_type_with_options | &quot;DECLARE&quot; &quot;SELECT&quot; function name &quot;NO&quot; &quot;CHECK&quot; '(' typed_names ')' ; declare_func_stmt: &quot;DECLARE&quot; function name '(' func_params ')' data_type_with_options | &quot;DECLARE&quot; &quot;SELECT&quot; function name '(' params ')' data_type_with_options | &quot;DECLARE&quot; function name '(' func_params ')' &quot;CREATE&quot; data_type_with_options | &quot;DECLARE&quot; &quot;SELECT&quot; function name '(' params ')' '(' typed_names ')' ; procedure: &quot;PROC&quot; | &quot;PROCEDURE&quot; ; declare_proc_no_check_stmt: &quot;DECLARE&quot; procedure name &quot;NO&quot; &quot;CHECK&quot; ; declare_proc_stmt: &quot;DECLARE&quot; procedure name '(' params ')' | &quot;DECLARE&quot; procedure name '(' params ')' '(' typed_names ')' | &quot;DECLARE&quot; procedure name '(' params ')' &quot;USING&quot; &quot;TRANSACTION&quot; | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; '(' typed_names ')' | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; '(' typed_names ')' &quot;USING&quot; &quot;TRANSACTION&quot; | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; &quot;UNION&quot; '(' typed_names ')' | &quot;DECLARE&quot; procedure name '(' params ')' &quot;OUT&quot; &quot;UNION&quot; '(' typed_names ')' &quot;USING&quot; &quot;TRANSACTION&quot; ; declare_interface_stmt: &quot;DECLARE&quot; &quot;INTERFACE&quot; name '(' typed_names ')' | &quot;INTERFACE&quot; name '(' typed_names ')' ; create_proc_stmt: &quot;CREATE&quot; procedure name '(' params ')' &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; inout: &quot;IN&quot; | &quot;OUT&quot; | &quot;INOUT&quot; ; typed_name: name data_type_with_options | shape_def | name shape_def ; typed_names: typed_name | typed_name ',' typed_names ; func_param: param | name &quot;CURSOR&quot; ; func_params: /* nil */ | func_param | func_param ',' func_params ; param: name data_type_with_options | inout name data_type_with_options | shape_def | name shape_def ; params: /* nil */ | param | param ',' params ; declare_value_cursor: &quot;DECLARE&quot; name &quot;CURSOR&quot; shape_def | &quot;CURSOR&quot; name shape_def | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;LIKE&quot; select_stmt | &quot;CURSOR&quot; name &quot;LIKE&quot; select_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;LIKE&quot; '(' typed_names ')' | &quot;CURSOR&quot; name &quot;LIKE&quot; '(' typed_names ')' ; declare_forward_read_cursor_stmt: &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; select_stmt | &quot;CURSOR&quot; name &quot;FOR&quot; select_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; explain_stmt | &quot;CURSOR&quot; name &quot;FOR&quot; explain_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; call_stmt | &quot;CURSOR&quot; name &quot;FOR&quot; call_stmt | &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FOR&quot; expr | &quot;CURSOR&quot; name &quot;FOR&quot; expr ; declare_fetched_value_cursor_stmt: &quot;DECLARE&quot; name &quot;CURSOR&quot; &quot;FETCH&quot; &quot;FROM&quot; call_stmt | &quot;CURSOR&quot; name &quot;FETCH&quot; &quot;FROM&quot; call_stmt ; declare_type_stmt: &quot;DECLARE&quot; name &quot;TYPE&quot; data_type_with_options | &quot;TYPE&quot; name data_type_with_options ; declare_vars_stmt: &quot;DECLARE&quot; name_list data_type_with_options | &quot;VAR&quot; name_list data_type_with_options | declare_value_cursor ; call_stmt: &quot;CALL&quot; name '(' ')' | &quot;CALL&quot; name '(' call_expr_list ')' | &quot;CALL&quot; name '(' '*' ')' ; while_stmt: &quot;WHILE&quot; expr &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; switch_stmt: &quot;SWITCH&quot; expr switch_case switch_cases | &quot;SWITCH&quot; expr &quot;ALL&quot; &quot;VALUES&quot; switch_case switch_cases ; switch_case: &quot;WHEN&quot; expr_list &quot;THEN&quot; stmt_list | &quot;WHEN&quot; expr_list &quot;THEN&quot; &quot;NOTHING&quot; ; switch_cases: switch_case switch_cases | &quot;ELSE&quot; stmt_list &quot;END&quot; | &quot;END&quot; ; loop_stmt: &quot;LOOP&quot; fetch_stmt &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; leave_stmt: &quot;LEAVE&quot; ; return_stmt: &quot;RETURN&quot; ; rollback_return_stmt: &quot;ROLLBACK&quot; &quot;RETURN&quot; ; commit_return_stmt: &quot;COMMIT&quot; &quot;RETURN&quot; ; throw_stmt: &quot;THROW&quot; ; trycatch_stmt: &quot;BEGIN&quot; &quot;TRY&quot; opt_stmt_list &quot;END&quot; &quot;TRY&quot; ';' &quot;BEGIN&quot; &quot;CATCH&quot; opt_stmt_list &quot;END&quot; &quot;CATCH&quot; ; continue_stmt: &quot;CONTINUE&quot; ; fetch_stmt: &quot;FETCH&quot; name &quot;INTO&quot; name_list | &quot;FETCH&quot; name ; fetch_cursor_from_blob_stmt: &quot;FETCH&quot; name &quot;FROM BLOB&quot; expr ; fetch_values_stmt: &quot;FETCH&quot; name opt_column_spec &quot;FROM&quot; &quot;VALUES&quot; '(' insert_list ')' opt_insert_dummy_spec | &quot;FETCH&quot; name opt_column_spec from_shape opt_insert_dummy_spec | &quot;FETCH&quot; name &quot;USING&quot; expr_names opt_insert_dummy_spec ; expr_names: expr_name | expr_name ',' expr_names ; expr_name: expr as_alias ; fetch_call_stmt: &quot;FETCH&quot; name opt_column_spec &quot;FROM&quot; call_stmt ; close_stmt: &quot;CLOSE&quot; name ; out_stmt: &quot;OUT&quot; name ; out_union_stmt: &quot;OUT&quot; &quot;UNION&quot; name ; out_union_parent_child_stmt: &quot;OUT&quot; &quot;UNION&quot; call_stmt &quot;JOIN&quot; child_results ; child_results: child_result | child_result &quot;AND&quot; child_results ; child_result: call_stmt &quot;USING&quot; '(' name_list ')' | call_stmt &quot;USING&quot; '(' name_list ')' &quot;AS&quot; name ; if_stmt: &quot;IF&quot; expr &quot;THEN&quot; opt_stmt_list opt_elseif_list opt_else &quot;END&quot; &quot;IF&quot; ; opt_else: /* nil */ | &quot;ELSE&quot; opt_stmt_list ; elseif_item: &quot;ELSE IF&quot; expr &quot;THEN&quot; opt_stmt_list ; elseif_list: elseif_item | elseif_item elseif_list ; opt_elseif_list: /* nil */ | elseif_list ; control_stmt: commit_return_stmt | continue_stmt | leave_stmt | return_stmt | rollback_return_stmt | throw_stmt guard_stmt: &quot;IF&quot; expr control_stmt ; transaction_mode: /* nil */ | &quot;DEFERRED&quot; | &quot;IMMEDIATE&quot; | &quot;EXCLUSIVE&quot; ; begin_trans_stmt: &quot;BEGIN&quot; transaction_mode &quot;TRANSACTION&quot; | &quot;BEGIN&quot; transaction_mode ; rollback_trans_stmt: &quot;ROLLBACK&quot; | &quot;ROLLBACK&quot; &quot;TRANSACTION&quot; | &quot;ROLLBACK&quot; &quot;TO&quot; savepoint_name | &quot;ROLLBACK&quot; &quot;TRANSACTION&quot; &quot;TO&quot; savepoint_name | &quot;ROLLBACK&quot; &quot;TO&quot; &quot;SAVEPOINT&quot; savepoint_name | &quot;ROLLBACK&quot; &quot;TRANSACTION&quot; &quot;TO&quot; &quot;SAVEPOINT&quot; savepoint_name ; commit_trans_stmt: &quot;COMMIT&quot; &quot;TRANSACTION&quot; | &quot;COMMIT&quot; ; proc_savepoint_stmt: procedure &quot;SAVEPOINT&quot; &quot;BEGIN&quot; opt_stmt_list &quot;END&quot; ; savepoint_name: &quot;@PROC&quot; | name ; savepoint_stmt: &quot;SAVEPOINT&quot; savepoint_name ; release_savepoint_stmt: &quot;RELEASE&quot; savepoint_name | &quot;RELEASE&quot; &quot;SAVEPOINT&quot; savepoint_name ; echo_stmt: &quot;@ECHO&quot; name ',' str_literal ; alter_table_add_column_stmt: &quot;ALTER&quot; &quot;TABLE&quot; name &quot;ADD&quot; &quot;COLUMN&quot; col_def ; create_trigger_stmt: &quot;CREATE&quot; opt_temp &quot;TRIGGER&quot; opt_if_not_exists trigger_def opt_delete_version_attr ; trigger_def: name trigger_condition trigger_operation &quot;ON&quot; name trigger_action ; trigger_condition: /* nil */ | &quot;BEFORE&quot; | &quot;AFTER&quot; | &quot;INSTEAD&quot; &quot;OF&quot; ; trigger_operation: &quot;DELETE&quot; | &quot;INSERT&quot; | &quot;UPDATE&quot; opt_of ; opt_of: /* nil */ | &quot;OF&quot; name_list ; trigger_action: opt_foreachrow opt_when_expr &quot;BEGIN&quot; trigger_stmts &quot;END&quot; ; opt_foreachrow: /* nil */ | &quot;FOR EACH ROW&quot; ; opt_when_expr: /* nil */ | &quot;WHEN&quot; expr ; trigger_stmts: trigger_stmt | trigger_stmt trigger_stmts ; trigger_stmt: trigger_update_stmt ';' | trigger_insert_stmt ';' | trigger_delete_stmt ';' | trigger_select_stmt ';' ; trigger_select_stmt: select_stmt_no_with ; trigger_insert_stmt: insert_stmt ; trigger_delete_stmt: delete_stmt ; trigger_update_stmt: basic_update_stmt ; enforcement_options: &quot;FOREIGN&quot; &quot;KEY&quot; &quot;ON&quot; &quot;UPDATE&quot; | &quot;FOREIGN&quot; &quot;KEY&quot; &quot;ON&quot; &quot;DELETE&quot; | &quot;JOIN&quot; | &quot;UPSERT&quot; &quot;STATEMENT&quot; | &quot;WINDOW&quot; function | &quot;WITHOUT&quot; &quot;ROWID&quot; | &quot;TRANSACTION&quot; | &quot;SELECT&quot; &quot;IF&quot; &quot;NOTHING&quot; | &quot;INSERT&quot; &quot;SELECT&quot; | &quot;TABLE&quot; &quot;FUNCTION&quot; | &quot;ENCODE&quot; &quot;CONTEXT COLUMN&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;INTEGER&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;LONG_INTEGER&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;REAL&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;BOOL&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;TEXT&quot; | &quot;ENCODE&quot; &quot;CONTEXT TYPE&quot; &quot;BLOB&quot; | &quot;IS TRUE&quot; | &quot;CAST&quot; | &quot;SIGN FUNCTION&quot; | &quot;CURSOR HAS ROW&quot; | &quot;UPDATE&quot; &quot;FROM&quot; ; enforce_strict_stmt: &quot;@ENFORCE_STRICT&quot; enforcement_options ; enforce_normal_stmt: &quot;@ENFORCE_NORMAL&quot; enforcement_options ; enforce_reset_stmt: &quot;@ENFORCE_RESET&quot; ; enforce_push_stmt: &quot;@ENFORCE_PUSH&quot; ; enforce_pop_stmt: &quot;@ENFORCE_POP&quot; ; opt_use_offset: /* nil */ | &quot;OFFSET&quot; ; blob_get_key_type_stmt: &quot;@BLOB_GET_KEY_TYPE&quot; name ; blob_get_val_type_stmt: &quot;@BLOB_GET_VAL_TYPE&quot; name ; blob_get_key_stmt: &quot;@BLOB_GET_KEY&quot; name opt_use_offset ; blob_get_val_stmt: &quot;@BLOB_GET_VAL&quot; name opt_use_offset ; blob_create_key_stmt: &quot;@BLOB_CREATE_KEY&quot; name opt_use_offset ; blob_create_val_stmt: &quot;@BLOB_CREATE_VAL&quot; name opt_use_offset ; blob_update_key_stmt: &quot;@BLOB_UPDATE_KEY&quot; name opt_use_offset ; blob_update_val_stmt: &quot;@BLOB_UPDATE_VAL&quot; name opt_use_offset ;  "},{"title":"Appendix 5: JSON Schema Grammar","type":0,"sectionRef":"#","url":"/cql-guide/x5","content":"","keywords":""},{"title":"Rules​","type":1,"pageTitle":"Appendix 5: JSON Schema Grammar","url":"/cql-guide/x5#rules","content":" json_schema: '{' '&quot;tables&quot;' ':' '[' opt_tables ']' ',' '&quot;virtualTables&quot;' ':' '[' opt_virtual_tables ']' ',' '&quot;views&quot;' ':' '[' opt_views ']' ',' '&quot;indices&quot;' ':' '[' opt_indices ']' ',' '&quot;triggers&quot;' ':' '[' opt_triggers ']' ',' '&quot;attributes&quot;' ':' '[' opt_attribute_list ']' ',' '&quot;queries&quot;' ':' '[' opt_queries ']' ',' '&quot;inserts&quot;' ':' '[' opt_inserts ']' ',' '&quot;generalInserts&quot;' ':' '[' opt_inserts_general ']' ',' '&quot;updates&quot;' ':' '[' opt_updates ']' ',' '&quot;deletes&quot;' ':' '[' opt_deletes ']' ',' '&quot;general&quot;' ':' '[' opt_generals ']' ',' '&quot;declareProcs&quot;' ':' '[' opt_declare_procs']' ',' '&quot;declareFuncs&quot;' ':' '[' opt_declare_funcs']' ',' '&quot;interfaces&quot;' ':' '[' opt_interfaces ']' ',' '&quot;regions&quot;' ':' '[' opt_regions ']' ',' '&quot;adHocMigrationProcs&quot;' ':' '[' opt_ad_hoc_migrations ']' ',' '&quot;enums&quot;' ':' '[' opt_enums ']' ',' '&quot;constantGroups&quot;' ':' '[' opt_const_groups ']' ',' '&quot;subscriptions&quot;' ':' '[' opt_subscriptions ']' '}' ; BOOL_LITERAL: '0' | '1' ; opt_tables: | tables ; tables: table | table ',' tables ; opt_backing_details: | '&quot;isBacking&quot;' ':' '1' ',' | '&quot;isBacked&quot;' ':' '1' ',' '&quot;typeHash&quot;' ':' num_literal ',' ; opt_type_hash: | '&quot;typeHash&quot;' ':' num_literal ',' ; table: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;schema&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' BOOL_LITERAL ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;withoutRowid&quot;' ':' BOOL_LITERAL ',' '&quot;isAdded&quot;' ':' BOOL_LITERAL ',' opt_added_version '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version '&quot;isRecreated&quot;' ':' BOOL_LITERAL ',' opt_recreate_group_name opt_unsub_version opt_backing_details opt_region_info opt_table_indices opt_attributes '&quot;columns&quot;' ':' '[' columns ']' ',' '&quot;primaryKey&quot;' ':' '[' opt_column_names ']' ',' '&quot;primaryKeySortOrders&quot;' ':' '[' opt_sort_order_names ']' ',' opt_primary_key_name '&quot;foreignKeys&quot;' ':' '[' opt_foreign_keys ']' ',' '&quot;uniqueKeys&quot;' ':' '[' opt_unique_keys ']' ',' '&quot;checkExpressions&quot;' ':' '[' opt_check_expressions ']' '}' ; opt_primary_key_name: | '&quot;primaryKeyName&quot;' ':' STRING_LITERAL ',' ; opt_virtual_tables: | virtual_tables ; virtual_tables: virtual_table | virtual_table ',' virtual_tables ; virtual_table: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;schema&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' '0' ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;withoutRowid&quot;' ':' BOOL_LITERAL ',' '&quot;isAdded&quot;' ':' BOOL_LITERAL ',' opt_added_version '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version '&quot;isRecreated&quot;' ':' BOOL_LITERAL ',' opt_region_info '&quot;isVirtual&quot;' ':' '1' ',' '&quot;isEponymous&quot;' ':' BOOL_LITERAL ',' '&quot;module&quot;' ':' STRING_LITERAL ',' opt_module_args opt_attributes '&quot;columns&quot;' ':' '[' columns ']' ',' '&quot;primaryKey&quot;' ':' '[' opt_column_names ']' ',' '&quot;primaryKeySortOrders&quot;' ':' '[' opt_sort_order_names ']' ',' '&quot;foreignKeys&quot;' ':' '[' opt_foreign_keys ']' ',' '&quot;uniqueKeys&quot;' ':' '[' opt_unique_keys ']' ',' '&quot;checkExpressions&quot;' ':' '[' opt_check_expressions ']' '}' ; opt_module_args: | '&quot;moduleArgs&quot;' ':' STRING_LITERAL ',' ; opt_added_version: | '&quot;addedVersion&quot;' ':' any_integer ',' opt_added_migration_proc ; opt_added_migration_proc: | '&quot;addedMigrationProc&quot;' ':' STRING_LITERAL ',' ; opt_unsub_version: | '&quot;unsubscribedVersion&quot;' ':' any_integer ',' ; opt_deleted_version: | '&quot;deletedVersion&quot;' ':' any_integer ',' opt_deleted_migration_proc ; opt_deleted_migration_proc: | '&quot;deletedMigrationProc&quot;' ':' STRING_LITERAL ',' ; opt_recreate_group_name: | '&quot;recreateGroupName&quot;' ':' STRING_LITERAL ',' ; opt_index_names: | index_names ; index_names: STRING_LITERAL | STRING_LITERAL ',' index_names ; opt_arg_names: | arg_names ; arg_names: STRING_LITERAL | STRING_LITERAL ',' arg_names ; opt_column_names: | column_names ; column_names: STRING_LITERAL | STRING_LITERAL ',' column_names ; opt_table_names: | table_names ; table_names: STRING_LITERAL | STRING_LITERAL ',' table_names ; opt_view_names: | view_names ; view_names: STRING_LITERAL | STRING_LITERAL ',' view_names ; opt_procedure_names: | procedure_names ; procedure_names: STRING_LITERAL | STRING_LITERAL ',' procedure_names ; opt_sort_order_names: | sort_order_names ; sort_order_names: STRING_LITERAL | STRING_LITERAL ',' sort_order_names ; columns: column | column ',' columns ; column: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' opt_attributes '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL ',' '&quot;isAdded&quot;' ':' BOOL_LITERAL ',' opt_added_version '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version opt_default_value opt_collate opt_check_expr opt_type_hash '&quot;isPrimaryKey&quot;' ':' BOOL_LITERAL ',' '&quot;isUniqueKey&quot;' ':' BOOL_LITERAL ',' '&quot;isAutoIncrement&quot;' ':' BOOL_LITERAL '}' ; opt_collate : | '&quot;collate&quot;' ':' STRING_LITERAL ',' ; opt_check_expr: | '&quot;checkExpr&quot;' ':' STRING_LITERAL ',' '&quot;checkExprArgs&quot;' ':' '[' opt_arg_names ']' ',' ; opt_default_value: | '&quot;defaultValue&quot;' ':' any_literal ',' ; opt_foreign_keys : | foreign_keys ; opt_kind: | '&quot;kind&quot;' ':' STRING_LITERAL ',' ; opt_is_sensitive: | '&quot;isSensitive&quot;' ':' '1' ',' ; foreign_keys : foreign_key | foreign_key ',' foreign_keys ; foreign_key : '{' opt_name '&quot;columns&quot;' ':' '[' column_names ']' ',' '&quot;referenceTable&quot;' ':' STRING_LITERAL ',' '&quot;referenceColumns&quot;' ':' '[' column_names ']' ',' '&quot;onUpdate&quot;' ':' STRING_LITERAL ',' '&quot;onDelete&quot;' ':' STRING_LITERAL ',' '&quot;isDeferred&quot;' ':' BOOL_LITERAL '}' ; opt_unique_keys : | unique_keys ; unique_keys : unique_key | unique_key ',' unique_keys ; unique_key: '{' opt_name '&quot;columns&quot;' ':' '[' column_names ']' ',' '&quot;sortOrders&quot;' ':' '[' sort_order_names ']' '}' ; opt_check_expressions: | check_expressions ; check_expressions: check_expression | check_expression ',' check_expressions ; check_expression: '{' opt_name '&quot;checkExpr&quot;' ':' STRING_LITERAL ',' '&quot;checkExprArgs&quot;' ':' '[' ']' '}' ; opt_name: | '&quot;name&quot;' ':' STRING_LITERAL ',' ; opt_table_indices: | table_indices ; table_indices: '&quot;indices&quot;' ':' '[' opt_index_names ']' ',' ; opt_attributes: | attributes ; attributes: '&quot;attributes&quot;' ':' '[' attribute_list ']' ',' ; opt_attribute_list: | attribute_list ; attribute_list: attribute | attribute ',' attribute_list ; attribute: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;value&quot;' ':' attribute_value '}' ; attribute_array: '[' opt_attribute_value_list ']' ; opt_attribute_value_list: | attribute_value_list ; attribute_value_list: attribute_value | attribute_value ',' attribute_value_list ; attribute_value: any_literal | attribute_array ; any_integer: BOOL_LITERAL | INT_LITERAL ; any_literal: BOOL_LITERAL | INT_LITERAL | '-' INT_LITERAL | LONG_LITERAL | '-' LONG_LITERAL | REAL_LITERAL | '-' REAL_LITERAL | STRING_LITERAL | NULL_LITERAL ; num_literal: BOOL_LITERAL | INT_LITERAL | '-' INT_LITERAL | LONG_LITERAL | '-' LONG_LITERAL | REAL_LITERAL | '-' REAL_LITERAL ; opt_views: | views ; views: view | view ',' views ; view: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' BOOL_LITERAL ',' '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version opt_region_info opt_attributes projection '&quot;select&quot;' ':' STRING_LITERAL ',' '&quot;selectArgs&quot;' ':' '[' ']' ',' dependencies '}' ; opt_region_info: | '&quot;region&quot;' ':' STRING_LITERAL ',' | '&quot;region&quot;' ':' STRING_LITERAL ',' '&quot;deployedInRegion&quot;' ':' STRING_LITERAL ',' ; opt_projection: | projection ; projection: '&quot;projection&quot;' ':' '[' projected_columns ']' ',' ; projected_columns: projected_column | projected_column ',' projected_columns ; projected_column: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; opt_indices: | indices ; indices: index | index ',' indices ; index: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;isUnique&quot;' ':' BOOL_LITERAL ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version opt_region_info opt_partial_index_where opt_attributes '&quot;columns&quot;' ':' '[' column_names ']' ',' '&quot;sortOrders&quot;' ':' '[' sort_order_names ']' '}' ; opt_partial_index_where: | '&quot;where&quot;' ':' STRING_LITERAL ',' ; opt_triggers: | triggers ; triggers: trigger | trigger ',' triggers ; trigger: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' '&quot;target&quot;' ':' STRING_LITERAL ',' '&quot;isTemp&quot;' ':' BOOL_LITERAL ',' '&quot;ifNotExists&quot;' ':' BOOL_LITERAL ',' '&quot;isDeleted&quot;' ':' BOOL_LITERAL ',' opt_deleted_version before_after_instead ',' delete_insert_update ',' opt_for_each_row opt_when_expr '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' ',' opt_region_info opt_attributes dependencies '}' ; before_after_instead: '&quot;isBeforeTrigger&quot;' ':' '1' | '&quot;isAfterTrigger&quot;' ':' '1' | '&quot;isInsteadOfTrigger&quot;' ':' '1' ; delete_insert_update: '&quot;isDeleteTrigger&quot;' ':' '1' | '&quot;isInsertTrigger&quot;' ':' '1' | '&quot;isUpdateTrigger&quot;' ':' '1' ; opt_for_each_row: | '&quot;forEachRow&quot;' ':' BOOL_LITERAL ',' ; opt_when_expr: | '&quot;whenExpr&quot;' ':' STRING_LITERAL ',' '&quot;whenExprArgs&quot;' ':' '[' opt_arg_names ']' ',' ; dependencies: opt_insert_tables opt_update_tables opt_delete_tables opt_from_tables opt_uses_procedures opt_uses_views '&quot;usesTables&quot;' ':' '[' opt_table_names ']' ; opt_uses_views: | '&quot;usesViews&quot;' ':' '[' opt_view_names ']' ',' ; opt_insert_tables: | '&quot;insertTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_update_tables: | '&quot;updateTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_delete_tables: | '&quot;deleteTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_from_tables: | '&quot;fromTables&quot;' ':' '[' opt_table_names ']' ',' ; opt_uses_procedures : | '&quot;usesProcedures&quot;' ':' '[' opt_procedure_names ']' ',' ; opt_queries: | queries ; queries: query | query ',' queries ; query: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes projection '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_args: | args ; args: arg | arg ',' args ; arg: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;argOrigin&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; opt_inserts: | inserts ; inserts: insert | insert ',' inserts ; insert : '{' insert_details ',' '&quot;values&quot;' ':' '[' opt_values ']' '}' ; opt_inserts_general: | inserts_general ; inserts_general: insert_general | insert_general ',' inserts_general ; insert_details: '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' ',' '&quot;statementType&quot;' ':' STRING_LITERAL ',' '&quot;columns&quot;' ':' '[' column_names ']' insert_general : '{' insert_details '}' ; opt_values: | values ; values: value | value ',' values ; value: '{' '&quot;value&quot;' ':' STRING_LITERAL ',' '&quot;valueArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_updates: | updates ; updates: update | update ',' updates ; update : '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_deletes: | deletes ; deletes: delete | delete ',' deletes ; delete : '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_args ']' ',' dependencies ',' opt_region_info opt_attributes '&quot;table&quot;' ':' STRING_LITERAL ',' '&quot;statement&quot;' ':' STRING_LITERAL ',' '&quot;statementArgs&quot;' ':' '[' opt_arg_names ']' '}' ; opt_generals: | generals ; generals: general | general ',' generals ; general: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' '&quot;args&quot;' ':' '[' opt_complex_args ']' ',' dependencies ',' opt_regions opt_attributes opt_projection opt_result_contract '&quot;usesDatabase&quot;' ':' BOOL_LITERAL '}' ; opt_result_contract: | '&quot;hasSelectResult&quot;' ':' '1' ',' | '&quot;hasOutResult&quot;' ':' '1' ',' | '&quot;hasOutUnionResult&quot;' ':''1' ',' ; opt_complex_args: | complex_args ; complex_args: complex_arg | complex_arg ',' complex_args ; complex_arg: '{' binding '&quot;name&quot;' ':' STRING_LITERAL ',' opt_arg_origin '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; binding: | '&quot;binding&quot;' ':' '&quot;inout&quot;' ',' | '&quot;binding&quot;' ':' '&quot;out&quot;' ',' ; opt_arg_origin: | arg_origin ; arg_origin: '&quot;argOrigin&quot;' ':' STRING_LITERAL ',' ; opt_enums: | enums ; enums: enum | enum ',' enums ; enum: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' '&quot;isNotNull&quot;' ':' '1' ',' '&quot;values&quot;' ':' '[' enum_values ']' '}' ; enum_values: enum_value | enum_value ',' enum_values ; enum_value: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;value&quot;' ':' num_literal '}' ; opt_declare_procs: | declare_procs ; declare_procs: declare_proc | declare_proc ',' declare_procs declare_proc: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;args&quot;' ':' '[' opt_complex_args ']' ',' opt_attributes opt_projection '&quot;usesDatabase&quot;' ':' BOOL_LITERAL '}' ; opt_declare_funcs: | declare_funcs ; declare_funcs: declare_func | declare_func ',' declare_funcs ; declare_func: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;args&quot;' ':' '[' opt_complex_args ']' ',' opt_attributes opt_return_type '&quot;createsObject&quot;' ':' BOOL_LITERAL '}' ; opt_return_type: | '&quot;returnType&quot;' ':' return_type ',' ; return_type: '{' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind opt_is_sensitive '&quot;isNotNull&quot;' ':' BOOL_LITERAL '}' ; opt_interfaces: | interfaces ; interfaces: interface | interface ',' interfaces ; interface: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;definedInFile&quot;' ':' STRING_LITERAL ',' '&quot;definedOnLine&quot;' ':' INT_LITERAL ',' opt_attributes '&quot;projection&quot;' ':' '[' projected_columns ']' '}' ; opt_subscriptions: | subscriptions ; subscriptions: subscription | subscription ',' subscriptions ; subscription: '{' '&quot;type&quot;' ':' STRING_LITERAL ',' '&quot;table&quot;' ':' STRING_LITERAL ',' opt_region_info '&quot;version&quot;' ':' any_integer '}' ; opt_const_groups: | const_groups ; const_groups: const_group | const_group ',' const_groups ; const_group: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;values&quot;' ':' '[' const_values ']' '}' ; const_values: const_value | const_value ',' const_values ; const_value: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind '&quot;isNotNull&quot;' ':' BOOL_LITERAL ',' '&quot;value&quot;' ':' num_literal '}' | '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;type&quot;' ':' STRING_LITERAL ',' opt_kind '&quot;isNotNull&quot;' ':' BOOL_LITERAL ',' '&quot;value&quot;' ':' STRING_LITERAL '}' ; opt_regions: | regions ; regions: region | region ',' regions ; region: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;isDeployableRoot&quot;' ':' BOOL_LITERAL ',' '&quot;deployedInRegion&quot;' ':' STRING_LITERAL ',' '&quot;using&quot;' ':' '[' opt_region_names ']' ',' '&quot;usingPrivately&quot;' ':' '[' opt_bool_list ']' '}' ; opt_region_names: | region_names ; region_names: STRING_LITERAL | STRING_LITERAL ',' region_names ; opt_bool_list: | bool_list ; bool_list: BOOL_LITERAL | BOOL_LITERAL ',' bool_list ; opt_ad_hoc_migrations: | ad_hoc_migrations ; ad_hoc_migrations: ad_hoc_migration | ad_hoc_migration ',' ad_hoc_migrations ; ad_hoc_migration: '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' opt_attributes '&quot;version&quot;' ':' any_integer '}' | '{' '&quot;name&quot;' ':' STRING_LITERAL ',' '&quot;crc&quot;' ':' STRING_LITERAL ',' opt_attributes '&quot;onRecreateOf&quot;' ':' STRING_LITERAL '}' ;  "},{"title":"Appendix 6: CQL In 20 Minutes","type":0,"sectionRef":"#","url":"/cql-guide/x6","content":"Appendix 6: CQL In 20 Minutes What follows is a series of examples intended to illustrate the most important features of the CQL language. This appendix was significantly influenced by a similar article on Python at https://learnxinyminutes.com/docs/python/ Also of interest: http://sqlite.orghttps://learnxinyminutes.com/docs/sql And with no further delay, CQL in 20 minutes... -- Single line comments start with two dashes /* C style comments also work * * C pre-processor features like #include and #define are generally available * CQL is typically run through the C pre-processor before it is compile. */ /********************************************************** * 1. Primitive Datatypes and Operators *********************************************************/ -- You have numbers 3 -- an integer 3L -- a long integer 3.5 -- a real literal 0x10 -- 16 in hex -- Math is what you would expect 1 + 1 --&gt; 2 8 - 1 --&gt; 7 10 * 2 --&gt; 20 35.0 / 5 --&gt; 7.0 -- Modulo operation, same as C and SQLite 7 % 3 --&gt; 1 -7 % 3 --&gt; -1 7 % -3 --&gt; 1 -7 % 3 --&gt; -1 -- Bitwise operators bind left to right like in SQLite 1 | 4 &amp; 3 --&gt; 1 (not 0) -- Enforce precedence with parentheses 1 + 3 * 2 --&gt; 7 (1 + 3) * 2 --&gt; 8 -- Use true and false for bools, nullable bool is possible true --&gt; how to true false --&gt; how to false null --&gt; null means &quot;unknown&quot; in CQL like SQLite -- Negate with not not true --&gt; false not false --&gt; true not null --&gt; null (not unknown is unknown) -- Logical Operators 1 and 0 --&gt; 0 0 or 1 --&gt; 1 0 and x --&gt; 0 and x not evaluated 1 or x --&gt; 1 and x not evaluated -- Remember null is &quot;unknown&quot; null or false --&gt; null null or true --&gt; true null and false --&gt; false null and true --&gt; null -- Non-zero values are truthy 0 --&gt; false 4 --&gt; true -6 --&gt; true 0 and 2 --&gt; 0 (false) -5 or 0 --&gt; 1 (true) -- Equality is == or = 1 == 1 --&gt; true 1 = 1 --&gt; true (= and == are the same thing) 2 == 1 --&gt; false -- Note that null is not equal to anything (like SQL) null == 1 --&gt; null (hence not true) null == null --&gt; null (hence not true) &quot;x&quot; == &quot;x&quot; --&gt; true -- IS lets you compare against null 1 IS 1 --&gt; true 2 IS 1 --&gt; false null IS 1 --&gt; false null IS null --&gt; true (Unknown is Unknown? Yes it is!) &quot;x&quot; IS &quot;x&quot; --&gt; true -- x IS NOT y is the same as NOT (x IS y) 1 IS NOT 1 --&gt; false 2 IS NOT 1 --&gt; true null IS NOT 1 --&gt; true null IS NOT null --&gt; false &quot;x&quot; IS NOT &quot;x&quot; --&gt; false -- Inequality is != or &lt;&gt; 1 != 1 --&gt; false 2 &lt;&gt; 1 --&gt; true null != 1 --&gt; null null &lt;&gt; null --&gt; null -- More comparisons 1 &lt; 10 --&gt; true 1 &gt; 10 --&gt; false 2 &lt;= 2 --&gt; true 2 &gt;= 2 --&gt; true 10 &lt; null --&gt; null -- To test if a value is in a range 1 &lt; 2 and 2 &lt; 3 --&gt; true 2 &lt; 3 and 3 &lt; 2 --&gt; false -- BETWEEN makes this look nicer 2 between 1 and 3 --&gt; true 3 between 2 and 2 --&gt; false -- Strings are created with &quot;x&quot; or 'x' &quot;This is a string.\\n&quot; -- can have C style escapes (no embedded nulls) &quot;Th\\x69s is a string.\\n&quot; -- even hex literals 'This isn''t a C style string' -- use '' to escape single quote ONLY /********************************************************** * 2. Simple Variables *********************************************************/ -- CQL can call simple libc methods with a no-check declaration -- we'll need this for later examples so we can do something -- with our expressions (i.e. print them) declare procedure printf no check; call printf(&quot;I'm CQL. Nice to meet you!\\n&quot;); -- Variables are declared with DECLARE. -- Keywords and identifiers are not case sensitive. declare x integer not null; -- You can call it X, it is the same thing. set X := 0; -- All variables begin with a null value if allowed, else a zero value. declare y integer not null; if y == 0 then call printf(&quot;Yes, this will run.\\n&quot;); end if; -- A nullable variable (i.e. not marked with not null) is initialized to null declare z real; if z is null then call printf(&quot;Yes, this will run.\\n&quot;); end if; -- The various types declare a_blob blob; declare a_string text; declare a_real real; declare an_int integer; declare a_long long; declare an_object object; -- There are some typical SQL synonyms declare an_int int; declare a_long long integer; declare a_long long int; declare a_long long_int; -- The basic types can be tagged to make them less miscible declare m real&lt;meters&gt;; declare kg real&lt;kilos&gt;; set m := kg; -- error! -- Object variables can also be tagged so that they are not mixed-up easily declare dict object&lt;dict&gt; not null; declare list object&lt;list&gt; not null; set dict := create_dict(); -- an external function that creates a dict set dict := create_list(); -- error set list := create_list(); -- ok set list := dict; -- error -- Implied type initialization LET i := 1; -- integer not null LET l := 1L; -- long not null LET t := &quot;x&quot;; -- text not null LET b := x IS y; -- bool not null LET b := x = y; -- bool (maybe not null depending on x/y) -- The psuedo function &quot;nullable&quot; converts the type of its arg to the nullable -- version of the same thing. LET n_i := nullable(1); -- nullable integer variable initialized to 1 LET l_i := nullable(1L); -- nullable long variable initialized to 1 /********************************************************** * 3. Control Flow *********************************************************/ -- Just make a variable declare some_var integer not null; set some_var := 5 -- Here is an IF statement if some_var &gt; 10 then call printf(&quot;some_var is totally bigger than 10.\\n&quot;) else if some_var &lt; 10 then -- else if is optional call printf(&quot;some_var is smaller than 10.\\n&quot;) else -- else is optional call printf(&quot;some_var is indeed 10.\\n&quot;) end if; -- WHILE loops iterate as usual declare i integer not null; set i := 0; while i &lt; 5 begin call printf(&quot;%d\\n&quot;, i); set i := i + 1; end; -- Use LEAVE to end a loop early declare i integer not null; set i := 0; while i &lt; 500 begin if i &gt;= 5 then -- we are not going to get anywhere near 500 leave; end if; call printf(&quot;%d\\n&quot;, i); set i := i + 1; end; -- Use CONTINUE to go back to the loop test declare i integer not null; set i := 0; while i &lt; 500 begin set i := i + 1; if i % 2 then -- Note: we to do this after &quot;i&quot; is incremented! -- to avoid an infinite loop continue; end if; -- odd numbers will not be printed because of continue above call printf(&quot;%d\\n&quot;, i); end; /********************************************************** * 4. Complex Expression Forms *********************************************************/ -- Case is an expression, so it is more like the C &quot;?:&quot; operator -- than a switch statement. It is like &quot;?:&quot; on steroids. case i -- a switch expression is optional when 1 then &quot;one&quot; -- one or more cases when 2 then &quot;two&quot; else &quot;other&quot; -- else is optional end; -- Case with no common expression is a series of independent tests case when i == 1 then &quot;i = one&quot; -- booleans could be completely unrelated when j == 2 then &quot;j = two&quot; -- first match wins else &quot;other&quot; end; -- If nothing matches the cases, the result is null. -- The following expression yields null because 7 is not 1. case 7 when 1 then &quot;one&quot; end -- Case is just an expression, so it can nest case X when 1 case y when 1 &quot;x:1 y:1&quot; else &quot;x:1 y:other&quot; end else case when z == 1 &quot;x:other z:1&quot; else &quot;x:other z:other&quot; end end; -- IN is used to test for membership 5 IN (1, 2, 3, 4, 5) --&gt; true 7 IN (1, 2) --&gt; false null in (1, 2, 3) --&gt; null null in (1, null, 3) --&gt; null (null == null is not true) 7 NOT IN (1, 2) --&gt; true null not in (null, 3) --&gt; null /********************************************************** * 4. Working with and &quot;getting rid of&quot; null *********************************************************/ -- Null can be annoying, you might need a not null value. -- In most operations null is radioactive: null + x --&gt; null null * x --&gt; null null == null --&gt; null -- IS and IS NOT always return 0 or 1 null is 1 -&gt; 0 1 is not null -&gt; 1 -- COALESCE returns the first non null arg, or the last arg if all were null. -- If the last arg is not null, you get a non null result for sure. -- The following is never null, but it's false if either x or y is null COALESCE(x==y, false) -&gt; thought excercise: how is this different than x IS y? -- IFNULL is coalesce with 2 args only (COALESCE is more general) IFNULL(x, -1) --&gt; use -1 if x is null -- The reverse, NULLIF, converts a sentinel value to unknown, more exotic NULLIF(x, -1) --&gt; if x is -1 then use null -- the else part of a case can get rid of nulls CASE when x == y then 1 else 0 end; --&gt; true iff x = y and neither is null -- CASE can be used to give you a default value after various tests -- The following expression is never null; &quot;other&quot; is returned if x is null. CASE when x &gt; 0 then &quot;pos&quot; when x &lt; 0 then &quot;neg&quot; else &quot;other&quot; end; -- You can &quot;throw&quot; out of the current procedure (see exceptions below) declare x integer not null; set x := ifnull_throw(nullable_int_expr); -- returns non null, throws if null -- If you have already tested the expression then control flow analysis -- improves its type to &quot;not null&quot;. Many common check patterns are recognized. if nullable_int_expr is not null then -- nullable_int_expression is known to be not null in this context set x := nullable_int_expr; end if; /********************************************************** * 5. Tables, Views, Indices, Triggers *********************************************************/ -- Most forms of data definition language DDL are supported. -- &quot;Loose&quot; DDL (outside of any procedure) simply declares -- schema, it does not actually create it; the schema is assumed to -- exist as you specified. create table T1( id integer primary key, t text, r real ); create table T2( id integer primary key references T1(id), l long, b blob ); -- CQL can take a series of schema declarations (DDL) and -- automatically create a procedure that will materialize -- that schema and even upgrade previous versions of the schema. -- This system is discussed in Chapter 10 of The Guide. -- To actually create tables and other schema you need -- procedures that look like the below: create proc make_tables() begin create table T1 if not exists ( id integer primary key, t text, r real ); end; -- Views are supported create view V1 as (select * from T1); -- Triggers are supported create trigger if not exists trigger1 before delete on T1 begin delete from T2 where id = old.id; end; -- Indices are supported create index I1 on T1(t); create index I2 on T1(r); -- The various drop forms are supported drop index I1; drop index I2; drop view V1; drop table T2; drop table T1; -- A complete discussion of DDL is out of scope, refer to sqlite.org /********************************************************** * 6. Selecting Data *********************************************************/ -- We will use this scratch variable in the following examples declare rr real; -- First observe CQL is a two-headed language set rr := 1+1; -- this is evaluated in generated C code set rr := (select 1+1); -- this expresion goes to SQLite; SQLite does the addition -- CQL tries to do most things the same as SQLite in the C context -- but some things are exceedingly hard to emulate correctly. -- Even simple looking things such as: set rr := (select cast(&quot;1.23&quot; as real)); --&gt; rr := 1.23 set rr := cast(&quot;1.23&quot; as real); --&gt; error (not safe to emulate SQLite) -- In general, numeric/text conversions have to happen in SQLite context -- because the specific library that does the conversion could be and usually -- is different than the one CQL would use. It would not do to give different answers -- in one context or another so those conversions are simply not supported. -- Loose concatenation is not supported because of the implied conversions. -- Loose means &quot;not in the context of a SQL statement&quot;. set r := 1.23; set r := (select cast(&quot;100&quot;||r as real)); --&gt; 1001.23 (a number) set r := cast(&quot;100&quot;||r as real); --&gt; error, concat not supported in loose expr -- A simple insertion insert into T1 values (1, &quot;foo&quot;, 3.14); -- Finally, reading from the database set r := (select r from T1 where id = 1); --&gt; r = 3.14 -- The (select ...) form requires the result to have at least one row. -- You can use IF NOTHING forms to handle other cases such as: set r := (select r from T1 where id = 2 if nothing -1); --&gt; r = -1 -- If the SELECT statement might return a null result you can handle that as well set r := (select r from T1 where id = 2 if nothing or null -1); --&gt; r = -1 -- With no IF NOTHING clause, lack of a row will cause the SELECT expression to throw -- an exception. IF NOTHING THROW merely makes this explicit. set r := (select r from T1 where id = 2 if nothing throw); --&gt; will throw /********************************************************** * 6. Procedures, Results, Exceptions *********************************************************/ -- Procedures are a list of statements that can be executed, with arguments. create proc hello() begin call printf(&quot;Hello, world\\n&quot;); end; -- IN, OUT, and INOUT parameters are possible create proc swizzle(x integer, inout y integer, out z real not null) begin set y := x + y; -- any computation you like -- bizarre way to compute an id but this is just an illustration set z := (select r from T1 where id = x if nothing or null -1); end; -- Procedures like &quot;hello&quot; (above) have a void signature -- they return nothing -- as nothing can go wrong. Procedures that use the database like &quot;swizzle&quot; (above) -- can return an error code if there is a problem. -- &quot;will_fail&quot; (below) will always return SQLITE_CONSTRAINT, the second insert -- is said to &quot;throw&quot;. In CQL exceptions are just result codes. create proc will_fail() begin insert into T1 values (1, &quot;x&quot;, 1); insert into T1 values (1, &quot;x&quot;, 1); --&gt; duplicate key end; -- DML that fails generates an exception and -- exceptions can be caught. Here is a example: create proc upsert_t1( id_ integer primary key, t_ text, r_ real ) begin begin try -- try to insert insert into T1(id, t, r) values (id_, t_, r_); end try; begin catch -- if the insert fails, try to update update T1 set t = t_, r = r_ where id = id_; end catch; end; -- Shapes can be very useful in avoiding boilerplate code -- the following is equivalent to the above. -- More on shapes later. create proc upsert_t1(LIKE t1) -- my args are the same as the columns of T1 begin begin try insert into T1 from arguments end try; begin catch update T1 set t = t_, r = r_ where id = id_; end catch; end; -- You can (re)throw an error explicitly. -- If there is no current error you get SQLITE_ERROR create proc upsert_wrapper(LIKE t1) -- my args are the same as the columns of T1 begin if r_ &gt; 10 then throw end if; -- throw if r_ is too big call upsert_t1(from arguments); end; -- Procedures can also produce a result set. -- The compiler generates the code to create this result set -- and helper functions to read rows out of it. create proc get_low_r(r_ real) begin -- optionally insert some rows or do other things select * from T1 where T1.r &lt;= r_; end; -- A procedure can choose between various results, the choices must be compatible. -- The last &quot;select&quot; to run controls the ultimate result. create proc get_hi_or_low(r_ real, hi_not_low bool not null) begin -- trying to do this with one query would result in a poor plan, so -- instead we use two economical queries. if hi_not_low then select * from T1 where T1.r &gt;= r_; else select * from T1 where T1.r &lt;= r_; end if; end; -- Using IF to create to nice selects above is a powerful thing. -- SQLite has no IF, if we tried to create a shared query we get -- something that does not use indices at all. As in the below. -- The two-headed CQL beast has its advantages! select * from T1 where case hi_not_low then T1.r &gt;= r_ else T1.r &lt;= r_ end; -- You can get the current return code and use it in your CATCH logic. -- This upsert is a bit better than the first: create proc upsert_t1(LIKE t1) -- my args are the same as the columns of T1 begin begin try insert into T1 from arguments end try; begin catch; if @rc == 19 /* SQLITE_CONSTRAINT */ then update T1 set t = t_, r = r_ where id = id_; else throw; -- rethrow, something bad happened. end if; end catch; end; -- By convention, you can call a procedure that has an OUT argument -- as its last argument using function notation. The out argument -- is used as the return value. If the called procedure uses the -- database then it could throw which causes the caller to throw -- as usual. create proc fib(n integer not null, out result integer not null) begin set result := case n &lt;= 2 then 1 else fib(n-1) + fib(n-2) end; end; /********************************************************** * 7. Statement Cursors *********************************************************/ -- Statement cursors let you iterate over a select result. -- Here we introduce cursors, LOOP and FETCH. create proc count_t1(r_ real, out rows_ integer not null) begin declare rows integer not null; -- starts at zero guaranteed declare C cursor for select * from T1 where r &lt; r_; loop fetch C -- iterate until fetch returns no row begin -- goofy code to illustrate you can process the cursor -- in whatever way you deem appropriate if C.r &lt; 5 then rows := rows + 1; -- count rows with C.r &lt; 5 end if; end; set rows_ := rows; end; -- Cursors can be tested for presence of a row -- and they can be closed before the enumeration is finished. -- As before the below is somewhat goofy example code. create proc peek_t1(r_ real, out rows_ integer not null) begin /* rows_ is set to zero for sure! */ declare C cursor for select * from T1 where r &lt; r_ limit 2; open C; -- this is no-op, present because other systems have it fetch C; -- fetch might find a row or not if C then -- cursor name as bool indicates presence of a row set rows_ = rows_ + (C.r &lt; 5); fetch C; set rows_ = rows_ + (C and C.r &lt; 5); end if; close C; -- cursors auto-closed at end of method but early close possible end; -- The FETCH...INTO form can be used to fetch directly into variables fetch C into id_, t_, r_; --&gt; loads named locals instead of C.id, C.t, C.r -- A procedure can be the source of a cursor declare C cursor for call get_low_r(3.2); -- valid cursor source -- OUT can be used to create a result set that is just one row create proc one_t1(r_ real) begin declare C cursor for select * from T1 where r &lt; r_ limit 1; fetch C; out C; -- emits a row if we have one, no row is ok too, empty result set. end; /********************************************************** * 8. Value Cursors, Out, and Out Union *********************************************************/ -- To consume a procedure that uses &quot;out&quot; you can declare a value cursor. -- By itself such as cursor does not imply use of the database, but often -- the source of the cursor uses the database. In this example -- consume_one_t1 uses the database because of the call to one_t1. create proc consume_one_t1() begin -- a cursor whose shape matches the one_t1 &quot;out&quot; statement declare C cursor like one_t1; -- load it from the call fetch C from call one_t1(7); if C.r &gt; 10 then -- use values as you see fit call printf(&quot;woohoo&quot;); end if; end; -- You can do the above in one step with the compound form: declare C cursor fetch from call one_t1(7); -- declare and fetch -- Value cursors can come from anywhere and can be a procedure result create proc use_t1_a_lot() begin -- T1 is the same shape as one_t1, this will work, too declare C cursor like T1; fetch C from call one_t1(7); -- load it from the call -- some arbitrary logic might be here -- load C again with different args fetch C from call one_t1(12); -- load it again -- some arbitrary logic might be here -- now load C yet again with explicit args fetch C using 1 id, &quot;foo&quot; t, 8.2 r; -- now return it out C; end; -- Make a complex result set one row at a time create proc out_union_example() begin -- T1 is the same shape as one_t1, this will work, too declare C cursor like T1; -- load it from the call fetch C from call one_t1(7); -- note out UNION rather than just out, indicating potentially many rows out union C; -- load it again with different args fetch C from call one_t1(12); out union C; -- do something, then maybe load it again with explicit args fetch C using 1 id, &quot;foo&quot; t, 8.2 r; out union C; -- we have generated a 3 row result set end; -- Consume the above create proc consume_result() begin declare C cursor for call out_union_example(); loop fetch C begin -- use builtin cql_cursor_format to make the cursor into a string call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); --&gt; prints every column and value end; end; /********************************************************** * 9. Named Types and Enumerations *********************************************************/ -- Create a simple named types declare my_type type integer not null; -- make an alias for integer not null declare i my_type; -- use it, &quot;i&quot; is an integer -- Mixing in type kinds is helpful declare distance type real&lt;meters&gt;; -- e.g., distances to be measured in meters declare time type real&lt;seconds&gt;; -- e.g., time to be measured in seconds declare job_id type long&lt;job_id&gt;; declare person_id type long&lt;person_id&gt;; -- With the above done -- * vars/cols of type &quot;distance&quot; are incompatible with those of type &quot;time&quot; -- * vars/cols of types job_id are incompatible with person_id -- This is true even though the underlying type is the same for both! -- ENUM declarations can have any numeric type as their base type declare enum implement integer ( pencil, -- values start at 1 unless you use = to choose something pen, -- the next enum gets previous + 1 as its value (2) brush = 7 -- with = expression you get the indicated value ); -- The above also implicitly does this declare implement type integer&lt;implement&gt; not null; -- Using the enum -- simply use dot notation declare impl implement; set impl := implement.pen; -- value &quot;2&quot; -- You can emit an emum into the current .h file we are going to generate. -- Do not put this directive in an include file, you want it to go to one place. -- Instead, pick one compiland that will &quot;own&quot; the emission of the enum. -- C code can then #include that one .h file. @emit_enums implement; /********************************************************** * 10. Shapes and Their Uses *********************************************************/ -- Shapes first appeared to help define value cursors like so: -- A table or view name defines a shape declare C cursor like T1; -- The result of a proc defines a shape declare D cursor like one_t1; -- A dummy select statement defines a shape (the select does not run) -- this one is the same as (x integer not null, y text not null) declare E cursor like select 1 x, &quot;2&quot; y; -- Another cursor defines a shape declare F cursor like C; -- The arguments of a procedure define a shape. If you have -- create proc count_t1(r_ real, out rows_ integer not null) ... -- the shape will be: -- (r_ real, rows_ integer not null) declare G cursor like count_t1 arguments; -- A loaded cursor can be used to make a call call count_t1(from G); -- the args become G.r_, G.rows_ -- A shape can be used to define a procedures args, or some of the args -- In the following &quot;p&quot; will have arguments:s id_, t_, and r_ with types -- matching table T1. -- Note: To avoid ambiguity, an _ was added to each name! create proc p(like T1) begin -- do whatever you like end; -- The arguments of the current procedure are a synthetic shape -- called &quot;arguments&quot; and can used where other shapes can appear. -- For instance, you can have &quot;q&quot; shim to &quot;p&quot; using this form: create proc q(like T1, print bool not null) begin -- maybe pre-process, silly example set id_ := id_ + 1; -- shim to p call p(from arguments); -- pass my args through, whatever they are -- maybe post-process, silly example set r_ := r_ - 1; if print then -- convert args to cursor declare C like q arguments; fetch C from arguments; call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); --&gt; prints every column and value end if; -- insert a row based on the args insert into T1 from arguments; end; -- You an use a given shape more than once if you name each use. -- This would be more exciting if T1 was like a &quot;person&quot; or something. create proc r(a like T1, b like T1) begin call p(from a); call p(from b); -- you can refer to a.id, b.id etc. declare C like a; fetch C from a; call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); fetch C from b; call printf(&quot;%s\\n&quot;, cql_cursor_format(C)); end; -- Shapes can be subsetted, for instance in the following example -- only the arguments that match C are used in the FETCH. fetch C from arguments(like C); -- Fetch the columns of D into C using the cursor D for the data source. -- Other columns get default values. fetch C(like D) from D; -- Use the D shape to load C, dummy values for the others. -- In this example, dummy_seed means use the provided value, 11, for -- any numerics that are not specified (not in D) and and use -- &quot;col_name_11&quot; for any strings/blobs. This pattern is useful in test code -- to create dummy data, hence the name. fetch C(like D) from D @dummy_seed(11); -- Use the Z shape to control which fields are copied. -- Use the dummy value even if the field is nullable and null would have be ok. fetch C(like Z) from D(like Z) @dummy_seed(11) @dummy_nullables; -- The above patterns also work for insert statements -- The shape constraints are generally useful. The dummy data -- sources are useful for inserting test data. insert into T1(like Z) from D(like Z) @dummy_seed(11) @dummy_nullables; -- We'll need this dummy procedure some_shape so we can use its return -- value in the examples that follow. We will never actual create this -- proc, we only declare it to define the shape, so this is kind of like -- a typedef. declare proc some_shape() (x integer not null, y integer not null, z integer not null); -- You can make a helper procedure to create test args that are mostly constant -- or computable. create get_foo_args(X like some_shape, seed_ integer not null) begin declare C cursor like foo arguments; -- any way of loading C could work this is one fetch C(like X) from X @dummy_seed(seed_); out C; end; -- Now we can use the &quot;get_foo_args&quot; to get full set of arguments for &quot;foo&quot; and then -- call &quot;foo&quot; with those arguments. In this example we're providing -- some of the arguments explicitly, &quot;some_shape&quot; is the part of the args that -- needs to manually vary in each test iteration, the rest of the arguments will -- be dummy values. There could be zillions of args in either category. -- In the below &quot;some_shape&quot; is going to get the manual values 1, 2, 3 while 100 -- will be the seed for the dummy args. declare foo_args cursor fetch from call get_foo_args(1,2,3, 100); call foo(from foo_args); /********************************************************** * 11. INSERT USING and FETCH USING *********************************************************/ -- This kind of thing is a pain insert into foo(a, b, c, d, e, f, g) values(1, 2, 3, null, null, 5, null); -- Instead, write this form: insert into foo USING 1 a, 2 b, 3 c, null d, null e, 5 f, null g; -- The FETCH statement can also be &quot;fetch using&quot; declare C cursor like foo; fetch C USING 1 a, 2 b, 3 c, null d, null e, 5 f, null g; If you've read this far you know more than most now. :)","keywords":""},{"title":"Appendix 7: CQL Anti-patterns","type":0,"sectionRef":"#","url":"/cql-guide/x7","content":"","keywords":""},{"title":"Common Schema​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#common-schema","content":"For these examples let's create a couple of tables we might need for examples CREATE TABLE foo ( id integer primary key, name text ); CREATE TABLE bar ( id integer primary key, rate real );  "},{"title":"Declarations​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#declarations","content":"DECLARE v LONG NOT NULL; SET v := 1;  better LET v := 1L; -- long literals have the L suffix like in C  Similarly DECLARE v REAL NOT NULL; SET v := 1;  better LET v := 1.0; -- use scientific notation or add .0 to make a real literal  "},{"title":"Casts​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#casts","content":"Redundant casts fatten the code and don't really add anything to readability. Sometimems it's necessary to cast NULL to a particular type so that you can be sure that generated result set has the right data type, but most of the casts below are not necessary.  SELECT CAST(foo.id as INTEGER) as id, CAST(foo.name as TEXT) as name, CAST(NULL as REAL) as rate FROM foo UNION ALL SELECT CAST(bar.id as INTEGER) as id, CAST(NULL as TEXT) as name, CAST(bar.rate as REAL) as rate FROM bar  Better  SELECT foo.id, foo.name, CAST(NULL as REAL) as rate FROM foo UNION ALL SELECT bar.id, CAST(NULL as TEXT) as name, bar.rate FROM bar  It's possible to do the following to make this even cleaner: -- somewhere central #define NULL_TEXT CAST(NULL as TEXT) #define NULL_REAL CAST(NULL as REAL) #define NULL_INT CAST(NULL as INTEGER) #define NULL_LONG CAST(NULL as LONG)  Then you can write  SELECT foo.id, foo.name, NULL_REAL as rate FROM foo UNION ALL SELECT bar.id, NULL_TEXT as name, bar.rate FROM bar  Booleans​ TRUE and FALSE can be used as boolean literals. SQLite doesn't care about the type but CQL will get the type information it needs to make the columns of type BOOL  SELECT foo.id, foo.name, NULL_REAL as rate, TRUE as has_name, -- this is a bit artificial but you get the idea FALSE as has_rate FROM foo UNION ALL SELECT bar.id, NULL_TEXT as name, bar.rate, FALSE as has_name, TRUE as has_rate FROM bar  "},{"title":"Boolean expressions and CASE/WHEN​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#boolean-expressions-and-casewhen","content":"It's easy to get carried away with the power of CASE expressions, I've seen this kind of thing: CAST(CASE WHEN foo.name IS NULL THEN 0 ELSE 1 END AS BOOL)  But this is simply foo.name IS NOT NULL  In general, if your case alternates are booleans a direct boolean expression would have served you better. "},{"title":"CASE and CAST and NULL​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#case-and-cast-and-null","content":"Somtimes there's clamping or filtering going on in a case statement CAST(CASE WHEN foo.name &gt; 'm' THEN foo.name ELSE NULL END AS TEXT)  Here the CAST is not needed at all so we could go to CASE WHEN foo.name &gt; 'm' THEN foo.name ELSE NULL END  NULL is already the default value for the ELSE clause so you never need ELSE NULL So better: CASE WHEN foo.name &gt; 'm' THEN foo.name END  "},{"title":"Filtering out NULLs​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#filtering-out-nulls","content":"Consider SELECT * FROM foo WHERE foo.name IS NOT NULL AND foo.name &gt; 'm';  There's no need to test for NOT NULL here, the boolean will result in NULL if foo.name is null which is not true so the WHERE test will fail. Better: SELECT * FROM foo WHERE foo.name &gt; 'm';  "},{"title":"Not null boolean expressions​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#not-null-boolean-expressions","content":"In this statement we do not want to have a null result for the boolean expression SELECT id, name, CAST(IFNULL(name &gt; 'm', 0) AS BOOL) AS name_bigger_than_m FROM FOO;  So now we've made several mistakes. We could have used the usual FALSE defintion to avoid the cast. But even that would have left us with an IFNULL that's harder to read. Here's a much simpler formulation: SELECT id, name, name &gt; 'm' IS TRUE AS name_bigger_than_m FROM FOO;  Even without the TRUE macro you could do IS 1 above and still get a result of type BOOL NOT NULL "},{"title":"Using IS when it makes sense to do so​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#using-is-when-it-makes-sense-to-do-so","content":"This kind of boolean expression is also verbose for no reason  rate IS NOT NULL AND rate = 20  In a WHERE clause probably rate = 20 suffices but even if you really need a NOT NULL BOOLresult the expression above is exactly what the IS operator is for. e.g.  rate IS 20  The IS operator is frequently avoided except for IS NULL and IS NOT NULL but it's a general equality operator with the added semantic that it never returns NULL. NULL IS NULL is true. NULL IS [anything not null] is false. "},{"title":"Left joins that are not left joins​","type":1,"pageTitle":"Appendix 7: CQL Anti-patterns","url":"/cql-guide/x7#left-joins-that-are-not-left-joins","content":"Consider  SELECT foo.id, foo.name, bar.rate FROM foo LEFT JOIN bar ON foo.id = bar.id WHERE bar.rate &gt; 5;  This is no longer a left join because the WHERE clause demands a value for at least one column from bar. Better:  SELECT foo.id, foo.name, bar.rate FROM foo INNER JOIN bar ON foo.id = bar.id WHERE bar.rate &gt; 5;  "},{"title":"Appendix 8: CQL Best Practices","type":0,"sectionRef":"#","url":"/cql-guide/x8","content":"","keywords":""},{"title":"Data Definition Language (DDL)​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#data-definition-language-ddl","content":"ALTER TABLE ADD COLUMNCREATE INDEXCREATE PROCCREATE TABLECREATE TRIGGERCREATE VIEWCREATE VIRTUAL TABLEDROP INDEXDROP TABLEDROP TRIGGERDROP VIEW These statements almost never appear in normal procedures and generally should be avoided. The normal way of handling schema in CQL is to have one or more files declare all the schema you need and then let CQL create a schema upgrader for you. This means you'll never manually drop tables or indices etc. The create declarations with their annotations will totally drive the schema. Any ad hoc DDL is usually a very bad sign. Test code is an obvious exception to this as it often does setup and teardown of schema to set up things for the test. "},{"title":"Ad Hoc Migrations​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#ad-hoc-migrations","content":"@SCHEMA_AD_HOC_MIGRATION This is a special upgrade step that should be taken at the version indicated in the statement. These can be quite complex and even super important but should not be used lightly. Any migration procedure has to be highly tolerant of a variety of incoming schema versions and previous partial successes. In any case this directive should not appear in normal code. It should be part of the schema DDL declarations. "},{"title":"Transactions​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#transactions","content":"BEGIN TRANSACTIONCOMMIT TRANSACTIONROLLBACK TRANSACTION Transactions do not nest and most procedures do not know the context in which they will be called, so the vast majority of procedures will not and should not actually start transactions. You can only do this if you know, somehow, for sure, that the procedure in question is somehow a &quot;top level&quot; procedure. So generally, don't use these statements. "},{"title":"Savepoints​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#savepoints","content":"SAVEPOINTROLLBACK TO SAVEPOINTRELEASE SAVEPOINTPROC SAVEPOINTCOMMIT RETURNROLLBACK RETURN Savepoints are the preferred tool for having interim state that can be rolled back if needed. You can use ad hoc savepoints, just give your save point and name then use RELEASE SAVEPOINT to commit it, or else ROLLBACK TO SAVEPOINTfollowed by a RELEASE to abort it. Note that you always RELEASE savepoints in both the rollback and the commit case. Managing savepoints can be tricky, especially given the various error cases. They combine nicely with TRY CATCH to do this job. However, even that is a lot of boilerplate. The best way to use savepoints is with PROC SAVEPOINT BEGIN .. END; When you use PROC SAVEPOINT, a savepoint is created for you with the name of your procedure. When the block exits the savepoint is released (committed). However you also get an automatically generated try/catch block which will rollback the savepoint if anything inside the block were to invoke THROW. Also, you may not use a regular RETURNinside this block, you must use either ROLLBACK RETURN or COMMIT RETURN. Both of these directly indicate the fate of the automatically generated statement when they run. This gives you useful options to early-out (with no error) while keeping or abandoning any work in progress. Of course you can use THROW to return an error and abandon the work in progress. "},{"title":"Compilation options​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#compilation-options","content":"@ENFORCE_NORMAL@ENFORCE_POP@ENFORCE_PUSH@ENFORCE_RESET@ENFORCE_STRICT CQL allows you to specify a number of useful options such as &quot;do not allow Window Functions&quot; or &quot;all foreign keys must choose some update or delete strategy&quot;. These additional enforcements are designed to prevent errors. Because of this they should be established once, somewhere central and they should be rarely if ever overridden. For instance @ENFORCE_NORMAL WINDOW FUNCTION would allow you to use window functions again, but this is probably a bad idea. If strict mode is on, disallowing them, that probably means your project is expected to target versions of SQLite that do not have window functions. Overriding that setting is likely to lead to runtime errors. In general you don't want to see these options in most code. "},{"title":"Previous Schema​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#previous-schema","content":"@PREVIOUS_SCHEMA CQL can ensure that the current schema is compatible with the previous schema, meaning that an upgrade script could reasonably be generated to go from the previous to the current. This directive demarks the start of the previous schema section when that validation happens. This directive is useless except for creating that schema validation so it should never appear in normal procedures. "},{"title":"Schema Regions​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#schema-regions","content":"@BEGIN_SCHEMA_REGION@DECLARE_DEPLOYABLE_REGION@DECLARE_SCHEMA_REGION@END_SCHEMA_REGION CQL allows you to declare arbitrary schema regions and limit what parts of the schema any given region may consume. This helps you to prevent schema from getting entangled. There is never a reason to use this directives inside normal procedures; They should appear only in your schema declaration files. "},{"title":"Schema Version​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#schema-version","content":"@SCHEMA_UPGRADE_SCRIPT@SCHEMA_UPGRADE_VERSION The @SCHEMA_UPGRADE_SCRIPT directive is only used by CQL itself to declare that the incoming file is an autogenerated schema upgrade script. These scripts have slightly different rules for schema declaration that are not useful outside of such scripts. So you should never use this. @SCHEMA_UPGRADE_VERSION on the other hand is used if you are creating a manual migration script. You need this script to run in the context of the schema version that it affects. Use this directive at the start of the file to do so. Generally manual migration scripts are to be avoided so hopefully this directive is rarely if ever used. "},{"title":"C Text Echo​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#c-text-echo","content":"@ECHO This directive emits plain text directly into the compiler's output stream. It can be invaluable for adding new runtime features and for ensuring that (e.g.) additional #include or #define directives are present in the output but you can really break things by over-using this feature. Most parts of the CQL output are subject to change so any use of this should be super clean. The intended use was, as mentioned, to allow an extra #include in your code so that CQL could call into some library. Most uses of this combine with DECLARE FUNCTION or DECLARE PROCEDURE to declare an external entity. "},{"title":"Enumerations​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#enumerations","content":"DECLARE ENUM@EMIT_ENUMS Avoid embedded constants whenever possible. Instead declare a suitable enumeration. Use @EMIT_ENUMS Some_Enum to get the enumeration constants into the generated .h file for C. But be sure to do this only from one compiland. You do not want the enumerations in every .h file. Choose a single .sql file (not included by lots of other things) to place the @EMIT_ENUMS directive. You can make a file specifically for this purpose if nothing else is serviceable. "},{"title":"Cursor Lifetime​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#cursor-lifetime","content":"CLOSEOPEN The OPEN statement is a no-op, SQLite has no such notion. It was included because it is present in MYSQL and other variants and its inclusion can ease readability sometimes. But it does nothing. The CLOSE statement is normally not necessary because all cursors are closed at the end of the procedure they are declared in (unless they are boxed, see below). You only need CLOSE if you want to close a global cursor (which has no scope) or if you want to close a local cursor &quot;sooner&quot; because waiting to the end of the procedure might be a very long time. Using close more than once is safe, the second and later close operations do nothing. "},{"title":"Procedure Calls and Exceptions​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#procedure-calls-and-exceptions","content":"CALLTHROWTRY CATCH Remember that if you call a procedure and it uses THROW or else uses some SQL that failed, this return code will cause your code to THROW when the procedure returns. Normally that's exactly what you want, the error will ripple out and some top-levelCATCH will cause a ROLLBACK and the top level callers sees the error. If you have your own rollback needs be sure to install your own TRY/CATCH block or else use PROC SAVEPOINT as above to do it for you. Inside of a CATCH block you can use the special variable @RC to see the most recent return code from SQLite. "},{"title":"Control Flow with \"Big Moves\"​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#control-flow-with-big-moves","content":"CONTINUELEAVERETURN These work as usual but beware, you can easily use any of these to accidentally leave a block with a savepoint or transaction and you might skip over the ROLLBACK or COMMIT portions of the logic. Avoid this problem by using PROC SAVEPOINT. "},{"title":"Getting access to external code​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#getting-access-to-external-code","content":"DECLARE FUNCTIONDECLARE SELECT FUNCTIONDECLARE PROCEDURE The best practice is to put any declarations into a shared header file which you can #include in all the places it is needed. This is especially important should you have to forward declare a procedure. CQL normally provides exports for all procedures so you basically get an automatically generated and certain-to-be-correct #include file. But, if the procedures are being compiled together then an export file won't have been generated yet at the time you need it; To work around this you use the DECLARE PROCEDUREform. However, procedure declarations are tricky; they include not just the type of the arguments but the types of any/all of the columns in any result set the procedure might have. This must not be wrong or callers will get unpredictable failures. The easiest way to ensure it is correct is to use the same trick as you would in C -- make sure that you #include the declaration the in the translation unit with the definition. If they don't match there will be an error. A very useful trick: the error will include the exact text of the correct declaration. So if you don't know it, or are too lazy to figure it out; simply put ANY declaration in the shared header file and then paste in the correct declaration from the error. should the definition ever change you will get a compilation error which you can again harvest to get the correct declaration. In this way you can be sure the declarations are correct. Functions have no CQL equivalent, but they generally don't change very often. Use DECLARE FUNCTION to allow access to some C code that returns a result of some kind. Be sure to add the CREATE option if the function returns a reference that the caller owns. Use DECLARE SELECT FUNCTION to tell CQL about any User Defined Functions you have added to SQLite so that it knows how to call them. Note that CQL does not register those UDFs, it couldn't make that call lacking the essential C information required to do so. If you find that you are getting errors when calling a UDF the most likely reason for the failure is that the UDF was declared but never registered with SQLite at runtime. This happens in test code a lot -- product code tends to have some central place to register the UDFs and it normally runs at startup, e.g. right after the schema is upgraded. "},{"title":"Regular Data Manipulation Language (DML)​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#regular-data-manipulation-language-dml","content":"DELETEINSERTSELECTUPDATEUPSERT These statements are the most essential and they'll appear in almost every procedure. There are a few general best practices we can go over. Try to do as much as you can in one batch rather than iterating; e.g. don't write a loop with a DELETE statement that deletes one row if you can avoid it, write a delete statement that deletes all you need to deletedon't write a loop with of SELECT statement that fetches one row, try to fetch all the rows you need with one select Make sure UPSERT is supported on the SQLite system you are using, older versions do not support it Don't put unnecessary casts in your SELECT statements, they just add fat Don't use CASE/WHEN to compute a boolean, the boolean operations are more economical (e.g. use IS) Don't use COUNT if all you need to know is whether a row exists or not, use EXISTS Don't use GROUP BY, ORDER BY, or DISTINCT on large rowsets, the sort is expensive and it will make your SELECT statements write to disk rather than just read Always use the INSERT INTO FOO USING form of the INSERT statement, it's much easier to read than the standard form and compiles to the same thing "},{"title":"Variable and Cursor declarations​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#variable-and-cursor-declarations","content":"DECLARE OUT CALLDECLARELETSET These are likely to appear all over as well. If you can avoid a variable declaration by using LET then do so; The code will be more concise and you'll get the exact variable type you need. This is the same as var x = foo(); in other languages. Once the variable is declared use SET. You can save yourself a lot of declarations of OUT variables with DECLARE OUT CALL. That declaration form automatically declares the OUT variables used in the call you are about to make with the correct type. If the number of arguments changes you just have to add the args you don't have to also add new declarations. The LIKE construct can be used to let you declare things whose type is the same as another thing. Patterns like DECLARE ARGS CURSOR LIKE FOO ARGUMENTSsave you a lot of typing and also enhance correctness. There's a whole chapter dedicated to &quot;shapes&quot; defined by LIKE. "},{"title":"Query Plans​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#query-plans","content":"EXPLAIN Explain can be used in front of other queries to generate a plan. The way SQLite handles this is that you fetch the rows of the plan as usual. So basicallyEXPLAIN is kind of like SELECT QUERY PLAN OF. This hardly ever comes up in normal coding. CQL has an output option where it will generate code that gives you the query plan for a procedures queries rather than the normal body of the procedure. "},{"title":"Fetching Data from a Cursor or from Loose Data​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#fetching-data-from-a-cursor-or-from-loose-data","content":"FETCHUPDATE CURSOR The FETCH statement has many variations, all are useful at some time or another. There are a few helpful guidelines. If fetching from loose values into a cursor use the FETCH USING form (as you would with INSERT INTO USING) because it is less error proneFETCH INTO is generally a bad idea, you'll have to declare a lot of variables, instead just rely on automatic storage in the cursor e.g.fetch my_cursor rather than fetch my_cursor into a, b, cIf you have data already in a cursor you can mutate some of the columns using UPDATE CURSOR, this can let you adjust values or apply defaults "},{"title":"Control Flow​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#control-flow","content":"IFLOOPSWITCHWHILE These are your bread and butter and they will appear all over. One tip: Use the ALL VALUES variant of switch whenever possible to ensure that you haven't missed any cases. "},{"title":"Manual Control of Results​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#manual-control-of-results","content":"OUT OUT UNION If you know you are producing exactly one row OUT is more economical than SELECT If you need complete flexibility on what rows to produce (e.g. skip some, add extras, mutate some) then OUT UNION will give you that, use it only when needed, it's more expensive than just SELECT "},{"title":"CTEs and Shared Fragments​","type":1,"pageTitle":"Appendix 8: CQL Best Practices","url":"/cql-guide/x8#ctes-and-shared-fragments","content":"To understand what kinds of things you can reasonably do with fragments, really you just have to understand the things that you can do with common table expressions or CTEs. For those who don't know, CTEs are the things you declare in the WITH clause of a SELECT statement. They're kind of like local views. Well, actually, they are exactly like local views. Query fragments help you to define useful CTEs so basically what you can do economically in a CTE directly determines what you can do economically in a fragment. To demonstrate some things that happen with CTEs we're going to use these three boring tables. create table A ( id integer primary key, this text not null ); create table B ( id integer primary key, that text not null ); create table C ( id integer primary key, other text not null );  Let's start with a very simple example, the first few examples are like control cases. explain query plan select * from A inner join B on B.id = A.id; QUERY PLAN |--SCAN TABLE A \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  OK as we can see A is not constrained so it has to be scanned but B isn't scanned, we use its primary key for the join. This is the most common kind of join: a search based on a key of the table you are joining to. Let's make it a bit more realistic. explain query plan select * from A inner join B on B.id = A.id where A.id = 5; QUERY PLAN |--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?)  Now A is constrained by the WHERE clause so we can use its index and then use the B index. So we get a nice economical join from A to B and no scans at all. Now suppose we try this with some CTE replacements for A and B. Does this make it worse? explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select * from B) select * from AA left join BB on BB.id = AA.id where AA.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  The answer is a resounding no. The CTE AA was not materialized it was expanded in place, as was the CTE BB. We get exactly the same query plan. Now this means that the inner expressions like select * from A could have been fragments such as: @attribute(cql:shared_fragment) create proc A_() begin select * from A; end; @attribute(cql:shared_fragment) create proc B_() begin select * from B; end; explain query plan with (call A_()), -- short for A_(*) AS (call A_()) (call B_()) -- short for B_(*) AS (call B_()) select * from A_ left join B_ on B_.id = A_.id where A_.id = 5;  Note: I'll use the convention that A_ is the fragment proc that could have generated the CTE AA, likewise with B_ and so forth. The above will expand into exactly what we had before and hence will have the exactly same good query plan. Of course this is totally goofy, why make a fragment like that -- it's just more typing. Well now lets generalize the fragments just a bit. @attribute(cql:shared_fragment) create proc A_(experiment bool not null) begin -- data source might come from somewhere else due to an experiment if not experiment then select * from A; else select id, this from somewhere_else; end if; end; @attribute(cql:shared_fragment) create proc B_() begin -- we don't actually refer to &quot;B&quot; if the filter is null if b_filter is not null then -- applies b_filter if specified select * from B where B.other like b_filter; else -- generates the correct shape but zero rows of it select null as id, null as that where false; end if; end; create proc getAB( id_ integer not null, experiment bool not null, b_filter text) begin with (call A_(experiment)), (call B_(b_filter)) select * from A_ left join B_ on B_.id = A_.id where A_.id = id_; end;  The above now has 4 combos economically encoded and all of them have a good plan. Importantly though, if b_filter is not specified then we don't actually join to B. The B_ CTE will have no reference to B, it just has zero rows. Now lets look at some things you don't want to do. Consider this form: explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select A.id, B.that from A left join B on B.id = A.id) select * from AA left join BB on BB.id = AA.id where AA.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  Note that here we get 3 joins. Now a pretty cool thing happened here -- even though the expression for BB does not include a WHERE clause SQLite has figured out the AA.id being 5 forces A.id to be 5 which in turn gives a constraint on BB. Nice job SQLite. If it hadn't been able to figure that out then the expansion of BB would have resulted in a table scan. Still, 3 joins is bad when we only need 2 joins to do the job. What happened? Well, when we did the original fragments with extensions and stuff we saw this same pattern in fragment code. Basically the fragment for BB isn't just doing the B things it's restarting from A and doing its own join to get B. This results in a wasted join. And it might result in a lot of work on the A table as well if the filtering was more complex and couldn't be perfectly inferred. You might think, &quot;oh, no problem, I can save this, I'll just refer to AA instead of A in the second query.&quot; This does not help (but it's going in the right direction): explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select AA.id, B.that from AA left join B on B.id = AA.id) select * from AA left join BB on BB.id = AA.id where AA.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  In terms of fragments the anti-pattern is this. @attribute(cql:shared_fragment) create proc B_() begin select B.* from A left join B on B.id = A.id; end;  The above starts the query for B again from the root. You can save this, the trick is to not try to generate just the B columns and then join them later. You can get a nice data flow going with chain of CTEs. explain query plan with AA(id, this) as (select * from A), AB(id, this, that) as (select AA.*, B.that from AA left join B on B.id = AA.id) select * from AB where AB.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?)  And we're right back to the perfect plan. The good form creates a CTE chain where we only need the result of the final CTE. A straight line of CTEs each depending on the previous one results in a excellent data flow. In terms of fragments this is now: @attribute(cql:shared_fragment) create proc A_() begin select * from A; end; @attribute(cql:shared_fragment) create proc AB_() begin with (call A_) select A_.*, B.that from A_ left join B on B.id = A_.id end; with (call AB_()) select * from AB_ where AB_.id = 5;  For brevity I didn't include the possibility of using IF and such. Another option that makes the same good query plan. We can generalize AB_ so that it doesn't know where the base data is coming from and can be used in more cases. @attribute(cql:shared_fragment) create proc A_() begin select * from A; end; @attribute(cql:shared_fragment) create proc AB_() begin with source(*) like A -- you must provide some source that is the same shape as A select source.*, B.that from source left join B on B.id = source.id end; with (call A_()) (call AB_() using A_ as source) select * from AB_ where AB_.id = 5;  Again this results in a nice straight chain of CTEs and even though the where clause is last the A table is constrained properly. It's important not to fork the chain... if you do that then whatever came before the fork must be materialized for use in both branches. That can be quite bad because then the filtering might come after the materialization. This is an example that is quite bad. explain query plan with AA(id, this) as (select * from A), BB(id, that) as (select AA.id, B.that from AA left join B on B.id = AA.id), CC(id, other) as (select AA.id, C.other from AA left join C on C.id = AA.id) select * from AA left join BB on BB.id = AA.id left join CC on CC.id = AA.id where AA.id = 5; QUERY PLAN |--MATERIALIZE 2 | |--SCAN TABLE A | \\--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?) |--MATERIALIZE 3 | |--SCAN TABLE A | \\--SEARCH TABLE C USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SCAN SUBQUERY 2 \\--SEARCH SUBQUERY 3 USING AUTOMATIC COVERING INDEX (id=?)  Things have gone way of the rails here. As you can see A is now scanned twice. and there are many more joins. We could make this a lot better by moving the A condition all the way up into the first CTE. With fragments that would just mean creating something like @attribute(cql:shared_fragment) create proc A_(id_) begin select * from A where A.id = id_; end;  At least then if we have to materialize we'll get only one row. This could be a good thing to do universally, but it's especially important if you know that forking in the query shape is mandatory for some reason. A better pattern might be this: explain query plan with AA(id, this) as (select * from A), AB(id, this, that) as (select AA.*, B.that from AA left join B on B.id = AA.id), ABC(id, this, that, other) as (select AB.*, C.other from AB left join C on C.id = AB.id) select * from ABC where ABC.id = 5; QUERY PLAN |--SEARCH TABLE A USING INTEGER PRIMARY KEY (rowid=?) |--SEARCH TABLE B USING INTEGER PRIMARY KEY (rowid=?) \\--SEARCH TABLE C USING INTEGER PRIMARY KEY (rowid=?)  Here we've just extended the chain. With shared fragments you could easily build anAB_ proc as before and then build an ABC_ proc either by calling AB_ directly or by having a table parameter that is LIKE AB_. Both cases will give you a great plan. So the most important things are: Avoid forking the chain of CTEs/fragments, a straight chain works great.Avoid re-joining to tables, even unconstrained CTEs result in great plans if they don't have to be materialized.If you do need to fork in your CTE chain, because of your desired shape, be sure to move as many filters as you can further upstream so that by the time you materialize only a very small number of rows need to be materialiized. These few rules will go far in helping you to create shapes. One last thing, without shared fragments, if you wanted to create a large 10 way join or something you had to type that join into your file and it would be very much in your face. Now that join might be hidden from you in a nice easy-to-use fragment. Which you might then decide you want to use 3 times... And now with a tiny amount of code you have 30 joins. The thing is shared fragments make it easy to generate a lot of SQL. It's not bad that shared fragments make things easy, but with great power comes great responsibility, so give a care as to what it is you are assembling. Understanding your fragments, especially any big ones, will help you to create great code. "},{"title":"Appendix 4: CQL Error Codes","type":0,"sectionRef":"#","url":"/cql-guide/x4","content":"","keywords":""},{"title":"CQL0001: operands must be an integer type, not real​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0001-operands-must-be-an-integer-type-not-real","content":"integer math operators like &lt;&lt; &gt;&gt; &amp; and | are not compatible with real-valued arguments  "},{"title":"CQL0002: left operand cannot be an object in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0002-left-operand-cannot-be-an-object-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on objects. Basically comparison is all you can do.  "},{"title":"CQL0003: left operand cannot be an object in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0003-left-operand-cannot-be-an-object-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on objects. Basically comparison is all you can do.  "},{"title":"CQL0004: left operand cannot be a blob in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0004-left-operand-cannot-be-a-blob-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on blobs. Basically comparison is all you can do.  "},{"title":"CQL0005: right operand cannot be a blob in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0005-right-operand-cannot-be-a-blob-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on blobs. Basically comparison is all you can do.  "},{"title":"CQL0007: left operand cannot be a string in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0007-left-operand-cannot-be-a-string-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on strings. Basically comparison is all you can do.  "},{"title":"CQL0008: right operand cannot be a string in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0008-right-operand-cannot-be-a-string-in-operator","content":"Most arithmetic operators (e.g. +, -, *) do not work on strings. Basically comparison is all you can do.  "},{"title":"CQL0009: incompatible types in expression 'subject'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0009-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required a TEXT as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0010: incompatible types in expression 'subject'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0010-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required an OBJECT as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0011: incompatible types in expression 'subject'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0011-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required a BLOB as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0012: incompatible types in expression 'subject'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0012-incompatible-types-in-expression-subject","content":"The expression type indicated by subject required a numeric as the next item and found something else. This could be a binary operator, part of a CASE expression, the parts of an IN expression or any other place where several expressions might need to be compatible with each other.  "},{"title":"CQL0013: cannot assign/copy possibly null expression to not null target 'target'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0013-cannot-assigncopy-possibly-null-expression-to-not-null-target-target","content":"Here assign/copy can be the simplest case of assigning to a local variable or an OUT parameter but this error also appears when calling functions. You should think of the IN arguments as requiring that the actual argument be assignable to the formal variable and OUT arguments requiring that the formal be assignable to the actual argument variable.  "},{"title":"CQL0014: cannot assign/copy sensitive expression to not null target 'target'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0014-cannot-assigncopy-sensitive-expression-to-not-null-target-target","content":"Here assign/copy can be the simplest case of assigning to a local variable or an OUT parameter but this error also appears when calling functions. You should think of the IN arguments as requiring that the actual argument be assignable to the formal variable and OUT arguments requiring that the formal be assignable to the actual argument variable.  "},{"title":"CQL0015: expected numeric expression 'context'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0015-expected-numeric-expression-context","content":"Many SQL clauses require a numeric expression such as WHERE/HAVING/LIMIT/OFFSET. This expression indicates the expression in the given context is not a numeric.  "},{"title":"CQL0016: duplicate table name in join 'table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0016-duplicate-table-name-in-join-table","content":"When this error is produced it means the result of the join would have the same table twice with no disambiguation between the two places. The conflicting name is provided. To fix this, make an alias both tables. e.g. SELECT T1.id AS parent_id, T2.id AS child_id FROM foo AS T1 INNER JOIN foo AS T2 ON T1.id = T2.parent_id;   "},{"title":"CQL0017: index was present but now it does not exist (use @delete instead) 'index'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0017-index-was-present-but-now-it-does-not-exist-use-delete-instead-index","content":"The named index is in the previous schema bit it is not in the current schema. All entities need some kind of tombstone in the schema so that they can be correctly deleted if they are still present.  "},{"title":"CQL0018: duplicate index name 'index'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0018-duplicate-index-name-index","content":"An index with the indicated name already exists.  "},{"title":"CQL0019: create index table name not found 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0019-create-index-table-name-not-found-table_name","content":"The table part of a CREATE INDEX statement was not a valid table name.  "},{"title":"CQL0020: duplicate constraint name in table 'constraint_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0020-duplicate-constraint-name-in-table-constraint_name","content":"A table contains two constraints with the same name.  "},{"title":"CQL0021: foreign key refers to non-existent table 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0021-foreign-key-refers-to-non-existent-table-table_name","content":"The table in a foreign key REFERENCES clause is not a valid table.  "},{"title":"CQL0022: exact type of both sides of a foreign key must match (expected expected_type; found actual_type) 'key_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0022-exact-type-of-both-sides-of-a-foreign-key-must-match-expected-expected_type-found-actual_type-key_name","content":"The indicated foreign key has at least one column with a different type than corresponding column in the table it references. This usually means that you have picked the wrong table or column in the foreign key declaration.  "},{"title":"CQL0023: number of columns on both sides of a foreign key must match​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0023-number-of-columns-on-both-sides-of-a-foreign-key-must-match","content":"The number of column in the foreign key must be the same as the number of columns specified in the foreign table. This usually means a column is missing in the REFERENCES part of the declaration.  CQL0024: no longer in use  "},{"title":"CQL0025: version number in annotation must be positive​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0025-version-number-in-annotation-must-be-positive","content":"In an @create or @delete annotation, the version number must be &gt; 0. This error usually means there is a typo in the version number.  "},{"title":"CQL0026: duplicate version annotation​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0026-duplicate-version-annotation","content":"There can only be one @create, @delete, or @recreate annotation for any given table/column. More than one @create is redundant. This error usually means the @create was cut/paste to make an @delete and then not edited or something like that.  "},{"title":"CQL0027: a procedure can appear in only one annotation 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0027-a-procedure-can-appear-in-only-one-annotation-procedure_name","content":"The indicated migration procedure e.g. the foo in @create(5, foo) appears in another annotation. Migration steps should happen exactly once. This probably means the annotation was cut/paste and the migration proc was not removed.  "},{"title":"CQL0028: FK reference must be exactly one column with the correct type 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0028-fk-reference-must-be-exactly-one-column-with-the-correct-type-column_name","content":"When a foreign key is specified in the column definition it is the entire foreign key. That means the references part of the declaration can only be for that one column. If you need more columns, you have to declare the foreign key independently.  "},{"title":"CQL0029: autoincrement column must be [LONG_]INTEGER PRIMARY KEY 'column name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0029-autoincrement-column-must-be-long_integer-primary-key-column-name","content":"SQLite is very fussy about autoincrement columns. The column in question must be either a LONG INTEGER or an INTEGER and it must be PRIMARY KEY. In fact, CQL will rewrite LONG INTEGER into INTEGER because only that exact form is supported, but SQLite INTEGERs can hold LONG values so that's ok. Any other autoincrement form results in this error.  "},{"title":"CQL0030: a column attribute was specified twice on the same column 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0030-a-column-attribute-was-specified-twice-on-the-same-column-column_name","content":"This error indicates a pattern like &quot;id text not null not null&quot; was found. The same attribute shouldn't appear twice.  "},{"title":"CQL0031: column can't be primary key and also unique key 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0031-column-cant-be-primary-key-and-also-unique-key-column","content":"In a column definition, the column can only be marked with at most one of PRIMARY KEY or UNIQUE  "},{"title":"CQL0032: created columns must be at the end and must be in version order\", 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0032-created-columns-must-be-at-the-end-and-must-be-in-version-order-column","content":"The SQLite ALTER TABLE ADD COLUMN statement is used to add new columns to the schema. This statement puts the columns at the end of the table. In order to make the CQL schema align as closely as possible to the actual sqlite schema you will get you are required to add columns where SQLite will put them. This will help a lot if you ever connect to such a database and start doing select * from &lt;somewhere with creates&gt;  "},{"title":"CQL0033: columns in a table marked @recreate cannot have @create or @delete, 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0033-columns-in-a-table-marked-recreate-cannot-have-create-or-delete-column","content":"If the table is using the @recreate plan then you can add and remove columns (and other things freely) you don't need to mark columns with @create or @delete just add/remove them. This error prevents the build up of useless annotations.  "},{"title":"CQL0034: create/delete version numbers can only be applied to columns that are nullable or have a default value 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0034-createdelete-version-numbers-can-only-be-applied-to-columns-that-are-nullable-or-have-a-default-value-column","content":"Any new column added to a schema must have a default value or be nullable so that its initial state is clear and so that all existing insert statements do not have to be updated to include it. Either make the column nullable or give it a default value. Similarly, any column being deleted must be nullable or have a default value. The column can't actually be deleted (not all versions of SQLite support this) so it will only be &quot;deprecated&quot;. But if the column is not null and has no default then it would be impossible to write a correct insert statement for the table with the deleted column. As a consequence you can neither add nor remove columns that are not null and have no default.  "},{"title":"CQL0035: column delete version can't be <= column create version\", 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0035-column-delete-version-cant-be--column-create-version-column","content":"You can't @delete a column in a version before it was even created. Probably there is a typo in one or both of the versions.  "},{"title":"CQL0036: column delete version can't be <= the table create version 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0036-column-delete-version-cant-be--the-table-create-version-column","content":"The indicated column is being deleted in a version that is before the table it is found in was even created. Probably there is a typo in the delete version.  "},{"title":"CQL0037: column delete version can't be >= the table delete version​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0037-column-delete-version-cant-be--the-table-delete-version","content":"The indicated column is being deleted in a version that is after the table has already been deleted. This would be redundant. Probably one or both have a typo in their delete version.  "},{"title":"CQL0038: column create version can't be <= the table create version 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0038-column-create-version-cant-be--the-table-create-version-column","content":"The indicated column is being created in a version that is before the table it is found in was even created. Probably there is a typo in the delete version.  "},{"title":"CQL0039: column create version can't be >= the table delete version 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0039-column-create-version-cant-be--the-table-delete-version-column","content":"The indicated column is being created in a version that that is after it has already been supposedly deleted. Probably there is a typo in one or both of the version numbers.  "},{"title":"CQL0040: table can only have one autoinc column 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0040-table-can-only-have-one-autoinc-column-column","content":"The indicated column is the second column to be marked with AUTOINCREMENT in its table. There can only be one such column.  "},{"title":"CQL0041: tables cannot have object columns 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0041-tables-cannot-have-object-columns-column","content":"The OBJECT data type is only for use in parameters and local variables. SQLite has no storage for object references. The valid data types include INTEGER, LONG INTEGER, REAL, BOOL, TEXT, BLOB  "},{"title":"CQL0042: left operand must be a string in 'LIKE/MATCH/GLOB'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0042-left-operand-must-be-a-string-in-likematchglob","content":"The indicated operator can only be used to compare two strings.  "},{"title":"CQL0043: right operand must be a string in 'LIKE/MATCH/GLOB'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0043-right-operand-must-be-a-string-in-likematchglob","content":"The indicated operator can only be used to compare two strings.  "},{"title":"CQL0044: operator may only appear in the context of a SQL statement 'MATCH'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0044-operator-may-only-appear-in-the-context-of-a-sql-statement-match","content":"The MATCH operator is a complex sqlite primitive. It can only appear within SQL expressions. See the CQL documentation about it being a two-headed-beast when it comes to expression evaluation.  "},{"title":"CQL0045: blob operand not allowed in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0045-blob-operand-not-allowed-in-operator","content":"None of the unary math operators e.g. '-' and '~' allow blobs as an operand.  "},{"title":"CQL0046: object operand not allowed in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0046-object-operand-not-allowed-in-operator","content":"None of the unary math operators e.g. '-' and '~' allow objects as an operand.  "},{"title":"CQL0047: string operand not allowed in 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0047-string-operand-not-allowed-in-operator","content":"None of the unary math operators e.g. '-' and '~' allow strings as an operand.  "},{"title":"CQL0051: argument can only be used in count() ''​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0051-argument-can-only-be-used-in-count-","content":"The '' special operator can only appear in the COUNT function. e.g. `select count() from some_table` It is not a valid function argument in any other context.  "},{"title":"CQL0052: select * cannot be used with no FROM clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0052-select--cannot-be-used-with-no-from-clause","content":"Select statements of the form select 1, 'foo'; are valid but select '*'; is not. The * shortcut for columns only makes sense if there is something to select from. e.g. select * from some_table; is valid.  "},{"title":"CQL0053: select [table].* cannot be used with no FROM clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0053-select-table-cannot-be-used-with-no-from-clause","content":"Select statements of the form select 1, 'foo'; are valid but select 'T.*'; is not. The T.* shortcut for all the columns from table T only makes sense if there is something to select form. e.g. select T.* from some_table T; is valid.  "},{"title":"CQL0054: table not found 'table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0054-table-not-found-table","content":"The indicated table was used in a select statement like select T.* from ... but no such table was present in the FROM clause.  "},{"title":"CQL0055: all columns in the select must have a name​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0055-all-columns-in-the-select-must-have-a-name","content":"Referring to the select statement on the failing line, that select statement was used in a context where all the columns must have a name. Examples include defining a view, a cursor, or creating a result set from a procedure. The failing code might look something like this.select 1, 2 B; it needs to look like this select 1 A, 2 B;  "},{"title":"CQL0056: NULL expression has no type to imply a needed type 'variable'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0056-null-expression-has-no-type-to-imply-a-needed-type-variable","content":"In some contexts the type of a constant is used to imply the type of the expression. The NULL literal cannot be used in such contexts because it has no specific type. In a SELECT statement the NULL literal has no type. If the type of the column cannot be inferred then it must be declared specifically. In a LET statement, the same situation arises LET x := NULL; doesn't specify what type 'x' is to be. You can fix this error by changing the NULL to something like CAST(NULL as TEXT). A common place this problem happens is in defining a view or returning a result set from a stored procedure. In those cases all the columns must have a name and a type.  "},{"title":"CQL0057: if multiple selects, all must have the same column count​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0057-if-multiple-selects-all-must-have-the-same-column-count","content":"If a stored procedure might return one of several result sets, each of the select statements it might return must have the same number of columns. Likewise, if several select results are being combined with UNION or UNION ALL they must all have the same number of columns.  "},{"title":"CQL0058: if multiple selects, all column names must be identical so they have unambiguous names; error in column N: 'X' vs. 'Y'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0058-if-multiple-selects-all-column-names-must-be-identical-so-they-have-unambiguous-names-error-in-column-n-x-vs-y","content":"If a stored procedure might return one of several result sets, each of the select statements must have the same column names for its result. Likewise, if several select results are being combined with UNION or UNION ALL they must all have the same column names. This is important so that there can be one unambiguous column name for every column for group of select statements. e.g. select 1 A, 2 B union select 3 A, 4 C;  Would provoke this error. In this case the error would report that the problem was in column 2 and that error was 'B' vs. 'C'  "},{"title":"CQL0059: a variable name might be ambiguous with a column name, this is an anti-pattern 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0059-a-variable-name-might-be-ambiguous-with-a-column-name-this-is-an-anti-pattern-name","content":"The referenced name is the name of a local or a global in the same scope as the name of a column. This can lead to surprising results as it is not clear which name takes priority (previously the variable did rather than the column, now it's an error). example: create proc foo(id integer) begin -- this is now an error, in all cases the argument would have been selected select id from bar T1 where T1.id != id; end;  To correct this, rename the local/global. Or else pick a more distinctive column name, but usually the local is the problem.  "},{"title":"CQL0060: referenced table can be independently recreated so it cannot be used in a foreign key, 'referenced_table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0060-referenced-table-can-be-independently-recreated-so-it-cannot-be-used-in-a-foreign-key-referenced_table","content":"The referenced table is marked recreate so it must be in the same recreate group as the current table or in a recreate group that does not introduce a cyclic foreign key dependency among recreate groups. Otherwise, the referenced table might be recreated away leaving all the foreign key references in current table as orphans. So we check the following: If the referenced table is marked recreate then any of the following result in CQL0060 the containing table is not recreate at all (non-recreate table can't reference recreate tables at all), ORthe new foreign key dependency between the referenced table and the current table introduces a cycle The referenced table is a recreate table and one of the 4 above conditions was not met. Either don't reference it or else put the current table and the referenced table into the same recreate group.  "},{"title":"CQL0061: if multiple selects, all columns must be an exact type match (expected expected_type; found actual_type) 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0061-if-multiple-selects-all-columns-must-be-an-exact-type-match-expected-expected_type-found-actual_type-column","content":"In a stored proc with multiple possible selects providing the result, all of the columns of all the selects must be an exact type match. e.g. if x then select 1 A, 2 B else select 3 A, 4.0 B; end if;  Would provoke this error. In this case 'B' would be regarded as the offending column and the error is reported on the second B.  "},{"title":"CQL0062: if multiple selects, all columns must be an exact type match (including nullability) (expected expected_type; found actual_type) 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0062-if-multiple-selects-all-columns-must-be-an-exact-type-match-including-nullability-expected-expected_type-found-actual_type-column","content":"In a stored proc with multiple possible selects providing the result, all of the columns of all the selects must be an exact type match. This error indicates that the specified column differs by nullability.  "},{"title":"CQL0063: can't mix and match out statement with select/call for return values 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0063-cant-mix-and-match-out-statement-with-selectcall-for-return-values-procedure_name","content":"If the procedure is using SELECT to create a result set it cannot also use the OUT keyword to create a one-row result set.  "},{"title":"CQL0064: object variables may not appear in the context of a SQL statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0064-object-variables-may-not-appear-in-the-context-of-a-sql-statement","content":"SQLite doesn't understand object references, so that means you cannot try to use a variable or parameter of type object inside of a SQL statement. e.g. create proc foo(X object) begin select X is null; end;  In this example X is an object parameter, but even to use X for an is null check in a select statement would require binding an object which is not possible. On the other hand this compiles fine. create proc foo(X object, out is_null bool not null) begin set is_null := X is null; end;  This is another example of XQL being a two-headed beast.  "},{"title":"CQL0065: identifier is ambiguous 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0065-identifier-is-ambiguous-name","content":"There is more than one variable/column with indicated name in equally near scopes. The most common reason for this is that there are two column in a join with the same name and that name has not been qualified elsewhere. e.g. SELECT A FROM (SELECT 1 AS A, 2 AS B) AS T1 INNER JOIN (SELECT 1 AS A, 2 AS B) AS T2;  There are two possible columns named A. Fix this by using T1.A or T2.A.  "},{"title":"CQL0066: if a table is marked @recreate, its indices must be in its schema region 'index_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0066-if-a-table-is-marked-recreate-its-indices-must-be-in-its-schema-region-index_name","content":"If a table is marked @recreate that means that when it changes it is dropped and created during schema maintenance. Of course when it is dropped its indices are also dropped. So the indices must also be recreated when the table changes. So with such a table it makes no sense to have indices that are in a different schema region. This can only work if they are all always visible together. Tables on the @create plan are not dropped so their indices can be maintained separately. So they get a little extra flexibility. To fix this error move the offending index into the same schema region as the table. And probably put them physically close for maintenance sanity.  "},{"title":"CQL0067: cursor was not used with 'fetch [cursor]' 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0067-cursor-was-not-used-with-fetch-cursor--cursor_name","content":"The code is trying to access fields in the named cursor but the automatic field generation form was not used so there are no such fields. e.g. declare a integer; declare b integer; declare C cursor for select 1 A, 2 B; fetch C into a, b; -- C.A and C.B not created (!) if (C.A) then -- error ... end if;  Correct usage looks like this: declare C cursor for select 1 A, 2 B; fetch C; -- automatically creates C.A and C.B if (C.A) then ... end if;   "},{"title":"CQL0068: field not found in cursor 'field'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0068-field-not-found-in-cursor-field","content":"The indicated field is not a valid field in a cursor expression. e.g. declare C cursor for select 1 A, 2 B; fetch C; -- automatically creates C.A and C.B if (C.X) then -- C has A and B, but no X ... end if;   "},{"title":"CQL0069: name not found 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0069-name-not-found-name","content":"The indicated name could not be resolved in the scope in which it appears. Probably there is a typo. But maybe the name you need isn't available in the scope you are trying to use it in.  "},{"title":"CQL0070: incompatible object type 'incompatible_type'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0070-incompatible-object-type-incompatible_type","content":"Two expressions of type object are holding a different object type e.g. declare x object&lt;Foo&gt;; declare y object&lt;Bar&gt;; set x := y;  Here the message would report that 'Bar' is incompatible. The message generally refers to the 2nd object type as the first one was ok by default then the second one caused the problem.  "},{"title":"CQL0071: first operand cannot be a blob in 'BETWEEN/NOT BETWEEN'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0071-first-operand-cannot-be-a-blob-in-betweennot-between","content":"The BETWEEN operator works on numerics and strings only.  "},{"title":"CQL0072: first operand cannot be a blob in 'BETWEEN/NOT BETWEEN'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0072-first-operand-cannot-be-a-blob-in-betweennot-between","content":"The BETWEEN operator works on numerics and strings only.  "},{"title":"CQL0073: CAST may only appear in the context of SQL statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0073-cast-may-only-appear-in-the-context-of-sql-statement","content":"The CAST function does highly complex and subtle conversions, including date/time functions and other things. It's not possibly to emulate this accurately and there is no sqlite helper to do the job directly from a C call. Consequently it's only supported in the context of CQL statements. It can be used in normal expressions by using the nested SELECT form (select ...)  "},{"title":"CQL0074: too few arguments provided 'coalesce'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0074-too-few-arguments-provided-coalesce","content":"There must be at least two arguments in a call to coalesce.  "},{"title":"CQL0075: incorrect number of arguments 'ifnull'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0075-incorrect-number-of-arguments-ifnull","content":"The ifnull function requires exactly two arguments.  "},{"title":"CQL0076: NULL literal is useless in function 'ifnull/coalesce'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0076-null-literal-is-useless-in-function-ifnullcoalesce","content":"Adding a NULL literal to IFNULL or COALESCE is a no-op. It's most likely an error.  "},{"title":"CQL0077: encountered arg known to be not null before the end of the list, rendering the rest useless 'expression'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0077-encountered-arg-known-to-be-not-null-before-the-end-of-the-list-rendering-the-rest-useless-expression","content":"In an IFNULL or COALESCE call, only the last argument may be known to be not null. If a not null argument comes earlier in the list, then none of the others could ever be used. That is almost certainly an error. The most egregious form of this error is if the first argument is known to be not null in which case the entireIFNULL or COALESCE can be removed.  "},{"title":"CQL0078: [not] in (select ...) is only allowed inside of select lists, where, on, and having clauses​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0078-not-in-select--is-only-allowed-inside-of-select-lists-where-on-and-having-clauses","content":"The (select...) option for IN or NOT IN only makes sense in certain expression contexts. Other uses are most likely errors. It cannot appear in a loose expression because it fundamentally requires sqlite to process it.  "},{"title":"CQL0079: function got incorrect number of arguments 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0079-function-got-incorrect-number-of-arguments-name","content":"The indicated function was called with the wrong number of arguments. There are various functions supported each with different rules. See the SQLite documentation for more information about the specified function.  "},{"title":"CQL0080: function may not appear in this context 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0080-function-may-not-appear-in-this-context-name","content":"Many functions can only appear in certain contexts. For instance most aggregate functions are limited to the select list or the HAVING clause. They cannot appear in, for instance, a WHERE, or ON clause. The particulars vary by function.  "},{"title":"CQL0081: aggregates only make sense if there is a FROM clause 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0081-aggregates-only-make-sense-if-there-is-a-from-clause-name","content":"The indicated aggregate function was used in a select statement with no tables. For instance select MAX(7);  Doesn't make any sense.  "},{"title":"CQL0082: argument must be numeric​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0082-argument-must-be-numeric","content":"The argument of function must be numeric.  "},{"title":"CQL0083: argument must be numeric 'SUM'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0083-argument-must-be-numeric-sum","content":"The argument of SUM must be numeric.  "},{"title":"CQL0084: second argument must be a string in function 'group_concat'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0084-second-argument-must-be-a-string-in-function-group_concat","content":"The second argument of group_concat is the separator, it must be a string. The first argument will be converted to a string.  "},{"title":"CQL0085: all arguments must be strings 'strftime'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0085-all-arguments-must-be-strings-strftime","content":"The strftime function does complex data formatting. All the arguments are strings. See the sqlite documentation for more details on the options (there are many).  "},{"title":"CQL0086: first argument must be a string in function 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0086-first-argument-must-be-a-string-in-function-function","content":"The first argument of the function is the formatting string. The other arguments are variable and many complex conversions will apply.  "},{"title":"CQL0087: first argument must be of type real 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0087-first-argument-must-be-of-type-real-function","content":"The first argument of the function (e.g. round) should be of type 'real'.  "},{"title":"CQL0088: user function may not appear in the context of a SQL statement 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0088-user-function-may-not-appear-in-the-context-of-a-sql-statement-function_name","content":"External C functions declared with declare function ... are not for use in sqlite. They may not appear inside statements.  "},{"title":"CQL0089: user function may only appear in the context of a SQL statement 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0089-user-function-may-only-appear-in-the-context-of-a-sql-statement-function_name","content":"SQLite user defined functions (or builtins) declared with declare select function may only appear inside of sql statements. In the case of user defined functions they must be added to sqlite by the appropriate C APIs before they can be used in CQL stored procs (or any other context really). See the sqlite documentation on how to add user defined functions. Create Or Redefine SQL Functions  "},{"title":"CQL0090: object<T SET> has a T that is not a procedure with a result set, 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0090-objectt-set-has-a-t-that-is-not-a-procedure-with-a-result-set-name","content":"The data type object&lt;T SET&gt; refers to the shape of a result set of a particular procedure. In this case the indicated name is not such a procedure. The most likely source of this problem is that there is a typo in the indicated name. Alternatively the name might be a valid shape like a cursor name or some other shape name but it's a shape that isn't coming from a procedure.  CQL0091: -- generalized so that this is not an error anymore  "},{"title":"CQL0092: RAISE may only be used in a trigger statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0092-raise-may-only-be-used-in-a-trigger-statement","content":"SQLite only supports this kind of control flow in the context of triggers, certain trigger predicates might need to unconditionally fail and complex logic can be implemented in this way. However this sort of thing is not really recommended. In any case this is not a general purpose construct.  "},{"title":"CQL0093: RAISE 2nd argument must be a string​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0093-raise-2nd-argument-must-be-a-string","content":"Only forms with a string as the second argument are supported by SQLite.  "},{"title":"CQL0094: function not yet implemented 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0094-function-not-yet-implemented-function","content":"The indicated function is not implemented in CQL. Possibly you intended to declare it with declare function as an external function or declare select function as a sqlite builtin. Note not all sqlite builtins are automatically declared.  "},{"title":"CQL0095: table/view not defined 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0095-tableview-not-defined-name","content":"The indicated name is neither a table nor a view. It is possible that the table/view is now deprecated with @delete and therefore will appear to not exist in the current context.  "},{"title":"CQL0096: join using column not found on the left side of the join 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0096-join-using-column-not-found-on-the-left-side-of-the-join-column_name","content":"In the JOIN ... USING(x,y,z) form, all the columns in the using clause must appear on both sides of the join. Here the indicated name is not present on the left side of the join.  "},{"title":"CQL0097: join using column not found on the right side of the join 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0097-join-using-column-not-found-on-the-right-side-of-the-join-column_name","content":"In the JOIN ... USING(x,y,z) form, all the columns in the using clause must appear on both sides of the join. Here the indicated name is not present on the right side of the join.  "},{"title":"CQL0098: left/right column types in join USING(...) do not match exactly 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0098-leftright-column-types-in-join-using-do-not-match-exactly-column_name","content":"In the JOIN ... USING(x,y,z) form, all the columns in the using clause must appear on both sides of the join and have the same data type. Here the data types differ in the named column.  "},{"title":"CQL0099: HAVING clause requires GROUP BY clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0099-having-clause-requires-group-by-clause","content":"The HAVING clause makes no sense unless there is also a GROUP BY clause. SQLite enforces this as does CQL.  "},{"title":"CQL0100: duplicate common table name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0100-duplicate-common-table-name-name","content":"In a WITH clause, the indicated common table name was defined more than once.  "},{"title":"CQL0101: too few column names specified in common table expression 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0101-too-few-column-names-specified-in-common-table-expression-name","content":"In a WITH clause the indicated common table expression doesn't include enough column names to capture the result of the select statement it is associated with. e.g. WITH foo(a) as (SELECT 1 A, 2 B) ...`  The select statement produces two columns the foo declaration specifies one.  "},{"title":"CQL0102: too many column names specified in common table expression 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0102-too-many-column-names-specified-in-common-table-expression-name","content":"In a WITH clause the indicated common table expression has more column names than the select expression it is associated with. e.g. WITH foo(a, b) as (SELECT 1) ... `  The select statement produces one column the foo declaration specifies two.  "},{"title":"CQL0103: duplicate table/view name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0103-duplicate-tableview-name-name","content":"The indicated table or view must be unique in its context. The version at the indicated line number is a duplicate of a previous declaration.  "},{"title":"CQL0104: view was present but now it does not exist (use @delete instead) 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0104-view-was-present-but-now-it-does-not-exist-use-delete-instead-name","content":"During schema validation, CQL found a view that used to exist but is now totally gone. The correct procedure is to mark the view with @delete (you can also make it stub with the same name to save a little space). This is necessary so that CQL can know what views should be deleted on client devices during an upgrade. If the view is eradicated totally there would be no way to know that the view should be deleted if it exists.  "},{"title":"CQL0105: object was a view but is now a table 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0105-object-was-a-view-but-is-now-a-table-name","content":"Converting a view into a table, or otherwise creating a table with the same name as a view is not legal.  "},{"title":"CQL0106: trigger was present but now it does not exist (use @delete instead) 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0106-trigger-was-present-but-now-it-does-not-exist-use-delete-instead-name","content":"During schema validation, CQL found a trigger that used to exist but is now totally gone. The correct procedure is to mark the trigger with @delete (you can also make it stub with the same name to save a little space). This is necessary so that CQL can know what triggers should be deleted on client devices during an upgrade. If the trigger is eradicated totally there would be no way to know that the trigger should be deleted if it exists. That would be bad.  "},{"title":"CQL0107: delete version can't be <= create version 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0107-delete-version-cant-be--create-version-name","content":"Attempting to declare that an object has been deleted before it was created is an error. Probably there is a typo in one or both of the version numbers of the named object.  "},{"title":"CQL0108: table in drop statement does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0108-table-in-drop-statement-does-not-exist-table_name","content":"The indicated table was not declared anywhere. Note that CQL requires that you declare all tables you will work with, even if all you intend to do with the table is drop it. When you put a CREATE TABLE statement in global scope this only declares a table, it doesn't actually create the table. See the documentation on DDL for more information.  "},{"title":"CQL0109: cannot drop a view with drop table 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0109-cannot-drop-a-view-with-drop-table-view_name","content":"The object named in a DROP TABLE statement must be a table, not a view.  "},{"title":"CQL0110: view in drop statement does not exist 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0110-view-in-drop-statement-does-not-exist-view_name","content":"The indicated view was not declared anywhere. Note that CQL requires that you declare all views you will work with, even if all you intend to do with the view is drop it. When you put a CREATE VIEW statement in global scope this only declares a view, it doesn't actually create the view. See the documentation on DDL for more information.  "},{"title":"CQL0111: cannot drop a table with drop view 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0111-cannot-drop-a-table-with-drop-view-name","content":"The object named in a DROP VIEW statement must be a view, not a table.  "},{"title":"CQL0112: index in drop statement was not declared 'index_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0112-index-in-drop-statement-was-not-declared-index_name","content":"The indicated index was not declared anywhere. Note that CQL requires that you declare all indices you will work with, even if all you intend to do with the index is drop it. When you put a CREATE INDEX statement in global scope this only declares an index, it doesn't actually create the index. See the documentation on DDL for more information.  "},{"title":"CQL0113: trigger in drop statement was not declared 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0113-trigger-in-drop-statement-was-not-declared-name","content":"The indicated trigger was not declared anywhere. Note that CQL requires that you declare all triggers you will work with, even if all you intend to do with the trigger is drop it. When you put a CREATE TRIGGER statement in global scope this only declares a trigger, it doesn't actually create the trigger. See the documentation on DDL for more information.  "},{"title":"CQL0114: current schema can't go back to recreate semantics for 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0114-current-schema-cant-go-back-to-recreate-semantics-for-table_name","content":"The indicated table was previously marked with @create indicating it has precious content and should be upgraded carefully. The current schema marks the same table with @recreate meaning it has discardable content and should be upgraded by dropping it and recreating it. This transition is not allowed. If the table really is non-precious now you can mark it with @delete and then make a new similar table with @recreate. This really shouldn't happen very often if at all. Probably the error is due to a typo or wishful thinking.  "},{"title":"CQL0115: current create version not equal to previous create version for 'table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0115-current-create-version-not-equal-to-previous-create-version-for-table","content":"The indicated table was previously marked with @create at some version (x) and now it is being created at some different version (y !=x ). This not allowed (if it were then objects might be created in the wrong/different order during upgrade which would cause all kinds of problems).  "},{"title":"CQL0116: current delete version not equal to previous delete version for 'table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0116-current-delete-version-not-equal-to-previous-delete-version-for-table","content":"The indicated table was previously marked with @delete at some version (x) and now it is being deleted at some different version (y != x). This not allowed (if it were then objects might be deleted in the wrong/different order during upgrade which would cause all kinds of problems).  "},{"title":"CQL0117: @delete procedure changed in object 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0117-delete-procedure-changed-in-object-table_name","content":"The @delete attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0118: @create procedure changed in object 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0118-create-procedure-changed-in-object-table_name","content":"The @create attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0119: column name is different between previous and current schema 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0119-column-name-is-different-between-previous-and-current-schema-name","content":"Since there is no sqlite operation that allows for columns to be renamed, attempting to rename a column is not allowed. NOTE: you can also get this error if you remove a column entirely, or add a column in the middle of the list somewhere. Since columns (also) cannot be reordered during upgrade, CQL expects to find all the columns in exactly the same order in the previous and new schema. Any reordering, or deletion could easily look like an erroneous rename. New columns must appear at the end of any existing columns.  "},{"title":"CQL0120: column type is different between previous and current schema 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0120-column-type-is-different-between-previous-and-current-schema-name","content":"It is not possible to change the data type of a column during an upgrade, SQLite provides no such options. Attempting to do so results in an error. This includes nullability.  "},{"title":"CQL0121: column current create version not equal to previous create version 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0121-column-current-create-version-not-equal-to-previous-create-version-name","content":"The indicated column was previously marked with @create at some version (x) and now it is being created at some different version (y !=x ). This not allowed (if it were then objects might be created in the wrong/different order during upgrade which would cause all kinds of problems). "},{"title":"CQL0122: column current delete version not equal to previous delete version 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0122-column-current-delete-version-not-equal-to-previous-delete-version-name","content":"The indicated column was previously marked with @delete at some version (x) and now it is being deleted at some different version (y != x). This not allowed (if it were then objects might be deleted in the wrong/different order during upgrade which would cause all kinds of problems).  "},{"title":"CQL0123: column @delete procedure changed 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0123-column-delete-procedure-changed-name","content":"The @delete attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0124: column @create procedure changed 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0124-column-create-procedure-changed-name","content":"The @create attribute can optional include a &quot;migration proc&quot; that is run when the upgrade happens. Once set, this proc can never be changed.  "},{"title":"CQL0125: column current default value not equal to previous default value 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0125-column-current-default-value-not-equal-to-previous-default-value-column","content":"The default value of a column may not be changed in later versions of the schema. There is no SQLite operation that would allow this.  "},{"title":"CQL0126: table was present but now it does not exist (use @delete instead) 'table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0126-table-was-present-but-now-it-does-not-exist-use-delete-instead-table","content":"During schema validation, CQL found a table that used to exist but is now totally gone. The correct procedure is to mark the table with @delete. This is necessary so that CQL can know what tables should be deleted on client devices during an upgrade. If the table is eradicated totally there would be no way to know that the table should be deleted if it exists. That would be bad.  "},{"title":"CQL0127: object was a table but is now a view 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0127-object-was-a-table-but-is-now-a-view-name","content":"The indicated object was a table in the previous schema but is now a view in the current schema. This transformation is not allowed.  "},{"title":"CQL0128: table has a column that is different in the previous and current schema 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0128-table-has-a-column-that-is-different-in-the-previous-and-current-schema-column","content":"The indicated column changed in one of its more exotic attributes, examples: its FOREIGN KEY rules changed in some wayits PRIMARY KEY status changedits UNIQUE status changed Basically the long form description of the column is now different and it isn't different in one of the usual way like type or default value. This error is the catch all for all the other ways a column could change such as &quot;the FK rule for what happens when an update fk violation occurs is now different&quot; -- there are dozens of such errors and they aren't very helpful anyway.  "},{"title":"CQL0129: a column was removed from the table rather than marked with @delete 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0129-a-column-was-removed-from-the-table-rather-than-marked-with-delete-column_name","content":"During schema validation, CQL found a column that used to exist but is now totally gone. The correct procedure is to mark the column with @delete. This is necessary so that CQL can know what columns existed during any version of the schema, thereby allowing them to be used in migration scripts during an upgrade. If the column is eradicated totally there would be no way to know that the exists, and should no longer be used. That would be bad. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0130: table has columns added without marking them @create 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0130-table-has-columns-added-without-marking-them-create-column_name","content":"The indicated column was added but it was not marked with @create. The table in question is not on the @recreate plan so this is an error. Add a suitable @create annotation to the column declaration.  "},{"title":"CQL0131: table has newly added columns that are marked both @create and @delete 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0131-table-has-newly-added-columns-that-are-marked-both-create-and-delete-column_name","content":"The indicated column was simultaneously marked @create and @delete. That's surely some kind of typo. Creating a column and deleting it in the same version is weird.  "},{"title":"CQL0132: table has a facet that is different in the previous and current schema 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0132-table-has-a-facet-that-is-different-in-the-previous-and-current-schema-table_name","content":"The indicated table has changes in one of its non-column features. These changes might be: a primary key declarationa unique key declarationa foreign key declaration None of these are allowed to change. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0133: non-column facets have been removed from the table 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0133-non-column-facets-have-been-removed-from-the-table-name","content":"The error indicates that the table has had some stuff removed from it. The &quot;stuff&quot; might be: a primary key declarationa unique key declarationa foreign key declaration Since there is no way to change any of the constraints after the fact, they may not be changed at all if the table is on the @create plan. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0134: table has a new non-column facet in the current schema 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0134-table-has-a-new-non-column-facet-in-the-current-schema-table_name","content":"The error indicates that the table has had some stuff added to it. The &quot;stuff&quot; might be: a primary key declarationa unique key declarationa foreign key declaration Since there is no way to change any of the constraints after the fact, they may not be changed at all if the table is on the @create plan. Of course @recreate tables will never get this error because they can be altered at whim.  "},{"title":"CQL0135: table create statement attributes different than previous version 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0135-table-create-statement-attributes-different-than-previous-version-table_name","content":"The 'flags' on the CREATE TABLE statement changed between versions. These flags capture the options like theTEMP in CREATE TEMP TABLE and the IF NOT EXISTS. Changing these is not allowed.  "},{"title":"CQL0136: trigger already exists 'trigger_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0136-trigger-already-exists-trigger_name","content":"Trigger names may not be duplicated. Probably there is copy/pasta going on here.  "},{"title":"CQL0137: table/view not found 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0137-tableview-not-found-name","content":"In a CREATE TRIGGER statement, the indicated name is neither a table or a view. Either a table or a view was expected in this context.  "},{"title":"CQL0138: a trigger on a view must be the INSTEAD OF form 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0138-a-trigger-on-a-view-must-be-the-instead-of-form-name","content":"In a CREATE TRIGGER statement, the named target of the trigger was a view but the trigger type is not INSTEAD OF. Only INSTEAD OF can be applied to views because views are not directly mutable so none of the other types make sense. e.g. there can be no delete operations, on a view, so BEFORE DELETE or AFTER DELETE are not really a thing.  "},{"title":"CQL0139: temp objects may not have versioning annotations 'object_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0139-temp-objects-may-not-have-versioning-annotations-object_name","content":"The indicated object is a temporary. Since temporary do not survive sessions it makes no sense to try to version them for schema upgrade. They are always recreated on demand. If you need to remove one, simply delete it entirely, it requires no tombstone.  "},{"title":"CQL0140: columns in a temp table may not have versioning attributes 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0140-columns-in-a-temp-table-may-not-have-versioning-attributes-column_name","content":"The indicated column is part of a temporary table. Since temp tables do not survive sessions it makes no sense to try to version their columns for schema upgrade. They are always recreated on demand.  "},{"title":"CQL0141: table has an AUTOINCREMENT column; it cannot also be WITHOUT ROWID 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0141-table-has-an-autoincrement-column-it-cannot-also-be-without-rowid-table_name","content":"SQLite uses its ROWID internally for AUTOINCREMENT columns. Therefore WITHOUT ROWID is not a possibility if AUTOINCREMENT is in use.  "},{"title":"CQL0142: duplicate column name 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0142-duplicate-column-name-column_name","content":"In a CREATE TABLE statement, the indicated column was defined twice. This is probably a copy/pasta issue.  "},{"title":"CQL0143: more than one primary key in table 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0143-more-than-one-primary-key-in-table-table_name","content":"The indicated table has more than one column with the PRIMARY KEY attribute or multiple PRIMARY KEY constraints, or a combination of these things. You'll have to decide which one is really intended to be primary.  "},{"title":"CQL0144: cannot alter a view 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0144-cannot-alter-a-view-view_name","content":"In an ALTER TABLE statement, the table to be altered is actually a view. This is not allowed.  "},{"title":"CQL0144: table in alter statement does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0144-table-in-alter-statement-does-not-exist-table_name","content":"In an ALTER TABLE statement, the table to be altered was not defined, or perhaps was marked with @delete and is no longer usable in the current schema version. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0145: version annotations not valid in alter statement 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0145-version-annotations-not-valid-in-alter-statement-column_name","content":"In an ALTER TABLE statement, the attributes on the column may not include @create or @delete. Those annotations go on the columns declaration in the corresponding CREATE TABLE statement. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0146: adding an auto increment column is not allowed 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0146-adding-an-auto-increment-column-is-not-allowed-column_name","content":"In an ALTER TABLE statement, the attributes on the column may not include AUTOINCREMENT. SQLite does not support the addition of new AUTOINCREMENT columns. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0147: adding a not nullable column with no default value is not allowed 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0147-adding-a-not-nullable-column-with-no-default-value-is-not-allowed-column_name","content":"In an ALTER TABLE statement the attributes on the named column must include a default value or else the column must be nullable. This is so that SQLite knows what value to put on existing rows when the column is added and also so that any existing insert statements will not suddenly all become invalid. If the column is nullable or has a default value then the existing insert statements that don't specify the column will continue to work, using either NULL or the default. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0148: added column must already be reflected in declared schema, with @create, exact name match required 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0148-added-column-must-already-be-reflected-in-declared-schema-with-create-exact-name-match-required-column_name","content":"In CQL loose schema is a declaration, it does not actually create anything unless placed inside of a procedure. A column that is added with ALTER TABLE is not actually declared as part of the schema by the ALTER. Rather the schema declaration is expected to include any columns you plan to add. Normally the way this all happens is that you put @create notations on a column in the schema and the automatic schema upgrader then creates suitable ALTER TABLE statements to arrange for that column to be added. If you manually write an ALTER TABLE statement it isn't allowed to add columns at whim; in some sense it must be creating the reality already described in the declaration. This is exactly what the automated schema upgrader does -- it declares the end state and then alters the world to get to that state. It's important to remember that from CQL's perspective the schema is fixed for any given compilation, so runtime alterations to it are not really part of the type system. They can't be. Even DROP TABLE does not remove the table from type system -- it can't -- the most likely situation is that you are about to recreate that same table again for another iteration with the proc that creates it. This particular error is saying that the column you are trying to add does not exist in the declared schema. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0149: added column must be an exact match for the column type declared in the table 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0149-added-column-must-be-an-exact-match-for-the-column-type-declared-in-the-table-column_name","content":"In CQL loose schema is a declaration, it does not actually create anything unless placed inside of a procedure. A column that is added with ALTER TABLE is not actually declared as part of the schema by the ALTER. Rather the schema declaration is expected to include any columns you plan to add. Normally the way this all happens is that you put @create notations on a column in the schema and the automatic schema upgrader then creates suitable ALTER TABLE statements to arrange for that column to be added. If you manually write an ALTER TABLE statement it isn't allowed to add columns at whim; in some sense it must be creating the reality already described in the declaration. This is exactly what the automated schema upgrader does -- it declares the end state and then alters the world to get to that state. It's important to remember that from CQL's perspective the schema is fixed for any given compilation, so runtime alterations to it are not really part of the type system. They can't be. Even DROP TABLE does not remove the table from type system -- it can't -- the most likely situation is that you are about to recreate that same table again for another iteration with the proc that creates it. This particular error is saying that the column you are trying to add exists in the declared schema, but its definition is different than you have specified in the ALTER TABLE statement. NOTE: ALTER TABLE is typically not used directly; the automated schema upgrade script generation system uses it.  "},{"title":"CQL0150: expected numeric expression in IF predicate​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0150-expected-numeric-expression-in-if-predicate","content":"In an IF statement the condition (predicate) must be a numeric. The body of the IF runs if the value is not null and not zero.  "},{"title":"CQL0151: table in delete statement does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0151-table-in-delete-statement-does-not-exist-table_name","content":"In a DELETE statement, the indicated table does not exist. Probably it's a spelling mistake, or else the table has been marked with @delete and may no longer be used in DELETE statements.  "},{"title":"CQL0152: cannot delete from a view 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0152-cannot-delete-from-a-view-view_name","content":"In a DELETE statement, the target of the delete must be a table, but the indicated name is a view.  "},{"title":"CQL0153: duplicate target column name in update statement 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0153-duplicate-target-column-name-in-update-statement-column_name","content":"In an UPDATE statement, you can only specify any particular column to update once. e.g. UPDATE coordinates set x = 1, x = 3; will produce this error. UPDATE coordinates set x = 1, y = 3; might be correct. This error is most likely caused by a typo or a copy/pasta of the column names, especially if they were written one per line.  "},{"title":"CQL0154: table in update statement does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0154-table-in-update-statement-does-not-exist-table_name","content":"In an UPDATE statement, the target table does not exist. Probably it's a spelling mistake, or else the table has been marked with @delete and may no longer be used in UPDATE statements.  "},{"title":"CQL0155: cannot update a view 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0155-cannot-update-a-view-view_name","content":"In an UPDATE statement, the target of the update must be a table but the name of a view was provided.  "},{"title":"CQL0156: seed expression must be a non-nullable integer​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0156-seed-expression-must-be-a-non-nullable-integer","content":"The INSERT statement statement supports the notion of synthetically generated values for dummy data purposes. A 'seed' integer is used to derive the values. That seed (in the @seed() position) must be a non-null integer. The most common reason for this error is that the seed is an input parameter and it was not declared NOT NULL.  "},{"title":"CQL0157: count of columns differs from count of values​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0157-count-of-columns-differs-from-count-of-values","content":"In an INSERT statement of the form INSERT INTO foo(a, b, c) VALUES(x, y, z) the number of values (x, y, z) must be the same as the number of columns (a, b, c). Note that there are many reasons you might not have to specify all the columns of the table but whichever columns you do specify should have values.  "},{"title":"CQL0158: required column missing in INSERT statement 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0158-required-column-missing-in-insert-statement-column_name","content":"In an INSERT statement such as INSERT INTO foo(a,b,c) VALUES(x,yz) this error is indicating that there is a column in foo (the one indicated in the error) which was not in the list (i.e. not one of a, b, c) and that column is neither nullable, nor does it have a default value. In order to insert a row a value must be provided. To fix this include the indicated column in your insert statement.  "},{"title":"CQL0159: cannot add an index to a virtual table 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0159-cannot-add-an-index-to-a-virtual-table-table_name","content":"Adding an index to a virtual table isn't possible, the virtual table includes whatever indexing its module provides, no further indexing is possible. From the SQLite documentation: &quot;One cannot create additional indices on a virtual table. (Virtual tables can have indices but that must be built into the virtual table implementation. Indices cannot be added separately using CREATE INDEX statements.)&quot;  "},{"title":"CQL0160: table in insert statement does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0160-table-in-insert-statement-does-not-exist-table_name","content":"In an INSERT statement attempting to insert into the indicated table name is not possible because there is no such table. This error might happen because of a typo, or it might happen because the indicated table has been marked with @delete and is logically hidden.  "},{"title":"CQL0161: cannot insert into a view 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0161-cannot-insert-into-a-view-view_name","content":"In an INSERT statement attempting to insert into the indicated name is not possible because that name is a view not a table. Inserting into views is not supported.  "},{"title":"CQL0162: cannot add a trigger to a virtual table 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0162-cannot-add-a-trigger-to-a-virtual-table-table_name","content":"Adding a trigger to a virtual table isn't possible. From the SQLite documentation: &quot;One cannot create a trigger on a virtual table.&quot;  "},{"title":"CQL0163: FROM ARGUMENTS construct is only valid inside a procedure​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0163-from-arguments-construct-is-only-valid-inside-a-procedure","content":"Several statements support the FROM ARGUMENTS sugar format like INSERT INTO foo(a,b,c) FROM ARGUMENTS which causes the arguments of the current procedure to be used as the values. This error is complaining that you have used this form but the statement does not occur inside of a procedure so there can be no arguments. This form does not make sense outside of any procedure.  "},{"title":"CQL0164: cannot use ALTER TABLE on a virtual table 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0164-cannot-use-alter-table-on-a-virtual-table-table_name","content":"This is not supported by SQLite. From the SQLite documentation: &quot;One cannot run ALTER TABLE ... ADD COLUMN commands against a virtual table.&quot;  "},{"title":"CQL0165: fetch values is only for value cursors, not for sqlite cursors 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0165-fetch-values-is-only-for-value-cursors-not-for-sqlite-cursors-cursor_name","content":"Cursors come in two flavors. There are &quot;statement cursors&quot; which are built from something like this: declare C cursor for select * from foo; fetch C; -- or -- fetch C into a, b, c;  That is, they come from a SQLite statement and you can fetch values from that statement. The second type comes from procedural values like this. declare C cursor like my_table; fetch C from values(1, 2, 3);  In the second example C's data type will be the same as the columns in my_table and we will fetch its values from 1,2,3 -- this version has no database backing at all, it's just data. This error says that you declared the cursor in the first form (with a SQL statement) but then you tried to fetch it using the second form, the one for data. These forms don't mix. If you need a value cursor for a row you can copy data from one cursor into another.  "},{"title":"CQL0166: count of columns differs from count of values​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0166-count-of-columns-differs-from-count-of-values","content":"In a value cursor, declared something like this: declare C cursor like my_table; fetch C from values(1, 2, 3);  The type of the cursor ( in this case from my_table) requires a certain number of columns, but that doesn't match the number that were provided in the values. To fix this you'll need to add/remove values so that the type match.  "},{"title":"CQL0167: required column missing in FETCH statement 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0167-required-column-missing-in-fetch-statement-column_name","content":"In a value cursor, declared something like this: declare C cursor like my_table; fetch C(a,b,c) from values(1, 2, 3);  This error is saying that there is some other field in the table 'd' and it was not specified in the values. Nor was there a usable dummy data for that column that could be used. You need to provide a value for the missing column.  "},{"title":"CQL0168: CQL has no good way to generate dummy blobs; not supported for now​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0168-cql-has-no-good-way-to-generate-dummy-blobs-not-supported-for-now","content":"In a value cursor with dummy data specified, one of the columns in the cursor is of type blob. There's no good way to create dummy data for blobs so that isn't supported.  "},{"title":"CQL0169: enum not found 'enum_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0169-enum-not-found-enum_name","content":"The indicated name was used in a context where an enumerated type name was expected but there is no such type. Perhaps the enum was not included (missing a #include) or else there is a typo.  "},{"title":"CQL0170: cast is redundant, remove to reduce code size 'expression'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0170-cast-is-redundant-remove-to-reduce-code-size-expression","content":"The operand of the CAST expression is already the type that it is being cast to. The cast will do nothing but waste space in the binary and make the code less clear. Remove it.  "},{"title":"CQL0171: name not found 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0171-name-not-found-name","content":"In a scoped name list, like the columns of a cursor (for a fetch), or the columns of a particular table (for an index) a name appeared that did not belong to the universe of legal names. Trying to make a table index using a column that is not in the table would produce this error. There are many instances where a list of names belongs to some limited scope.  "},{"title":"CQL0172: name list has duplicate name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0172-name-list-has-duplicate-name-name","content":"In a scoped name list, like the columns of a cursor (for a fetch), or the columns of a particular table (for an index) a name appeared twice in the list where the names must be unique. Trying to make a table index using the same column twice would produce this error.  "},{"title":"CQL0173: variable not found 'variable_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0173-variable-not-found-variable_name","content":"In a SET statement, the target of the assignment is not a valid variable name in that scope.  "},{"title":"CQL0174: cannot set a cursor 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0174-cannot-set-a-cursor-cursor_name","content":"In a SET statement, the target of the assignment is a cursor variable, you cannot assign to a cursor variable.  "},{"title":"CQL0175: duplicate parameter name 'parameter_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0175-duplicate-parameter-name-parameter_name","content":"In a parameter list for a function or a procedure, the named parameter appears more than once. The formal names for function arguments must be unique.  "},{"title":"CQL0176: indicated procedure or group already has a recreate action 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0176-indicated-procedure-or-group-already-has-a-recreate-action-name","content":"There can only be one migration rule for a table or group, the indicated item already has such an action. If you need more than one migration action you can create a containing procedure that dispatches to other migrators.  "},{"title":"CQL0177: global constants must be either constant numeric expressions or string literals 'constant_definition'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0177-global-constants-must-be-either-constant-numeric-expressions-or-string-literals-constant_definition","content":"Global constants must be either a combination other constants for numeric expressions or else string literals. The indicated expression was not one of those. This can happen if the expression uses variables, or has other problems that prevent it from evaluating, or if a function is used that is not supported.  "},{"title":"CQL0178: proc has no result 'like_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0178-proc-has-no-result-like_name","content":"In an argument list, the LIKE construct was used to create arguments that are the same as the return type of the named procedure. However the named procedure does not produce a result set and therefore has no columns to mimic. Probably the name is wrong.  "},{"title":"CQL0179: shared fragments must consist of exactly one top level statement 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0179-shared-fragments-must-consist-of-exactly-one-top-level-statement-procedure_name","content":"Any shared fragment can have only one statement. There are three valid forms -- IF/ELSE, WITH ... SELECT, and SELECT. This error indicates the named procedure, which is a shared fragment, has more than one statement.  "},{"title":"CQL0180: duplicate column name in result not allowed 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0180-duplicate-column-name-in-result-not-allowed-column_name","content":"In a procedure that returns a result either with a loose SELECT statement or in a place where the result of a SELECT is captured with a FETCH statement the named column appears twice in the projection of the SELECT in question. The column names must be unique in order to have consistent cursor field names or consistent access functions for the result set of the procedure. One instance of the named column must be renamed with something like select T1.foo first_foo, T2.foo second_foo.  "},{"title":"CQL0181: autodrop temp table does not exist 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0181-autodrop-temp-table-does-not-exist-name","content":"In a cql:autodrop annotation, the given name is unknown entirely.  "},{"title":"CQL0182: autodrop target is not a table 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0182-autodrop-target-is-not-a-table-name","content":"In a cql:autodrop annotation, the given name is not a table (it's probably a view).  "},{"title":"CQL0183: autodrop target must be a temporary table 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0183-autodrop-target-must-be-a-temporary-table-name","content":"In a cql:autodrop annotation, the given name is a table but it is not a temp table. The annotation is only valid on temp tables, it's not for &quot;durable&quot; tables.  "},{"title":"CQL0184: stored procedures cannot be nested 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0184-stored-procedures-cannot-be-nested-name","content":"The CREATE PROCEDURE statement may not appear inside of another stored procedure. The named procedure appears in a nested context.  "},{"title":"CQL0185: proc name conflicts with func name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0185-proc-name-conflicts-with-func-name-name","content":"In a CREATE PROCEDURE statement, the given name conflicts with an already declared function (DECLARE FUNCTION or DECLARE SELECT FUNCTION). You'll have to choose a different name.  "},{"title":"CQL0186: duplicate stored proc name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0186-duplicate-stored-proc-name-name","content":"In a CREATE PROCEDURE statement, the indicated name already corresponds to a created (not just declared) stored procedure. You'll have to choose a different name.  "},{"title":"CQL0187: @schema_upgrade_version not declared or doesn't match upgrade version N for proc 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0187-schema_upgrade_version-not-declared-or-doesnt-match-upgrade-version-n-for-proc-name","content":"The named procedure was declared as a schema migration procedure in an @create or @delete annotation for schema version N. In order to correctly type check such a procedure it must be compiled in the context of schema version N. This restriction is required so that the tables and columns the procedure sees are the ones that existed in version N not the ones that exist in the most recent version as usual. To create this condition, the procedure must appear in a file that begins with the line: @schema_upgrade_version &lt;N&gt;;  And this declaration must come before any CREATE TABLE statements. If there is no such declaration, or if it is for the wrong version, then this error will be generated.  "},{"title":"CQL0188: procedure is supposed to do schema migration but it doesn't have any DML 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0188-procedure-is-supposed-to-do-schema-migration-but-it-doesnt-have-any-dml-name","content":"The named procedure was declared as a schema migration procedure in an @create or @delete annotation, however the procedure does not have any DML in it. That can't be right. Some kind of data reading and writing is necessary.  "},{"title":"CQL0189: procedure declarations/definitions do not match 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0189-procedure-declarationsdefinitions-do-not-match-name","content":"The named procedure was previously declared with a DECLARE PROCEDURE statement but when the CREATE PROCEDURE was encountered, it did not match the previous declaration.  "},{"title":"CQL0190: duplicate column name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0190-duplicate-column-name-name","content":"In a context with a typed name list (e.g. id integer, t text) the named column occurs twice. Typed name lists happen in many contexts, but a common one is the type of the result in a declared procedure statement or declared function statement.  "},{"title":"CQL0191: declared functions must be top level 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0191-declared-functions-must-be-top-level-function_name","content":"A DECLARE FUNCTION statement for the named function is happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure.  "},{"title":"CQL0192: func name conflicts with proc name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0192-func-name-conflicts-with-proc-name-name","content":"The named function in a DECLARE FUNCTION statement conflicts with an existing declared or created procedure. One or the other must be renamed to resolve this issue.  "},{"title":"CQL0193: duplicate function name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0193-duplicate-function-name-name","content":"The named function in a DECLARE FUNCTION statement conflicts with an existing declared function, or it was declared twice. One or the other declaration must be renamed or removed to resolve this issue.  "},{"title":"CQL0194: declared procedures must be top level 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0194-declared-procedures-must-be-top-level-name","content":"A DECLARE PROCEDURE statement for the named procedure is itself happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure.  "},{"title":"CQL0195: proc name conflicts with func name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0195-proc-name-conflicts-with-func-name-name","content":"The named procedure in a DECLARE PROCEDURE statement conflicts with an existing declared function. One or the other declaration must be renamed or removed to resolve this issue.  "},{"title":"CQL0196: procedure declarations/definitions do not match 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0196-procedure-declarationsdefinitions-do-not-match-name","content":"The named procedure was previously declared with a DECLARE PROCEDURE statement. Now there is another declaration and it does not match the previous declaration  "},{"title":"CQL0197: duplicate variable name in the same scope 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0197-duplicate-variable-name-in-the-same-scope-name","content":"In a DECLARE statement, a variable of the same name already exists in that scope. Note that CQL does not have block level scope, all variables are procedure level, so they are in scope until the end of the procedure. To resolve this problem, either re-use the old variable if appropriate or rename the new variable.  "},{"title":"CQL0198: global variable hides table/view name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0198-global-variable-hides-tableview-name-name","content":"In a DECLARE statement, the named variable is a global (declared outside of any procedure) and has the same name as a table or view. This creates a lot of confusion and is therefore disallowed. To correct the problem, rename the variable. Global variables generally are problematic, but sometimes necessary.  "},{"title":"CQL0199: cursor requires a procedure that returns a result set via select 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0199-cursor-requires-a-procedure-that-returns-a-result-set-via-select-procedure_name","content":"In a DECLARE statement that declares a CURSOR FOR CALL the procedure that is being called does not produce a result set with the SELECT statement. As it has no row results it is meaningless to try to put a cursor on it. Probably the error is due to a copy/pasta of the procedure name.  "},{"title":"CQL0200: variable is not a cursor 'another_cursor'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0200-variable-is-not-a-cursor-another_cursor","content":"In a DECLARE statement that declares a CURSOR LIKE another cursor, the indicated name is a variable but it is not a cursor, so we cannot make another cursor like it. Probably the error is due to a typo in the 'like_name'.  "},{"title":"CQL0201: expanding FROM ARGUMENTS, there is no argument matching 'required_arg'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0201-expanding-from-arguments-there-is-no-argument-matching-required_arg","content":"In an INSERT or FETCH statement using the form FROM ARGUMENTS(LIKE [name])The shape [name] had columns that did not appear in as arguments to the current procedure. Maybe arguments are missing or maybe the name in the like part is the wrong name.  "},{"title":"CQL0202: must be a cursor, proc, table, or view 'like_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0202-must-be-a-cursor-proc-table-or-view-like_name","content":"In a DECLARE statement that declares a CURSOR LIKE some other name, the indicated name is not the name of any of the things that might have a valid shape to copy, like other cursors, procedures, tables, or views. Probably there is a typo in the name.  "},{"title":"CQL0203: cursor requires a procedure that returns a cursor with OUT 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0203-cursor-requires-a-procedure-that-returns-a-cursor-with-out-cursor_name","content":"In the DECLARE [cursor_name] CURSOR FETCH FROM CALL &lt;something&gt; form, the code is trying to create the named cursor by calling a procedure that doesn't actually produce a single row result set with the OUT statement. The procedure is valid (that would be a different error) so it's likely that the wrong procedure is being called rather than being an outright typo. Or perhaps the procedure was changed such that it no longer produces a single row result set. This form is equivalent to: DECLARE [cursor_name] LIKE procedure; FETCH [cursor_name] FROM CALL procedure(args);  It's the declaration that's failing here, not the call.  CQL0204 : Unused.  "},{"title":"CQL0205: not a cursor 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0205-not-a-cursor-name","content":"The indicated name appeared in a context where the name of a cursor was expected, but the name does not refer to a cursor.  "},{"title":"CQL0206: duplicate name in list 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0206-duplicate-name-in-list-name","content":"There are many contexts where a list of names appears in the CQL grammar and the list must not contain duplicate names. Some examples are: the column names in a JOIN ... USING(x,y,z,...) clausethe fetched variables in a FETCH [cursor] INTO x,y,z... statementthe column names listed in a common table expression CTE(x,y,z,...) as (SELECT ...)the antecedent schema region names in @declare_schema_region &lt;name&gt; USING x,y,z,... The indicated name was duplicated in such a context.  "},{"title":"CQL0207: expected a variable name for OUT or INOUT argument 'param_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0207-expected-a-variable-name-for-out-or-inout-argument-param_name","content":"In a procedure call, the indicated parameter of the procedure is an OUT or INOUT parameter but the call site doesn't have a variable in that position in the argument list. Example: declare proc foo(out x integer); -- the constant 1 cannot be used in the out position when calling foo call foo(1); '   "},{"title":"CQL0208: shared fragments cannot have any out or in/out parameters 'param_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0208-shared-fragments-cannot-have-any-out-or-inout-parameters-param_name","content":"A shared fragment will be expanded into the body of a SQL select statement, as such it can have no side-effects such as out arguments.  "},{"title":"CQL0209: proc out parameter: arg must be an exact type match (expected expected_type; found actual_type) 'param_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0209-proc-out-parameter-arg-must-be-an-exact-type-match-expected-expected_type-found-actual_type-param_name","content":"In a procedure call, the indicated parameter is in an 'out' position, it is a viable local variable but it is not an exact type match for the parameter. The type of variable used to capture out parameters must be an exact match. declare proc foo(out x integer); create proc bar(out y real) begin call foo(y); -- y is a real variable, not an integer. end;  The above produces: CQL0209: proc out parameter: arg must be an exact type match (expected integer; found real) 'y'   "},{"title":"CQL0210: proc out parameter: arg must be an exact type match (even nullability)​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0210-proc-out-parameter-arg-must-be-an-exact-type-match-even-nullability","content":"(expected expected_type; found actual_type) 'variable_name' In a procedure call, the indicated parameter is in an 'out' position, it is a viable local variable of the correct type but the nullability does not match. The type of variable used to capture out parameters must be an exact match. declare proc foo(out x integer not null); create proc bar(out y integer) begin call foo(y); -- y is nullable but foo is expecting not null. end;  The above produces: CQL0210: proc out parameter: arg must be an exact type match (even nullability) (expected integer notnull; found integer) 'y'   "},{"title":"CQL0211: procedure without trailing OUT parameter used as function 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0211-procedure-without-trailing-out-parameter-used-as-function-procedure_name","content":"In a function call, the target of the function call was a procedure, procedures can be used like functions but their last parameter must be marked out. That will be the return value. In this case the last argument was not marked as out and so the call is invalid. Example: declare proc foo(x integer); create proc bar(out y integer) begin set y := foo(); -- foo does not have an out argument at the end end;   "},{"title":"CQL0212: too few arguments provided to procedure 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0212-too-few-arguments-provided-to-procedure-name","content":"In a procedure call to the named procedure, not enough arguments were provided to make the call. One or more arguments may have been omitted or perhaps the called procedure has changed such that it now requires more arguments.  "},{"title":"CQL0213: procedure had errors, can't call. 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0213-procedure-had-errors-cant-call-procedure_name","content":"In a procedure call to the named procedure, the target of the call had compilation errors. As a consequence this call cannot be checked and therefore must be marked in error, too. Fix the errors in the named procedure.  "},{"title":"CQL0214: procedures with results can only be called using a cursor in global context 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0214-procedures-with-results-can-only-be-called-using-a-cursor-in-global-context-name","content":"The named procedure results a result set, either with the SELECT statement or the OUT statement. However it is being called from outside of any procedure. Because of this, its result cannot then be returned anywhere. As a result, at the global level the result must be capture with a cursor. Example: create proc foo() begin select * from bar; end; call foo(); -- this is not valid declare cursor C for call foo(); -- C captures the result of foo, this is ok.   "},{"title":"CQL0215: value cursors are not used with FETCH C, or FETCH C INTO 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0215-value-cursors-are-not-used-with-fetch-c-or-fetch-c-into-cursor_name","content":"In a FETCH statement of the form FETCH [cursor] or FETCH [cursor] INTO the named cursor is a value cursor. These forms of the FETCH statement apply only to statement cursors. Example:good -- value cursor shaped like a table declare C cursor for select * from bar; --ok, C is fetched from the select results fetch C;  Example: bad -- value cursor shaped like a table declare C cursor like bar; -- invalid, there is no source for fetching a value cursor fetch C; -- ok assuming bar is made up of 3 integers fetch C from values(1,2,3);  statement cursors come from SQL statements and can be fetchedvalue cursors are of a prescribed shape and can only be loaded from value sources  "},{"title":"CQL0216: FETCH variable not found 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0216-fetch-variable-not-found-cursor_name","content":"In a FETCH statement, the indicated name, which is supposed to be a cursor, is not in fact a valid name at all. Probably there is a typo in the name. Or else the declaration is entirely missing.  "},{"title":"CQL0217: number of variables did not match count of columns in cursor 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0217-number-of-variables-did-not-match-count-of-columns-in-cursor-cursor_name","content":"In a FETCH [cursor] INTO [variables] the number of variables specified did not match the number of columns in the named cursor. Perhaps the source of the cursor (a select statement or some such) has changed.  "},{"title":"CQL0218: continue must be inside of a 'loop' or 'while' statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0218-continue-must-be-inside-of-a-loop-or-while-statement","content":"The CONTINUE statement may only appear inside of looping constructs. CQL only has two LOOP FETCH ... and WHILE  "},{"title":"CQL0219: leave must be inside of a 'loop', 'while', or 'switch' statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0219-leave-must-be-inside-of-a-loop-while-or-switch-statement","content":"The LEAVE statement may only appear inside of looping constructs or the switch statement. CQL has two loop types: LOOP FETCH ... and WHILE and of course the SWITCH statement. The errant LEAVE statement is not in any of those.  "},{"title":"CQL0220: savepoint has not been mentioned yet, probably wrong 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0220-savepoint-has-not-been-mentioned-yet-probably-wrong-name","content":"In a ROLLBACK statement that is rolling back to a named savepoint, the indicated savepoint was never mentioned before. It should have appeared previously in a SAVEPOINT statement. This probably means there is a typo in the name.  "},{"title":"CQL0221: savepoint has not been mentioned yet, probably wrong 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0221-savepoint-has-not-been-mentioned-yet-probably-wrong-name","content":"In a RELEASE SAVEPOINT statement that is rolling back to a named savepoint, the indicated savepoint was never mentioned before. It should have appeared previously in a SAVEPOINT statement. This probably means there is a typo in the name.  "},{"title":"CQL0222: out cursor statement only makes sense inside of a procedure​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0222-out-cursor-statement-only-makes-sense-inside-of-a-procedure","content":"The statement form OUT [cursor_name] makes a procedure that returns a single row result set. It doesn't make any sense to do this outside of any procedure because there is no procedure to return that result. Perhaps the OUT statement was mis-placed.  "},{"title":"CQL0223: cursor was not fetched with the auto-fetch syntax 'fetch [cursor]' 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0223-cursor-was-not-fetched-with-the-auto-fetch-syntax-fetch-cursor-cursor_name","content":"The statement form OUT [cursor_name] makes a procedure that returns a single row result set that corresponds to the current value of the cursor. If the cursor never held values directly then there is nothing to return. Example: declare C cursor for select * from bar; out C; -- error C was never fetched declare C cursor for select * from bar; fetch C into x, y, z; -- error C was used to load x, y, z so it's not holding any data out C; declare C cursor for select * from bar; -- create storage in C to hold bar columns (e.g. C.x, C,y, C.z) fetch C; -- ok, C holds data out C;   "},{"title":"CQL0224: a CALL statement inside SQL may call only a shared fragment i.e. @attribute(cql:shared_fragment)​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0224-a-call-statement-inside-sql-may-call-only-a-shared-fragment-ie-attributecqlshared_fragment","content":"Inside of a WITH clause you can create a CTE by calling a shared fragment like so: WITH my_shared_something(*) AS (CALL shared_proc(5)) SELECT * from my shared_something;  or you can use a nested select expression like  SELECT * FROM (CALL shared_proc(5)) as T;  However shared_proc must define a shareable fragment, like so: @attribute(cql:shared_fragment) create proc shared_proc(lim_ integer) begin select * from somewhere limit lim_; end;  Here the target of the CALL is not a shared fragment.  "},{"title":"CQL0225: switching to previous schema validation mode must be outside of any proc​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0225-switching-to-previous-schema-validation-mode-must-be-outside-of-any-proc","content":"The @previous_schema directive says that any schema that follows should be compared against what was declared before this point. This gives CQL the opportunity to detect changes in schema that are not supportable. The previous schema directive must be outside of any stored procedure. Example: @previous_schema; -- ok here create proc foo() begin @previous schema; -- nope end;   "},{"title":"CQL0226: schema upgrade declaration must be outside of any proc​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0226-schema-upgrade-declaration-must-be-outside-of-any-proc","content":"The @schema_upgrade_script directive tells CQL that the code that follows is intended to upgrade schema from one version to another. This kind of script is normally generated by the --rt schema_upgrade option discussed elsewhere. When processing such a script, a different set of rules are used for DDL analysis. In particular, it's normal to declare the final versions of tables but have DDL that creates the original version and more DDL to upgrade them from wherever they are to the final version (as declared). Ordinarily these duplicate definitions would produce errors. This directive allows those duplications. This error is reporting that the directive happened inside of a stored procedure, this is not allowed. Example: @schema_upgrade_script; -- ok here create proc foo() begin @schema_upgrade_script; -- nope end;   "},{"title":"CQL0227: schema upgrade declaration must come before any tables are declared​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0227-schema-upgrade-declaration-must-come-before-any-tables-are-declared","content":"The @schema_upgrade_script directive tells CQL that the code that follows is intended to upgrade schema from one version to another. This kind of script is normally generated by the --rt schema_upgrade option discussed elsewhere. When processing such a script, a different set of rules are used for DDL analysis. In particular, it's normal to declare the final versions of tables but have DDL that creates the original version and more DDL to upgrade them from wherever they are to the final version (as declared). Ordinarily these duplicate definitions would produce errors. This directive allows those duplications. In order to do its job properly the directive must come before any tables are created with DDL. This error tells you that the directive came too late in the stream. Or perhaps there were two such directives and one is late in the stream.  "},{"title":"CQL0228: schema upgrade version must be a positive integer​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0228-schema-upgrade-version-must-be-a-positive-integer","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that the version you are trying to view is not a positive integer version (e.g version -2)  "},{"title":"CQL0229: schema upgrade version declaration may only appear once​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0229-schema-upgrade-version-declaration-may-only-appear-once","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that a second @schema_upgrade_version directive has been found.  "},{"title":"CQL0230: schema upgrade version declaration must be outside of any proc​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0230-schema-upgrade-version-declaration-must-be-outside-of-any-proc","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that the @schema_upgrade_version directive was found inside of a stored procedure. This is not allowed.  "},{"title":"CQL0231: schema upgrade version declaration must come before any tables are declared​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0231-schema-upgrade-version-declaration-must-come-before-any-tables-are-declared","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive, the code in that procedure expects to see the schema as it existed at the version it is to migrate. The @schema_upgrade_version directive allows you to set the visible schema version to something other than the latest. There can only be one such directive. This error says that the @schema_upgrade_version directive came after tables were already declared. This is not allowed, the directive must come before any DDL.  "},{"title":"CQL0232: nested select expression must return exactly one column​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0232-nested-select-expression-must-return-exactly-one-column","content":"In a SELECT expression like set x := (select id from bar) the select statement must return exactly one column as in the example provided. Note that a runtime error will ensue if the statement returns zero rows, or more than one row, so this form is very limited. To fix this error change your select statement to return exactly one column. Consider how many rows you will get very carefully also, that cannot be checked at compile time.  "},{"title":"CQL0233: procedure previously declared as schema upgrade proc, it can have no args 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0233-procedure-previously-declared-as-schema-upgrade-proc-it-can-have-no-args-procedure_name","content":"When authoring a schema migration procedure that was previously declared in an @create or @delete directive that procedure will be called during schema migration with no context available. Therefore, the schema migration proc is not allowed to have any arguments.  "},{"title":"CQL0234: autodrop annotation can only go on a procedure that returns a result set 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0234-autodrop-annotation-can-only-go-on-a-procedure-that-returns-a-result-set-procedure_name","content":"The named procedure has the autodrop annotation (to automatically drop a temporary table) but the procedure in question doesn't return a result set so it has no need of the autodrop feature. The purpose that that feature is to drop the indicated temporary tables once all the select results have been fetched.  "},{"title":"CQL0235: too many arguments provided to procedure 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0235-too-many-arguments-provided-to-procedure-procedure_name","content":"In a CALL statement, or a function call, the named procedure takes fewer arguments than were provided. This error might be due to some copy/pasta going on or perhaps the argument list of the procedure/function changed to fewer items. To fix this, consult the argument list and adjust the call accordingly.  "},{"title":"CQL0236: autodrop annotation can only go on a procedure that uses the database 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0236-autodrop-annotation-can-only-go-on-a-procedure-that-uses-the-database-name","content":"The named procedure has the autodrop annotation (to automatically drop a temporary table) but the procedure in question doesn't even use the database at all, much less the named table. This annotation is therefore redundant.  "},{"title":"CQL0237: strict FK validation requires that some ON UPDATE option be selected for every foreign key​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0237-strict-fk-validation-requires-that-some-on-update-option-be-selected-for-every-foreign-key","content":"@enforce_strict has been use to enable strict foreign key enforcement. When enabled every foreign key must have an action for the ON UPDATE rule. You can specify NO ACTION but you can't simply leave the action blank.  "},{"title":"CQL0238: strict FK validation requires that some ON DELETE option be selected for every foreign key​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0238-strict-fk-validation-requires-that-some-on-delete-option-be-selected-for-every-foreign-key","content":"@enforce_strict has been use to enable strict foreign key enforcement. When enabled every foreign key must have an action for the ON DELETE rule. You can specify NO ACTION but you can't simply leave the action blank.  "},{"title":"CQL0239: 'annotation' column does not exist in result set 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0239-annotation-column-does-not-exist-in-result-set-column_name","content":"The @attribute(cql:identity=(col1, col2, ...)) form has been used to list the identity columns of a stored procedures result set. These columns must exist in the result set and they must be unique. The indicated column name is not part of the result of the procedure that is being annotated. The @attribute(cql:vault_sensitive=(col1, col2, ...) form has been used to list the columns of a stored procedures result set. These columns must exist in the result set. The indicated column name will be encoded if they are sensitive and the cursor that produced the result_set is a DML.  "},{"title":"CQL0240: identity annotation can only go on a procedure that returns a result set 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0240-identity-annotation-can-only-go-on-a-procedure-that-returns-a-result-set-procedure_name","content":"The @attribute(cql:identity=(col1, col2,...)) form has been used to list the identity columns of a stored procedures result set. These columns must exist in the result set and they must be unique. In this case, the named procedure doesn't even return a result set. Probably there is a copy/pasta going on. The identity attribute can likely be removed.  "},{"title":"CQL0241: CONCAT may only appear in the context of SQL statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0241-concat-may-only-appear-in-the-context-of-sql-statement","content":"The SQLite || operator has complex string conversion rules making it impossible to faithfully emulate. Since there is no helper function for doing concatenations, CQL choses to support this operator only in contexts where it will be evaluated by SQLite. That is, inside of some SQL statement. Examples: declare X text; set X := 'foo' || 'bar'; -- error set X := (select 'foo' || 'bar'); -- ok  If concatenation is required in some non-sql context, use the (select ..) expression form to let SQLite do the evaluation.  "},{"title":"CQL0242: lossy conversion from type 'type'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0242-lossy-conversion-from-type-type","content":"There is an explicit (set x := y) or implicit assignment (e.g. conversion of a parameter) where the storage for the target is a smaller numeric type than the expression that is being stored. This usually means a variable that should have been declared LONG is instead declared INTEGER or that you are typing to pass a LONG to a procedure that expects an INTEGER  "},{"title":"CQL0243: blob operand must be converted to string first in '||'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0243-blob-operand-must-be-converted-to-string-first-in-","content":"We explicitly do not wish to support string concatenation for blobs that holds non-string data. If the blob contains string data, make your intent clear by converting it to string first using CAST before doing the concatenation.  "},{"title":"CQL0244: unknown schema region 'region'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0244-unknown-schema-region-region","content":"In a @declare_schema_region statement one of the USING regions is not a valid region name. Or in @begin_schema_region the region name is not valid. This probably means there is a typo in your code.  "},{"title":"CQL0245: schema region already defined 'region'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0245-schema-region-already-defined-region","content":"The indicated region was previously defined, it cannot be redefined.  "},{"title":"CQL0246: schema regions do not nest; end the current region before starting a new one​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0246-schema-regions-do-not-nest-end-the-current-region-before-starting-a-new-one","content":"Another @begin_schema_region directive was encountered before the previous @end_schema_region was found.  "},{"title":"CQL0247: you must begin a schema region before you can end one​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0247-you-must-begin-a-schema-region-before-you-can-end-one","content":"An @end_schema_region directive was encountered but there was no corresponding @begin_schema_region directive.  "},{"title":"CQL0248: schema region directives may not appear inside of a procedure​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0248-schema-region-directives-may-not-appear-inside-of-a-procedure","content":"All of the *_schema_region directives must be used at the top level of your program, where tables are typically declared. They do not belong inside of procedures. If you get this error, move the directive out of the procedure near the DDL that it affects.  "},{"title":"CQL0249: function is not a table-valued-function 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0249-function-is-not-a-table-valued-function-function_name","content":"The indicated identifier appears in the context of a table, it is a function, but it is not a table-valued function. Either the declaration is wrong (use something like declare select function foo(arg text) (id integer, t text)) or the name is wrong. Or both.  "},{"title":"CQL0250: table-valued function not declared 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0250-table-valued-function-not-declared-function_name","content":"In a select statement, there is a reference to the indicated table-valued-function. For instance: -- the above error happens if my_function has not been declared -- as a table valued function select * from my_function(1,2,3);  However , my_function has not been declared as a function at all. A correct declaration might look like this: declare select function my_function(a int, b int, c int) (x int, y text);  Either there is a typo in the name or the declaration is missing, or both...  "},{"title":"CQL0251: fragment must end with exactly 'SELECT * FROM CTE'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0251-fragment-must-end-with-exactly-select--from-cte","content":"Query fragments have an exact prescription for their shape. This prescription includes select * from CTE as the final query where CTE is the common table expression that they define. This the error message includes the specific name that is required in this context.  "},{"title":"CQL0252: @PROC literal can only appear inside of procedures​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0252-proc-literal-can-only-appear-inside-of-procedures","content":"An @PROC literal was used outside of any procedure. It cannot be resolved if it isn't inside a procedure.  "},{"title":"CQL0253: base fragment must have only a single CTE named the same as the fragment 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0253-base-fragment-must-have-only-a-single-cte-named-the-same-as-the-fragment-name","content":"Query fragments have an exact prescription for their shape. This prescription includes select * from CTE where CTE is the single common table expression with the same name as the base query. This error says that the final select came from something other than the single CTE that is the base name or there was more than one CTE in the fragment. You can also get this error if you have an extension fragment but you accidentally marked it as a base fragment.  "},{"title":"CQL0254: switching to previous schema validation mode not allowed if @schema_upgrade_version was used​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0254-switching-to-previous-schema-validation-mode-not-allowed-if-schema_upgrade_version-was-used","content":"When authoring a schema migration script (a stored proc named in an @create or @delete annotation) you must create that procedure in a file that is marked with @schema_upgrade_verison specifying the schema version it is upgrading. If you do this, then the proc (correctly) only sees the schema as it existed at that version. However that makes the schema unsuitable for analysis using @previous_schema because it could be arbitrarily far in the past. This error prevents you from combining those features. Previous schema validation should only happen against the current schema.  "},{"title":"CQL0255: fragment name is not a previously declared base fragment 'bad_fragment_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0255-fragment-name-is-not-a-previously-declared-base-fragment-bad_fragment_name","content":"In an extension or assembly fragment declaration, the specified base fragment name has not been previously defined and that is not allowed. Probably there is a typo, or the declarations are in the wrong order. The base fragment has to come first.  "},{"title":"CQL0256: fragment name conflicts with existing base fragment 'NAME'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0256-fragment-name-conflicts-with-existing-base-fragment-name","content":"Extension query fragment can only be created with a custom procedure name different from all existing base fragment names otherwise we throw this error.  "},{"title":"CQL0257: argument must be a string or numeric in 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0257-argument-must-be-a-string-or-numeric-in-function","content":"The indicated function (usually min or max) only works on strings and numerics. NULL literals, blobs, or objects are not allowed in this context.  "},{"title":"CQL0258: extension fragment must add exactly one CTE; found extra named 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0258-extension-fragment-must-add-exactly-one-cte-found-extra-named-name","content":"The extension fragment includes more than one additional CTE, it must have exactly one. In the following example, ext2 is not valid, you have to stop at ext1 -- example bad extension fragment @attribute(cql:extension_fragment=core) create proc test_bad_extension_fragment_three() begin with core(x,y,z) as (select 1,nullable(&quot;a&quot;),nullable(3L)), ext1(x,y,z,a) as (select core.*, extra1.* from core left outer join extra1), ext2(x,y,z,b) as (select core.*, extra2.* from core left outer join extra2) select * from ext2; end;   "},{"title":"CQL0259: extension fragment CTE must select T.* from base CTE​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0259-extension-fragment-cte-must-select-t-from-base-cte","content":"For the select expression in extension fragment, it must include all columns from the base table. This error indicates the select expression doesn't select from the base table. It should look like this select core.*, ... from core  Here core is the name of its base table.  "},{"title":"CQL0260: extension fragment CTE must be a simple left outer join from 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0260-extension-fragment-cte-must-be-a-simple-left-outer-join-from-table_name","content":"Extension fragments may add columns to the result, to do this without lose any rows you must always left outer join to the new data that you with to include. There is a specific prescription for this. It has to look like this: @attribute(cql:extension_fragment=core) create proc an_extension() begin with core(x,y,z) as (select 1,nullable(&quot;a&quot;),nullable(3L)), ext1(x,y,z,a) as (select core.*, extra_column from core left outer join extra_stuff), select * from ext1; end;  Here extension ext1 is adding extra_column which came from extra_stuff. There could have been any desired join condition or indeed any join expression at all but it has to begin with from core left outer join so that all the core columns will be present and now rows can be removed.  "},{"title":"CQL0261: cursor did not originate from a SQLite statement, it only has values 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0261-cursor-did-not-originate-from-a-sqlite-statement-it-only-has-values-cursor_name","content":"The form:  SET [name] FROM CURSOR [cursor_name]  Is used to wrap a cursor in an object so that it can be returned for forwarded. This is the so-called &quot;boxing&quot; operation on the cursor. The object can then be &quot;unboxed&quot; later to make a cursor again. However the point of this is to keep reading forward on the cursor perhaps in another procedure. You can only read forward on a cursor that has an associated SQLite statement. That is the cursor was created with something like this  DECLARE [name] CURSOR FOR SELECT ... | CALL ...  If the cursor isn't of this form it's just values, you can't move it forward and so &quot;boxing&quot; it is of no value. Hence not allowed. You can return the cursor values with OUT instead.  "},{"title":"CQL0262: LIKE ... ARGUMENTS used on a procedure with no arguments 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0262-like--arguments-used-on-a-procedure-with-no-arguments-procedure_name","content":"The LIKE [procedure] ARGUMENTS form creates a shape for use in a cursor or procedure arguments. The indicated name is a procedure with no arguments so it cannot be used to make a shape.  "},{"title":"CQL0263: non-ANSI joins are forbidden if strict join mode is enabled.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0263-non-ansi-joins-are-forbidden-if-strict-join-mode-is-enabled","content":"You can enable strict enforcement of joins to avoid the form select * from A, B;  which sometimes confuses people (the above is exactly the same as select * from A inner join B on 1;  Usually there are constraints on the join also in the WHERE clause but there don't have to be. @enforce_strict join turns on this mode.  "},{"title":"CQL0264: duplicate assembly fragments of base fragment​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0264-duplicate-assembly-fragments-of-base-fragment","content":"For each base fragment, it only allows to exist one assembly fragment of that base fragment.  "},{"title":"CQL0265: assembly fragment can only have one CTE​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0265-assembly-fragment-can-only-have-one-cte","content":"Assembly fragment can only have one base table CTE.  "},{"title":"CQL0266: extension fragment name conflicts with existing fragment​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0266-extension-fragment-name-conflicts-with-existing-fragment","content":"Two or more extension fragments share the same name.  "},{"title":"CQL0267: extension fragments of same base fragment share the same cte column​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0267-extension-fragments-of-same-base-fragment-share-the-same-cte-column","content":"Two or more extension fragments which have the same base fragments share the same name for one of their unique columns. E.g. the base table is core(x,y) and one extension table is plugin_one(x,y,z) and another is plugin_two(x,y,z). Here, z in both extension fragments share the same name but may refer to different values.  "},{"title":"CQL0268: extension/assembly fragment must have the CTE columns same as the base fragment​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0268-extensionassembly-fragment-must-have-the-cte-columns-same-as-the-base-fragment","content":"Extension and assembly fragments have an exact prescription for their shape. For each extension and assembly fragment, the first CTE must be a stub for their base table. This error means this stub in extension/assembly fragment differs from the definition of the base table.  "},{"title":"CQL0269: at least part of this unique key is redundant with previous unique keys​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0269-at-least-part-of-this-unique-key-is-redundant-with-previous-unique-keys","content":"The new unique key must have at least one column that is not in a previous key AND it must not have all the columns from any previous key. e.g: create table t1 ( a int, b long, c text, d real, UNIQUE (a, b), UNIQUE (a, b, c), -- INVALID (a, b) is already unique key UNIQUE (b, a), -- INVALID (b, a) is the same as (a, b) UNIQUE (c, d, b, a), -- INVALID subset (b, a) is already unique key UNIQUE (a), -- INVALID a is part of (a, b) key UNIQUE (a, c), -- VALID UNIQUE (d), -- VALID UNIQUE (b, d) -- VALID );   "},{"title":"CQL0270: use FETCH FROM for procedures that returns a cursor with OUT 'cursor'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0270-use-fetch-from-for-procedures-that-returns-a-cursor-with-out-cursor","content":"If you are calling a procedure that returns a value cursor (using OUT) then you accept that cursor using the pattern DECLARE C CURSOR FETCH FROM CALL foo(...);  The pattern DECLARE C CURSOR FOR CALL foo(...);  Is used for procedures that provide a full select statement. Note that in the former cause you don't then use fetch on the cursor. There is at most one row anyway and it's fetched for you so a fetch would be useless. In the second case you fetch as many rows as there are and/or you want.  "},{"title":"CQL0271: OFFSET clause may only be used if LIMIT is also present​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0271-offset-clause-may-only-be-used-if-limit-is-also-present","content":"select * from foo offset 1;  Is not supported by SQLite. OFFSET may only be used if LIMIT is also present. Also, both should be small because offset is not cheap. There is no way to do offset other than to read and ignore the indicated number of rows. So something like offset 1000 is always horrible.  "},{"title":"CQL0272: columns referenced in the foreign key statement should match exactly a unique key in parent table​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0272-columns-referenced-in-the-foreign-key-statement-should-match-exactly-a-unique-key-in-parent-table","content":"If you're creating a table t2 with foreign keys on table t1, then the set of t1's columns reference in the foreign key statement for table t2 should be: A primary key in t1 e.g: create table t1(a text primary key); create table t2(a text primary key, foreign key(a) references t1(a));  A unique key in t1 e.g: create table t1(a text unique); create table t2(a text primary key, foreign key(a) references t1(a));  A group of unique key in t1 e.g: create table t1(a text, b int, unique(a, b)); create table t2(a text, b int, foreign key(a, b) references t1(a, b));  A group of primary key in t1 e.g: create table t1(a text, b int, primary key(a, b)); create table t2(a text, b int, foreign key(a, b) references t1(a, b));  A unique index in t1 e.g: create table t1(a text, b int); create unique index unq on t1(a, b); create table t2(a text, b int, foreign key(a, b) references t1(a, b));   "},{"title":"CQL0273: autotest attribute has incorrect format (...) in 'dummy_test'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0273-autotest-attribute-has-incorrect-format--in-dummy_test","content":"In a cql:autotest annotation, the given dummy_test info (table name, column name, column value) has incorrect format.  "},{"title":"CQL0274: autotest attribute 'dummy_test' has non existent table​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0274-autotest-attribute-dummy_test-has-non-existent-table","content":"In a cql:autotest annotation, the given table name for dummy_test attribute does not exist.  "},{"title":"CQL0275: autotest attribute 'dummy_test' has non existent column​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0275-autotest-attribute-dummy_test-has-non-existent-column","content":"In a cql:autotest annotation, the given column name for dummy_test attribute does not exist.  "},{"title":"CQL0276: autotest attribute 'dummy_test' has invalid value type in​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0276-autotest-attribute-dummy_test-has-invalid-value-type-in","content":"In a cql:autotest annotation, the given column value's type for dummy_test attribute does not match the column type.  "},{"title":"CQL0277: autotest has incorrect format​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0277-autotest-has-incorrect-format","content":"In a cql:autotest annotation, the format is incorrect.  "},{"title":"CQL0278: autotest attribute name is not valid​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0278-autotest-attribute-name-is-not-valid","content":"In a cql:autotest annotation, the given attribute name is not valid.  "},{"title":"CQL0279: columns referenced in an UPSERT conflict target must exactly match a unique key the target table​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0279-columns-referenced-in-an-upsert-conflict-target-must-exactly-match-a-unique-key-the-target-table","content":"If you're doing an UPSERT on table T, the columns listed in the conflict target should be: A primary key in TA unique key in TA group of unique key in TA group of primary key in TA unique index in T  "},{"title":"CQL0280: upsert statement requires a where clause if the insert clause uses select​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0280-upsert-statement-requires-a-where-clause-if-the-insert-clause-uses-select","content":"When the INSERT statement to which the UPSERT is attached takes its values from a SELECT statement, there is a potential parsing ambiguity. The SQLite parser might not be able to tell if the ON keyword is introducing the UPSERT or if it is the ON clause of a join. To work around this, the SELECT statement should always include a WHERE clause, even if that WHERE clause is just WHERE 1 (always true). Note: The CQL parser doesn't have this ambiguity because it treats &quot;ON CONFLICT&quot; as a single token so this is CQL reporting that SQLite might have trouble with the query as written. e.g: insert into foo select id from bar where 1 on conflict(id) do nothing;   "},{"title":"CQL0281: upsert statement does not include table name in the update statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0281-upsert-statement-does-not-include-table-name-in-the-update-statement","content":"The UPDATE statement of and UPSERT should not include the table name because the name is already known from the INSERT statement part of the UPSERT e.g: insert into foo select id from bar where 1 on conflict(id) do update set id=10;   "},{"title":"CQL0282: update statement require table name​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0282-update-statement-require-table-name","content":"The UPDATE statement should always include a table name except if the UPDATE statement is part of an UPSERT statement. e.g: update foo set id=10; insert into foo(id) values(1) do update set id=10;   "},{"title":"CQL0283: upsert syntax only support INSERT INTO​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0283-upsert-syntax-only-support-insert-into","content":"The INSERT statement part of an UPSERT statement can only uses INSERT INTO ... e.g: insert into foo(id) values(1) on conflict do nothing; insert into foo(id) values(1) on conflict do update set id=10;   "},{"title":"CQL0284: ad hoc schema migration directive must provide a procedure to run​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0284-ad-hoc-schema-migration-directive-must-provide-a-procedure-to-run","content":"@schema_ad_hoc_migration must provide both a version number and a migrate procedure name. This is unlike the other version directives like @create where the version number is optional. This is because the whole point of this migrator is to invoke a procedure of your choice.  "},{"title":"CQL0285: ad hoc schema migration directive version number changed 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0285-ad-hoc-schema-migration-directive-version-number-changed-procedure_name","content":"In @schema_ad_hoc_migration you cannot change the version number of the directive once it has been added to the schema because this could cause inconsistencies when upgrading. You can change the body of the method if you need to but this is also not recommended because again there could be inconsistencies. However careful replacement and compensation is possible. This is like going to 110% on the reactor... possible, but not recommended.  "},{"title":"CQL0286: ad hoc schema migration directive was removed; this is not allowed 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0286-ad-hoc-schema-migration-directive-was-removed-this-is-not-allowed-procedure_name","content":"An @schema_ad_hoc_migration cannot be removed because it could cause inconsistencies on upgrade. You can change the body of the method if you need to but this is also not recommended because again there could be inconsistencies. However careful replacement and compensation is possible. This is like going to 110% on the reactor... possible, but not recommended.  "},{"title":"CQL0287: extension/assembly fragment must add stub \"​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0287-extensionassembly-fragment-must-add-stub-","content":" "},{"title":"CQL0288: extension/assembly fragment stub for base CTE column must be \"​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0288-extensionassembly-fragment-stub-for-base-cte-column-must-be-","content":" "},{"title":"CQL0289: upsert statement are forbidden if strict upsert statement mode is enabled​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0289-upsert-statement-are-forbidden-if-strict-upsert-statement-mode-is-enabled","content":"@enforce_strict has been use to enable strict upsert statement enforcement. When enabled all sql statement should not use the upsert statement. This is because sqlite version running in some iOS and Android version is old. Upsert statement was added to sqlite in the version 3.24.0 (2018-06-04).  "},{"title":"CQL0290: fragments can only have one statement in the statement list and it must be a WITH...SELECT​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0290-fragments-can-only-have-one-statement-in-the-statement-list-and-it-must-be-a-withselect","content":"All of the extendable query fragment types consist of a procedure with exactly one statement and that statement is a WITH...SELECT statement. If you have more than one statement or some other type of statement you'll get this error.  "},{"title":"CQL0291: region links into the middle of a deployable region; you must point to the root of <deployable_region> not into the middle: <error_region>​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0291-region-links-into-the-middle-of-a-deployable-region-you-must-point-to-the-root-of-deployable_region-not-into-the-middle-error_region","content":"Deployable regions have an &quot;inside&quot; that is in some sense &quot;private&quot;. In order to keep the consistent (and independently deployable) you can't peek into the middle of such a region, you have to depend on the root (i.e. &lt;deployable_region&gt; itself). This allows the region to remain independently deployable and for its internal logical regions to be reorganized in whatever manner makes sense. To fix this error probably you should change error_region so that it depends directly on deployable_region  "},{"title":"CQL0292: explain statement is only available in dev mode because its result set may vary between sqlite versions​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0292-explain-statement-is-only-available-in-dev-mode-because-its-result-set-may-vary-between-sqlite-versions","content":"The EXPLAIN statement is intended for interactive debugging only. It helps engineer understand how Sqlite will execute their query and the cost attached to it. This is why this grammar is only available in dev mode in CQL and should never be used in production.  "},{"title":"CQL0293: only [EXPLAIN QUERY PLAN ...] statement is supported​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0293-only-explain-query-plan--statement-is-supported","content":"CQL only support [EXPLAIN QUERY PLAN stmt] sql statement.  "},{"title":"CQL0294: window function invocations can only appear in the select list of a select statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0294-window-function-invocations-can-only-appear-in-the-select-list-of-a-select-statement","content":"Not all SQLite builtin function can be used as a window function.  "},{"title":"CQL0295: window name is not defined​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0295-window-name-is-not-defined","content":"Window name referenced in the select list should be defined in the Window clause of the same select statement.  "},{"title":"CQL0296: window name definition is not used​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0296-window-name-definition-is-not-used","content":"Window name defined in the window clause of a select statement should always be used within that statement.  "},{"title":"CQL0297: FROM [shape] is redundant if column list is empty​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0297-from-shape-is-redundant-if-column-list-is-empty","content":"In this form: insert into YourTable() FROM your_cursor; The () means no columns are being specified, the cursor will never be used. The only source of columns is maybe dummy data (if it was specified) or the default values or null. In no case will the cursor be used. If you really want this use FROM VALUES() and don't implicate a cursor or an argument bundle.  "},{"title":"CQL0298: cannot read from a cursor without fields 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0298-cannot-read-from-a-cursor-without-fields-cursor_name","content":"The cursor in question has no storage associated with it. It was loaded with something like: fetch C into x, y, z; You can only use a cursor as a source of data if it was fetched with its own storage like fetch C This results in a structure for the cursor. This gives you C.x, C.y, C.z etc. If you fetched the cursor into variables then you have to use the variables for any inserting.  "},{"title":"CQL0299: [cursor] has too few fields, 'shape_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0299-cursor-has-too-few-fields-shape_name","content":"The named shape was used in a fetch statement but the number of columns fetched is smaller than the number required by the statement we are processing. If you need to use the cursor plus some other data then you can't use this form, you'll have to use each field individually like from values(C.x, C.y, C.z, other_stuff). The shape with too few fields might be the source or the target of the statement.  "},{"title":"CQL0300: argument must be an integer (between 1 and max integer) in function 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0300-argument-must-be-an-integer-between-1-and-max-integer-in-function-function_name","content":"The argument of the function should be an integer.  "},{"title":"CQL0301: second argument must be an integer (between 0 and max integer) in function 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0301-second-argument-must-be-an-integer-between-0-and-max-integer-in-function-function_name","content":"The second argument of the function should be an integer between 0 and INTEGER_MAX.  "},{"title":"CQL0302: first and third arguments must be compatible in function 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0302-first-and-third-arguments-must-be-compatible-in-function-function_name","content":"The first and third arguments of the function have to be of the same type because the third argument provide a default value in cause the first argument is NULL.  "},{"title":"CQL0303: second argument must be an integer between 1 and max integer in function 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0303-second-argument-must-be-an-integer-between-1-and-max-integer-in-function-function_name","content":"The second argument of the function must be and integer between 1 and INTEGER_MAX.  "},{"title":"CQL0304: DISTINCT may only be used with one explicit argument in an aggregate function​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0304-distinct-may-only-be-used-with-one-explicit-argument-in-an-aggregate-function","content":"The keyword DISTINCT can only be used with one argument in an aggregate function.  "},{"title":"CQL0305: DISTINCT may only be used in function that are aggregated or user defined​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0305-distinct-may-only-be-used-in-function-that-are-aggregated-or-user-defined","content":"Only aggregated functions and user defined functions can use the keyword DISTINCT. Others type of functions are not allowed to use it.  "},{"title":"CQL0306: FILTER clause may only be used in function that are aggregated or user defined​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0306-filter-clause-may-only-be-used-in-function-that-are-aggregated-or-user-defined","content":" "},{"title":"CQL0307: return statement should be in a procedure and not at the top level​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0307-return-statement-should-be-in-a-procedure-and-not-at-the-top-level","content":"There are basically two checks here both of which have to do with the &quot;nesting level&quot; at which the return occurs. A loose return statement (not in a procedure) is meaningless so that produce an error. There is nothing to return from. If the return statement is not inside of an &quot;if&quot; or something like that then it will run unconditionally. Nothing should follow the return (see CQL0308) so if we didn't fall afoul of CQL0308 and we're at the top level then the return is the last thing in the proc, in which case it is totally redundant. Both these situations produce an error.  "},{"title":"CQL0308: statement should be the last thing in a statement list​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0308-statement-should-be-the-last-thing-in-a-statement-list","content":"Control flow will exit the containing procedure after a return statement, so any statements that follow in its statement list will certainly not run. So the return statement must be the last statement, otherwise there are dead/unreachable statements which is most likely done by accident. To fix this probably the things that came after the return should be deleted. Or alternately there was a condition on the return that should have been added but wasn't, so the return should have been inside a nested statement list (like the body of an if maybe).  "},{"title":"CQL0309: new table must be added with @create([number]) or later 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0309-new-table-must-be-added-with-createnumber-or-later-table_name","content":"The indicated table was newly added -- it is not present in the previous schema. However the version number it was added at is in the past. The new table must appear at the current schema version or later. That version is provided in the error message. To fix this, change the @create annotation on the table to be at the indicated version or later.  "},{"title":"CQL0310: new column must be added with @create([number]) or later\" 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0310-new-column-must-be-added-with-createnumber-or-later-column_name","content":"The indicated column was newly added -- it is not present in the previous schema. However the version number it was added at is in the past. The new column must appear at the current schema version or later. That version is provided in the error message. To fix this, change the @create annotation on the table to be at the indicated version or later.  "},{"title":"CQL0311: object's deployment region changed from '<previous_region>' to '<current_region>' 'object_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0311-objects-deployment-region-changed-from-previous_region-to-current_region-object_name","content":"An object may not move between deployment regions, because users of the schema will depend on its contents. New objects can be added to a deployment region but nothing can move from one region to another. The indicated object appears to be moving.  "},{"title":"CQL0312: window function invocation are forbidden if strict window function mode is enabled​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0312-window-function-invocation-are-forbidden-if-strict-window-function-mode-is-enabled","content":"@enforce_strict has been use to enable strict window function enforcement. When enabled all sql statement should not invoke window function. This is because sqlite version running in some iOS version is old. Window function was added to SQLite in the version 3.25.0 (2018-09-15).  "},{"title":"CQL0313: blob literals may only appear in the context of a SQL statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0313-blob-literals-may-only-appear-in-the-context-of-a-sql-statement","content":"CQL (currently) limits use of blob literals to inside of SQL fragments. There's no easy way to get a blob constant variable into the data section so any implementation would be poor. These don't come up very often in any case so this is a punt basically. You can fake it with (select x'1234') which makes it clear that you are doing something expensive. This is not recommended. Better to pass the blob you need into CQL rather than cons it from a literal. Within SQL it's just text and SQLite does the work as usual so that poses no problems. And of course non-literal blobs (as args) work find and are bound as usual.  "},{"title":"CQL0314: select function does not require a declaration, it is a CQL built-in​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0314-select-function-does-not-require-a-declaration-it-is-a-cql-built-in","content":"CQL built-in function does not require a select function declaration. You can used it directly in your SQL statement.  "},{"title":"CQL0315: mandatory column with no default value in INSERT INTO name DEFAULT VALUES statement.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0315-mandatory-column-with-no-default-value-in-insert-into-name-default-values-statement","content":"Columns on a table must have default value or be nullable in order to use INSERT INTO &lt;table&gt; DEFAULT VALUES statement.  "},{"title":"CQL0316: upsert-clause is not compatible with DEFAULT VALUES​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0316-upsert-clause-is-not-compatible-with-default-values","content":"INSERT statement with DEFAULT VALUES can not be used in a upsert statement. This form is not supported by SQLite.  "},{"title":"CQL0317: char function arguments must be integer​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0317-char-function-arguments-must-be-integer","content":"All parameters of the built-In scalar CQL functions char(...) must be of type integer.  "},{"title":"CQL0318: more than one fragment annotation on procedure 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0318-more-than-one-fragment-annotation-on-procedure-procedure_name","content":"The indicated procedure has several cql:*_fragment annotations such as cql:base_fragment and cql:extension_fragment. You can have at most one of these. example: @attribute(cql:assembly_fragment=foo) @attribute(cql:base_fragment=goo) create proc mixed_frag_types3(id_ integer) begin ... end;   "},{"title":"CQL0319: name of the assembly procedure must match the name of the base fragment 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0319-name-of-the-assembly-procedure-must-match-the-name-of-the-base-fragment-procedure_name","content":"The name of the procedure that carries the assembly attribute (cql:assembly_fragment) has to match the name of the base fragment. This is because the code that is generated for the extension fragments refers to some shared code that is generated in the assembly fragment. If the assembly fragment were allowed to have a distinct name the linkage could never work. example: -- correct example -- note: 'foo' must be a valid base fragment, declared elsewhere @attribute(cql:assembly_fragment=foo) create proc foo(id_ integer) begin ... end; -- incorrect example @attribute(cql:assembly_fragment=foo) create proc bar(id_ integer) begin ... end;   "},{"title":"CQL0320: extension fragment CTE must have a FROM clause and no other top level clauses 'frag_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0320-extension-fragment-cte-must-have-a-from-clause-and-no-other-top-level-clauses-frag_name","content":"In the extension fragment form that uses LEFT OUTER JOIN to add columns you cannot include top level restrictions/changes like WHERE, ORDER BY, LIMIT and so forth. Any of these would remove or reorder the rows from the core fragment and that is not allowed, you can only add columns. Hence you must have a FROM clause and you can have no other top level clauses. You can use any clauses you like in a nested select to get your additional columns. Note: you could potentially add rows with a LEFT OUTER JOIN and a generous ON clause. That's allowed but not recommended. The ON clause can't be forbidden because it's essential in the normal case.  "},{"title":"CQL0321: migration proc not allowed on object 'object_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0321-migration-proc-not-allowed-on-object-object_name","content":"The indicated name is an index or a trigger. These objects may not have a migration script associated with them when they are deleted. The reason for this is that both these types of objects are attached to a table and the table itself might be deleted. If the table is deleted it becomes impossible to express even a tombstone for the deleted trigger or index without getting errors. As a consequence the index/trigger must be completely removed. But if there had been a migration procedure on it then some upgrade sequences would have run it, but others would not (anyone who upgraded after the table was deleted would not get the migration procedure). To avoid this problem, migration procedures are not allowed on indices and triggers.  "},{"title":"CQL0322: fragment parameters must be exactly '[arguments]' 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0322-fragment-parameters-must-be-exactly-arguments-procedure_name","content":"The named procedure is an extension fragment or an assembly fragment. It must have exactly the same arguments as the base fragment. These arguments are provided. Recall that the code for the procedure that does the select comes from the assembly fragment, so its arguments are in some sense the only ones that matter. But the extension fragments are also allowed to use the arguments. Of course we must ensure that the extension fragments do not use any arguments that aren't in the assembly, and so the only choice we have is to make sure the extensions conform to the base. And so for that to work the assembly also has to conform to the base. So the base fragment must define the args for all the other fragments. You could imagine a scheme where the extension fragments are allowed to use a subset of the parameters defined in the base but if that were the case you might have names that mean different things in different fragments and then you could get errors or different semantics when the fragments were assembled. To avoid all of these problems, and for simplicity, we demand that the arguments of all fragments match exactly.  "},{"title":"CQL0323: calls to undeclared procedures are forbidden; declaration missing or typo 'procedure'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0323-calls-to-undeclared-procedures-are-forbidden-declaration-missing-or-typo-procedure","content":"If you get this error it means that there is a typo in the name of the procedure you are trying to call, or else the declaration for the procedure is totally missing. Maybe a necessary #include needs to be added to the compiland. Previously if you attempted to call an unknown CQL would produce a generic function call. If you need to do this, especially a function with varargs, then you must declare the function with something like: DECLARE PROCEDURE printf NO CHECK; This option only works for void functions. For more complex signatures check DECLARE FUNCTION and DECLARE SELECT FUNCTION. Usually these will require a simple wrapper to call from CQL. In all cases there must be some kind of declaration,to avoid mysterious linker failures or argument signature mismatches.  "},{"title":"CQL0324: referenced table was created in a later version so it cannot be used in a foreign key 'referenced_table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0324-referenced-table-was-created-in-a-later-version-so-it-cannot-be-used-in-a-foreign-key-referenced_table","content":"In a foreign key, we enforce the following rules: @recreate tables can see any version they like, if the name is in scope that's good enoughother tables may only &quot;see&quot; the same version or an earlier version. Normal processing can't actually get into this state because if you tried to create the referencing table with the smaller version number first you would get errors because the name of the referenced table doesn't yet exist. But if you created them at the same time and you made a typo in the version number of the referenced table such that it was accidentally bigger you'd create a weirdness. So we check for that situation here and reject it to prevent that sort of typo. If you see this error there is almost certainly a typo in the version number of the referenced table; it should be fixed.  "},{"title":"CQL0325: ok_table_scan attribute must be a name​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0325-ok_table_scan-attribute-must-be-a-name","content":"The values for the attribute ok_table_scan can only be names. CQL attributes can have a variety of values but in this case the attribute refers to the names of tables so no other type of attribute is reasonable.  "},{"title":"CQL0326: table name in ok_table_scan does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0326-table-name-in-ok_table_scan-does-not-exist-table_name","content":"The names provided to ok_table_scan attribute should be names of existing tables. The attribute indicates tables that are ok to scan in this procedure even though they are typically not ok to scan due to 'no_table_scan'. Therefore the attribute must refer to an existing table. There is likely a typo in the the table name that needs to be corrected.  "},{"title":"CQL0327: a value should not be assigned to 'attribute_name' attribute​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0327-a-value-should-not-be-assigned-to-attribute_name-attribute","content":"The attribute attribute_name doesn't take a value. When marking a statement with @attribute(cql:&lt;attribute_name&gt;) there is no need for an attribute value.  "},{"title":"CQL0328: 'attribute_name' attribute may only be added to a 'statement_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0328-attribute_name-attribute-may-only-be-added-to-a-statement_name","content":"The attribute_name attribute can only be assigned to specific statements. The marking @attribute(cql:&lt;attribute_name&gt;) only makes sense on specific statement. It's likely been put somewhere strange, If it isn't obviously on the wrong thing, look into possibly how the source is after macro expansion.  "},{"title":"CQL0329: ok_table_scan attribute can only be used in a create procedure statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0329-ok_table_scan-attribute-can-only-be-used-in-a-create-procedure-statement","content":"The ok_table_scan can only be placed on a create procedure statement. The marking @attribute(cql:ok_table_scan=...) indicates that the procedure may scan the indicated tables. This marking doesn't make sense on other kinds of statements.  "},{"title":"CQL0330: fragment must start with exactly 'SELECT * FROM CTE'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0330-fragment-must-start-with-exactly-select--from-cte","content":"Query fragments have an exact prescription for their shape. This prescription includes select * from CTE in the first branch of the UNION ALL operator when using that form. Here CTE is the common table expression that they define. This the error message includes the specific name that is required in this context.  "},{"title":"CQL0331: extension fragment CTE must have not have ORDER BY or LIMIT clauses 'frag_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0331-extension-fragment-cte-must-have-not-have-order-by-or-limit-clauses-frag_name","content":"In the extension fragment form that uses UNION ALL to add rows you cannot include the top level operators ORDER BY, or LIMIT Any of these would remove or reorder the rows from the core fragment and that is not allowed, you can only add new rows. You can use any clauses you like in a nested selects as they will not remove rows from the base query.  "},{"title":"CQL0332: all extension fragments that use UNION ALL must come before those that use LEFT OUTER JOIN 'frag_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0332-all-extension-fragments-that-use-union-all-must-come-before-those-that-use-left-outer-join-frag_name","content":"Query fragments that add rows using the UNION ALL form have no way to refer columns that may have been added before them in the part of the query that adds rows (the second and subsequent branches of UNION ALL). As a result, in order to get a assembled query that makes sense the row-adding form must always come before any columns were added. Hence all of these fragments must come before any of the LEFT OUTER JOIN form. If you get this error, you should re-order your fragments such that the UNION ALL form comes before any LEFT OUTER JOIN fragments. The offending fragment is named in the error.  "},{"title":"CQL0333: all the compound operators in this CTE must be UNION ALL​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0333-all-the-compound-operators-in-this-cte-must-be-union-all","content":"The compound operators in CTE must and always be an UNION ALL.  "},{"title":"CQL0334: @dummy_seed @dummy_nullables @dummy_defaults many only be used with a single VALUES row​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0334-dummy_seed-dummy_nullables-dummy_defaults-many-only-be-used-with-a-single-values-row","content":"Dummy insert feature makes only sense when it's used in a VALUES clause that is not part of a compound select statement. "},{"title":"CQL0336: select statement with VALUES clause requires a non empty list of values​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0336-select-statement-with-values-clause-requires-a-non-empty-list-of-values","content":"VALUES clause requires at least a value for each of the values list. Empty values list are not supported.  "},{"title":"CQL0337: number of columns values for each row should be identical in VALUES clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0337-number-of-columns-values-for-each-row-should-be-identical-in-values-clause","content":"The number of values for each values list in VALUES clause should always be the same.  "},{"title":"CQL0338: name of a migration procedure may not end in '_crc' 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0338-name-of-a-migration-procedure-may-not-end-in-_crc-procedure_name","content":"To avoid name conflicts in the upgrade script, migration procedures are not allowed to end in '_crc' this suffix is reserved for internal use.  "},{"title":"CQL0339: WITHOUT ROWID tables are forbidden if strict without rowid mode is enabled​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0339-without-rowid-tables-are-forbidden-if-strict-without-rowid-mode-is-enabled","content":"@enforce_strict has been used to enable strict WITHOUT ROWID enforcement. When enabled no CREATE TABLE statement can have WITHOUT ROWID clause.  "},{"title":"CQL0340: FROM ARGUMENTS used in a procedure with no arguments 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0340-from-arguments-used-in-a-procedure-with-no-arguments-procedure_name","content":"The named procedure has a call that uses the FROM ARGUMENTS pattern but it doesn't have any arguments. This is almost certainly a cut/paste from a different location that needs to be adjusted.  "},{"title":"CQL0341: argument must be a variable in function 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0341-argument-must-be-a-variable-in-function-function_name","content":"The argument for the CQL builtin function 'function_name' should always be a variable. It can not be an expression for example  "},{"title":"CQL0342: cursor arguments must have identical column count 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0342-cursor-arguments-must-have-identical-column-count-function_name","content":"The number of column in the cursor arguments must be identical to accurately do diffing between two cursors.  "},{"title":"CQL0343: all arguments must be blob 'cql_get_blob_size'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0343-all-arguments-must-be-blob-cql_get_blob_size","content":"The argument for the CQL builtin function cql_get_blob_size should always be of type blob  "},{"title":"CQL0344: argument must be a nullable type (but not constant NULL) in 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0344-argument-must-be-a-nullable-type-but-not-constant-null-in-function","content":"Functions like ifnull_crash only make sense if the argument is nullable. If it's already not null the operation is uninteresting/redundant. The most likely cause is that the function call in question is vestigial and you can simply remove it.  "},{"title":"CQL0345: arguments must be of type blob 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0345-arguments-must-be-of-type-blob-function_name","content":"The indicated function accepts only a single argument of type blob.  "},{"title":"CQL0346: expression must be of type object<T cursor> where T is a valid shape name 'variable'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0346-expression-must-be-of-type-objectt-cursor-where-t-is-a-valid-shape-name-variable","content":"It's possible to take the statement associated with a statement cursor and store it in an object variable. Using the form: declare C cursor for X;  The object variable 'X' must be declared as follows: declare X object&lt;T cursor&gt;;  Where T refers to a named object with a shape, like a table, a view, or a stored procedure that returns a result set. This type T must match the shape of the cursor exactly i.e. having the column names and types. The reverse operation, storing a statement cursor in a variable is also possible with this form: set X from cursor C;  This has similar constraints on the variable X. This error indicates that the variable in question (X in this example) is not a typed object variable so it can't be the source of a cursor, or accept a cursor. See Chapter 5 of the CQL Programming Language.  "},{"title":"CQL0347: select function may not return type OBJECT 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0347-select-function-may-not-return-type-object-function_name","content":"The indicated function was declared with DECLARE SELECT FUNCTION meaning it is to be used in the context of SQLite statements. However, SQLite doesn't understand functions that return type object at all. Therefore declaration is illegal. When working with pointer type through SQLite it is often possibly to encode the object as an long integer assuming it can pass through unchanged with no retain/release semantics or any such thing. If that is practical you can move objects around by returning long integers.  "},{"title":"CQL0348: collate applied to a non-text column 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0348-collate-applied-to-a-non-text-column-column_name","content":"Collation order really only makes sense on text fields. Possibly blob fields but we're taking a stand on blob for now. This can be relaxed later if that proves to be a mistake. For now, only text  "},{"title":"CQL0349: column definitions may not come after constraints 'column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0349-column-definitions-may-not-come-after-constraints-column_name","content":"In a CREATE TABLE statement, the indicated column name came after a constraint. SQLite expects all the column definitions to come before any constraint definitions. You must move the offending column definition above the constraints.  "},{"title":"CQL0350: statement must appear inside of a PROC SAVEPOINT block​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0350-statement-must-appear-inside-of-a-proc-savepoint-block","content":"The ROLLBACK RETURN and COMMIT RETURN forms are only usable inside of a PROC SAVEPOINT block because they rollback or commit the savepoint that was created at the top level.  "},{"title":"CQL0351: statement should be in a procedure and at the top level​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0351-statement-should-be-in-a-procedure-and-at-the-top-level","content":"The indicated statement may only appear inside procedure and not nested. The classic example of this is the PROC SAVEPOINT form which can only be used at the top level of procedures.  "},{"title":"CQL0352: use COMMIT RETURN or ROLLBACK RETURN in within a proc savepoint block​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0352-use-commit-return-or-rollback-return-in-within-a-proc-savepoint-block","content":"The normal RETURN statement cannot be used inside of PROC SAVEPOINT block, you have to indicate if you want to commit or rollback the savepoint when you return. This makes it impossible to forget to do so which is in some sense the whole point of PROC SAVEPOINT.  "},{"title":"CQL0353: evaluation of constant failed​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0353-evaluation-of-constant-failed","content":"The constant expression could not be evaluated. This is most likely because it includes an operator that is not supported or a function call which is not support. Very few functions can be used in constant expressions The supported functions include iif, which is rewritten; abs; ifnull, nullif, and coalesce.  "},{"title":"CQL0354: duplicate enum member 'enum_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0354-duplicate-enum-member-enum_name","content":"While processing a declare enum statement the indicated member of the enum appeared twice. This is almost certainly a copy/paste of the same enum member twice.  "},{"title":"CQL0355: evaluation failed 'enum_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0355-evaluation-failed-enum_name","content":"While processing a declare enum statement the indicated member of the enum could not be evaluated as a constant expression. There could be a non-constant in the expression or there could be a divide-by-zero error.  "},{"title":"CQL0356: enum definitions do not match 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0356-enum-definitions-do-not-match-name","content":"The two described declare enum statements have the same name but they are not identical. The error output contains the full text of both declarations to compare.  "},{"title":"CQL0357: enum does not contain 'enum_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0357-enum-does-not-contain-enum_name","content":"The indicated member is not part of the enumeration.  "},{"title":"CQL0358: declared enums must be top level 'enum'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0358-declared-enums-must-be-top-level-enum","content":"A DECLARE ENUM statement for the named enum is happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure.  "},{"title":"CQL0359: duplicate type declaration 'type_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0359-duplicate-type-declaration-type_name","content":"The name of a declared type should always be unique.  "},{"title":"CQL0360: unknown type 'type_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0360-unknown-type-type_name","content":"The indicated name is not a valid type name.  "},{"title":"CQL0361: return data type in a create function declaration can only be Text, Blob or Object​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0361-return-data-type-in-a-create-function-declaration-can-only-be-text-blob-or-object","content":"Return data type in a create function definition can only be TEXT, BLOB or OBJECT. These are the only reference types and so CREATE makes sense only with those types. An integer, for instance, can't start with a +1 reference count.  "},{"title":"CQL0362: HIDDEN column attribute must be the first attribute if present​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0362-hidden-column-attribute-must-be-the-first-attribute-if-present","content":"In order to ensure that SQLite will parse HIDDEN as part of the type it has to come before any other attributes like NOT NULL. This limitation is due to the fact that CQL and SQLite use slightly different parsing approaches for attributes and in SQLite HIDDEN isn't actually an attribute. The safest place to put the attribute is right after the type name and before any other attributes as it is totally unambiguous there so CQL enforces this.  "},{"title":"CQL0363: all arguments must be names 'vault_sensitive'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0363-all-arguments-must-be-names-vault_sensitive","content":"vault_sensitive attribution only allow names. Integer, string literal, c string or blob are not allowed, only IDs should be provided.  "},{"title":"CQL0364: vault_sensitive annotation can only go on a procedure that uses the database​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0364-vault_sensitive-annotation-can-only-go-on-a-procedure-that-uses-the-database","content":"The named procedure has the vault_sensitive annotation to automatically encode sensitive value in the result set. Encoding value require the database, but the procedure in question doesn't even use the database at all. This annotation is therefore useless.  "},{"title":"CQL0365: @enforce_pop used but there is nothing to pop​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0365-enforce_pop-used-but-there-is-nothing-to-pop","content":"Each @enforce_pop should match an @enforce_push, but there is nothing to pop on the stack now.  "},{"title":"CQL0366: transaction operations disallowed while STRICT TRANSACTION enforcement is on​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0366-transaction-operations-disallowed-while-strict-transaction-enforcement-is-on","content":"@enforce_strict transaction has been used, while active no transaction operations are allowed. Savepoints may be used. This is typically done to prevent transactions from being used in any ad hoc way because they don't nest and typically need to be used with some &quot;master plan&quot; in mind.  "},{"title":"CQL0367: an attribute was specified twice 'attribute_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0367-an-attribute-was-specified-twice-attribute_name","content":"In the indicated type declaration, the indicated attribute was specified twice. This is almost certainly happening because the line in question looks like thisdeclare x type_name not null; but type_name is already not null.  "},{"title":"CQL0368: strict select if nothing requires that all (select ...) expressions include 'if nothing'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0368-strict-select-if-nothing-requires-that-all-select--expressions-include-if-nothing","content":"@enforce_strict select if nothing has been enabled. This means that select expressions must includeif nothing throw (the old default) if nothing [value] or if nothing or null [value]. This options exists because commonly the case where a row does not exist is not handled correctly when (select ...) is used without the if nothing options. If your select expression uses a built-in aggregate function, this check may not be enforced because they can always return a row. But there are exceptions. The check is still enforced when one of the following is in the expression: a GROUP BY clausea LIMIT that evaluates to less than 1, or is a variablean OFFSET clauseYou have a min or max function with more than 1 argument. Those are scalar functions.  "},{"title":"CQL0369: (SELECT ... IF NOTHING) construct is for use in top level expressions, not inside of other DML​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0369-select--if-nothing-construct-is-for-use-in-top-level-expressions-not-inside-of-other-dml","content":"This form allows for error control of (select...) expressions. But SQLite does not understand the form at all, so it can only appear at the top level of expressions where CQL can strip it out. Here are some examples: good:  set x := (select foo from bar where baz if nothing 0); if (select foo from bar where baz if nothing 1) then ... end if;  bad:  select foo from bar where (select something from somewhere if nothing null); delete from foo where (select something from somewhere if nothing 1);  Basically if you are already in a SQL context, the form isn't usable because SQLite simply doesn't understand if nothing at all. This error makes it so that you'll get a build time failure from CQL rather than a run time failure from SQLite.  "},{"title":"CQL0370: due to a memory leak bug in old SQLite versions, the select part of an insert must not have a top level join or compound operator. Use WITH and a CTE, or a nested select to work around this.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0370-due-to-a-memory-leak-bug-in-old-sqlite-versions-the-select-part-of-an-insert-must-not-have-a-top-level-join-or-compound-operator-use-with-and-a-cte-or-a-nested-select-to-work-around-this","content":"There is an unfortunate memory leak in older versions of SQLite (research pending on particular versions, but 3.28.0 has it). It causes this pattern to leak: -- must be autoinc table create table x ( pk integer primary key autoincrement ); -- any join will do (this is a minimal repro) insert into x select NULL pk from (select 1) t1 inner join (select 1) t2;  You can workaround this with a couple of fairly simple rewrites. This form is probably the cleanest. with cte (pk) as (select .. anything you need) insert into x select * from cte;  Simply wrapping your desired select in a nested select also suffices. So long as the top level is simple. insert into x select * from ( select anything you need.... );   "},{"title":"CQL0371: table valued function used in a left/right/cross context; this would hit a SQLite bug. Wrap it in a CTE instead.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0371-table-valued-function-used-in-a-leftrightcross-context-this-would-hit-a-sqlite-bug--wrap-it-in-a-cte-instead","content":"This error is generated by @enforce_strict table function. It is there to allow safe use of Table Valued Functions (TVFs) even though there was a bug in SQLite prior to v 3.31.0 when joining against them. The bug appears when the TVF is on the right of a left join. For example: select * from foo left join some_tvf(1);  In this case the join becomes an INNER join even though you wrote a left join. Likewise select * from some_tvf(1) right join foo;  Becomes an inner join even though you wrote a right join. The same occurs when a TVF is on either side of a cross join. The workaround is very simple. You don't want the TVF to be the target of the join directly. Instead: with tvf_(*) as (select * from some_tvf(1)) select * from foo left join tvf_;  OR select * from foo left join (select * from some_tvf(1));   "},{"title":"CQL0372: SELECT ... IF NOTHING OR NULL NULL is redundant; use SELECT ... IF NOTHING NULL instead.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0372-select--if-nothing-or-null-null-is-redundant-use-select--if-nothing-null-instead","content":"It is always the case that SELECT ... IF NOTHING OR NULL NULL is equivalent to SELECT ... IF NOTHING NULL. As such, do not do this: select foo from bar where baz if nothing or null null  Do this instead: select foo from bar where baz if nothing null   "},{"title":"CQL0373: comparing against NULL always yields NULL; use IS and IS NOT instead.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0373-comparing-against-null-always-yields-null-use-is-and-is-not-instead","content":"Attempting to check if some value x is NULL via x = NULL or x == NULL, or isn't NULL via x &lt;&gt; NULL or x != NULL, will always produce NULL regardless of the value of x. Instead, use x IS NULL or x IS NOT NULL to get the expected boolean result.  "},{"title":"CQL0374: SELECT expression is equivalent to NULL.​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0374-select-expression-is-equivalent-to-null","content":"CQL found a redundant select operation (e.g., set x := (select NULL);). There is no need to write a select expression that always evaluates to NULL. Simply use NULL instead (e.g., set x := NULL;).  CQL 0375 : unused, this was added to prevent merge conflicts at the end on literally every checkin  CQL 0376 : unused, this was added to prevent merge conflicts at the end on literally every checkin  "},{"title":"CQL0377: table transitioning from @recreate to @create must use @create(nn,cql:from_recreate) 'table name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0377-table-transitioning-from-recreate-to-create-must-use-createnncqlfrom_recreate-table-name","content":"The indicated table is moving from @recreate to @create meaning it will now be schema managed in an upgradable fashion. When this happens end-user databases might have some stale version of the table from a previous installation. This stale version must get a one-time cleanup in order to ensure that the now current schema is correctly applied. The cql:from_recreate annotation does this. It is required because otherwise there would be no record that this table &quot;used to be recreate&quot; and therefore might have an old version still in the database. A correct table might look something like this: create table correct_migration_to_create( id integer primary key, t text ) @create(7, cql:from_recreate);   "},{"title":"CQL0378: built-in migration procedure not valid in this context 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0378-built-in-migration-procedure-not-valid-in-this-context-name","content":"The indicated name is a valid built-in migration procedure but it is not valid on this kind of item. For instance cql:from_recreate can only be applied to tables.  "},{"title":"CQL0379: unknown built-in migration procedure 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0379-unknown-built-in-migration-procedure-name","content":"Certain schema migration steps are built-in. Currently the only one is cql:from_recreate for moving to @create from @recreate. Others may be added in the future. The cql: prefix ensures that this name cannot conflict with a valid user migration procedure.  "},{"title":"CQL0380: WHEN expression cannot be evaluated to a constant​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0380-when-expression-cannot-be-evaluated-to-a-constant","content":"In a SWITCH statement each expression each expression in a WHEN clause must be made up of constants and simple numeric math operations. See the reference on the const(..) expression for the valid set. It's most likely that a variable or function call appears in the errant expression.  "},{"title":"CQL0381: case expression must be a not-null integral type​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0381-case-expression-must-be-a-not-null-integral-type","content":"The SWITCH statement can only switch over integers or long integers. It will be translated directly to the C switch statement form. TEXT, REAL, BLOB, BOOL, and OBJECT cannot be used in this way.  "},{"title":"CQL0382: type of a WHEN expression is bigger than the type of the SWITCH expression​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0382-type-of-a-when-expression-is-bigger-than-the-type-of-the-switch-expression","content":"The WHEN expression evaluates to a LONG INTEGER but the expression in the SWITCH is INTEGER.  "},{"title":"CQL0383: switch ... ALL VALUES is useless with an ELSE clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0383-switch--all-values-is-useless-with-an-else-clause","content":"The ALL VALUES form of switch means that: the SWITCH expression is an enumerated typethe WHEN cases will completely cover the values of the enum If you allow the ELSE form then ALL VALUES becomes meaningless because of course they are all covered. So with ALL VALUES there can be no ELSE. You can list items that have no action with this form:  WHEN 10, 15 THEN NOTHING -- explicitly do nothing in these cases so they are still covered  No code is generated for such cases.  "},{"title":"CQL0384: switch statement did not have any actual statements in it​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0384-switch-statement-did-not-have-any-actual-statements-in-it","content":"Either there were no WHEN clauses at all, or they were all WHEN ... THEN NOTHING so there is no actual code to execute. You need to add some cases that do work.  "},{"title":"CQL0385: WHEN clauses contain duplicate values 'value'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0385-when-clauses-contain-duplicate-values-value","content":"In a SWITCH statement all of the values in the WHEN clauses must be unique. The indicated errant entry is a duplicate.  "},{"title":"CQL0386: SWITCH ... ALL VALUES is used but the switch expression is not an enum type​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0386-switch--all-values-is-used-but-the-switch-expression-is-not-an-enum-type","content":"In a SWITCH statement with ALL VALUES specified the switch expression was not an enumerated type.ALL VALUES is used to ensure that there is a case for every value of an enumerated type so this switch cannot be so checked. Either correct the expression, or remove ALL VALUES.  "},{"title":"CQL0387: a value exists in the enum that is not present in the switch 'enum_member'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0387-a-value-exists-in-the-enum-that-is-not-present-in-the-switch-enum_member","content":"In a SWITCH statement with ALL VALUES specified the errant enum member did not appear in any WHEN clause. All members must be specified when ALL VALUES is used.  "},{"title":"CQL0388: a value exists in the switch that is not present in the enum 'numeric_value'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0388-a-value-exists-in-the-switch-that-is-not-present-in-the-enum-numeric_value","content":"In a SWITCH statement with ALL VALUES specified the errant integer value appeared in in a WHEN clause. This value is not part of the members of the enum. Note that enum members that begin with '_' are ignored as they are, by convention, considered to be pseudo-members. e.g. in declare enum v integer (v0 = 0, v1 =1, v2 =2, _count = 3) _count is a pseudo-member. The errant entry should probably be removed. Alternatively, ALL VALUES isn't appropriate as the domain of the switch is actually bigger than the domain of the enumeration. One of these changes must happen.  "},{"title":"CQL0389: DECLARE OUT requires that the procedure be already declared​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0389-declare-out-requires-that-the-procedure-be-already-declared","content":"The purpose of the DECLARE OUT form is to automatically declare the out parameters for that procedure. This cannot be done if the type of the procedure is not yet known.  "},{"title":"CQL0390: DECLARE OUT CALL used on a procedure with no missing OUT arguments​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0390-declare-out-call-used-on-a-procedure-with-no-missing-out-arguments","content":"The DECLARE OUT CALL form was used, but the procedure has no OUT arguments that need any implicit declaration. Either they have already all been declared or else there are noOUT arguments at all, or even no arguments of any kind.  "},{"title":"CQL0391: CLOSE cannot be used on a boxed cursor​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0391-close-cannot-be-used-on-a-boxed-cursor","content":"When a cursor is boxed—i.e., wrapped in an object—the lifetime of the box and underlying statement are automatically managed via reference counting. Accordingly, it does not make sense to manually call CLOSE on such a cursor as it may be retained elsewhere. Instead, to allow the box to be freed and the underlying statement to be finalized, set all references to the cursor to NULL. Note: As with all other objects, boxed cursors are automatically released when they fall out of scope. You only have to set a reference to NULL if you want to release the cursor sooner, for some reason.  "},{"title":"CQL0392: when deleting a virtual table you must specify @delete(nn, cql:module_must_not_be_deleted_see_docs_for_CQL0392) as a reminder not to delete the module for this virtual table​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0392-when-deleting-a-virtual-table-you-must-specify-deletenn-cqlmodule_must_not_be_deleted_see_docs_for_cql0392-as-a-reminder-not-to-delete-the-module-for-this-virtual-table","content":"When the schema upgrader runs, if the virtual table is deleted it will attempt to do DROP TABLE IF EXISTS on the indicated table. This table is a virtual table. SQLite will attempt to initialize the table even when you simply try to drop it. For that to work the module must still be present. This means modules can never be deleted! This attribute is here to remind you of this fact so that you are not tempted to delete the module for the virtual table when you delete the table. You may, however, replace it with a shared do-nothing stub. The attribute itself does nothing other than hopefully cause you to read this documentation.  "},{"title":"CQL0393: not deterministic user function cannot appear in a constraint expression 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0393-not-deterministic-user-function-cannot-appear-in-a-constraint-expression-function_name","content":"CHECK expressions and partial indexes (CREATE INDEX with a WHERE clause) require that the expressions be deterministic. User defined functions may or may not be deterministic. Use @attribute(cql:deterministic) on a UDF declaration (declare select function...) to mark it deterministic and allow its use in an index.  "},{"title":"CQL0394: nested select expressions may not appear inside of a constraint expression​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0394-nested-select-expressions-may-not-appear-inside-of-a-constraint-expression","content":"SQLite does not allow the use of correlated subqueries or other embedded select statements inside of a CHECK expression or the WHERE clauses of a partial index. This would require additional joins on every such operation which would be far too expensive.  "},{"title":"CQL0395: table valued functions may not be used in an expression context 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0395-table-valued-functions-may-not-be-used-in-an-expression-context-function_name","content":"A table valued function should be used like a table e.g. -- this is right select * from table_valued_func(5);  Not like a value e.g. -- this is wrong select table_valued_func(5); -- this is also wrong select 1 where table_valued_func(5) = 3;   "},{"title":"CQL0396: versioning attributes may not be used on DDL inside a procedure​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0396-versioning-attributes-may-not-be-used-on-ddl-inside-a-procedure","content":"If you are putting DDL inside of a procedure then that is going to run regardless of any @create,@delete, or @recreate attributes; DDL in entires do not get versioning attributes, attributes are reserved for schema declarations outside of any procedure.  "},{"title":"CQL0397: object is an orphan because its table is deleted. Remove rather than @delete 'object_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0397-object-is-an-orphan-because-its-table-is-deleted-remove-rather-than-delete-object_name","content":"This error is about either a trigger or an index. In both cases you are trying to use @delete on the index/trigger but the table that the named object is based on is itself deleted, so the object is an orphan. Because of this, the orphaned object doesn't need, or no longer needs, an @delete tombstone because when the table is dropped, all of its orphaned indices and triggers will also be dropped. To fix this error, remove the named object entirely rather than marking it @delete. Note: if the index/trigger was previously deleted and now the table is also deleted, it is now safe to remove the index/trigger @delete tombstone and this error reminds you to do so.  "},{"title":"CQL0398: a compound select cannot be ordered by the result of an expression​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0398-a-compound-select-cannot-be-ordered-by-the-result-of-an-expression","content":"When specifying an ORDER BY for a compound select, you may only order by indices (e.g., 3) or names (e.g., foo) that correspond to an output column, not by the result of an arbitrary expression (e.g., foo + bar). For example, this is allowed: SELECT x, y FROM t0 UNION ALL select x, y FROM t1 ORDER BY y  The equivalent using an index is also allowed: SELECT x, y FROM t0 UNION ALL select x, y FROM t1 ORDER BY 2  This seemingly equivalent version containing an arbitrary expression, however, is not: SELECT x, y FROM t0 UNION ALL select x, y FROM t1 ORDER BY 1 + 1;   "},{"title":"CQL0399: table must leave @recreate management with @create(nn) or later 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0399-table-must-leave-recreate-management-with-createnn-or-later-table_name","content":"The indicated table changed from @recreate to @create but it did so in a past schema version. The change must happen in the current schema version. That version is indicated by the value of nn. To fix this you can change the @create annotation so that it matches the number in this error message  "},{"title":"CQL0400: encode context column can't be sensitive​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0400-encode-context-column-cant-be-sensitive","content":"The encode context column will be used to encode sensitive fields, it can't be exposed to encode functions  "},{"title":"CQL0401: encode context column must be specified if strict encode context column mode is enabled​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0401-encode-context-column-must-be-specified-if-strict-encode-context-column-mode-is-enabled","content":"encode context column must be specified in vault_sensitive attribute with format: @attribute(cql:vault_sensitive=(encode_context_col, (col1, col2, ...))  "},{"title":"CQL0402: encode context column in vault_sensitive attribute must match the specified type in strict mode​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0402-encode-context-column-in-vault_sensitive-attribute-must-match-the-specified-type-in-strict-mode","content":"encode context column must match the specified type in vault_sensitive attribute with format: @attribute(cql:vault_sensitive=(encode_context_col, (col1, col2, ...))  "},{"title":"CQL0403: operator may not be used because it is not supported on old versions of SQLite, 'operator'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0403-operator-may-not-be-used-because-it-is-not-supported-on-old-versions-of-sqlite-operator","content":"The indicated operator has been suppressed with @enforce_strict is true because it is not available on older versions of sqlite.  "},{"title":"CQL0404: procedure cannot be both a normal procedure and an unchecked procedure, 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0404-procedure-cannot-be-both-a-normal-procedure-and-an-unchecked-procedure-procedure_name","content":"The construct: DECLARE PROCEDURE printf NO CHECK;  Is used to tell CQL about an external procedure that might take any combination of arguments. The canonical example isprintf. All the arguments are converted from CQL types to basic C types when making the call (e.g. TEXT variables become temporary C strings). Once a procedure has been declared in this way it can't then also be declared as a normal CQL procedure via CREATE or DECLARE PROCEDURE. Likewise a normal procedure can't be redeclared with the NO CHECKpattern.  "},{"title":"CQL0405: procedure of an unknown type used in an expression 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0405-procedure-of-an-unknown-type-used-in-an-expression-procedure_name","content":"If a procedure has no known type—that is, it was originally declared with NO CHECK, and has not been subsequently re-declared with DECLARE FUNCTION orDECLARE SELECT FUNCTION—it is not possible to use it in an expression. You must either declare the type before using it or call the procedure outside of an expression via a CALL statement: DECLARE PROCEDURE some_external_proc NO CHECK; -- This works even though `some_external_proc` has no known type -- because we're using a CALL statement. CALL some_external_proc(&quot;Hello!&quot;); DECLARE FUNCTION some_external_proc(t TEXT NOT NULL) INT NOT NULL; -- Now that we've declared the type, we can use it in an expression. let result := some_external_proc(&quot;Hello!&quot;);   "},{"title":"CQL0406: substr uses 1 based indices, the 2nd argument of substr may not be zero\"​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0406-substr-uses-1-based-indices-the-2nd-argument-of-substr-may-not-be-zero","content":"A common mistake with substr is to assume it uses zero based indices like C does. It does not. In fact the result when using 0 as the second argument is not well defined. If you want the firstn characters of a string you use substr(haystack, 1, n).  CQL 0407 : unused, this was added to prevent merge conflicts at the end on literally every checkin  "},{"title":"CQL0408: encode context column can be only specified once​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0408-encode-context-column-can-be-only-specified-once","content":"The encode context column can be only specified once in @vault_sensitive attribute  "},{"title":"CQL0409: cannot use IS NULL or IS NOT NULL on a value of a NOT NULL type 'nonnull_expr'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0409-cannot-use-is-null-or-is-not-null-on-a-value-of-a-not-null-type-nonnull_expr","content":"If the left side of an IS NULL or IS NOT NULL expression is of a NOT NULLtype, the answer will always be the same (FALSE or TRUE, respectively). Such a check often indicates confusion that may lead to unexpected behavior (e.g., checking, incorrectly, if a cursor has a row via cursor IS NOT NULL). NOTE: Cursor fields of cursors without a row and uninitialized variables of a NOT NULL reference type are exceptions to the above rule: Something may be NULL even if it is of a NOT NULL type in those cases. CQL will eventually eliminate these exceptions. In the cursor case, one can check whether or not a cursor has a row by using the cursor-as-boolean-expression syntax (e.g., IF cursor THEN ... END IF;, IF NOT cursor ROLLBACK RETURN;, et cetera). In the uninitialized variables case, writing code that checks for initialization is not recommended (and, indeed, use before initialization will soon be impossible anyway): One should simply always initialize the variable.  CQL 0410 : unused, this was added to prevent merge conflicts at the end on literally every checkin  "},{"title":"CQL0411: duplicate flag in substitution 'flag'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0411-duplicate-flag-in-substitution-flag","content":"The same flag cannot be used more than once per substitution within a format string.  "},{"title":"CQL0412: cannot combine '+' flag with space flag​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0412-cannot-combine--flag-with-space-flag","content":"It is not sensible to use both the + flag and the space flag within the same substitution (e.g., %+ d) as it is equivalent to just using the + flag (e.g., %+d).  "},{"title":"CQL0413: width required when using flag in substitution 'flag'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0413-width-required-when-using-flag-in-substitution-flag","content":"The flag used (- or 0) for a substitution within a format string does not make sense unless accompanied by a width (e.g., %-10d).  "},{"title":"CQL0414: 'l' length specifier has no effect; consider 'll' instead​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0414-l-length-specifier-has-no-effect-consider-ll-instead","content":"The use of the l length specifier within a format string, e.g. %ld, has no effect in SQLite. If the argument is to be a LONG, use ll instead (e.g.,%lld). If the argument is to be an INTEGER, simply omit the length specifier entirely (e.g., %d).  "},{"title":"CQL0415: length specifier cannot be combined with '!' flag​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0415-length-specifier-cannot-be-combined-with--flag","content":"Length specifiers are only for use with integer type specifiers (e.g. %lld) and the ! flag is only for use with non-integer type specifiers (e.g. %!10sand %!f). It therefore makes no sense to use both within the same substitution.  "},{"title":"CQL0416: type specifier not allowed in CQL 'type_specifier'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0416-type-specifier-not-allowed-in-cql-type_specifier","content":"The type specifier used is accepted by SQLite, but it would be either useless or unsafe if used within the context of CQL.  "},{"title":"CQL0417: unrecognized type specifier 'type_specifier'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0417-unrecognized-type-specifier-type_specifier","content":"The type specifier used within the format string is not known to SQLite.  "},{"title":"CQL0418: type specifier combined with inappropriate flags 'type_specifier'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0418-type-specifier-combined-with-inappropriate-flags-type_specifier","content":"The type specifier provided does not make sense given one or more flags that appear within the same substitution. For example, it makes no sense to have a substitution like %+u: the + indicates the sign of the number will be shown, while the u indicates the number will be shown as an unsigned integer.  "},{"title":"CQL0419: type specifier cannot be combined with length specifier 'type_specifier'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0419-type-specifier-cannot-be-combined-with-length-specifier-type_specifier","content":"The type specifier provided cannot be used with a length specifier. For example,%lls makes no sense because ll only makes sense with integer types and sis a type specifier for strings.  "},{"title":"CQL0420: incomplete substitution in format string​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0420-incomplete-substitution-in-format-string","content":"The format string ends with a substitution that is incomplete. This can be the case if a format string ends with a % (e.g., &quot;%d %s %&quot;). If the intent is to have a literal % printed, use %% instead (e.g., &quot;%d %s %%&quot;`).  "},{"title":"CQL0421: first argument must be a string literal 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0421-first-argument-must-be-a-string-literal-function","content":"The first argument to the function must be a string literal.  "},{"title":"CQL0422: more arguments provided than expected by format string 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0422-more-arguments-provided-than-expected-by-format-string-function","content":"More arguments were provided to the function than its format string indicates are necessary. The most likely cause for this problem is that the format string is missing a substitution.  "},{"title":"CQL0423: fewer arguments provided than expected by format string 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0423-fewer-arguments-provided-than-expected-by-format-string-function","content":"Fewer arguments were provided to the function than its format string indicates are necessary. The most likely cause for this problem is that an argument was accidentally omitted.  "},{"title":"CQL0424: procedure with INOUT parameter used as function 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0424-procedure-with-inout-parameter-used-as-function-procedure_name","content":"If a procedure has an INOUT parameter, it cannot be used as a function: It may only be called via a CALL statement.  "},{"title":"CQL0425: procedure with non-trailing OUT parameter used as function 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0425-procedure-with-non-trailing-out-parameter-used-as-function-procedure_name","content":"For a procedure to be used as a function, it must have exactly one OUTparameter, and that parameter must be the last parameter of the procedure. In all other cases, procedures with one or more OUT parameters may only be called via a CALL statement.  "},{"title":"CQL0426: OUT or INOUT argument cannot be used again in same call 'variable'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0426-out-or-inout-argument-cannot-be-used-again-in-same-call-variable","content":"When a variable is passed as an OUT or INOUT argument, it may not be used as another argument within the same procedure call. It can, however, be used within a subexpression of another argument. For example: CREATE PROC some_proc(IN a TEXT, OUT b TEXT) BEGIN ... END DECLARE t TEXT; -- This is NOT legal. CALL some_proc(t, t); -- This, however, is perfectly fine. CALL some_proc(some_other_proc(t), t);   "},{"title":"CQL0427: LIKE CTE form may only be used inside a shared fragment at the top level i.e. @attribute(cql:shared_fragment) 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0427-like-cte-form-may-only-be-used-inside-a-shared-fragment-at-the-top-level-ie-attributecqlshared_fragment-procedure_name","content":"When creating a shared fragment you can specify &quot;table parameters&quot; by defining their shape like so: @attribute(cql:shared_fragment) create proc shared_proc(lim_ integer) begin with source(*) LIKE any_shape select * from source limit lim_; end;  However this LIKE form only makes sense withing a shared fragment, and only as a top level CTE in such a fragment. So either: the LIKE appeared outside of any procedurethe LIKE appeared in a procedure, but that procedure is not a shared fragmentthe LIKE appeared in a nested WITH clause  "},{"title":"CQL0428: duplicate binding of table in CALL/USING clause 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0428-duplicate-binding-of-table-in-callusing-clause-table_name","content":"In a CALL clause to access a shared fragment there is a duplicate table name in the USING portion. Example: my_cte(*) AS (call my_fragment(1) USING something as param1, something_else as param1),  Here param1 is supposed to take on the value of both something and something_else. Each parameter may appear only once in the USING clause. "},{"title":"CQL0429: called procedure has no table arguments but a USING clause is present 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0429-called-procedure-has-no-table-arguments-but-a-using-clause-is-present-procedure_name","content":"In a CALL clause to access a shared fragment there are table bindings but the shared fragment that is being called does not have any table bindings. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin select * from a_location limit lim; end; -- here we try to use my_fragment with table parameter but it has none with my_cte(*) AS (call my_fragment(1) USING something as param) select * from my_cte;   "},{"title":"CQL0430: no actual table was provided for the table parameter 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0430-no-actual-table-was-provided-for-the-table-parameter-table_name","content":"In a CALL clause to access a shared fragment the table bindings are missing a table parameter. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE source_shape select * from source limit lim; end; -- here we try to use my_fragment but no table was specified to play the role of &quot;source&quot; with my_cte(*) AS (call my_fragment(1)) select * from my_cte;   "},{"title":"CQL0431: an actual table was provided for a table parameter that does not exist 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0431-an-actual-table-was-provided-for-a-table-parameter-that-does-not-exist-table_name","content":"In a CALL clause to access a shared fragment the table bindings refer to a table parameter that does not exist. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE source_shape select * from source limit lim; end; -- here we try to use my_fragment but there is a table name &quot;soruce&quot; that doesn't match source with my_cte(*) AS (call my_fragment(1) USING something as soruce) select * from my_cte;   "},{"title":"CQL0432: table provided must have the same number of columns as the table parameter 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0432-table-provided-must-have-the-same-number-of-columns-as-the-table-parameter-table_name","content":"In a CALL clause to access a shared fragment the table bindings are trying to use a table that has the wrong number of columns. The column count, names, and types must be compatible. Extra columns for instance are not allowed because they might create ambiguities that were not present in the shared fragment. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE (select 1 x, 2 y) select * from source limit lim; end; -- here we try to use my_fragment but we provided 3 columns not 2 with my_source(*) AS (select 1 x, 2 y, 3 z), my_cte(*) AS (call my_fragment(1) USING my_source as source) select * from my_cte;  Here my_fragment wants a source table with 2 columns (x, y). But 3 were provided.  "},{"title":"CQL0433: table argument 'formal_name' requires column 'column_name' but it is missing in provided table 'actual_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0433-table-argument-formal_name-requires-column-column_name-but-it-is-missing-in-provided-table-actual_name","content":"In a CALL clause to access a shared fragment the table bindings are trying to use a table that is missing a required column. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin with source LIKE (select 1 x, 2 y) select * from source limit lim; end; -- here we try to use my_fragment but we passed in a table with (w,x) not (x,y) with my_source(*) AS (select 1 w, 2 x), my_cte(*) AS (call my_fragment(1) USING my_source as source) select * from my_cte;   "},{"title":"CQL0434: shared fragments may not be called outside of a SQL statement 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0434-shared-fragments-may-not-be-called-outside-of-a-sql-statement-procedure_name","content":"The indicated name is the name of a shared fragment, these fragments may be used inside of SQL code (e.g. select statements) but they have no meaning in a normal call outside of a SQL statement. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin select * from somewhere limit lim; end; call my_fragment();  Here my_fragment is being used like a normal procedure. This is not valid. A correct use of a fragment might look something like this: with (call my_fragment()) select * from my_fragment;   "},{"title":"CQL0435: must use qualified form to avoid ambiguity with alias 'column'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0435-must-use-qualified-form-to-avoid-ambiguity-with-alias-column","content":"In a SQLite SELECT expression, WHERE, GROUP BY, HAVING, and WINDOWclauses see the columns of the FROM clause before they see any aliases in the expression list. For example, assuming some table t has columns x and y, the following two expressions are equivalent: SELECT x AS y FROM t WHERE y &gt; 100 SELECT x AS y FROM t WHERE t.y &gt; 100  In the first expression, the use of y &gt; 100 makes it seem as though the yreferred to could be the y resulting from x as y in the expression list, but that is not the case. To avoid such confusion, CQL requires the use of the qualified form t.y &gt; 100 instead.  "},{"title":"CQL0436: alias referenced from WHERE, GROUP BY, HAVING, or WINDOW clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0436-alias-referenced-from-where-group-by-having-or-window-clause","content":"Unlike many databases (e.g., PostgreSQL and SQL Server), SQLite allows the aliases of a SELECT expression list to be referenced from clauses that are evaluated before the expression list. It does this by replacing all such alias references with the expressions to which they are equivalent. For example, assuming t does not have a column x, the following two expressions are equivalent: SELECT a + b AS x FROM t WHERE x &gt; 100 SELECT a + b AS x FROM t WHERE a + b &gt; 100  This can be convenient, but it is also error-prone. As mentioned above, the above equivalency only holds if x is not a column in t: If x is a column in t, the WHERE clause would be equivalent to t.x &gt; 100 instead, and there would be no syntactically obvious way to know this without first manually determining all of the columns present in t. To avoid such confusion, CQL disallows referencing expression list aliases fromWHERE, GROUP BY, HAVING, and WINDOW clauses altogether. Instead, one should simply use the expression to which the alias is equivalent (as is done in the second example above).  "},{"title":"CQL0437: common table name shadows previously declared table or view 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0437-common-table-name-shadows-previously-declared-table-or-view-name","content":"The name of a common table expression may not shadow a previously declared table or view. To rectify the problem, simply use a different name.  "},{"title":"CQL0438: variable possibly used before initialization 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0438-variable-possibly-used-before-initialization-name","content":"The variable indicated must be initialized before it is used because it is of a reference type (BLOB, OBJECT, or TEXT) that is also NOT NULL. CQL is usually smart enough to issue this error only in cases where initialization is truly lacking. Be sure to verify that the variable will be initialized before it is used for all possible code paths.  "},{"title":"CQL0439: nonnull reference OUT parameter possibly not always initialized 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0439-nonnull-reference-out-parameter-possibly-not-always-initialized-name","content":"The parameter indicated must be initialized before the procedure returns because it is of a reference type (BLOB, OBJECT, or TEXT) that is also NOT NULL. CQL is usually smart enough to issue this error only in cases where initialization is truly lacking. Be sure to verify that the parameter will be initialized both before the end of the procedure and before all cases ofRETURN and ROLLBACK RETURN. (Initialization before THROW is not required.)  "},{"title":"CQL0440: fragments may not have an empty body 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0440-fragments-may-not-have-an-empty-body-procedure_name","content":"The indicated procedure is one of the fragment types but has an empty body. This is not valid for any fragment type. Example: @attribute(cql:shared_fragment) create proc my_fragment(lim integer not null) begin /* something has to go here */ end;   "},{"title":"CQL0441: shared fragments may only have IF, SELECT, or WITH...SELECT at the top level 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0441-shared-fragments-may-only-have-if-select-or--withselect-at-the-top-level-procedure_name","content":"A shared fragment may consist of just one SELECT statement (including WITH...SELECT) or it can be an IF/ELSE statement that has a series of compatible select statements. There are no other valid options.  "},{"title":"CQL0442: shared fragments with conditionals must include an else clause 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0442-shared-fragments-with-conditionals-must-include-an-else-clause-procedure_name","content":"In shared fragment with conditionals (i.e. it has an IF statement at the top) the fragment must have an ELSE block so that it is guaranteed to create chunk of SQL text in its expansion. When no rows are required you can do so with something like: IF ... THEN ... ELSE select 1 x, '2' y WHERE 0; END IF;  If the ELSE condition indicates that some join should not happen you might generate default values or NULLs for the join result like so: IF ... THEN ... ELSE select input_stuff.*, NULL x, -1 y; END IF;   "},{"title":"CQL0443: shared fragments with conditionals must have exactly one SELECT or WITH...SELECT in each statement list 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0443-shared-fragments-with-conditionals-must-have-exactly-one-select-or-withselect-in-each-statement-list-procedure_name","content":"In a shared fragment with conditionals the top level statement is an &quot;IF&quot;. All of the statement lists in the IF must have exactly one valid select statement. This error indicates that a statement list has the wrong number or type of statement.  "},{"title":"CQL0444: this use of the named shared fragment is not legal because of name conflict 'procedure_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0444-this-use-of-the-named-shared-fragment-is-not-legal-because-of-name-conflict-procedure_name","content":"This error will be followed by additional diagnostic information about the call chain that is problematic. For instance: Procedure innermost has a different CTE that is also named foo The above originated from CALL inner USING foo AS source The above originated from CALL middle USING foo AS source The above originated from CALL outer USING foo AS source  This indicates that you are trying to call outer which in turn calls middle which in turn called inner. The conflict happened when the foo parameter was passed in to inner because it already has a CTE named foo that means something else. The way to fix this problem is to rename the CTE in probably the outermost call as that is likely the one you control. Renaming it in the innermost procedure might also be wise if that procedure is using a common name likely to conflict. It is wise to name the CTEs in shared fragments such that they are unlikely to eclipse outer CTEs that will be needed as table parameters.  "},{"title":"CQL0445: @attribute(cql:try_is_proc_body) accepts no values​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0445-attributecqltry_is_proc_body-accepts-no-values","content":"The attribute cql:try_is_proc_body cannot be used with any values (e.g.,cql:try_is_proc_body=(...)).  "},{"title":"CQL0446: @attribute(cql:try_is_proc_body) cannot be used more than once per procedure​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0446-attributecqltry_is_proc_body-cannot-be-used-more-than-once-per-procedure","content":"The purpose of cql:try_is_proc_body is to indicate that a particular TRYblock contains what should be considered to be the true body of the procedure. As it makes no sense for a procedure to have multiple bodies,cql:try_is_proc_body must appear only once within any given procedure.  "},{"title":"CQL0447: virtual table 'table' claims to be eponymous but its module name 'module' differs from its table name​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0447-virtual-table-table-claims-to-be-eponymous-but-its-module-name-module-differs-from-its-table-name","content":"By definition, an eponymous virtual table has the same name as its module. If you use the @eponymous notation on a virtual table, you must also make the module and table name match.  "},{"title":"CQL0448: table was marked @delete but it needs to be marked @recreate @delete 'table'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0448-table-was-marked-delete-but-it-needs-to-be-marked-recreate-delete-table","content":"The indicated table was on the recreate plan and was then deleted by adding an @delete(version) attribute. However, the previous @recreate annotation was removed. This would make the table look like it was a baseline table that had been deleted, and it isn't. To correctly drop a table on the @recreate you leave the recreate directive as it was and simply add @delete. No version information is required because the table is on the recreate plan anyway. Example: create table dropping_this ( f1 integer, f2 text ) @recreate(optional_group) @delete;  This error indicates that the @recreate(optional_group) annotation was removed. You should put it back.  "},{"title":"CQL0449: unsubscribe does not make sense on non-physical tables 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0449-unsubscribe-does-not-make-sense-on-non-physical-tables-table_name","content":"The indicated table was marked for blob storage or is a backed table. In both cases there is no physical schema associated with it so unsubscribe does not make any sense there. If it's a backed table perhaps the intent was to remove the backing table? "},{"title":"CQL0450: a shared fragment used like a function must be a simple SELECT with no FROM clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0450-a-shared-fragment-used-like-a-function-must-be-a-simple-select-with-no-from-clause","content":"When using a shared fragment like an expression, the shared fragment must consist of a simple SELECT without a FROM clause. That SELECT, however, may contain a nested SELECT expression which, itself, may have a FROM clause. Additional constraints: the target of the call is a shared fragment the target therefore a single select statementthe target therefore has no out-arguments the target has no select clauses other than the select list, e.g. no FROM, WHERE, LIMIT etc.the target returns exactly one column, i.e. it's just one SQL expression  "},{"title":"CQL0451: procedure as function call is not compatible with DISTINCT or filter clauses​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0451-procedure-as-function-call-is-not-compatible-with-distinct-or-filter-clauses","content":"Certain built-in functions like COUNT can be used with DISTINCT or FILTER options like so: select count(distinct ...); select sum(...) filter(where ...) over (...)  These options are not valid when calling a procedure as a function and so they generate errors if used.  "},{"title":"CQL0452: function may not be used in SQL because it is not supported on old versions of SQLite 'function'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0452-function-may-not-be-used-in-sql-because-it-is-not-supported-on-old-versions-of-sqlite-function","content":"Due to an enabled enforcement (e.g., @enforce_strict sign function;), the indicated function may not be used within SQL because it is not supported on old versions of SQLite.  "},{"title":"CQL0453: blob type is not a valid table 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0453-blob-type-is-not-a-valid-table-table_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like so: DECLARE blob_var blob&lt;table_name&gt;  But the named table table_name is not a table.  "},{"title":"CQL0454: cursor was not declared for storage 'cursor_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0454-cursor-was-not-declared-for-storage-cursor_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the cursor variable have storage associated with it. This means it must be a value cursor or else a cursor that was fetched using the fetch C form and not the fetch C into [variables] form. The indicated cursor was either not fetched at all, or else is using only the fetch into form so it does not have storage that could be used to create a blob.  "},{"title":"CQL0455: blob variable must have a type kind for type safety, 'blob_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0455-blob-variable-must-have-a-type-kind-for-type-safety-blob_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like so: DECLARE blob_name blob;  But it must be: DECLARE blob_name blob&lt;table_name&gt;;  Where table_name is a suitable table.  "},{"title":"CQL0456: blob type is a view, not a table 'view_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0456-blob-type-is-a-view-not-a-table-view_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like: DECLARE blob_var blob&lt;view_name&gt;  Where the named type view_name is a view, not a table.  "},{"title":"CQL0457: the indicated table is not marked with @attribute(cql:blob_storage) 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0457-the-indicated-table-is-not-marked-with-attributecqlblob_storage-table_name","content":"The CQL forms SET [blob] FROM CURSOR [cursor] and FETCH [cursor] FROM [blob] require that the blob variable be declared with a type kind and the type of the blob matches a suitable table. In this case the blob was declared like: DECLARE blob_var blob&lt;table_name&gt;  but the indicated table is missing the necessary attribute @attribute(cql:blob_storage). This attribute is necessary so that CQL can enforce additional rules on the table to ensure that it is viable for blob storage. For instance, the table can have no primary key, no foreign keys, and may not be used in normal SQL statements.  "},{"title":"CQL0458: the indicated table may only be used for blob storage 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0458-the-indicated-table-may-only-be-used-for-blob-storage-table_name","content":"The indicated table has been marked with @attribute(cql:blob_storage). This means that it isn't a real table -- it will have no SQL schema. Since it's only a storage shape, it cannot be used in normal operations that use tables such as DROP TABLE,CREATE INDEX, or inside of SELECT statements. The CREATE TABLE construct is used to declare a blob storage type because it's the natural way to define a structure in SQL and also because the usual versioning rules are helpful for such tables. But otherwise, blob storage isn't really a table at all.  "},{"title":"CQL0459: table is not suitable for use as blob storage: [reason] 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0459-table-is-not-suitable-for-use-as-blob-storage-reason-table_name","content":"The indicated table was marked with @attribute(cql:blob_storage). This indicates that the table is going to be used to define the shape of blobs that could be stored in the database. It isn't going to be a &quot;real&quot; table. There are a number of reasons why a table might not be a valid as blob storage. For instance: it has a primary keyit has foreign keysit has constraintsit is a virtual table This error indicates that one of these items is present. The specific cause is included in the text of the message.  "},{"title":"CQL0460: field of a nonnull reference type accessed before verifying that the cursor has a row 'cursor.field'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0460-field-of-a-nonnull-reference-type-accessed-before-verifying-that-the-cursor-has-a-row-cursorfield","content":"If a cursor has a field of a nonnull reference type (e.g., TEXT NOT NULL), it is necessary to verify that the cursor has a row before accessing the field (unless the cursor has been fetched in such a way that it must have a row, e.g., via FETCH ... FROM VALUES or LOOP FETCH). The reason for this is that, should the cursor not have a row, the field will be NULL despite the nonnull type. Assume we have the following: create table t (x text not null); declare proc requires_text_notnull(x text not null);  The following code is illegal: declare c cursor for select * from t; fetch c; -- ILLEGAL because `c` may not have a row and thus -- `c.x` may be `NULL` call requires_text_notnull(c.x);  To fix it, the cursor must be verified to have a row before the field is accessed: declare c cursor for select * from t; fetch c; if c then -- legal due to the above check call requires_text_notnull(c.x); end if;  Alternatively, one can perform a &quot;negative&quot; check by returning (or using another control flow statement) when the cursor does not have a row: declare c cursor for select * from t; fetch c; if not c then call some_logging_function(&quot;no rows in t&quot;); return; end if; -- legal as we would have returned if `c` did not -- have a row call requires_text_notnull(c.x);  If you are sure that a row must be present, you can throw to make that explicit: declare c cursor for select * from t; fetch c; if not c throw; -- legal as we would have thrown if `c` did not -- have a row call requires_text_notnull(c.x);   "},{"title":"CQL0461: fetch from blob operand is not a blob​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0461-fetch-from-blob-operand-is-not-a-blob","content":"The blob operand in the form FETCH [cursor] FROM BLOB [blob]must be a blob. The given expression is of some other type.  "},{"title":"CQL0462: group declared variables must be top level 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0462-group-declared-variables-must-be-top-level-name","content":"A DECLARE GROUP statement for the named enum is happening inside of a procedure. This is not legal. To correct this, move the declaration outside of the procedure.  CQL0463: variable definitions do not match in group 'name'​ The two described DECLARE GROUP statements have the same name but they are not identical. The error output contains the full text of both declarations to compare.  "},{"title":"CQL0464: group not found 'group_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0464-group-not-found-group_name","content":"The indicated name was used in a context where a variable group name was expected but there is no such group. Perhaps the group was not included (missing an #include) or else there is a typo.  "},{"title":"CQL0465 avaiable for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0465-avaiable-for-re-use","content":" "},{"title":"CQL0466: the table/view named in an @unsub directive does not exist 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0466-the-tableview-named-in-an-unsub-directive-does-not-exist-name","content":"The indicated name is not a valid table or view.  "},{"title":"CQL0467 available for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0467-available-for-re-use","content":" "},{"title":"CQL0468: @attribute(cql:shared_fragment) may only be placed on a CREATE PROC statement 'proc_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0468-attributecqlshared_fragment-may-only-be-placed-on-a-create-proc-statement-proc_name","content":"In order to use a shared fragment the compiler must see the full body of the fragment, this is because the fragment will be inlined into the SQL in which it appears. As a consequence it makes no sense to try to apply the attribute to a procedure declaration. Instead put the shared fragment you want to use somewhere where it can be #included in full. example error: @attribute(cql:shared_fragment) declare proc x() (x integer); create proc y() begin with (call x()) select * from x; end;  Instead, include the entire body like so (this example is ultra simple). @attribute(cql:shared_fragment) create proc x() begin select 1 x; -- the procedure body must be present end;   "},{"title":"CQL0469: table/view is already deleted 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0469-tableview-is-already-deleted-name","content":"In an @unsub directive, the indicated table/view has already been deleted. It can no longer be managed via subscriptions.  "},{"title":"CQL0470 available for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0470-available-for-re-use","content":" "},{"title":"CQL0471 available for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0471-available-for-re-use","content":" "},{"title":"CQL0472: table/view is already unsubscribed 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0472-tableview-is-already-unsubscribed-name","content":"In an @unsub directive, the indicated table/view has already been unsubscribed. It doesn't need another unsubscription.  "},{"title":"CQL0473: @unsub is invalid because the table/view is still used by 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0473-unsub-is-invalid-because-the-tableview-is-still-used-by-name","content":"This error indicates that you are attempting to @unsub a table/view while there are still other tables/views that refer to it (e.g. by FK). You must @unsub all of those as well in order to safely @unsub the present table/view. All such dependencies will be listed. Note that some of those might, in turn, have the same issue. In short, a whole subtree has to be removed in order to do this operation safely.  "},{"title":"CQL0474 available for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0474-available-for-re-use","content":" "},{"title":"CQL0475 available for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0475-available-for-re-use","content":" "},{"title":"CQL0476 available for re-use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0476-available-for-re-use","content":" "},{"title":"CQL0477: interface name conflicts with func name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0477-interface-name-conflicts-with-func-name-name","content":"In a DECLARE INTERFACE statement, the given name conflicts with an already declared function (DECLARE FUNCTION or DECLARE SELECT FUNCTION). You'll have to choose a different name. "},{"title":"CQL0478: interface name conflicts with procedure name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0478-interface-name-conflicts-with-procedure-name-name","content":"In a DECLARE INTERFACE statement, the indicated name already corresponds to a created or declared stored procedure. You'll have to choose a different name. "},{"title":"CQL0479: interface declarations do not match 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0479-interface-declarations-do-not-match-name","content":"The interface was previously declared with a DECLARE INTERFACE statement but when subsequent DECLARE INTERFACE was encountered, it did not match the previous declaration. "},{"title":"CQL0480: declared interface must be top level 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0480-declared-interface-must-be-top-level-name","content":"A DECLARE INTERFACE statement is happening inside of a procedure. This is not legal. To correct this move the declaration outside of the procedure. "},{"title":"CQL0481: proc name conflicts with interface name 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0481-proc-name-conflicts-with-interface-name-name","content":"In a CREATE PROCEDURE / DECLARE PROCEDURE statement, the given name conflicts with an already declared interface (DECLARE INTERFACE). You'll have to choose a different name. "},{"title":"CQL0482: interface not found 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0482-interface-not-found-name","content":"Interface with the name provided in cql:implements attribute does not exist "},{"title":"CQL0483: table is not suitable for use as backing storage: [reason] 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0483-table-is-not-suitable-for-use-as-backing-storage-reason-table_name","content":"The indicated table was marked with @attribute(cql:backing_table). This indicates that the table is going to be used to as a generic storage location stored in the database. There are a number of reasons why a table might not be a valid as backing storage. For instance: it has foreign keysit has constraintsit is a virtual tableit has schema versioning This error indicates that one of these items is present. The specific cause is included in the text of the message. "},{"title":"CQL0484: procedure '%s' is missing column '%s' of interface '%s'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0484-procedure-s-is-missing-column-s-of-interface-s","content":"Procedure should return all columns defined by the interface (and possibly others). The columns may be returned in any order. "},{"title":"CQL0485: column types returned by proc need to be the same as defined on the interface​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0485-column-types-returned-by-proc-need-to-be-the-same-as-defined-on-the-interface","content":"Procedure should return at least all columns defined by the interface and column type should be the same. "},{"title":"CQL0486: function cannot be both a normal function and an unchecked function, 'function_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0486-function-cannot-be-both-a-normal-function-and-an-unchecked-function-function_name","content":"The same function cannot be declared as a function with unchecked parameters with the NO CHECK clause and then redeclared with typed parameters, or vice versa. --- Declaration of an external function foo with unchecked parameters. DECLARE SELECT FUNCTION foo NO CHECK t text; ... --- A redeclaration of foo with typed paramters. This would be invalid if the previous declaration exists. DECLARE SELECT FUNCTION foo() t text;  Make sure the redeclaration of the function is consistent with the original declaration, or remove the redeclaration. "},{"title":"CQL0487: table is not suitable as backed storage: [reason] 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0487-table-is-not-suitable-as-backed-storage-reason-table_name","content":"The indicated table was marked with @attribute(cql:backing_table). This indicates that the table is going to be used to as a generic storage location stored in the database. There are a number of reasons why a table might not be a valid as backing storage. For instance: it has foreign keysit has constraintsit is a virtual tableit has schema versioning This error indicates that one of these items is present. The specific cause is included in the text of the message. "},{"title":"CQL0488: the indicated table is not declared for backed storage 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0488-the-indicated-table-is-not-declared-for-backed-storage-table_name","content":"When declaring a backed table, you must specify the physical table that will hold its data. The backed table is marked with @attribute(cql:backed_by=table_name). The backing table is marked with @attribute(cql:backing). The backing and backed_by attributes applies extra checks to tables to ensure they are suitable candidates. This error indicates that the named table is not marked as a backed table. "},{"title":"CQL0489: the indicated column is not present in the named backed storage 'table_name.column_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0489-the-indicated-column-is-not-present-in-the-named-backed-storage-table_namecolumn_name","content":"The named table is a backed table, but it does not have the indicated column. "},{"title":"CQL0490: argument must be table.column where table is a backed table 'function\"​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0490-argument-must-be-tablecolumn-where-table-is-a-backed-table-function","content":"The database blob access functions cql_blob_get, cql_blob_create, cql_blob_update all allow you to specify the backed table name and column you are trying to read/create/update. The named function was called with a table.column combination where the table is not a backed table, hence the call is invalid. Note that normally this error doesn't happen because these functions are typically called by CQL itself as part of the rewriting process for backed tables. However it is possible to use them manually, hence they are error checked. "},{"title":"CQL0491: argument 1 must be a table name that is a backed table 'cql_blob_create'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0491-argument-1-must-be-a-table-name-that-is-a-backed-table-cql_blob_create","content":"When using the cql_blob_create helper function, the first argument must be a valid backed table (i.e. one that was marked with @attribute(cql:backed_by=some_backing_table)). The type signature of this table is used to create a hash valid for the type of the blob that is created. This error indicates that the first argument is not even an identifier, much less a table name that is backed. There are more specific errors if the table is not found or the table is not backed. Note that normally this error doesn't happen because this functions is typically called by CQL itself as part of the rewriting process for backed tables. However it is possible to cql_blob_create manually, hence it is error checked. "},{"title":"CQL0492 available for use​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0492-available-for-use","content":""},{"title":"CQL0493: backed storage tables may not be used in indexes/triggers/drop 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0493-backed-storage-tables-may-not-be-used-in-indexestriggersdrop-table_name","content":"The indicated table name was marked as backed storage. Therefore it does not have a physical manifestation, and therefore it cannot be used in an index or in a trigger. You may be able to get the index or trigger you want by creating an index on the backing storage and then using the blob access functions to index a column or check colulmns in a trigger. For instance this index is pretty normal: @attribute(cql:backing_table) create table backing ( k blob primary key, v blob not null ); create index backing_type_index on backing(cql_blob_get_type(k));  This gives you a useful index on the type field of the blob for all backed tables that use backing_table. But generally, physical operations like indices, triggers, and drop are not applicable to backed tables. "},{"title":"CQL0494: mixing adding and removing columns from a shape 'name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0494-mixing-adding-and-removing-columns-from-a-shape-name","content":"When selecting columns from a shape you can use this form LIKE some_shape(name1, name2)  to extract the named columns or this form LIKE some_shape(-name1, -name2)  to extract everything but the named columns. You can't mix the positive and negative forms "},{"title":"CQL0495: no columns were selected in the LIKE expression​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0495-no-columns-were-selected-in-the-like-expression","content":"An expression that is supposed to select some columns from a shape such as LIKE some_shape(-name1, -name2)  ended up removing all the columns from some_shape. "},{"title":"CQL0496: SELECT NOTHING may only appear in the else clause of a shared fragment​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0496-select-nothing-may-only-appear-in-the-else-clause-of-a-shared-fragment","content":"A common case for conditional shared fragments is that there are rows that should be optionally included. The normal way this is handled is to have a condition like this IF something THEN SELECT your_data; ELSE SELECT dummy data WHERE 0; END IF;  The problem here is that dummy_data could be complex and involve a lot of typing to get nothing. To make this less tedious CQL allows: IF something THEN SELECT your_data; ELSE SELECT NOTHING; END IF;  However this is the only place SELECT NOTHING is allowed. It must be: in a procedurewhich is a conditional shared fragmentin the else clause Any violation results in the present error. "},{"title":"CQL0497: FROM clause not supported when updating backed table, 'table_name'​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0497-from-clause-not-supported-when-updating-backed-table-table_name","content":"SQLite supports an extended format of the update statement with a FROM clause. At this time backed tables cannot be updated using this form. This is likely to change fairly soon. "},{"title":"CQL0498: strict UPDATE ... FROM validation requires that the UPDATE statement not include a FROM clause​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0498-strict-update--from-validation-requires-that-the-update-statement-not-include-a-from-clause","content":"@enforce_strict has been use to enable strict update enforcement. When enabled update statements may not include a FROM clause. This is done if the code expects to target SQLite version 3.33 or lower. "},{"title":"CQL0499: alias_of attribute may only be added to a declare function statement​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0499-alias_of-attribute-may-only-be-added-to-a-declare-function-statement","content":"cql:alias_of attributes may only be used in DECLARE FUNC statements or DECLARE PROC statements. "},{"title":"CQL0500: alias_of attribute must be a non-empty string argument​","type":1,"pageTitle":"Appendix 4: CQL Error Codes","url":"/cql-guide/x4#cql0500-alias_of-attribute-must-be-a-non-empty-string-argument","content":"cql:alias_of must have a string argument to indicate the underlying function name that the aliased function references. For example: @attribute(cql:alias_of=foo) declare function bar() int  All subsequent calls to bar() in CQL will call the foo() function. "},{"title":"Appendix 9: Using the CQL Amalgam","type":0,"sectionRef":"#","url":"/cql-guide/x9","content":"","keywords":""},{"title":"Building the Amalgam​","type":1,"pageTitle":"Appendix 9: Using the CQL Amalgam","url":"/cql-guide/x9#building-the-amalgam","content":"The amalgam has to include the results of bison and flex, so a normal build must run first. The simplest way to build it starting from the sources directory is: make ./make_amalgam.sh  The result goes in out/cql_amalgam.c. It can then be built using cc with whatever flags you might desire. With a few -D directives it can readily be compiled with Microsoft C and it also works with Emscripten (emcc) basically unchanged. Clang and Gcc of course also work. The standard test script test.sh builds the amalgam and attempts to compile it as well, which ensures that the amalgam can at least compile at all times. "},{"title":"Testing the Amalgam​","type":1,"pageTitle":"Appendix 9: Using the CQL Amalgam","url":"/cql-guide/x9#testing-the-amalgam","content":"Of course you can do whatever tests you might like by simply compiling the amalgam as is and then using it to compile things. But importantly the test script test.sh can test the amalgam build like so: test.sh --use_amalgam  This runs all the normal tests using the binary built from the amalgam rather than the normal binary. Normal CQL development practices result in this happening pretty often so the amalgam tends to stay in good shape. The code largely works in either form with very few affordances for the amalgam build needed. Most developers don't even think about the amalgam build flavor; to a first approximation &quot;it just works&quot;. "},{"title":"Using the Amalgam​","type":1,"pageTitle":"Appendix 9: Using the CQL Amalgam","url":"/cql-guide/x9#using-the-amalgam","content":"To use the amalgam you'll want to do something like this: #define CQL_IS_NOT_MAIN 1 // Suppresses a bunch of warnings because the code // is in an #include context // PR's to remove these are welcome :D #pragma clang diagnostic ignored &quot;-Wnullability-completeness&quot; #include &quot;cql_amalgam.c&quot; void go_for_it(const char *your_buffer) { YY_BUFFER_STATE my_string_buffer = yy_scan_string(your_buffer); // Note: &quot;--in&quot; is irrelevant because the scanner is // going to read from the buffer above. // // If you don't use yy_scan_string, you could use &quot;--in&quot; // to get data from a file. int argc = 4; char *argv[] = { &quot;cql&quot;, &quot;--cg&quot;, &quot;foo.h&quot;, &quot;foo.c&quot; }; cql_main(argc, argv); yy_delete_buffer(my_string_buffer); }  So the general pattern is: predefine the options you want to use (see below)include the amalgamadd any functions you want that will call the amalgam Most amalgam functions are static to avoid name conflicts. You will want to create your own public functions such as go_for_it above that use the amalgam in all the ways you desire. You'll want to avoid calling any internal functions other than cql_main because they are liable to change. NOTE: The amalgam is C code not C++ code. Do not attempt to use it inside of an extern &quot;C&quot; block in a C++ file. It won't build. If you want a C++ API, expose the C functions you need and write a wrapper class. "},{"title":"CQL Amalgam Options​","type":1,"pageTitle":"Appendix 9: Using the CQL Amalgam","url":"/cql-guide/x9#cql-amalgam-options","content":"The amalgam includes the following useful #ifdef options to allow you to customize it. CQL_IS_NOT_MAINCQL_NO_SYSTEM_HEADERSCQL_NO_DIAGNOSTIC_BLOCKcql_emit_errorcql_emit_outputcql_open_file_for_writecql_write_file CQL_IS_NOT_MAIN​ If this symbol is defined then cql_main will not be redefined to be main. As the comments in the source say: #ifndef CQL_IS_NOT_MAIN // Normally CQL is the main entry point. If you are using CQL // in an embedded fashion then you want to invoke its main at // some other time. If you define CQL_IS_NOT_MAIN then cql_main // is not renamed to main. You call cql_main when you want. #define cql_main main #endif  Set this symbol so that you own main and cql_main is called at your pleasure. CQL_NO_SYSTEM_HEADERS​ The amalgam includes the normal #include directives needed to make it compile, things like stdio and such. In your situation these headers may not be appropriate. If CQL_NO_SYSTEM_HEADERS is defined then the amalgam will not include anything; you can then add whatever headers you need before you include the amalgam. CQL_NO_DIAGNOSTIC_BLOCK​ The amalgam includes a set of recommended directives for warnings to suppress and include. If you want to make other choices for these you can suppress the defaults by defining CQL_NO_DIAGNOSTIC_BLOCK; you can then add whatever diagnostic pragmas you want/need. cql_emit_error​ The amalgam uses cql_emit_error to write its messages to stderr. The documentation is included in the code which is attached here. If you want the error messages to go somewhere else, define cql_emit_erroras the name of your error handling function. It should accept a const char * and record that string however you deem appropriate. #ifndef cql_emit_error // CQL &quot;stderr&quot; outputs are emitted with this API. // // You can define it to be a method of your choice with // &quot;#define cql_emit_error your_method&quot; and then your method // will get the data instead. This will be whatever output the // compiler would have emitted to stderr. This includes // semantic errors or invalid argument combinations. Note that // CQL never emits error fragments with this API; you always // get all the text of one error. This is important if you // are filtering or looking for particular errors in a test // harness or some such. // // You must copy the memory if you intend to keep it. &quot;data&quot; will // be freed. // // Note: you may use cql_cleanup_and_exit to force a failure from // within this API but doing so might result in unexpected cleanup // paths that have not been tested. void cql_emit_error(const char *err) { fprintf(stderr, &quot;%s&quot;, err); if (error_capture) { bprintf(error_capture, &quot;%s&quot;, err); } } #endif  Typically you would #define cql_emit_error your_error_function before you include the amalgam and then define your_error_function elsewhere in that file (before or after the amalgam is included are both fine). cql_emit_output​ The amalgam uses cql_emit_output to write its messages to stdout. The documentation is included in the code which is attached here. If you want the standard output to go somewhere else, define cql_emit_outputas the name of your output handling function. It should accept a const char * and record that string however you deem appropriate. #ifndef cql_emit_output // CQL &quot;stdout&quot; outputs are emitted (in arbitrarily small pieces) // with this API. // // You can define it to be a method of your choice with // &quot;#define cql_emit_output your_method&quot; and then your method will // get the data instead. This will be whatever output the // compiler would have emitted to stdout. This is usually // reformated CQL or semantic trees and such -- not the normal // compiler output. // // You must copy the memory if you intend to keep it. &quot;data&quot; will // be freed. // // Note: you may use cql_cleanup_and_exit to force a failure from // within this API but doing so might result in unexpected cleanup // paths that have not been tested. void cql_emit_output(const char *msg) { printf(&quot;%s&quot;, msg); } #endif  Typically you would #define cql_emit_output your_output_function before you include the amalgam and then define your_error_function elsewhere in that file (before or after the amalgam is included are both fine). cql_open_file_for_write​ If you still want normal file i/o for your output but you simply want to control the placement of the output (such as forcing it to be on some virtual drive) you can replace this function by defining cql_open_file_for_write. If all you need to do is control the origin of the FILE * that is written to, you can replace just this function. #ifndef cql_open_file_for_write // Not a normal integration point, the normal thing to do is // replace cql_write_file but if all you need to do is adjust // the path or something like that you could replace // this method instead. This presumes that a FILE * is still ok // for your scenario. FILE *_Nonnull cql_open_file_for_write( const char *_Nonnull file_name) { FILE *file; if (!(file = fopen(file_name, &quot;w&quot;))) { cql_error(&quot;unable to open %s for write\\n&quot;, file_name); cql_cleanup_and_exit(1); } return file; } #endif  Typically you would #define cql_open_file_for_write your_open_function before you include the amalgam and then define your_open_function elsewhere in that file (before or after the amalgam is included are both fine). cql_write_file​ The amalgam uses cql_write_file to write its compilation outputs to the file system. The documentation is included in the code which is attached here. If you want the compilation output to go somewhere else, define cql_write_fileas the name of your output handling function. It should accept a const char * for the file name and another for the data to be written. You can then store those compilation results however you deem appropriate. #ifndef cql_write_file // CQL code generation outputs are emitted in one &quot;gulp&quot; with this // API. You can define it to be a method of your choice with // &quot;#define cql_write_file your_method&quot; and then your method will // get the filename and the data. This will be whatever output the // compiler would have emitted to one of it's --cg arguments. // You can then write it to a location of your choice. // You must copy the memory if you intend to keep it. &quot;data&quot; will // be freed. // Note: you *may* use cql_cleanup_and_exit to force a failure // from within this API. That's a normal failure mode that is // well-tested. void cql_write_file( const char *_Nonnull file_name, const char *_Nonnull data) { FILE *file = cql_open_file_for_write(file_name); fprintf(file, &quot;%s&quot;, data); fclose(file); } #endif  Typically you would #define cql_write_file your_write_function before you include the amalgam and then define your_write_function elsewhere in that file (before or after the amalgam is included are both fine). "},{"title":"Amalgam LEAN choices​","type":1,"pageTitle":"Appendix 9: Using the CQL Amalgam","url":"/cql-guide/x9#amalgam-lean-choices","content":"When you include the amalgam, you get everything by default. You may, however, only want some limited subset of the compiler's functions in your build. To customize the amalgam, there are a set of configuration pre-processor options. To opt-in to configuration, first define CQL_AMALGAM_LEAN. You then have to opt-in to the various pieces you might want. The system is useless without the parser, so you can't remove that; but you can choose from the list below. The options are: CQL_AMALGAM_LEAN` : enable lean mode; this must be set or you get everythingCQL_AMALGAM_CG_C : C codegenCQL_AMALGAM_CG_COMMON : common code generator piecesCQL_AMALGAM_GEN_SQL : the echoing featuresCQL_AMALGAM_JSON : JSON schema outputCQL_AMALGAM_OBJC : Objective-C code genCQL_AMALGAM_QUERY_PLAN : the query plan creatorCQL_AMALGAM_SCHEMA : the assorted schema output typesCQL_AMALGAM_SEM : semantic analysis (needed by most things)CQL_AMALGAM_TEST_HELPERS : test helper outputCQL_AMALGAM_UDF : the UDF stubs used by the query plan outputCQL_AMALGAM_UNIT_TESTS : some internal unit tests, which are pretty much needed by nobody Note that CQL_AMALGAM_SEM is necessary for any of the code generation features to work. Likewise, several generators require CQL_AMALGAM_CG_COMMON (e.g., C does). Pick what you want; stubs are created for what you omit to avoid linkage errors. "},{"title":"Other Notes​","type":1,"pageTitle":"Appendix 9: Using the CQL Amalgam","url":"/cql-guide/x9#other-notes","content":"The amalgam will use malloc/calloc for its allocations and it is designed to release all memory it has allocated when cql_main returns control to you, even in the face of error. Internal compilation errors result in an assert failure leading to an abort. This is not supposed to ever happen but there can always be bugs. Normal errors just prevent later phases of the compiler from running so you might not see file output, but rather just error output. In all cases things should be cleaned up. The compiler can be called repeatedly with no troubles; it re-initializes on each use. The compiler is not multi-threaded so if there is threading you should use some mutex arrangement to keep it safe. A thread-safe version would require extensive modifications. "},{"title":"Code Coverage CG/SQL","type":0,"sectionRef":"#","url":"/docs/code-coverage","content":"Code Coverage CG/SQL note Due to issues that we don't currently understand, gcovr on macOS with the latest XCode tools is not working. Consequently, the following script fails on macOS. See https://github.com/facebookincubator/CG-SQL/issues/92 for a more complete discussion. The problem doesn't seem to have anything to do with CG/SQL per se, but there it is... Run this command in the /sources directory: ./cov.sh This will run the test scripts with the coverage flag, which causes the coverage build. If the tests pass a coverage report is created. The same build options are available as cov.sh uses test.sh to do the heavy lifting.","keywords":""},{"title":"Developer Notes on CQL Development","type":0,"sectionRef":"#","url":"/docs/dev-notes","content":"Developer Notes on CQL Development We have extensive documentation at CQL Internals. If you aren't good with yacc/lex you probably should do some homework before you start. CQL development is all about building and walking a syntax tree. It's possible to make local changes without knowing the details but it can be hard to figure out where to make changes without context. CQL development is basically test driven, to create a new feature: Add the language feature to test.sqlrun test.sh; it will fail due to parse errorAdd the syntax to cql.y and create the necessary tree pieces in ast.hrun test.sh; accept any file differences to install this as the new reference baseline.Add a test case to sem_test.sql that uses your new feature. sem_test.sql can contain pattern matching for the semantic output.run test.sh; it will fail because it will find an AST node it doesn't understandedit sem.c to do the analysis for your new node typeadjust the verification in sem_test.sql accordinglyrun test.sh until it passes making fixes as neededthere will be new diff output and it will be spitting out the diffs; if you are happy with the new output, accept the diffs to update the reference outputs; note the pattern matching validations will still fail if the output goes bad even if the reference comparison is good, the reference output is a double checkadd code that uses your new feature to cg_test.sql, this is the code gen test, verifications using pattern matching are also allowed thererun test.shit will fail because codegen doesn't know about your new featureedit cg_c.c (or a different code gen if you're doing test helpers or some such) to support your new codecycle running test.sh until it passesaccept each diff when you're happy with the new outputAdd code that runs your new feature using run_test.sqlRun test.sh, if your codegen was perfect it could pass; it probably won't at firstfix your code until it's done; you shouldn't need to accept any more diffs at this pointrun cov.sh to confirm 100% coveragesanity check the GCC build (I use a linux box for this) Get a solid code review and land as usual. By the time you have done this you will have passed the tests dozens of times and you will know exactly what your code is doing to the entire battery of cql combinations. Missing tests can be painful and cause downstream regressions so be ruthless about adding enough combinations and validating the essential parts. The snapshot diffing is helpful but the real gating is done by the pattern matching logic. Note: none of this works unless you are standing the main source directory Note: the test scripts make a lot of turds, at this point almost everything should be going into the outdirectory but it wasn't always so. You can use make clean to get rid of the build stuff wherever it may be. Alternatively use source control to get rid of any junk.","keywords":""},{"title":"Getting Started with CG/SQL","type":0,"sectionRef":"#","url":"/docs/getting-started","content":"","keywords":""},{"title":"Building​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#building","content":"caution Please make sure you meet the requirements. Set your current directory to the CG/SQL sources directory, wherever that may be, then: make clean make  This compiles CQL and puts the result at out/cql. Now you can run it to show available command options (also documented here): $ out/cql  You might want to alias the location of out/cql. For example, by using the alias command in Linux or MacOS. "},{"title":"Next Steps​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#next-steps","content":"Go to the first chapter of the CQL Guide to write your first CQL program! The second chapter has a less trivial program that walks through how to query a SQLite database with CQL.CGL Language CheatsheetCQL Playground "},{"title":"Requirements​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#requirements","content":""},{"title":"MacOS Users​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#macos-users","content":"The default bison and flex on Mac are quite old. You'll need to replace them. The Build produces an error if this is happening. You can get a more recent versions like this:  brew install bison brew link bison --force brew install flex brew link flex --force  "},{"title":"Linux Users​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#linux-users","content":"The default SQLite on Ubuntu systems is also fairly old. Some of the tests (particularly the query plan tests) use features not available in this version. You'll want to link against a newer sqlite to pass all the tests. From a bare Ubuntu installation, you might need to add these components: sudo apt install makegccflexbisonsqlite3libsqlite3-dev After which I was able to do the normal installations. For the coverage build you need gcovr And if you want to do the AST visualizations in PNG form you need graphviz "},{"title":"Options​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#options","content":"If you add CGSQL_GCC to your environment the Makefile will add CFLAGS += -std=c99to try to be more interoperable with gcc. If you add SQLITE_PATH to your environment the Makefile will try to compile sqlite3-all.c from that path and it will link that in instead of using -lsqlite3. "},{"title":"Amalgam Build​","type":1,"pageTitle":"Getting Started with CG/SQL","url":"/docs/getting-started#amalgam-build","content":"The amalgam is created by ./make_amalgam.sh and the result is in out/cql_amalgam.c You can create and test the amalgam in one step (preferred) using ./test.sh --use_amalgam  This will cause the amalgam to be created and compiled. Then the test suite will run against that binary. "},{"title":"Introduction","type":0,"sectionRef":"#","url":"/docs/introduction","content":"Introduction CG/SQL is a code generation system for the popular SQLite library that allows developers to write stored procedures in a variant of Transact-SQL (T-SQL) and compile them into C code that uses SQLite’s C API to do the coded operations. CG/SQL enables engineers to create highly complex stored procedures with very large queries, without the manual code checking that existing methods require. The full system also includes features for managing and upgrading schema, creating test code for stored procedures, getting query plans for procedures, as well as interfacing with stored procedures from other languages, such as Java and Objective-C. The JSON output allows for the creation of even more analysis or interfacing code. The package includes extensive documentation on the language and system. tip When we write &quot;CQL&quot; in these documents we're referring to the compiler proper, its flags, outputs and so forth. In order to avoid confusion with the many similarly named systems (there are many!) we refer to the overall project as CG/SQL. This comprises the docs, runtime, samples, everything. What it does: The CQL compiler does most of the heavy lifting. This compiler reads the schema and procedures, providing a strongly typed language with hundreds of compile-time errors designed to prevent runtime SQL issues. The compiler carefully tracks the data types of variables as well as schema types, and reports inconsistencies (such as trying to assign nullable columns to non-nullable output variables), and otherwise ensures that the SQLite APIs are used consistently and properly. The generated code always checks the various return codes and always uses the correct column ordinals and column types when binding or reading data to or from the SQLite system — areas that are notoriously difficult to get right and keep right. Additionally, annotations on schema allow the system to automatically create stored procedures that will upgrade a database from any previous schema version to the current one, with dozens of checks in place to make this possible. Annotations on procedures can also be used to indicate that you would like supporting test code to create schema fragments and insert data into that schema. This allows procedures to be unit tested with minimal fuss and no dependency on an as-deployed system. Likewise, these facilities can create schema to allow query plans to be checked at compile time. Why it matters: SQLite is widely used, but creating well tested and maintainable data access layers can be challenging at best. Many teams use some kind of code generation to avoid having to change dozens of ordinals every time a column is added, but these can be error prone. The CQL compiler in CG/SQL allows you to create highly complex stored procedures with very large queries and with a combination of syntax helpers and strong typing these procedures are much easier to get right and keep right. The combination of strong typing in the language plus facilities for good unit testing can provide confidence that even very complex logic is correct. Syntax helpers convert safer code into the canonical SQL, allowing engineers to write less code that is more correct — and still runs everywhere. For example: create procedure insert_a_row(like your_table) begin insert into your_table from arguments; end; Creates a procedure that inserts into any table (e.g., your_table) whose arguments are exactly the columns of the table. You cannot possibly forget any columns, nor can you put the potentially dozens of arguments in the wrong order. These forms are not only brief but also highly error-resistant, making it easier for engineers to generate code without having to check every bit manually. Original Source: FB Engineering blog","keywords":""},{"title":"CG/SQL Playground","type":0,"sectionRef":"#","url":"/docs/playground","content":"","keywords":""},{"title":"CQL Playground​","type":1,"pageTitle":"CG/SQL Playground","url":"/docs/playground#cql-playground","content":"While we do not offer an interactive REPL environment, we have bootstrapped an environment to run simple CQL programs in the repl folder of our repository. You can run it by doing this from the CQL repository: $ cd repl $ ./go.sh  By default, you'll get this output: ../out/cql ready CQL Mini App Thingy Hello from CQL. Edit as you please  go.sh runs the go() stored procedure defined in go.sql. You can experiment with the CQL language by editing the go.sql file, as you please. The contents of go.sh also offers a basic demonstration of how CQL should be typically used to transpile files into a C executable. "},{"title":"Query Plan Playground​","type":1,"pageTitle":"CG/SQL Playground","url":"/docs/playground#query-plan-playground","content":"Within the same repl directory, we have a script that demonstrates CQL's query plan generation feature with go.sql. Run this script in the /repl directory of the CQL repository: $ cd repl $ ./go_query_plan.sh  The script will generate the output of EXPLAIN QUERY PLAN of the SQL statements used in go.sql. [&quot;Query&quot;, &quot;Stat&quot;, &quot;Graph&quot;], [ &quot;INSERT INTO my_table(str) VALUES(\\&quot;Hello from CQL.\\&quot;), (\\&quot;Edit as you please.\\&quot;)&quot;, [ [], [{&quot;value&quot;: &quot;SCAN&quot;, &quot;style&quot;: {&quot;fontSize&quot;: 14, &quot;color&quot;: &quot;red&quot;, &quot;fontWeight&quot;: &quot;bold&quot;}}, {&quot;value&quot;: 1, &quot;style&quot;: {&quot;fontSize&quot;: 14, &quot;color&quot;: &quot;red&quot;, &quot;fontWeight&quot;: &quot;bold&quot;}}], [] ], &quot;\\n?\\n|...SCAN 2 CONSTANT ROWS&quot; ], [ &quot;SELECT *\\n FROM my_table&quot;, [ [], [{&quot;value&quot;: &quot;SCAN&quot;, &quot;style&quot;: {&quot;fontSize&quot;: 14, &quot;color&quot;: &quot;red&quot;, &quot;fontWeight&quot;: &quot;bold&quot;}}, {&quot;value&quot;: 1, &quot;style&quot;: {&quot;fontSize&quot;: 14, &quot;color&quot;: &quot;red&quot;, &quot;fontWeight&quot;: &quot;bold&quot;}}], [] ], &quot;\\n?\\n|...SCAN TABLE my_table&quot; ],  info You might notice the above output has a lot of extraneous stuff, like what seems to be CSS styling in JSON format. This is something that will be addressed in the future. In the meantime, you can use something like jq to filter stuff out. For example: $ ./go_query_plan.sh | jq '.[0][0][1:-1][] | {&quot;query&quot;: .[0], &quot;explain&quot;: .[2]}'  "},{"title":"Testing CG/SQL","type":0,"sectionRef":"#","url":"/docs/testing","content":"Testing CG/SQL Run this command in the /sources directory: ./test.sh This will build and run the test suite ./test.sh --use_amalgam Does the same thing but it tests the built amalgam rather than the normal build See details in our CQL Internals documentation.","keywords":""}]