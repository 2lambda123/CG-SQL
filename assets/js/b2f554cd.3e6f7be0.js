"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"schema-notes-2022","metadata":{"permalink":"/blog/schema-notes-2022","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-11-07-schema-notes-2022.md","source":"@site/blog/2022-11-07-schema-notes-2022.md","title":"Some updates on the CQL schema upgrade system","description":"Foreword","date":"2022-11-07T00:00:00.000Z","formattedDate":"November 7, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":16.07,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"schema-notes-2022","title":"Some updates on the CQL schema upgrade system","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"nextItem":{"title":"Introducing Parent/Child Result Sets","permalink":"/blog/parent-child"}},"content":"## Foreword\\n\\nI was tempted to subtitle this article \\"How a great idea went horribly, horribly, wrong\\" but in the final\\nanalysis the outcome isn\'t actually at all horrible.  But there are some good lessons here, and it\'s useful\\nto capture the history while it is still fresh.\\n\\n## Introduction and Context\\n\\nThe CQL compiler can produce for you, starting from a set of table declarations and schema annotations,\\na schema upgrader that can upgrade your schema from any previous version to the current version, provided\\nsome simple rules are followed.  Helpfully, the compiler enforces those rules with plain error messages so\\nthat you can reasonably expect your upgrader to work provided all is well with your database connection.\\n\\nBroadly, the entities of the schema are on one of two plans,  \\"create\\", and \\"recreate\\".  These notions are all discussed in more detail in [Chapter 10](https://cgsql.dev/cql-guide/ch10) of the guide.\\n\\n### The Create Plan\\n\\nThis plan applies strictly to tables, and is used for tables that have precious data that cannot reasonably be restored from say the cloud or some on-device backup.  Typically the primary data is on this plan.\\n\\nOn this plan you are limited to these operations:\\n\\n* new tables can be created (including the so called baseline tables, those having no annotation at all)\\n* columns can be added to the end of a table such that an `ALTER TABLE ADD COLUMN` statement could add them\\n* columns can be deleted, making them illegal to use in queries but otherwise having no physical consequence\\n  * in CQL \\"select * from foo\\" will not include deleted columns hence \\"*\\" is fully expanded\\n* tables can be deleted, leaving a tombstone in the schema\\n  * the tombstone provides the clue to the upgrader that the table should be dropped if it is found\\n\\nThe primary directives for this plan use `@create` annotations, hence the name.\\n\\n### The Recreate Plan\\n\\nTriggers, Indicies, and Views are all on this plan and tables can be too if they are annotated with `@recreate`\\ninstead of `@create`.  The idea with this plan is that if the entity changes at all you simply drop the old version\\nand create the new version.  This means any change is possible but it also means the upgrade is always destructive:\\n\\n* if the upgrader is going to do anything at all it drops all views and all triggers at the start and recreates them at the end\\n  * this not destructive and takes a lot of weird failure modes off the table\\n  * note steps in the upgrade logic therefore cannot rely on the existence of views or triggers\\n\\n* if any index or table changes at all it is dropped and recreated\\n  * this is done by computing a 64 bit CRC of the entities schema and comparing it to the stored CRC\\n  * if the CRC is changed the recreate happens\\n\\nProbably the first thing you noticed once you create the notion of recreate for tables is that you really\\nwant to do the recreation in groups.  There are constellations of schema that have related information and\\nif one of them changes they all need to be updated.  This lets you have complex foreign key relationships\\nwithin this \\"recreate group\\".\\n\\nYou\'ll also notice that a recreate group can have foreign keys within itself and it can make foreign keys\\nto things that are on the create plan but you run into trouble if you try to make foreign keys to some\\nother recreate group.  That group might vanish on you, or rather, it might try to vanish and discover that it\\ncannot because of constraint violations.  Originally recreate groups could not refer to other groups but\\nrecently this was generalized to track a directed acyclic graph of groups.  This means that a core group recreating\\nforces the recreation of any groups that refer to it.  On this plan its common to end up with a snowflake type\\nschema where the outer parts of the snowflake update often and the inner parts hardly at all.\\n\\n### Overall CRC\\n\\nIn addition to the CRCs for the recreate groups, and indices there was a one CRC for overall schema.  The\\nupgrader checks this before anything else. If the overall schema CRC matches the current schema then nothing\\nneeds to be done (the upgrader has already done its job).  If it doesn\'t match then some steps have to be applied.\\n\\n## Immutable Schema Versions\\n\\nOther than the cross-group dependencies things began in the form above.  The recreate plan was CRC driven and the\\ncreate plan was version driven.  The original design simply generated the appropriate corrections at each\\nschema version and tracked the current version.  If the overall CRC had changed, whichever steps you needed were executed.\\n\\nThis turned out to be a disaster and it was changed within days.\\n\\nThe idea seems fine enough, but the first thing you run into is that two people might make a change to the schema\\ncreating say version 5.  The problem is if one person adds table X and the other adds table Y they will each\\nrun their own and mark themselves as schema 5.  When they merge their changes with some other developer, the\\nversion 5 upgrade will have already run and they will get nothing despite the fact that v5 includes more for both of them.  This is crazytown for developers.\\n\\nSo, rather than simply tracking the current schema version, each schema version got its own mini-CRC.  The upgrader\\nwould run the steps of each version if the CRC was absent or didn\'t match.  With steps like `CREATE TABLE IF NOT EXISTS` and so forth a merge would result in you getting the other half of the changes for your version and work\\ncould accumulate at say schema v5 with no problems.  Actual customers would never see this because they only saw\\ncompleted schema versions.\\n\\nThis worked a lot better and lasted about a year.\\n\\nThe problem is that the system is based on these \\"mostly immutable\\" schema versions.  You never restate the past\\nyou always give instructions on how to move forward.  With the versions being nearly immutable, and the upgrade steps\\nbeing idempotent, things seemed good.  But it turns out neither of those two assumptions was really exactly true.\\n\\n## Mutating Schema Versions\\n\\nThe reality of the schema we created for our platform was that there was one large uber schema that had all the possible schema you might need for a variety of features and any give product could opt in to the features it wanted, thereby getting the necessary schema.  The schema system had a good way to partition the schema using [regions](https://cgsql.dev/cql-guide/ch10#schema-regions).  The upgrader could work on a set of regions and provide the union of schema in those regions, omitting the rest.\\n\\nSuper.  Here\'s where things get exciting.  A schema consumer could reasonably decide at some time in the future that it wants new features and so it opts into additonal regions.  That\'s fair enough, but the net of this is that of course new tables appear.  Some of these are likely to be in the baseline schema (v0) and some might have appear later (e.g. v5, v10, v21).  This is all fine, the CRCs for those versions change and the schema upgrader runs again. Those versions execute and add the correct schema.  Perfect.\\n\\nActually no.\\n\\n## Zombie Tables\\n\\nAbout two years into the history of CQL we started noticing that some attempts to delete tables were failing.  The DROP commands claimed that there was a constraint problem -- but these should have been leaf tables.  What constraint could possibly be the issue?  This was the first time a major design flaw with this system was revealed.  Previous bugs had been few and had all been silly logic errors or off by one checks in version numbers, that kind of thing, easily fixed.  This was a puzzler.  But the answer was fortunately available in the set of annotations.\\n\\nBasically, imagine a table \\"zombie\\" had been created say in the baseline schema, and then later deleted; suppose it was deleted in version 20.  All is well, the upgrade steps for version 20 include a table drop.  However, now a team subscribes to more schema, causing the v0 schema to include a few more tables.  Here\'s the problem, when the steps for v0 run again they notice that \\"zombie\\" is missing and helpfully create it, thinking this is the right thing to do.  But this is a disaster... The \\"zombie\\" table is supposed to be deleted in v20 but that CRC is unchanged!  So now a table exists that has no business existing.  If \\"zombie\\" has an FK reference to some other table which we wnat to delete, then all attempts to drop that table will fail because \\"zombie\\" is there gumming up the works.  Even if it\'s empty...  which it will be in this case.\\n\\nThis problem was fixed by having all tables that need deleting be unconditionally deleted at the end of the upgrade and not in the steps for the version in which the delete happened.  This meant that the next upgrade purged all the zombies and enabled the correct table drops to start running with no errors.  The consequence of this was a 90% reduction in schema upgrade failures!\\n\\n## Unsubscription\\n\\nAnother reason for the \\"immutable\\" version history to (cough) mutate was a desire to opt out of tables.  As described in [this section](https://cgsql.dev/cql-guide/ch10#unsubscription-and-resubscription-features)\\nwe created an affordance to allow people to unsubscribe from some of the tables they had previously selected.\\nThis provided finer-grain control of the schema subscription and also made it possible to undo previous mistakes of over-subscription.  However, it was clear from the beginning that you might want to undo an unsubscription at some time in the future.  In keeping with schema directives that create a clear history the `@unsub` and `@resub` statements were added to the language with lots of rules for correctness.  The upgrader did the following things:\\n\\n* upon finding an unsubscription at version X that version includes DDL to drop the unsubscribed table\\n* changes to that table in an future versions were omitted\\n* upon finding a resubscription at version Y that version included DDL to create the table as it exists at version Y\\n* later changes to that table are once again emitted as usual\\n\\nThis was very nearly right except it had the same problem as the delete case above.  A table created in say the\\nbaseline might come back as a zombie even though it was unsubscribed. However, now wise to this issue a small fix\\ntakes care of the problem.\\n\\n* always drop tables in the unsubscribed state at the end just like delete tables\\n* no code is needed to do an unsubscribe at version x (the drop at the end will do the job)\\n* a resubscribe at version X first drops the table and then recreates as it exists at version X\\n\\nThis gives us a consistent upgrade path again and importantly avoids the problem of a resubscription finding a zombie that prevents it from doing its job.\\n\\n## Performance Optimization 1\\n\\nOn [July 1, 2022](https://github.com/facebookincubator/CG-SQL/commit/a4e030c715add5d88106c7f4381eac5bfb098aba) we\\nmade a modest change that reduced the number of SQLite statements required to do a full upgrade.  The opportuntity\\ncame from the many column existence checks we made before running `ALTER TABLE ADD COLUMN`.    Rather than\\nrun a statement that looked like this `(SELECT EXISTS(SELECT * FROM sqlite_master WHERE tbl_name = table_name AND sql GLOB column_declaration_pattern))` for each column we first selected all of the table schema out of the `sqlite_master` table and put it into a hash table keyed by the table name.  Reading even a few hundred table names was much faster than running a single statement for each column that needed to be checked -- especially when recreating the schema from scratch.  In the most relevant test case this was a 7% improvement.\\n\\nImportantly, it motivated us to add hash tables into `cqlrt_common.c` and generalize the mechanism for object management so that the `cqlrt` allows creation of new objects without having to add special support for each one.\\n\\nThis new hash table meant that we could do a hash lookup and substring match instead of a sqlite query for each column.\\n\\n## Performance Optimization 2\\n\\nOn [Oct 11, 2022](https://github.com/facebookincubator/CG-SQL/commit/fe921eac81c4000b690c4c4a8cec54dbbd56109a) we stopped using CRCs for the version checks on the create plan entirely.  This was in fact an optimization but it was motivated by a real, but rare, problem.\\n\\nWhat was happening was something like maybe 1 in 10^5 databases was missing columns.  The sequence of events that caused this was very hard to diagnose but the situation was very clear.  The database was at say schema version 100.  The columns had been added at say version 50.  The CRCs indicated that the v50 upgrade had already run so it didn\'t run again.  The columns would now never be added.\\n\\nWe had yet to come up with a set of steps that would adequately describe how this happened. I have to guess some combination of a resubscription ran because of one of those \\"the schema is not really immutable\\" changes and then \\"medium\\" version of the table say v25 was resubscribed but the columns added in say v50 never got readded because v50 thought it had already run.\\n\\nThis was getting to be a nightmare but there was a simple solution.\\n\\nWe already had created this dictionary that had all the tables and their schema from sqlite master, we were already using it to determine if we needed to add a particular column.  The only reason we had version CRCs at all was to allow us to skip steps, but since we could already skip column adds super fast all we needed was to be able to skip table adds -- there is nothing else.  Well the same hash table can obviously easily tell us if a table exists.  Non-existent tables have no schema and hence are absent from the hash table which is loaded directly from `sqlite_master`.\\n\\nSo the new algorithm, goes something like this:\\n\\n* use the version numbers only for ordering\\n* before adding a table, check if it exists in the table, this is faster htran running `CREATE TABLE IF NOT EXISTS`\\n* check the columns as before\\n* attempt each of these every time the overall schema changes, and trust that the fast checks are fast enough\\n\\nOn this plan we change the way `@unsub` and `@resub` are handled to something much simpler:\\n\\n* `@unsub` acts like an out of band `@delete` on the table or view to which it is applied\\n  * the drop happens at the end like before\\n* `@resub` resets that state so the table is considered not deleted if the last operation was `@resub`\\n\\nTo this we add one new rule:\\n  * the schema upgrader removes any reference to deleted tables entirely\\n    * they are removed from baseline\\n    * they are not included in any upgrade rules\\n    * they are only dropped at the end if they still exist\\n\\nThis vastly simplifies unsub/resub and delete.  An unsubscribed table will always get cleaned up at the end, just like deleted tables.  No strange interim states happen in resub.  If a table is resubcribed it just reappears in the schema and the various operations run as usual.\\n\\nThe only compromise to this is that we still have a single CRC for the overall baseline schema.  However even that could be removed at the expense of more hash table lookups.  There is a binary size win for fewer checks and since baseline by definition depends on nothing it seems like safe position to take.\\n\\nThis approach was about 13-15% faster in fact, the time saved examining and writing back schema CRCs more than paid for the extra hash table checks (which were ~100x faster than the db operations).  And the hash table already existed! The reduction of the CRC checks and removal of vestigial upgrade logic for deleted tables also resulted in a 2.2% reduction of upgrader size for our most important case.\\n\\n## The most recent change and further simplications in unsub/resub logic\\n\\nWith all of this in place it\'s clear that the various rules for unsubscription and resubscription and the\\nsort of historical playback that was used to try to create these immutable stages is moot.  The only thing\\nthat matters is if we end in the unsubscribed state.  Removing the unsubscribe upgrade steps from the upgrader\\nentirely just simplifies everything.  So no `@resub` is needed at all nor are `@unsub` version numbers.  Presently\\nset to land is set of changes that remove resubcription entirely, to resubscribe you simply remove the `@unsub`\\ndirective for your table/view.\\n\\nThis lets us eliminate a bunch of validations and test cases to get a simpler, clearer, and more easily verifiable\\nupgrader.  There\'s just much less to go wrong.\\n\\nEven crazy cases like \\"an unsubscription happens in v5, the resubscription happens in v7, a user runs the upgrade\\nand they might have a database that is v4, v5, v6, or v7 (having already been upgraded)\\".  All of these had\\ndifferent potential flows before.  Now they are all the same.  All the cases will roll the table forward to v7\\nfrom whatever version they might be on with the usual rules and states particular to unsubscription or resubscription.  The table is present or it isn\'t.  It is missing columns or it isn\'t.  Same as always.\\n\\n## A Versionless Future\\n\\nMore thinking is needed here but it\'s clear that now that we\'ve arrived at this simpler place ALL the version\\nnumbers are moot.  The only thing we really have to do with version numbers is run ad hoc migrations at\\nthe appropriate time, and only once.  The rules for migrators would have to change such that they are responsible\\nfor finding the state of the schema, and maybe some context could be provided for this.  But ad hoc data migrators\\nare very uncommon and regular annotations are much more so.\\n\\n## Conclusion\\n\\nThe fundamental assumption about how schema changes would happen was wrong.  Even so, it was close enough\\nthat upgrades were over 99.99% successful when the other parts of the system are working ok.  This is probably\\nabout the best we can hope for given the state of affairs with flash drives on Android devices.  The current\\nsystem is actually pretty close code-wise to what we had planned -- just look at the [Oct 11, 2022](https://github.com/facebookincubator/CG-SQL/commit/fe921eac81c4000b690c4c4a8cec54dbbd56109a) diff to see what I mean.  It\'s not that\\nbig of a difference in the end.  The new system has been deployed for nearly a month now and it is immune basically\\nall of the failure modes of the old.  It will take some time before we know what its true reliability is given\\nthe low failure rate of both.  But we do know the new system is significantly faster.  Optimizations 1 and 2 together are over 20% for full installations.\\n\\nI should note that someone who was obviously smarter than me told me that we would land on a solution like\\nthis and I didn\'t believe them.  They were of course correct.  You know who you are.  Sorry I doubted you."},{"id":"parent-child","metadata":{"permalink":"/blog/parent-child","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-10-06-parent-child.md","source":"@site/blog/2022-10-06-parent-child.md","title":"Introducing Parent/Child Result Sets","description":"Introduction and Context","date":"2022-10-06T00:00:00.000Z","formattedDate":"October 6, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":14.64,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"parent-child","title":"Introducing Parent/Child Result Sets","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Some updates on the CQL schema upgrade system","permalink":"/blog/schema-notes-2022"},"nextItem":{"title":"Introducing Backed Tables","permalink":"/blog/backed-tables"}},"content":"## Introduction and Context\\n\\nThere are many cases where you might want to nest one result set inside of another one.  In order to\\ndo this ecomomically there was a great desire to be able to run a parent query and a child query and\\nthen link the child rows to the parent rows.  One way to do this is of course to run one query for\\neach \\"child\\" but then you end up with `O(n)` child queries and if there are sub-children it would be\\n`O(n*m)` and so forth. What you really want to do here is something more like a join, only without\\nthe cross-product part of the join.  Many systems have such features, sometimes they are called\\n\\"chaptered rowsets\\" but in any case there is a general need for such a thing.\\n\\n\\nWe did a bunch of work in the name of Parent/Child results sets but like many goals of this kind it\\ncaused us to ripen the CQL language in a variety of ways and its interesting to talk about those\\nchanges.  Importantly, we wanted to be able to do work of this kind in the language while adding\\nthe fewest new notions and basically enabling the language to express a concept like a child rowset\\nin the first place.\\n\\nHere are some things that happened along the way that are interesting.\\n\\n## Cursor Types and Result Types\\nOne of the first problems we run into thinking about how a CQL program might express pieces of a rowset\\nand turn them into child results is that you need to be able to hash a row, append row data, and\\nextract a result set from a key.\\n\\nLet\'s think about that for just a second: in order to do anything at all with a child rowset,\\nno matter how we got such a thing, we have to be able to describe it in a type-safe way.\\nThese objects already exist at runtime but they do not appear anywhere in the language explicitly\\nand that was going to have to change.\\n\\nTo address this we added a new object type, kind of like we did with boxed statements.  A result set\\nhas a type that looks like this `object <proc_name set>`.  Here `proc_name` must the the name of a\\nprocedure that returns a result set and the object will represent a result set with the\\ncorresponding columns in it.\\n\\nThat step may seem like it\'s super important but actually it\'s kind of optional, it provides type-safety\\nbut the initial versions of the feature just used the type `object` which works fine provided you make\\nno mistakes... it turns out there are even more fundamental needs that aren\'t optional.\\n\\n## Creating New Cursor Types From Existing Cursor Types\\n\\nThe first thing you need to be able to to is take the type of the parent query and add to it one\\nmore columns to whole the child result set or sets (note that you can have more than one child\\nresult set per parent).  So for instance you might have a list of people, and one child result might\\nbe the names of the schools they attended and another is the names of the jobs they worked.\\n\\nSo while adding columns to existing rows might sound like a bizarre thing to do but actually it\'s\\nactually fundamental to the job here.  We must be able to create a new output row is that is the\\nsames as the parent but includes columns for the the child results too.  There was no good syntax for this.\\nThe cursor declaration forms were:\\n\\n```\\n/* option 1 */ declare C cursor like shape_name;\\n/* option 2 */ declare C cursor like select 1 x, \\"2\\" y, false z;\\n```\\n\\nThe first option implies that you already have a shape from (e.g.) a procedure or table and you want\\nto make an identical cursor.  That doesn\'t work here because we\'re trying to modify an existing shape,\\nnot use it as is.\\n\\nThe second form was supposed to be able to create any kind of cursor shape by simply declaring a `select`\\nstatement that is an example of what you want to capture.  In principle this can define almost anything.\\nHowever, there\'s a catch -- you can\'t get object types to come out of a `select` so it\'s hopeless for result set types.\\nAnd, maybe just as important, you can\'t just add a few columns to an existing type with any kind of ease,\\nyou have to list all columns.\\n\\nFortunately there was a pretty simple solution to this problem.  There were already lots of cases where\\na typed name list happens in the language -- for example in the return type of a function you can\\nspecify something like `(id integer, name text)`.  That construction also defines a shape just like a\\nselect statement and there was already code to handle all the correctness analysis.  Additionally,\\nthe `LIKE` construct can be used in such a list to refer to existing types.  So for instance a function that\\nreturns all the columns of tables A and B could be defined like so\\n\\n```\\ndeclare function foo() (LIKE A, LIKE B);\\n```\\n\\nSo we could solve all the cursor type problems by allowing a typed name list to be used to define a cursor shape.\\nProbably the approach that should have been taken in the first place. The select option seems weird by comparison.\\n\\nWith the already existing support for shapes in a type list we could make the result shape for this parent/child case\\nwith ease, like so:\\n\\n```\\ndeclare result cursor like (like parent, child_result object<child_proc set>);\\n```\\n\\nSo, all the parent columns plus a child result set.  Or more than one child result set if needed.\\n\\nLastly there were going to be cases where we needed to make a new cursor using only some of the field of an existing cursor.\\nThe case in particular I\'m thinking of is that we might have a big row from the parent and it might\\nhave only one or two columns that we need that form the key columns for the child.  We didn\'t have a good way to do that\\neither, but solving this turns out to be simple enough.  We already had this form:\\n\\n```\\ndeclare D cursor like C;\\n```\\n\\nwe just added:\\n\\n```\\ndeclare D cursor like C(a, b, c);\\n```\\n\\nWhich chooses just the 3 named fields from `C` and makes a cursor with only those.  Recently we added\\nthe form:\\n\\n```\\ndeclare D cursor like C(-x);\\n```\\n\\nTo mean take all the columns of `C` except `x`\\n\\nWith the a shape for the key fields defined, we can use existing syntax to load the fields\\neconomically:\\n\\n\\n```\\nfetch D from C(like D);\\n```\\n\\nWhich says we want to load `D` from the fields of `C`, but using only the columns of `D`.  That operation\\nis of course going to be an exact type match by construction.  So now we could describe the key columns from\\nchild rows, and the key columns from parent rows.  And we could add columns to the parent type to create space\\nto hold child result sets.  All of our type problems are solved.  Almost.\\n\\n### Cursor Arguments\\n\\nIt was clear that we would need to be able to do things like \\"hash a cursor\\" (any cursor) or \\"store this row\\ninto the appropriate partition\\" and this requirement meant that we had to be able to write functions that\\ncould take any cursor and dynamically do things to it based on its type information.  There is no good way\\nto write these generic helper things in CQL, but:\\n\\n* we don\'t need very many of them,\\n* it\'s pretty easy to do that job in C\\n\\nThe main thing we need is to create a way to declare such functions and call them a with cursor and the necessary shape info.\\n\\nSo we added this notion of being able to call an external function with any cursor.  Like so:\\n\\n```\\ndeclare function cursor_hash(C cursor) long not null;\\n```\\n\\nyou can call it like so:\\n\\n```\\nlet hash := cursor_hash(C);\\n```\\n\\nwhere `C` is any cursor.\\n\\nWhen such a call is made the C function `cursor_hash` gets passed what we call a \\"dynamic cursor\\".\\nThis includes:\\n* a pointer to the data for the cursor\\n* the count of fields\\n* the names of the fields\\n* the type/offset of every field in the cursor\\n\\nSo you can (e.g.) generically do the hash by applying a hash to each field and then combining all of those.\\nThis kind of function works on any cursor and all the extra data about the shape that\'s needed to make the\\ncall is static, so really the cost of the call stays modest.  Details of the dynamic cursor type are in\\n`cqlrt_common.h` and there are many example functions now in the `cqlrt_common.c` file.\\n\\nAgain, creating this facility was a pretty minor matter, the compiler already has all this data and uses it\\nto create result sets in the first place.  We just allowed other functions to use that same data and\\nmade a public type for it.\\n\\n## The Specific Parent/Child Functions\\nTo do the parent/child operations we needed three helper functions:\\n\\n```\\nDECLARE FUNC cql_partition_create ()\\n   CREATE OBJECT<partitioning> NOT NULL;\\n\\nDECLARE FUNC cql_partition_cursor (\\n  part OBJECT<partitioning> NOT NULL,\\n  key CURSOR,\\n  value CURSOR)\\n    BOOL NOT NULL;\\n\\nDECLARE FUNC cql_extract_partition (\\n  part OBJECT<partitioning> NOT NULL,\\n  key CURSOR)\\n    CREATE OBJECT NOT NULL;\\n```\\n\\nThe first function makes a new partitioning.\\n\\nThe second function hashes the key columns of a cursor (specified by the key argument) and appends\\nthe values provided into a bucket for that key.  By making a pass over the child rows you can easily\\ncreate a partitioning with each unique key combo having a buffer of all the matching rows.\\n\\nThe third function is used once the partitioning is done.  Given a key again, which you now presumably\\nget from the parent rows, you get the buffer you had accumulated and then make a result set out of it\\nand return that.  Note that this function returns the vanilla object type because it could be returning\\nany shape.\\n\\n## Result Set Sugar\\nWith the type system mentioned above you could now join together any kind of complex parent and\\nchild combo you needed, but it might be a lot of code, and it\'s error prone.  This is a good job\\nfor a little sugar.  So we added some simple syntax to specify the usual partitioning.\\n\\nIt looks like this:\\n\\n```\\n-- parent and child defined elsewhere\\ndeclare proc parent(x integer not null) (id integer not null, a integer, b integer);\\ndeclare proc child(y integer not null) (id integer not null, u text, v text);\\n\\n-- join together parent and child using \'id\'\\ncreate proc parent_child(x_ integer not null, y_ integer not null)\\nbegin\\n  out union call parent(x_) join call child(y_) using (id);\\nend;\\n```\\n\\nThe generated code is simple enough, even though there\'s a good bit of it.\\nBut it\'s a useful exercise to look at it once.  Comments added for clarity.\\n\\n```\\nCREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL)\\nBEGIN\\n  DECLARE __result__0 BOOL NOT NULL;\\n\\n  -- we need a cursor to hold just the key of the child row\\n  DECLARE __key__0 CURSOR LIKE child(id);\\n\\n  -- we need our partitioning object (there could be more than one per function\\n  -- so it gets a number, likewise everything else gets a number\\n  LET __partition__0 := cql_partition_create();\\n\\n  -- we invoke the child and then iterate its rows\\n  DECLARE __child_cursor__0 CURSOR FOR CALL child(y_);\\n  LOOP FETCH __child_cursor__0\\n  BEGIN\\n    -- we extract just the key fields (id in this case)\\n    FETCH __key__0(id) FROM VALUES(__child_cursor__0.id);\\n\\n    -- we add this child to the partition using its key\\n    SET __result__0 := cql_partition_cursor(__partition__0, __key__0, __child_cursor__0);\\n  END;\\n\\n  -- we need a shape for our result, it is the columns of the parent plus the child rowset\\n  DECLARE __out_cursor__0 CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER,\\n                                       child1 OBJECT<child SET> NOT NULL);\\n\\n  -- now we call the parent and iterate it\\n  DECLARE __parent__0 CURSOR FOR CALL parent(x_);\\n  LOOP FETCH __parent__0\\n  BEGIN\\n    -- we load the key values out of the parent this time, same key fields\\n    FETCH __key__0(id) FROM VALUES(__parent__0.id);\\n\\n    -- now we create a result row using the parent columns and the child result set\\n    FETCH __out_cursor__0(id, a, b, child1) FROM VALUES(__parent__0.id, __parent__0.a, __parent__0.b, cql_extract_partition(__partition__0, __key__0));\\n\\n    -- and then we emit that row\\n    OUT UNION __out_cursor__0;\\n  END;\\nEND;\\n```\\n\\nThis code iterates the child once and the parent once and only has two database calls,\\none for the child and one for the parent.  And this is enough to create parent/child result\\nsets for the most common examples.\\n\\n## Result Set Values\\nWhile the above is probably the most common case, another case can happen where you might\\nwant to make a procedure call for each parent row to compute the child.  And, more generally,\\nthere was no good way to work with result sets from procedure calls other than iterating them\\nwith a cursor.  The iteration pattern is very good if the data is coming from a select statement\\n-- we don\'t want to materialize all of the results if we can stream instead.  However, when working\\nwith result sets the whole point is to create materialized results for use elsewhere.\\nWe now had the power to express a result set type with `object<proc_name set>` but no way to\\nactually get such a set from an existing procedure.  Procedures generated them,\\nbut they could only be consumed in the C layer.\\n\\nFortunately this is also an easy problem to solve.  We already supported the ability to use\\nprocedures as functions in expressions if they had the right signature.  We now add the ability\\nto call a procedure that returns a result set and capture that result.\\nPreviously this was not supported and would have produced an error.\\n\\nWith the new features you can write:\\n\\n```\\ndeclare child_result object<child set>;\\nset child_result := child(args);\\n```\\n\\nor better still:\\n\\n```\\nlet child_result := child(args);\\n```\\n\\nWith this simple change we had the power to write something like this:\\n\\n\\n```\\ndeclare proc parent(x integer not null) (id integer not null, a integer, b integer);\\ndeclare proc child(id integer not null) (id integer not null, u text, v text);\\n\\ncreate proc parent_child(x_ integer not null, y_ integer not null)\\nbegin\\n  -- the result is like the parent with an extra column for the child\\n  declare result cursor like (like parent, child object<child set>);\\n\\n  -- call the parent and loop over the results\\n  declare P cursor for call parent(x_);\\n  loop fetch P\\n  begin\\n     -- compute the child for each P and then emit it\\n     fetch result from values(from P, child(P.id));\\n     out union result;\\n  end;\\nend;\\n```\\n\\nAfter the sugar is applied this compiles down to this program:\\n\\n```\\nDECLARE PROC parent (x INTEGER NOT NULL) (id INTEGER NOT NULL, a INTEGER, b INTEGER);\\nDECLARE PROC child (id INTEGER NOT NULL) (id INTEGER NOT NULL, u TEXT, v TEXT);\\n\\nCREATE PROC parent_child (x_ INTEGER NOT NULL, y_ INTEGER NOT NULL)\\nBEGIN\\n  DECLARE result CURSOR LIKE (id INTEGER NOT NULL, a INTEGER, b INTEGER,\\n                              child OBJECT<child SET>);\\n\\n  DECLARE P CURSOR FOR CALL parent(x_);\\n  LOOP FETCH P\\n  BEGIN\\n    FETCH result(id, a, b, child) FROM VALUES(P.id, P.a, P.b, child(P.id));\\n    OUT UNION result;\\n  END;\\nEND;\\n```\\n\\nThe `LIKE` and `FROM` forms are very powerful but they aren\'t new.  They do make\\nit a lot easier to express this notion of just adding one more column to the result.\\nNote that the code for emitting the `parent_child` result before the transformation\\ndoesn\'t need to specify what the columns of the parent are or the columns of the child,\\nonly that the parent has at least the `id` column.  Even that could have been removed.\\n\\nThis call could have been used instead:\\n\\n```\\nfetch result from values(from P, child(from P like child arguments));\\n```\\n\\nThat syntax would result in using the columns of P that match the arguments of `child` -- just\\n`P.id` in this case.  But if there were 7 such columns the sugar might be easier to understand.\\n\\n\\n## Additional Language Support\\n\\nLast, but not least, to make this more accessible we wanted more support in the generated code.\\nThe C interface would have produced generic object results for the child result columns.\\nThis isn\'t wrong exactly but it would mean that a cast would be required in every use case on the\\nnative side, and it\'s easy to get the cast wrong.  So the result type of column getters was\\nadjusted to be a `child_result_set_ref` instead of just `cql_object_ref`.\\n\\nSimilar transforms were needed if column setters were being emitted (yes that\'s an option!)\\nand of course the Java and Objective C output needed the same transform.\\n\\n## Conclusion\\n\\nThe prosecution of native support for parent/child result sets in CQL resulted in a bunch of\\nvery useful generalizations for declaring and managing cursors.  The old special case code for\\nblobs was actually replaced by these forms.  The language overall expressiveness increased far\\nmore than just the ability to do this one kind of join.  It\'s now possible to write general\\npurpose debug helpers for cursors.  It\'s possible to store and return pre-cooked result sets,\\ncreating useful caches and other such combinations.  The type extensions to allow extending\\nand narrowing existing types allow even more return flexibility while keeping everything\\nstrongly typed.\\n\\nParent/Child result sets exploit all of these things."},{"id":"backed-tables","metadata":{"permalink":"/blog/backed-tables","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-10-05-backed-tables.md","source":"@site/blog/2022-10-05-backed-tables.md","title":"Introducing Backed Tables","description":"Introduction and Context","date":"2022-10-05T00:00:00.000Z","formattedDate":"October 5, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":17.125,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"backed-tables","title":"Introducing Backed Tables","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Parent/Child Result Sets","permalink":"/blog/parent-child"},"nextItem":{"title":"Introducing Blob Storage","permalink":"/blog/blob-storage"}},"content":"## Introduction and Context\\n\\nMost production databases include some tables that are fairly generic, they use maybe a simple key-value combination to store some simple\\nsettings or something like that.  In the course of feature development this kind of thing comes up pretty often and in large client\\napplications (like Messenger, but certainly not limited to Messenger) there are many small features that need a little bit of state.\\nIt\'s easy enough to model whatever state you need with a table or two but this soon results in an explosion of tiny tables.  In some cases\\nthere are only a few rows of configuration data and indeed the situation can be so bad that the text of the schema for the little state table is\\nlarger than the sum of all the data you will ever store there.  This is a bit tragic because SQLite has initialization cost associated with\\neach table.  So these baby tables are really not paying for themselves at all.  What we\'d like to do is use some kind of generic table\\nas the backing store for many of these small tables while preserving type safety.  The cost of access might be a bit higher but since\\ndata volumes are expected to be low anyway this would be a good trade-off.  And we can have as many as we like.  In some cases the state doesn\'t\\neven need to be persisted, so we\'re talking about tables in an in-memory database.  Here low cost of initialization is especially important.\\nAnd lastly, if your product has dozens or even hundreds of small features like this, the likelihood that all of them are even used in a session\\nis quite low and so again, having a low fixed cost for the schema is a good thing.  No need to create 100 in-memory tables on the off chance that\\nthey are needed.\\n\\nSee also the related feature: [blob storage](https://cgsql.dev/blog/blob-storage).\\n\\n### How do I define one of these backed tables?\\n\\nFirst you need a place to store the data, we define a backing table in the usual way.  A simple backing table is just a key/value store and looks\\nlike this:\\n\\n```sql\\n@ATTRIBUTE(cql:backing_table)\\nCREATE TABLE backing(\\n  k BLOB PRIMARY KEY,\\n  v BLOB NOT NULL\\n);\\n```\\n\\nThe `backing_table` attribute indicates that the table we\'re about to define is to be used for backing storage.  At present it is signficantly restricted.\\nIt has to have exactly two columns, both of which are blobs, one is the key and one is the value.  It should be either baseline schema or annotated with\\n`@create` as it is expected to be precious data.  If it\'s an in-memory table the versioning is somewhat moot but really the backing store is not supposed\\nto change over time, that\'s the point.   In future versions we expect to allow some number of additional physical columns which can be used by the backed\\ntables (discussed below) but for now it\'s this simple pattern.\\n\\nBacked table looks like this:\\n\\n```sql\\n@ATTRIBUTE(cql:backed_by=backing)\\nCREATE TABLE backed(\\n  id INTEGER PRIMARY KEY,\\n  name TEXT NOT NULL,\\n  bias REAL\\n);\\n\\n@ATTRIBUTE(cql:backed_by=backing)\\nCREATE TABLE backed2(\\n  id INTEGER PRIMARY KEY,\\n  name TEXT NOT NULL\\n);\\n```\\n\\n\\nThe `backed_by` attribute indicates that the table we\'re about to define is not really going to be its own table.  As a result, you will not be\\nable to (e.g.) `DROP` the table or `CREATE INDEX`  or `CREATE TRIGGER` on it, and there will be no schema upgrade for it should you request one.\\nIt may not contain constraints as there would be no way to enforce them.  But as compensation for these restrictions it can be changed freely and\\nhas no physical schema cost associated with it.\\n\\n### How do I read this data?\\n\\nTo understand how this works imagine that we had a view for each backed table which simply read the blobs out of the backing store and then extracted the backed columns using some blob extraction functions. This would work, but then we\'d be trading view schema for table schema so the schema savings we\'re trying to achieve would go up in smoke.\\n\\nWe might be lost here but CQL already has something very \\"view like\\" and that\'s the shared fragment structure.  So what we do instead of views is to automatically create a shared fragment just like the view we could have made.  They look like this:\\n\\n```sql\\n@ATTRIBUTE(cql:shared_fragment)\\nCREATE PROC _backed ()\\nBEGIN\\n  SELECT\\n   rowid,\\n   cql_blob_get(T.k, backed.id) AS id,\\n   cql_blob_get(T.v, backed.name) AS name,\\n   cql_blob_get(T.v, backed.bias) AS bias\\n    FROM backing AS T\\n    WHERE cql_blob_get_type(T.k) = 2105552408096159860L;\\nEND;\\n```\\n\\nSo some things to notice right away:\\n\\nFirst, this fragment has the right shape, but the shared fragment doesn\'t directly call blob extractors.  Rather it uses these `cql_blob_get`.\\nThe point of this is to make the actual blob functions configurable.  The test suites include some very simple extraction functions for blobs\\nwith just integers in them, but you can create whatever blob format you want. You could use the [blob storage](https://cgsql.dev/blog/blob-storage)\\nfeature for encoding or you can encode it as you see fit.  You can even have different encodings in different backed tables.\\n\\nSecond, there is a type code embedded in the procedure.  The type code is a hash of the type name and the names and types of all the not-null fields\\nin the backed table.  The hash is arbitrary but repeatable, any system can compute the same hash and find the records they want without having to share\\nheaders. The actual hash is open source but it\'s just a SHA256 reduced to 64 bits with some name canonicalization.  Shortly the JSON will also include\\nthe relevant hashes so you can easily consume them without even having to know the hash function.\\n\\nHere\'s the slightly smaller shared fragment for `backed2`\\n```sql\\n@ATTRIBUTE(cql:shared_fragment)\\nCREATE PROC _backed2 ()\\nBEGIN\\n  SELECT\\n    rowid,\\n    cql_blob_get(T.k, backed2.id) AS id,\\n    cql_blob_get(T.v, backed2.name) AS name\\n    FROM backing AS T\\n    WHERE cql_blob_get_type(T.k) = -1844763880292276559L;\\nEND;\\n```\\nAs you can see it\'s very similar -- the type hash is different and of course it has different columns.\\n\\n### Why does the type hash include only the non-null fields?\\n\\nThe idea is that the backed table might change over time and you can add new optional fields without invalidating your existing data.  If you change\\nthe name of the type or if you add new not null fields the type identity changes and any data you have in the backing table will basically be\\nignored because the type hash will not match.\\n\\n### What do `cql_blob_get` and `cql_blob_get_type` turn into?\\n\\nYou can configure them as you see fit.  By default cql_blob_get turns into either `bgetkey` or `bgetval` depending on if you are\\nreading from the key blob or the value blob.  The directives for configuring this function are:\\n\\n```sql\\n@blob_get_key bgetkey offset;\\n@blob_get_val bgetval;\\n```\\n\\nYou can configure the system to ask for the column by offset (this is normal for the primary key because it has a fixed number of columns\\nfor any given key type and they are all mandatory), or by hash code (this is normal for the value type because it might be missing some\\ncolumns and so offset is probably not appropriate).  However both are configurable so you want to do key by hashcode simply omit the \\"offset\\"\\npart of the directive.  Likewise if your values are offset addressable you can add \\"offset\\" to the value directive.  Here the offset means\\nthe zero based ordinal of the column in the key or the value.\\n\\nThe type access functions are similarly configurable (they never need a code or an offset).\\n\\n```sql\\n@blob_get_key_type bgetkey_type;\\n@blob_get_val_type bgetval_type;\\n```\\n\\n### What does this end up looking like?\\n\\nArmed with these basic transforms we can already do a simple transform to make select statement work.  Suppose CQL sees:\\n\\n```sql\\ndeclare C cursor for select * from backed;\\n```\\n\\nWe can make this work with a simple transform:\\n\\n```sql\\n DECLARE C CURSOR FOR WITH\\n  backed (*) AS (CALL _backed())\\n  SELECT *\\n    FROM backed;\\n```\\n\\nNow remember  `_backed` was the automatically created shared fragment.  Basically, if we see a select statement that mentions any backed table\\nwe simply add a call to the corresponding shared fragment in the WITH clause.  This effectively creates that \\"view\\" we need.  And because we\'re\\nusing the shared fragment form, all users of this fragment will share the text.  So there\'s no schema and the text of the backed appears only\\nonce in the binary.  More precisely we get this:\\n\\n```sql\\nWITH\\nbacked (rowid, id, name, bias) AS (\\n  SELECT\\n    rowid,\\n    bgetkey(T.k, 0),                      -- 0 is offset of backed.id in key blob\\n    bgetval(T.v, -6639502068221071091L),  -- note hash of backed.name\\n    bgetval(T.v, -3826945563932272602L)   -- note hash of backed.bias\\n  FROM backing AS T\\n  WHERE bgetkey_type(T.k) = 2105552408096159860L)\\nSELECT rowid, id, name, bias\\n  FROM backed;\\n```\\n\\nNow with this in mind we can see that it would be very beneficial to also add this:\\n\\n```sql\\nCREATE INDEX backing_index ON backing(bgetkey_type(k));\\n```\\n\\nor more cleanly:\\n\\n```sql\\nCREATE INDEX backing_index ON backing(cql_blob_get_type(k));\\n```\\n\\nEither of these result in a computed index on the row type stored in the blob.  Other physical indices might be helpful too and these can potentially\\nbe shared by many backed tables, or used in partial indicies.\\n\\nOf course your type function might be named something other than the default `bgetkey_type`.\\n\\nNow consider a slightly more complex example:\\n\\nA slightly more complex example:\\n```\\nselect T1.* from backed T1 join backed2 T2 where T1.id = T2.id;\\n```\\n\\nbecomes:\\n\\n```\\nWITH\\n  backed (rowid, id, name, bias) AS (CALL _backed()),\\n  backed2 (rowid, id, name) AS (CALL _backed2())\\n  SELECT T1.*\\n    FROM backed AS T1\\n    INNER JOIN backed2 AS T2\\n    WHERE T1.id = T2.id;\\n```\\nNow even though two different backed tables will be using the backing store the select \\"just works\\".  All the compiler had to do was add both backed\\ntable fragments.  And of course if `backed` was joined against itself, that would also just work.\\n\\n### How do I insert data like this?\\n\\nConsider:\\n```sql\\ninsert into backed values (1, \\"n001\\", 1.2), (2, \\"n002\\", 3.7);\\n```\\n\\nThis has to insert into the backing storage and convert the various values into key and value blobs.  A simple transform does this job as well:\\n\\n```sql\\n WITH\\n  _vals (id, name, bias) AS (\\n    VALUES(1, \\"n001\\", 1.2), (2, \\"n002\\", 3.7)\\n  )\\n  INSERT INTO backing(k, v) SELECT\\n    cql_blob_create(backed, V.id, backed.id),\\n    cql_blob_create(backed,\\n      V.name, backed.name,\\n      V.bias, backed.bias)\\n    FROM _vals AS V;\\n```\\n\\nWhat\'s going on here? Well, the issue is that the data to be inserted can be arbitrarily complicated.  It might refer to all kinds of things.\\nIn this case it\'s just literal values but in general it could be anything.  So the transform takes the original values and puts them in a\\n _vals(...) CTE.  Then we insert into the backing store by converting _vals into blobs -- one for the key and one for the value.\\n There is only the one place we need to do this for any given insert statement no matter now many items or how complex the insertion is.\\n\\n`cql_blob_create` similarly expands to a user configured function with optional hash codes and mandatory field types.  There is default\\nconfiguration that corresponds to this:\\n\\n```sql\\n@blob_create_key bcreatekey offset;\\n@blob_create_val bcreateval;\\n```\\n\\n\\nThe final SQL looks like this:\\n\\n```sql\\nWITH\\n_vals (id, name, bias) AS (\\n  VALUES(1, \\"n001\\", 1.2), (2, \\"n002\\", 3.7)\\n)\\nINSERT INTO backing(k, v) SELECT\\n  bcreatekey(2105552408096159860, V.id, 1), -- type 1 is integer, offset implied\\n  bcreateval(2105552408096159860,\\n    -6639502068221071091, V.name, 4,  -- hash as before, type 4 is text,\\n    -3826945563932272602, V.bias, 3)  -- hash as before, type 3 is real,\\n  FROM _vals AS V\\n```\\n\\nNote that both blobs have the same overall type code (2105552408096159860) as before.  The key blob did not use per-field type codes, so the argument\\npositions give the implied offset.  In contrast the value blob is using hash codes (offset was not specified).  This configuration is typical.\\n\\nA more complex insert works just as well:\\n```sql\\ninsert into backed\\n  select id+10, name||\'x\', bias+3 from backed where id < 3;\\n```\\n\\nThe above insert statement is a bit of a mess.  It\'s taking some of the backed data and using it to create new backed data.  But the simple transforms we have  work just as before.  We add the needed `backed` CTE and create `_vals` like before.\\n\\n```sql\\nWITH\\n  backed (*) AS (CALL _backed()),\\n  _vals (id, name, bias) AS (\\n    SELECT id + 10, name || \'x\', bias + 3\\n    FROM backed\\n    WHERE id < 3\\n  )\\n  INSERT INTO backing(k, v)\\n   SELECT\\n     cql_blob_create(backed, V.id, backed.id),\\n     cql_blob_create(backed, V.name, backed.name, V.bias, backed.bias)\\n   FROM _vals AS V;\\n```\\n\\nLooking closely at the above we see a few things:\\n- `cql_blob_create` will expand as before (not shown)\\n- we added `backed(*)` as usual\\n- `_vals` once again just has the exact unchanged insert clause\\n- the `insert into backing(k, v)` part is identical, the same recipe always works\\n\\n\\n### How does the delete operation work?\\n\\nNow let\'s look at a simple delete example:\\n\\n```sql\\ndelete from backed where id = 7;\\n```\\n\\nNow remember we\'re again looking for a pattern that will generalize when the where condition gets crazy. But fortunately this is not so hard.\\nThe following form is fully general:\\n\\n```sql\\nWITH\\n  backed (*) AS (CALL _backed())\\nDELETE FROM backing\\n  WHERE rowid IN (\\n    SELECT rowid\\n    FROM backed\\n    WHERE id = 7\\n  );\\n```\\n\\nAll we had to do here was:\\n* add the usual `_backed` CTE\\n* move the original `WHERE` clause into a subordinate `SELECT` that gives us the rowids to delete.\\n\\nWith the backed table in scope, any `WHERE` clause works. If other backed tables are mentioned, the compiler\\nsimply adds those as usual.\\n\\nHere\'s a more complicated delete, it\'s a bit crazy but illustrative:\\n\\n```sql\\ndelete from backed where\\n  id in (select id from backed2 where name like \'%x%\');\\n```\\n\\nSo this is using rows in `backed2` to decide which rows to deleted in `backed`.  The same simple transform works directly.\\n\\n```sql\\nWITH\\n  backed2 (*) AS (CALL _backed2()),\\n  backed (*) AS (CALL _backed())\\nDELETE FROM backing\\n  WHERE rowid IN (\\n    SELECT rowid\\n    FROM backed\\n    WHERE id IN (\\n      SELECT id FROM backed2 WHERE name LIKE \'%x%\'\\n    )\\n  );\\n```\\n\\nWhat happened here:\\n* the `WHERE` clause went directly into the body of the rowid select\\n* `backed` was used as before but now we also need `backed2`\\n\\nThe delete pattern does not need any additional cql helpers beyond what we\'ve already seen.\\n\\n### What about updating tables?\\n\\nThe update statement is the most complicated of the bunch and it requires all the tricks from all the previous statements plus one more.\\n\\nFirst, we\'ll need two more blob helpers that are configurable.  By default they look like this:\\n\\n```sql\\n@blob_update_key bupdatekey offset;\\n@blob_update_val bupdateval;\\n```\\n\\nThese are used to replace particular columns in a stored blob.  Now let\'s start with a very simple update to see now it all works:\\n\\n```sql\\nupdate backed set name = \'foo\' where id = 5;\\n```\\n\\nFundamentally we need to do these things:\\n* the target of the update has to end up being the backing table\\n* we need the backed table CTE so we can do the filtering\\n* we want to use the rowid trick to figure out which rows to update which handles our where clause\\n* we need to modify the existing key and/or value blobs rather than create them from scratch\\n\\nLet\'s see how this looks:\\n\\n```sql\\nWITH\\n  backed (*) AS (CALL _backed())\\nUPDATE backing\\n  SET v = cql_blob_update(v, \'foo\', backed.name)\\n    WHERE rowid IN (SELECT rowid\\n    FROM backed\\n    WHERE id = 5);\\n```\\n\\nTearing this down a bit:\\n* we needed the normal CTE so that we can use `backed` rows\\n* the `WHERE` clause moved into a `WHERE rowid` sub-select just like in the `DELETE` case\\n* we changed the SET targets to be `k` and `v` very much like the `INSERT` case, except we used an update helper that takes the current blob and creates a new blob to store\\n  * the helper is varargs so as we\'ll see it can mutate many columns in one call\\n\\nThis gives us a working update statement... with one hitch.  It\'s possible to use the existing column values in the update expressions and there\'s no way to use our `backed` CTE to get them since the final update has to be all relative to the backing table.\\n\\nLet\'s look at another example to illustrate the problem:\\n\\n```sql\\nupdate backed set name = name || \'y\' where bias < 5;\\n```\\n\\nSo this is adding the letter \'y\' to some rows.  Kind of goofy but similar mutations do happen and have to work.  To make this work the reference to\\n`name` inside of the set expression has to change. We end up with something like this:\\n\\n```sql\\nWITH\\n  backed (*) AS (CALL _backed())\\nUPDATE backing\\n  SET v = cql_blob_update(v,\\n    cql_blob_get(v, backed.name) || \'y\',\\n    backed.name)\\n  WHERE rowid IN (SELECT rowid\\n    FROM backed\\n    WHERE bias < 5);\\n```\\n\\nImportantly the reference to `name` in the set expression was changed to `cql_blob_get(v, backed.name)` -- extracting the name from the value blob. After which it is appended with \'y\' as usual.\\n\\nThe rest of the pattern is just as it was, in fact literally everything else is unchanged.  But it\'s easy to see that the `WHERE` clause could be arbitrarily complex and it just works.  Since the `UPDATE` statement has no `FROM` clause only the fields in the target table might need to be rewritten, so in this case `name`, `id`, and `bias` were possible but only `name` was mentioned.\\n\\nAfter the `cql_blob_get` and `cql_blob_update` are expanded the result looks like this:\\n\\n```sql\\nWITH\\nbacked (rowid, id, name, bias) AS (\\n  SELECT\\n    rowid,\\n    bgetkey(T.k, 0),\\n    bgetval(T.v, -6639502068221071091L),\\n    bgetval(T.v, -3826945563932272602L)\\n  FROM backing AS T\\n  WHERE bgetkey_type(T.k) = 2105552408096159860L\\n)\\nUPDATE backing\\nSET v =\\n  bupdateval(\\n    v,\\n    -6639502068221071091L, bgetval(v, -6639502068221071091L) || \'y\', 4\\n  )\\n  WHERE rowid IN (SELECT rowid\\n  FROM backed\\n  WHERE bias < 5);\\n```\\nThe blob update function for the value blob requires the original blob, the hash or offset to update, the new value, and the type of the new value.\\nThe blob update function for the key blob is the same (blob, hash/offset, value) but the type is not required since the key blob necessarily has all\\nthe fields present because they are necessarily not null.  Therefore the type codes are already all present and so the type of every column is known.\\nThe value blob might be missing nullable values hence their type might not be stored/known.\\n\\nTo illustrate these cases we can make another small example; we\'ll set up yet another small table that uses the same backing store:\\n\\n```sql\\n@attribute(cql:backed_by=backing)\\ncreate table meta(\\n name text,\\n state long,\\n prev_state long,\\n primary key(name, state)\\n);\\n```\\n\\nThis update mixes all kinds of values around...\\n\\n```sql\\nupdate meta\\n set state = state + 1, prev_state = state\\n where name = \'foo\';\\n```\\n\\nAnd the final output will be:\\n\\n```sql\\nWITH\\nmeta (rowid, name, state, prev_state) AS (\\n  SELECT\\n    rowid,\\n    bgetkey(T.k, 0),\\n    bgetkey(T.k, 1),\\n    bgetval(T.v, -4464241499905806900)\\n  FROM backing AS T\\n  WHERE bgetkey_type(T.k) = 3397981749045545394\\n)\\nSET\\n  k = bupdatekey(k, bgetkey(k, 1) + 1, 1),\\n  v = bupdateval(v, -4464241499905806900, bgetkey(k, 1),  2)\\n  WHERE rowid IN (SELECT rowid\\n  FROM meta\\n  WHERE name = \'foo\');\\n```\\n\\nAs expected the `bupdatekey` call gets the column offset (1) but not the type code (2).  `bupdateval` gets a hash code and a type.\\n\\nAll of these transforms are live in the code as of a few days ago.\\n\\nThe upshot is that, if you write some simple encoding and decoding functions, you can have very flexible blob storage.\\n\\n### Appendix\\n\\nIf you want to refer to your blob functions in your own code, such as for indices you\'ll also need to do something like this:\\n\\n```sql\\ndeclare select function bgetkey_type(b blob) long;\\ndeclare select function bgetval_type(b blob) long;\\ndeclare select function bgetkey(b blob, iarg integer) long;\\ndeclare select function bgetval(b blob, iarg integer) long;\\ndeclare select function bcreateval no check blob;\\ndeclare select function bcreatekey no check blob;\\ndeclare select function bupdateval no check blob;\\ndeclare select function bupdatekey no check blob;\\n```\\n\\n`bgetval` and `bgetkey` are not readily declarable generally because their result is polymorphic so it\'s preferable to use `cql_blob_get` as above which then does the rewrite for you.  But it is helpful to have a UDF declaration for each of the above, especially if you want the `--rt query_plan` output to work seamlessly.  Typically `bgetval` would only be needed in the context of a `create index` statement."},{"id":"blob-storage","metadata":{"permalink":"/blog/blob-storage","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-03-17-blob-storage.md","source":"@site/blog/2022-03-17-blob-storage.md","title":"Introducing Blob Storage","description":"Introduction and Context","date":"2022-03-17T00:00:00.000Z","formattedDate":"March 17, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":9.625,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"blob-storage","title":"Introducing Blob Storage","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Backed Tables","permalink":"/blog/backed-tables"},"nextItem":{"title":"Using the FROM construct in more places","permalink":"/blog/from-general"}},"content":"## Introduction and Context\\n\\nThe general idea here is that you might want to store composite data in a single column in the database.  This is a common way to get more generic schema, the idea being that you can have one or more blob columns that store in tables a lot of data that doesn\'t have to be indexed. You could store it in other ways, like a JSON blob or some such, but we\'ll be using blobs as the basis for storage here -- hence\\nthe name blob \\"storage\\".\\n\\n### How do I define one of these blobs?\\nIn SQL/CQL, the main way you define structures, especially those that you want to maintain, is with tables.  Hence we introduce this\\n\\n```sql\\n@attribute(cql:blob_storage)\\ncreate table news_info(\\n  who text,\\n  what text,\\n  when_ long -- timestamp of some kind\\n);\\n```\\n\\nThe `blob_storage attribute` indicates that the thing we\'re about to define here is not really going to be a materialized table.  As a result, you will not be\\nable to (e.g.) `DROP` the table or `SELECT` from it, and there will be no schema upgrade for it should you request one. However, the usual schema rules still\\napply which help you to create compatible versions of this structure.  For instance, new columns can be added only at the end, and only if they are nullable.\\nHere we add `source` to the schema in a hypothetical \\"version 6\\".  Note that schema versions move forward globally in the schema, not locally in one table; this implies there are versions 1-5 elsewhere, not shown.\\n\\n```sql\\n@attribute(cql:blob_storage)\\ncreate table news_info(\\n  who text,\\n  what text,\\n  when_ long -- timestamp of some kind\\n  source text @create(6)\\n);\\n```\\n\\nAdditionally, since the storage is not backed by SQL with SQL\'s constraint system, default values and constraints are not allowed in a table marked\\nwith `cql:blob_storage`; it\'s just data. Similarly, triggers, views, and indices may not use the \\"table\\".\\n\\n### Where do you keep your blob storage?\\n\\nNaturally, blob storage goes in a blob field, but recall CQL has discriminated types so we could make something like this:\\n\\n```sql\\ncreate table info(\\n  id long primary key,\\n  news_info blob<news_info>\\n);\\n```\\n\\nFrom a SQL perspective `news_info` is just a blob.  That means if you want to do a `WHERE` clause or something like that on the info,\\nyou\'re out of luck.  Maybe you could write a user-defined function to crack the blob and so forth but really this isn\'t the point.\\nIf you\'re using this feature then, by construction, you don\'t need to index on this data.  It\'s simply not suitable for use at all\\nif you need field-at-a-time access within SQL.\\n\\n### How do I make one of these blobs?\\n\\nThe natural place that CQL stores structures is in value cursors so the most natural thing to do is to provide a variation of the `SET`\\nstatement that lets you load a blob from a cursor like so:\\n\\n```sql\\ncreate proc make_blob(like news_info, out result blob<news_info>)\\nbegin\\n  declare c cursor like news_info;\\n  fetch c from arguments;\\n  set result from cursor c;\\nEND;\\n```\\n\\nThis declares a cursor, loads it from argument values, and converts it to a blob.  Of course all of the usual cursor building forms can be\\nused to power your blob creation, you just do one serialization at the end.  The above is assembling a blob from arguments but you could\\nequally make the blob from data.\\n\\n```sql\\ncreate proc get_news_info(id_ long not null, out result blob<news_info>)\\nbegin\\n   -- use our columns sugar syntax for getting just news_info columns from\\n   -- a table with potentially lots of stuff (or an error if it\'s missing columns)\\n   declare c cursor for\\n     select columns(like news_info) from some_source_of_info where info.id = id_;\\n   fetch c;\\n   set result from cursor c;\\nEND;\\n```\\n\\nThere are *many* cursor fetch forms, including dummy data forms and other interesting bits of sugar.  You can fetch a cursor from arguments,\\nfrom other cursors, and even combinations.  We want all of that to work for blobs as well without adding tons of new syntax and code generation.\\nThe obvious way to accomplish that is for cursors to be the source of blobs.\\n\\n### How do I unpack one of these blobs?\\n\\nAgain, the normal way that you work with records in CQL is by creating suitable cursors. These can be economically accessed on a field-by-field basis.\\nWhat we need is a way to easily recreate a cursor from the blob so we can read the data values. This gives rise to this form:\\n\\n```sql\\nlet b := (select news_info from info where id = id_ if nothing null);\\ndeclare c cursor like b;\\nfetch c from b; -- note this can fail\\n-- now use c.who, c.what, etc.\\n```\\n\\nData loaded in a cursor is very economical to access on a field-by-field basis, and, since the deserialization of the blob happens all at once, that is also economical.\\nImportantly, we cannot assume that the blob is well formed, it could be coming from anywhere.  For secure-code reasons we must assume it is hostile.  Hence the\\ndecoding validates the shape, internal lengths, and so forth.\\n\\nIf we had instead started with something this:\\n\\n```sql\\nlet b := (select news_info from info where id = id_ if nothing null);\\n```\\n\\nThen maybe we might like to write:\\n```\\nif b.who == \'U2\') then ... end if;\\n```\\n\\nHowever, this sort of thing would be very uneconomical. For one thing, the blob does not have fixed-offset fields: It is carrying all the serialized data for the string fields\\nand so forth.  Each \\"dot\\" operation would be costly and, furthermore, each \\"dot\\" operation could fail if the blob is badly formed.  Having to deal with a `b.who` that might fail\\nseems very bad indeed.\\n\\nOnce you have the cursor you can make new blobs with different combinations, slice the cursor fields using the `LIKE` operator, return the cursor with `OUT`, or `OUT UNION`,\\nor pass the blob fields as arguments to functions using the `FROM` forms. Cursors already are super flexible in terms of what you can do with their contents.\\n\\n### What is the representation of one of these blobs?\\n\\nIt\'s important that we allow the blobs to evolve over time, so each blob has to be self-describing.  We also want to be able to throw an error if you use the wrong kind of blob when\\nloading a cursor, so the blob has to contain the following:\\n\\n* the number of columns in the blob data type when it was stored\\n* the type of each field is encoded as a single plain-text character\\n  * the types are bool, int, long, (double) real, (string) text, blob;\\n  * we use \'f\' (flag) for bools, hence \\"fildsb\\"\\n  * these are encoded with one letter each, upper case meaning \'not null\' so the storage might be \\"LFss\\"\\n  * the buffer begins with a null terminated string that serve for both the count and the types\\n* Each nullable field may be present or null; 1 bit is used to store this fact.  The bits are in an array of bytes that comes immediately after the type info (which implicitly tells us its size)\\n* Boolean values are likewise encoded as bits within the same array, so the total number of bits stored is nullables plus booleans (nullable booleans use 2 bits)\\n* If you are reading a newer version of a record from an older piece of data that is missing a column then the column is assumed to be NULL\\n* Any columns you add after the initial version (using @create) must be nullable; this is normal for adding columns to existing schema\\n* Integers and longs are stored in varint format after zigzag encoding\\n* Text is stored inline in null terminated strings (embedded nulls are not allowed in CQL text)\\n* Nested blobs are stored inline, with a length prefix encoded like any other int\\n\\n\\n### What about more than one row in a blob?\\n\\nWell, this is a bit more advanced but in principle this could be done as well.  To make it useful, we would want to make a new cursor type that can iterate over rows in a blob. The syntax leaves\\nroom for this, something like so:\\n\\n```sql\\ndeclare c cursor for blob b;\\nloop fetch c\\nbegin\\n  -- the usual stuff\\nend;\\n```\\n\\nThis cursor would be another variation; it would keep its current index into the blob to read data out of it.  Such a blob would also have to include a count of rows as part of its storage.\\n\\nHowever, that\'s future looking. There is no such support at present.\\n\\n### Conclusion\\n\\nWith a fairly modest amount of work, we now support structured storage natively and have pretty rich language constructs.  We carefully chose language constructs that lead to economical\\nserialization and deserialization patterns and a record format that is versioned well, without resorting to something super loose like JSON.\\n\\nAs with many other features, it\'s possible to replace the (de)serialization with code of your choice by supplying your own runtime methods.  So for instance, thrift encoding is possible;\\nthough it is more flexible than is strictly necessary for the few SQL data types, it might be convenient.\\n\\nStorage types that are going to be persisted in the database or go over a wire-protocol should be managed like schema with the usual validation rules.  On the other hand,\\nformats that will be used only transiently in memory can be changed at whim from version to version.  As mentioned above, the design specifically considers cases where a new\\nclient discovers and old-format blob (with fewer columns) and, the reverse, cases where an old client recieves a datagram from a new client with too many columns.\\n\\n#### Appendix\\n\\nA more complete example is included for reference.\\n\\n```sql\\n@attribute(cql:blob_storage)\\ncreate table news_info(\\n  who text,\\n  what text,\\n  when_ long -- timestamp of some kind\\n);\\n\\n-- a place where the blob appears in storage\\ncreate table some_table(\\n  x integer,\\n  y integer,\\n  news_blob blob<news_info>\\n);\\n\\n-- a procedure that creates the blob from loose args\\ncreate proc make_blob(like news_info, out result blob<news_info>)\\nbegin\\n  declare c cursor like news_info;\\n  fetch c from arguments;\\n  set result from cursor c;\\nend;\\n\\n-- a procedure that cracks the blob\\ncreate proc crack_blob(data blob<news_info>)\\nbegin\\n  declare c cursor like news_info;\\n  fetch c from data;\\n  out c;\\nend;\\n\\n-- a procedure that cracks the blob into loose args if needed\\n-- the OUT statement was created specifically to allow you to avoid this sort mass OUT awfulness\\ncreate proc crack_blob_to_vars(\\n  data blob<news_info>,\\n  out who text,\\n  out what text,\\n  out when_ long)\\nbegin\\n  declare c cursor like news_info;\\n  fetch c from data;\\n  set who := c.who;\\n  set what := c.what;\\n  set when_ := c.when_;\\nend;\\n\\n-- this just defines a shape for the part we are keeping from the original structure\\ndeclare proc my_basic_columns() (\\n  x int,\\n  y int\\n);\\n\\n-- this just defines a shape for the result we want\\n-- we\'re never actually defining this procedure\\ndeclare proc my_result_shape() (\\n  like my_basic_columns,\\n  like news_info\\n);\\n\\ncreate proc select_and_crack(whatever_condition bool)\\nbegin\\n  declare c cursor for select * from some_table where whatever_condition;\\n  loop fetch c\\n  begin\\n    -- crack the blob in c\\n    declare n cursor like news_info;\\n    fetch n from blob c.news_blob;\\n\\n    -- assemble the result we want from the parts we have\\n    declare result cursor like my_result_shape;\\n    fetch result from values (from c like my_basic_columns, from n);\\n\\n    -- emit one row\\n    out union result;\\n  end;\\nend;\\n```"},{"id":"from-general","metadata":{"permalink":"/blog/from-general","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-02-21-from-general.md","source":"@site/blog/2022-02-21-from-general.md","title":"Using the FROM construct in more places","description":"This new feature is a pretty simple generalization of the FROM construct as applied to expression lists.","date":"2022-02-21T00:00:00.000Z","formattedDate":"February 21, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":2.97,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"from-general","title":"Using the FROM construct in more places","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Blob Storage","permalink":"/blog/blob-storage"},"nextItem":{"title":"Introducing Expression Fragments","permalink":"/blog/expression-frags"}},"content":"This new feature is a pretty simple generalization of the `FROM` construct as applied to expression lists.\\nNote this isn\'t the same as using `FROM` the usual way in a `select` statement.   An example will clear\\nthis right up.\\n\\nSuppose you wanted to create a procedure that inserts a row into a table.  You could write this:\\n\\n```sql\\ncreate table Shape_xy (x int, y int);\\n\\ncreate proc insert_xy(like Shape_xy)\\nbegin\\n  insert into Shape_xy from arguments;\\nend;\\n```\\n\\nHere we\'re using `from` to introduce some shape of values.  It can appear in a lot of places.\\n\\nSuppose now I want to insert two of those shapes. I could write this slightly more complicated procedure:\\n\\n```sql\\ncreate proc insert_two_xy(xy1 like Shape_xy, xy2 like Shape_xy)\\nbegin\\n   call insert_xy(from xy1);\\n   call insert_xy(from xy2);\\nend;\\n```\\n\\nThis also composes with cursors, so maybe you need to get two `xy` values from diverse\\nlocations.  You can mix and match.\\n\\n```sql\\ncreate proc write_xy()\\nbegin\\n   declare C cursor for select T.x, T.y from somewhere T;\\n   fetch C;\\n   declare D cursor for select T.x, T.y from somewhere_else T;\\n   fetch D;\\n   if C and D then\\n     -- strange combos for illustration only\\n     call insert_two_xy(from C, from D);\\n     call insert_two_xy(from D, 5, 3);\\n     call insert_two_xy(4, 2, from C);\\n     call insert_two_xy(4, from C, 8);\\n   end if;\\nend;\\n```\\n\\nSo, as you can see, we can start from data in one or more cursors and we can turn that data,\\nplus other expressions, into arguments, composing them as we like. This gives you the ability\\nto call procedures and functions using shapes from a mixed set of sources.  None of this is new.\\n\\nHowever, the other places where expression lists happen -- `fetch`, `update cursor`, and `insert` -- only\\nallowed you specify a single object as the input source such as `insert into Shape_xy from C`.\\n\\nWith a little work, this is trivially generalized so that all value lists can use the `from` construct.\\n\\nHere\'s a complete example showing all the new forms.\\n\\n```sql\\ncreate table Shape_xy (x int, y int);\\ncreate table Shape_uv (u text, v text);\\ncreate table Shape_uvxy (like Shape_xy, like Shape_uv);\\n\\ncreate proc ShapeTrix()\\nbegin\\n  declare C cursor for select Shape_xy.*, \'1\' u, \'2\' v from Shape_xy;\\n  fetch C;\\n\\n  -- This new form is equivalent to the old form:\\n  --    insert into Shape_xy from C(like Shape_xy)\\n  -- but the values(...) form generalizes, see below.\\n  insert into Shape_xy values(from C like Shape_xy);\\n\\n  declare D cursor for select * from Shape_uv;\\n  fetch D;\\n\\n  declare R cursor like Shape_uvxy;\\n\\n  -- This form works just like the function call case\\n  -- that was previously supported (it uses the same code even).\\n  -- This form lets you load R from any combination of sources\\n  -- as long as you make a suitable row.\\n  fetch R from values (from C like Shape_xy, from D);\\n\\n  -- Same thing is supported in update cursor\\n  -- the x, y come from C and the u,v come from D.x, D.y.\\n  -- Note that C.u and C.v would not even be type compatible.\\n  update cursor R from values (from C like Shape_xy, from D);\\n\\n  -- And in a select-values clause\\n  declare S cursor for\\n    with cte(l,m,n,o) as (values (from C like Shape_xy, from D))\\n     select * from cte;\\n  fetch S;\\n  insert into Shape_uvxy from S;\\nend;\\n```\\n\\nAs you can see, you can choose a subset of the `from` shape using `like`.\\n\\nThese combinations let you flexibily assemble rows of data for\\ncursors, calls, and insertions, using any combination of data sources\\nyou might want, without resorting to listing every column by hand."},{"id":"expression-frags","metadata":{"permalink":"/blog/expression-frags","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-02-19-expression-frags.md","source":"@site/blog/2022-02-19-expression-frags.md","title":"Introducing Expression Fragments","description":"Following on the heels of shared fragments, we\'re introducing the same kind of thing","date":"2022-02-19T00:00:00.000Z","formattedDate":"February 19, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":4.42,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"expression-frags","title":"Introducing Expression Fragments","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Using the FROM construct in more places","permalink":"/blog/from-general"},"nextItem":{"title":"Using the LIKE form in the SELECT statement","permalink":"/blog/columns-sugar"}},"content":"Following on the heels of shared fragments, we\'re introducing the same kind of thing\\nfor shared fragments that are expressions rather than tables.  The syntax is as follows:\\n\\n```sql\\n-- this isn\'t very exciting because regular max would do the job\\n@attribute(cql:shared_fragment)\\ncreate proc max_func(x integer, y integer)\\nbegin\\n  select case when x >= y then x else y end;\\nend;\\n```\\n\\nThe above can be used in the context of a SQL statement like so:\\n\\n```sql\\nselect max_func(T1.column1, T1.column2) the_max from foo T1;\\n```\\n\\nThe consequence of the above is that the body of `max_func` is inlined into the generated SQL.  However, like\\nthe other shared fragments, this is done in such a way that the text can be shared between instances so\\nyou only pay for the cost of the text\\\\* in your program one time, no matter how many time you use it.\\n\\n\\\\* You still pay for the cost of a pointer to the text.\\n\\nIn particular, for the above, the compiler will generate the following SQL:\\n\\n\\n```sql\\nselect (\\n  select case when x >= y then x else y end\\n    from (select T1.column1 x, column2 y))\\n```\\n\\nBut each line will be its own string literal, so, more accurately, it will concatenate the following three strings:\\n\\n```c\\n\\"select (\\",                                      // string1\\n\\" select case when x >= y then x else y end\\",    // string2\\n\\" from (select T1.column1 x, column2 y))\\"        // string3\\n```\\n\\nImportantly, `string2` is fixed for any given fragment.  The only thing that changes is `string3`, i.e., the arguments.\\nThe C compiler, and then the linker, will unify the `string2` literal across all translation units so you only\\npay for the cost of that text one time.  It also means that the text of the arguments appears exactly one time,\\nno matter how complex they are.  For these benefits, we pay the cost of the select wrapper on the arguments.  This\\nis cost is frequently negative.  Consider this following:\\n\\n```sql\\nselect max_func((select max(T.m) from T), (select max(U.m) from U))\\n```\\n\\nA direct expansion of the above would result in something like this:\\n\\n```sql\\ncase when (select max(T.m) from T) >= (select max(U.m) from U)\\n   then (select max(T.m) from T)\\n   else (select max(U.m) from U)\\nend;\\n```\\n\\nThe above could be accomplished with a simple pre-processor macro, but the fragments code generates the following:\\n\\n\\n```sql\\nselect (\\n  select case when x >= y then x else y end\\n    from select (select max(T.m) from T) x, (select max(U.m) from U) y))\\n```\\n\\n\\nExpression fragments can nest, so you could write:\\n\\n```sql\\n@attribute(cql:shared_fragment)\\ncreate proc max3_func(x integer, y integer, z integer)\\nbegin\\n  select max_func(x, max_func(y, z));\\nend;\\n```\\n\\nAgain, this particular example is a waste because regular `max` would already do the job.\\n\\nTo give another example, common mappings from one kind of code to another using case/when can be written\\nand shared this way:\\n\\n```sql\\n-- this sort of thing happens all the time\\n@attribute(cql:shared_fragment)\\ncreate proc remap(x integer not null)\\nbegin\\n   select case x\\n     when 1 then 1001\\n     when 2 then 1057\\n     when 3 then 2010\\n     when 4 then 2011\\n     else 9999\\n   end;\\nend;\\n```\\n\\nIn the following:\\n\\n```sql\\nselect remap(T1.c), remap(T2.d), remap(T3.e) from C, D, E;\\n```\\n\\nThe text for `remap` will appear three times in the generated SQL query but only one time in your binary.\\n\\nRestrictions:\\n\\n* the function must consist of exactly one simple select statement\\n  * no `FROM`, `WHERE`, `HAVING`, etc. -- the result is an expression\\n* the select list must have exactly one value\\n  * Note: the expression can be a nested `SELECT` which could have all the usual `SELECT` elements\\n* the usual shared fragment rules apply, e.g. no out-parameters, exactly one statement, etc.\\n\\n\\nFAQ:\\n\\nQ: Why does the expression fragment have a `select` in it?\\n\\nA: Expression fragments are only interesting in SQL contexts where normal procedure and function calls are not available.\\nThe `select` keyword makes it clear to the author and the compiler that the expression will be evaluated by\\nSQLite and the rules for what is allowed to go in the expression are the SQLite rules.\\n\\nQ: Why no `FROM` clause?\\n\\nA: We\'re trying to produce an expression, not a table-value with one column.  If you want a table-value with\\none column, the original shared fragments solution already do exactly that.  This gives you a solution for\\nsharing code in, say, the `WHERE` clause or the select list.\\n\\nQ: Isn\'t this just the same as doing, say, `#define max_func(x,y) case when (x) >= (y) then x else y end;`?\\n\\nA: Macros can give you a ton of flexibility, but they have many problems:\\n* if the macro has an error, you see the error in the call site with really bad diagnostic info\\n* the compiler doesn\'t know that the sharing is going on so it won\'t be able to share text between call sites\\n* the arguments can be evaluated many times each which could be expensive, bloaty, or wrong\\n* there is no type-checking of arguments to the macro so you may or may not get compilation errors after expansion\\n* you have to deal with all the usual pre-processor hazards\\n\\nIn general, macros _can_ be used for meta-programming (as in C and C++), but that doesn\'t mean it\'s a good idea."},{"id":"columns-sugar","metadata":{"permalink":"/blog/columns-sugar","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2022-02-03-columns-sugar.md","source":"@site/blog/2022-02-03-columns-sugar.md","title":"Using the LIKE form in the SELECT statement","description":"One of the signature features of the CQL language is the ability to use the \\"LIKE\\" form to","date":"2022-02-03T00:00:00.000Z","formattedDate":"February 3, 2022","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":3.68,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"columns-sugar","title":"Using the LIKE form in the SELECT statement","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Expression Fragments","permalink":"/blog/expression-frags"},"nextItem":{"title":"Control Flow Analysis in CQL","permalink":"/blog/flow-analysis"}},"content":"One of the signature features of the CQL language is the ability to use the \\"LIKE\\" form to\\nslice out columns that conform to a shape.  This notion appears in many places in the language.\\nFor instance if I have a table `Foo`. I can make a cursor for that shape like so:\\n\\n```sql\\ndeclare C cursor like Foo;\\n```\\n\\nWhich says I want the columns of `C` to be like the columns of `Foo`.\\n\\nIf I have a cursor `D` that has the `Foo` columns but maybe more and maybe in a different order I can load `C`\\nas follows:\\n\\n```sql\\nfetch C from D(like Foo)\\n```\\n\\nWhich again saves me from having to list all the (potentially dozens) of Foo columns.  This construct is in many places:\\n\\n```sql\\ndeclare proc P(like Foo)\\nbegin\\n  insert into Foo from arguments;\\nend;\\n```\\n\\neven\\n\\n```sql\\ndeclare proc P(f like Foo, b like Bar)\\nbegin\\n  insert into Foo from f;\\n  insert into Bar from b;\\nend;\\n```\\n\\nAnd other examples...  This is discussed more fully in\\n[Chapter 5](https://cgsql.dev/cql-guide/ch05#reshaping-data-cursor-like-forms) of the Guide.\\n\\nHowever, one of the few places that shapes are interesting but not supported was in the select list.\\nAnd so, just a couple of days ago, we added the `COLUMNS` construct to the language which allows for\\na sugared syntax for extracting columns in bulk.  It\'s kind of a generalization of the `select T.*`\\npattern but with CQL-style slicing and type-checking.\\n\\nThese forms are supported:\\n\\n  * columns from a join table or tables\\n\\n```\\n-- same as A.*\\nselect columns(A) from ...;\\n\\n-- same as A.*, B.*\\nselect columns(A, B) from ...;\\n```\\n  * columns from a particular join table that match a shape\\n\\n```\\n-- the columns of A that match the shape Foo\\nselect columns(A like Foo) from ...;\\n\\n-- get the Foo shape from A and the Far shape from B\\nselect columns(A like Foo, B like Bar) from ...;\\n```\\n\\n* columns from any join table that match a shape\\n\\n```\\n--- get the Foo shape from anywhere in the join\\nselect columns(like Foo) from ...;\\n\\n-- get the Foo and Bar shapes, from anywhere in the join\\nselect columns(like Foo, like Bar) from ...;\\n```\\n  * specific columns\\n\\n```\\n-- x and y columns plus the foo shape\\nselect columns(T1.x, T2.y, like Foo) from ...;\\n```\\n\\n  * distinct columns from the above (not distinct values!)\\n\\n```\\n-- removes duplicate column names\\n-- e.g. there will be one copy of \'pk\'\\nselect columns(distinct A, B) from A join B using(pk);\\n\\n-- if both Foo and Bar have an (e.g.) \'id\' field you only get one copy\\nselect columns(distinct like Foo, like Bar) from ...;\\n\\n-- if a specific column is mentioned it is always included\\n-- but later clauses that are not a specific column will avoid it\\n-- if F or B has an x it won\'t appear again, just T.x\\nselect columns(distinct T.x, F like Foo, B like Bar) from F, B ..;\\n```\\n\\nOf course this is all just sugar, so it all ends up being a column list with table\\nqualifications -- but the syntax is very powerful.  For instance, for narrowing a\\nwide table, or for fusing joins that share common keys\\n\\n```\\n-- just the Foo columns\\nselect columns(like Foo) from Superset_Of_Foo_From_Many_Joins_Even;\\n\\n-- only one copy of \'pk\'\\nselect columns(distinct A,B,C) from\\n  A join B using (pk) join C using (pk);\\n```\\n\\nAnd of course you can define shapes however you like and then use them\\nto slice off column chucks of your choice.  There are many ways to build\\nup shapes from other shapes.  Probably the easiest is to declare procedures\\nthat return the shape you want and never actual create them.  E.g.\\n\\n```\\ndeclare proc shape1() (x integer, y real, z text);\\ndeclare proc shape2() (like shape1, u bool, v bool);\\n```\\n\\nWith this combination you can easily define common column shapes and slice them\\nout of complex queries without having to type the columns names over and over...\\n\\nNote that the `COLUMNS(...)` form is not a general replacement for the select list.\\nFor instance, general expressions are not allowed inside of `COLUMNS(...)` but,\\nwhere extraction of lots of columns is needed, or even re-ordering of colummns,\\nit\'s a very good option indeed and it composes well with the other `select` features.\\n\\nThis was the last significant area where shapes are useful but totally absent."},{"id":"flow-analysis","metadata":{"permalink":"/blog/flow-analysis","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-12-30-flow-analysis.md","source":"@site/blog/2021-12-30-flow-analysis.md","title":"Control Flow Analysis in CQL","description":"One of the biggest changes to CQL in 2021 was the addition of control flow","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":11.395,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"flow-analysis","title":"Control Flow Analysis in CQL","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Using the LIKE form in the SELECT statement","permalink":"/blog/columns-sugar"},"nextItem":{"title":"Introducing Shared Fragments","permalink":"/blog/shared-fragments-intro"}},"content":"One of the biggest changes to CQL in 2021 was the addition of control flow\\nanalysis. Given an understanding of how execution can flow within a user\'s\\nprogram, CQL can do things like infer when a nullable variable must contain a\\nnonnull value and improve its type appropriately, or issue an error when a\\nnonnull variable may be used before it has been initialized.\\n\\n### Improving Nullability\\n\\nAs of mid-2021, and with increasing sophistication throughout the remainder of\\nthe year, CQL has been able to infer that a variable of a nullable type must not\\nbe `NULL` within a portion of a user\'s program:\\n\\n```sql\\nDECLARE PROC another_proc(t0 TEXT NOT NULL, t1 TEXT NOT NULL);\\n\\nCREATE PROC some_proc(t0 TEXT, t1 TEXT)\\nBEGIN\\n  IF t0 IS NULL RETURN;\\n  -- `t0` must be nonnull here if we made it this far\\n\\n  IF t1 IS NOT NULL THEN\\n    -- `t0` and `t1` are nonnull here\\n    CALL another_proc(t0, t1);\\n  ELSE\\n    -- `t0` is nonnull here\\n    CALL another_proc(t0, \\"default\\");\\n  END IF;\\nEND;\\n```\\n\\nThe ability of the CQL compiler to infer non-nullability greatly reduces the\\nneed to use the functions `ifnull_crash` and `ifnull_throw` to coerce values to\\na nonnull type\u2014functions that, if they are ever used incorrectly, usually result\\nin programs misbehaving.\\n\\nFor a detailed description and many additional examples of what is possible\u2014CQL\\ncan handle much more than what is shown above\u2014see [the user guide\'s section on\\nnullability\\nimprovements](https://cgsql.dev/cql-guide/ch03#nullability-improvements).\\n\\n### Enforcing Initialization Before Use\\n\\nIn CQL, it is possible to declare a variable of a nonnull type without giving it\\na value. If the variable is of a non-reference type, it is assigned a default\\nvalue of `0`. If the variable is of a reference type (`BLOB`, `OBJECT`, or\\n`TEXT`), however, it is simply set to `NULL` despite the nonnull type as no\\ndefault value exists.\\n\\nTo help prevent accessing a reference variable of a nonnull type and getting\\nback `NULL`, CQL recently began enforcing that such variables are initialized\\nbefore use. The following code, therefore, now results in an error:\\n\\n```sql\\nDECLARE t TEXT NOT NULL;\\nCALL requires_text_notnull(t); -- error!\\n```\\n\\nUsing the same engine for control flow analysis that is behind nullability\\nimprovements, CQL can improve a variable to be initialized:\\n\\n```sql\\nDECLARE t TEXT NOT NULL;\\n\\nIF some_condition THEN\\n  SET t := \\"some example text\\";\\n  -- `t` is initialized here\\nELSE\\n  THROW;\\nEND IF;\\n-- `t` must be initialized here if we made it this far\\n\\nCALL requires_text_notnull(t); -- okay!\\n```\\n\\nThanks to CQL\'s ability to understand the control flow of users\' programs, the\\nabove example works just fine.\\n\\nCQL now also enforces that all procedures with `OUT` parameters of a nonnull\\nreference type properly initialize said parameters before they return:\\n\\n```sql\\nCREATE PROC some_proc(b BOOL NOT NULL, OUT t TEXT NOT NULL)\\nBEGIN\\n  IF b THEN\\n    SET t := another_proc(t);\\n    -- `t` is initialized here\\n  ELSE\\n    SET t := yet_another_proc(t);\\n    -- `t` is initialized here\\n  END IF;\\n  -- `t` must be initialized here because all possible\\n  -- branches initialized it, so `some_proc` is okay!\\nEND;\\n```\\n\\nAs with nullability improvements, understanding the nuances of what will be\\nconsidered initialized is easier if one has a sense for how control flow\\nanalysis works in the compiler.\\n\\n### Understanding Control Flow Analysis in CQL\\n\\nTo develop an intuition for how control flow analysis works in CQL, let\'s begin\\nby taking a look at the following example:\\n\\n```sql\\nDECLARE PROC p1(OUT t TEXT NOT NULL);\\nDECLARE PROC p2(i INTEGER NOT NULL, OUT t TEXT NOT NULL);\\n\\nCREATE PROC p0(b BOOL, i INTEGER, OUT t TEXT NOT NULL)\\nBEGIN\\n  IF i IS NULL THEN\\n    IF b THEN\\n      CALL p1(t);\\n    ELSE\\n      SET t := \\"\\";\\n    END IF;\\n    RETURN;\\n  END IF;\\n\\n  IF i == 0 THEN\\n    SET t := \\"\\";\\n  ELSE IF i > 0 THEN\\n    SET t := p2(i);\\n  ELSE\\n    THROW;\\n  END IF;\\nEND;\\n```\\n\\nThere are a couple of things we must verify in order to ensure the code is\\ntype-safe:\\n\\n- With regard to the parameters of `p0`: Since `t` is an `OUT` parameter of type\\n  `TEXT NOT NULL`, `p0` must always assign it a value before it returns. If it\\n  does not, a caller of `p0` may end up with a variable of a `NOT NULL` type\\n  that actually contains `NULL`.\\n\\n- With regard to the calling of `p2` in `p0`: Since `p2` requires a first\\n  argument of type `INTEGER NOT NULL`, some sort of check must be performed to\\n  ensure that `i` is not `NULL` before `p2(i)` is executed.\\n\\nIf we carefully study `p0`, we can determine that both of the above conditions\\nare satisfied. Making this determination, however, is not exactly trivial, and\\nreal-world code is often significantly more complicated than this\u2014and it evolves\\nover time. For these reasons, having a compiler that can make such\\ndeterminations automatically is critical; most modern production compilers\\nperform these sorts of checks.\\n\\nThe easiest way to understand how CQL does its job is to take the above example\\nline-by-line. This is not exactly how CQL works under the hood, but it should\\nprovide an intuitive sense of how control flow analysis works in the compiler:\\n\\n```sql\\n==> CREATE PROC p0(b BOOL, i INTEGER, OUT t TEXT NOT NULL)\\n    BEGIN\\n      ...\\n    END;\\n```\\n\\nRight away, CQL can see that `t` is declared both `OUT` and `TEXT NOT NULL` and\\nthus requires initialization before `p0` returns. CQL can, therefore, add a fact\\nabout what it is analyzing to its previously null set of facts:\\n\\n> * `t` requires initialization.\\n\\nWe can then continue:\\n\\n```sql\\n==>   IF i IS NULL THEN\\n        ...\\n      END IF;\\n```\\n\\nHere, the compiler notices that we\'re at an `IF` statement. In CQL, `IF`\\nstatements contain one or more **branches**, and the compiler considers every\\n`IF` to be the start of a **branch group**. The same line also indicates the\\ncondition for the first branch: `i IS NULL`. CQL can update its set of facts:\\n\\n> * `t` requires initialization.\\n> * In branch group:\\n>     * In branch when `i IS NULL`:\\n\\nIt then proceeds to the next line:\\n\\n```sql\\n      IF i IS NULL THEN\\n    ==> IF b THEN\\n          CALL p1(t);\\n        ELSE\\n          SET t := \\"\\";\\n        END IF;\\n        RETURN;\\n      END IF;\\n```\\n\\nAnother branch group and branch:\\n\\n> * `t` requires initialization.\\n> * In branch group:\\n>     * In branch when `i IS NULL`:\\n>         * In branch group:\\n>             * In branch when `b`:\\n\\nContinuing:\\n\\n```sql\\n      IF i IS NULL THEN\\n        IF b THEN\\n      ==> CALL p1(t);\\n        ELSE\\n          SET t := \\"\\";\\n        END IF;\\n        RETURN;\\n      END IF;\\n```\\n\\nSince `p1` takes an `OUT` argument of type `TEXT NOT NULL`, this call\\ninitializes `t`, and so CQL can update its set of facts once again:\\n\\n> * `t` requires initialization.\\n> * In branch group:\\n>     * In branch when `i IS NULL`:\\n>         * In branch group:\\n>             * In branch when `b`:\\n>                 * `t` is initialized.\\n\\nJumping ahead a couple of lines:\\n\\n```sql\\n      IF i IS NULL THEN\\n        IF b THEN\\n          CALL p1(t);\\n        ELSE\\n      ==> SET t := \\"\\";\\n        END IF;\\n        RETURN;\\n      END IF;\\n```\\n\\nAt this point, we\'re in another branch. We also have yet another fact to add\\nbecause `t` is initialized here as well due to the `SET`:\\n\\n> * `t` requires initialization.\\n> * In branch group:\\n>     * In branch when `i IS NULL`:\\n>         * In branch group:\\n>             * In branch when `b`:\\n>                 * `t` is initialized.\\n>             * In ELSE branch:\\n>                 * `t` is initialized.\\n\\nMoving ahead one more line, things get a bit more interesting:\\n\\n```sql\\n      IF i IS NULL THEN\\n        IF b THEN\\n          CALL p1(t);\\n        ELSE\\n          SET t := \\"\\";\\n    ==> END IF;\\n        RETURN;\\n      END IF;\\n```\\n\\nHere, we\'re at the end of an `IF`, and thus the end of a branch group. Whenever\\nCQL reaches the end of a branch group, it *merges* the effects of all of its\\nbranches.\\n\\nOne very important thing to note here is that the current branch group has an\\n`ELSE` branch, and so the set of branches covers all possible cases. That means\\nif something is initialized in every branch within the branch group, we can\\nconsider it to be initialized after the branch group has ended: Initialization\\nwill always occur. This allows CQL to simplify its set of facts as follows as it\\nleaves the branch group:\\n\\n> * `t` requires initialization.\\n> * In branch group:\\n>     * In branch when `i IS NULL`:\\n>         * `t` is initialized.\\n\\nStepping forward one line again, we reach a `RETURN`:\\n\\n```sql\\n      IF i IS NULL THEN\\n        ...\\n    ==> RETURN;\\n      END IF;\\n```\\n\\nWe\'re now at a point where we can exit the procedure. CQL will, therefore,\\nverify that if something requires initialization, it has been initialized. Since\\nwe have both the facts \\"`t` requires initialization\\" and \\"`t` is initialized\\",\\nall is well!\\n\\nThe fact that the current branch returns early is added to the set of facts:\\n\\n> * `t` requires initialization.\\n> * In branch group:\\n>     * In branch when `i IS NULL`:\\n>         * `t` is initialized.\\n>         * Returns.\\n\\nMoving ahead one more line, we reach the end of another branch and branch group,\\nand again something interesting happens:\\n\\n```sql\\n      ...\\n      IF i IS NULL THEN\\n        ...\\n  ==> END IF;\\n```\\n\\nUpon ending the branch group, we know that the branch group has exactly one\\nbranch, that the branch is entered only when `i IS NULL`, and that the branch\\nreturns. What that tells CQL is that, if execution is going to continue after\\nthe branch group, its sole branch must *not* have been taken, and so CQL knows\\nthe *opposite* of its condition for entry will be true from this point onward:\\n\\n> * `t` requires initialization.\\n> * `i` is not null.\\n\\nThe next `IF` is rather similar to what we\'ve seen already in its structure, so\\nwe can jump ahead several lines to the next point of interest:\\n\\n```sql\\n      IF i == 0 THEN\\n        SET t := \\"\\";\\n      ELSE IF i > 0 THEN\\n    ==> SET t := p2(i);\\n      ELSE\\n        THROW;\\n      END IF;\\n```\\n\\n*Before* we analyze the above-indicated line, we have the following set of facts:\\n\\n> * `t` requires initialization.\\n> * `i` is not null.\\n> * In branch group:\\n>     * In branch when `i == 0`:\\n>         * `t` is initialized.\\n>     * In branch when `i > 0`:\\n\\nIn the call `p2(i)`, we know that `i` was declared to have type `INTEGER` and\\nthat `p2` requires an `INTEGER NOT NULL`, but we also have the fact \\"`i` is not\\nnull\\". For this reason, we can consider `p2(i)` to be a valid call. We can also\\nadd the fact that `t` is initialized to our current set of facts:\\n\\n> * ...\\n>     * In branch when `i > 0`:\\n>         * `t` is initialized.\\n\\n**NOTE:** When it comes to code generation, it is not so simple as to say\\n`p2(i)` is valid and proceed as usual. That\'s because `p2` expects an argument\\nof type `INTEGER NOT NULL`, but we merely have a value of type `INTEGER` *that\\nwe happen to know* cannot be null: `INTEGER NOT NULL` and `INTEGER` do not share\\nthe same underlying representation, and so we cannot pass the declared-nullable\\nvariable `i` directly to `p2`. To solve this problem, CQL *rewrites the\\nexpression* such that `p2(i)` becomes `p2(cql_inferred_notnull(i))`, where\\n`cql_inferred_notnull` is an internal-only function that handles the\\nnullable-to-nonnull representational conversion for us. This explains its\\npresence in the following examples.\\n\\nJumping ahead again, we encounter a `THROW`:\\n\\n```sql\\n      IF i == 0 THEN\\n        SET t := \\"\\";\\n      ELSE IF i > 0 THEN\\n        SET t := p2(cql_inferred_notnull(i));\\n      ELSE\\n    ==> THROW;\\n      END IF;\\n```\\n\\nThe fact that the branch will throw is added to the current set of facts:\\n\\n> * `t` requires initialization.\\n> * `i` is not null.\\n> * In branch group:\\n>     * In branch when `i == 0`:\\n>         * `t` is initialized.\\n>     * In branch when `i > 0`:\\n>         * `t` is initialized.\\n>     * In ELSE branch:\\n>         * Throws.\\n\\nWe then proceed to the end of the `IF`:\\n\\n```sql\\n      IF i == 0 THEN\\n        SET t := \\"\\";\\n      ELSE IF i > 0 THEN\\n        SET t := p2(cql_inferred_notnull(i));\\n      ELSE\\n        THROW;\\n  ==> END IF;\\n```\\n\\nOnce again, CQL merges the effects of the branches in the branch group to finish\\nthe analysis of the `IF`. Since it can see that `t` was initialized in all\\nbranches except the one that throws, and since the branches cover all possible\\ncases, the set of facts is simplified as follows given the knowledge that, if\\n`THROW` was not encountered, `t` must have been initialized:\\n\\n> * `t` requires initialization.\\n> * `i` is not null.\\n> * `t` is initialized.\\n\\nMoving ahead one final time, we encounter the end of the procedure:\\n\\n```sql\\n    CREATE PROC p0(b BOOL, i INTEGER, OUT t TEXT NOT NULL)\\n    BEGIN\\n      ...\\n==> END;\\n```\\n\\nThe only thing left to do at this point is to validate that anything requiring\\ninitialization has been initialized. Since we have both \\"`t` requires\\ninitialization\\" and \\"`t` is initialized\\", everything is in order.\\n\\n### Looking Ahead\\n\\nAs a recently generalized piece of functionality within the CQL compiler,\\ncontrol flow analysis will soon be used to enforce additional properties of\\nusers\' programs. In particular, CQL will be able to ensure that cursors are\\nalways fetched before they\'re used and that cursors are always checked to have a\\nrow before their fields are accessed.\\n\\nHopefully you now understand the fundamentals of control flow analysis in CQL\\nand the benefits it brings to your programs. Best wishes for 2022!"},{"id":"shared-fragments-intro","metadata":{"permalink":"/blog/shared-fragments-intro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-12-14-shared-fragments.md","source":"@site/blog/2021-12-14-shared-fragments.md","title":"Introducing Shared Fragments","description":"Shared fragments are a real game-changer for CQL.","date":"2021-12-14T00:00:00.000Z","formattedDate":"December 14, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":7.845,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"shared-fragments-intro","title":"Introducing Shared Fragments","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Control Flow Analysis in CQL","permalink":"/blog/flow-analysis"},"nextItem":{"title":"Introducing @RC builtin variable","permalink":"/blog/result-variable"}},"content":"Shared fragments are a real game-changer for CQL.\\n\\nRemember, these are designed to let you write part of a query and then substitute in parameters.  So it\'s like a parameterized view in normal SQL terms.  But actually it\'s more powerful than that, fragments also provide features that are more like Java generics.  Let\'s do some examples.\\n\\nSuppose we have a procedure which looks something like this:\\n\\n```SQL\\nCREATE PROC get_stuff(to_include_ text, to_exclude_ text)\\nBEGIN\\n  WITH\\n  to_exclude_recursive_query (tok, rest) AS (\\n    SELECT\\n      \'\',\\n      to_exclude_ || \',\'\\n    UNION ALL\\n    SELECT\\n      substr(rest, 1, instr(rest, \',\') - 1),\\n      substr(rest, instr(rest, \',\') + 1)\\n    FROM to_exclude_recursive_query\\n    WHERE rest <> \'\'\\n  ),\\n  to_exclude (id) AS (\\n    SELECT CAST(tok AS LONG)\\n    FROM to_exclude_recursive_query\\n    WHERE tok <> \'\'\\n  )\\n  to_include_recursive_query (tok, rest) AS (\\n    SELECT\\n      \'\',\\n      to_include_ || \',\'\\n    UNION ALL\\n    SELECT\\n      substr(rest, 1, instr(rest, \',\') - 1),\\n      substr(rest, instr(rest, \',\') + 1)\\n    FROM to_include_recursive_query\\n    WHERE rest <> \'\'\\n  ),\\n  to_include (id) AS (\\n    SELECT CAST(tok AS LONG)\\n    FROM to_include_recursive_query\\n    WHERE tok <> \'\'\\n  )\\n  SELECT * from stuff S\\n  WHERE\\n    S.id in (select * from to_include) AND\\n    S.id not in (select * from to_exclude);\\nEND;\\n```\\n\\nWith shared fragments you could write something like this:\\n\\n```SQL\\n@attribute(cql:shared_fragment)\\nCREATE PROC split_commas(str text)\\nBEGIN\\n  WITH splitter(tok, rest) AS (\\n    SELECT \'\', IFNULL(str || \',\', \'\')\\n    UNION ALL\\n    SELECT\\n      substr(rest, 1, instr(rest, \',\') - 1),\\n      substr(rest, instr(rest, \',\') + 1)\\n    FROM splitter\\n    WHERE rest <> \'\')\\n  select tok from splitter where tok <> \'\';\\nEND;\\n\\n@attribute(cql:shared_fragment)\\nCREATE PROC ids_from_string(str text)\\nBEGIN\\n  WITH toks(tok) AS (CALL split_commas(str))\\n  SELECT CAST(tok AS LONG) AS id from toks;\\nEND;\\n```\\n\\nWe now have a shared fragment called `split_commas` which can be anywhere like maybe in a standard include file.  There are some immediate benefits:\\n\\n* the fragment is compiled on its own before usage so any errors are reported in the fragment\\n  * in contrast, with macros you get errors when you try to use the macro and they are all charged to the line the macro appears on so it\'s hopeless figuring out what\'s wrong\\n* the text of the shared fragment will be the same, so it can be re-used in all locations, this can be a big binary size savings\\n  * in contrast, macros are pre-processed before CQL ever sees the text so it doesn\'t \\"know\\" it\'s the same code\\n* fragments compose cleanly as we\'ll see; and they have typed arguments\\n* fragments can be independently tested outside of the context in which they appear\\n  * make a test context and explore the fragment, no worries about it breaking on edge cases later\\n\\n\\nThe first fragment called `split_commas` does exactly what it sounds like, it takes a string argument and makes a list of the strings in it.\\n\\nThe second fragment uses the first to split a string and then it converts all the strings to long integers.\\n\\nNow instead of the above we could write:\\n\\n```SQL\\n#include <stringsplit.sql> /* whereever you put the fragments */\\n\\nCREATE PROC get_stuff(to_include_ text, to_exclude_ text)\\nBEGIN\\n  WITH\\n    to_include(id) AS (CALL ids_from_string(to_include_)),\\n    to_exclude(id) AS (CALL ids_from_string(to_exclude_))\\n  SELECT * from stuff S\\n  WHERE\\n    S.id in (select * from to_include) AND\\n    S.id not in (select * from to_exclude);\\nEND;\\n```\\n\\nAnd of course since `ids_from_string` is somewhere shared (`stringsplit.sql`) so these fragments can be used\\nall over your code and you\'ll only pay for the text one time.  This gives you great flexibility, very much\\nlike parameterized views. You can have any number of these fragments, they will share code, they compose like crazy\\nand there is no schema cost!\\n\\n### Generics\\n\\nA series of useful fragments for generating data would go a long way but there are other applications\\nof fragments and you might want to operate on various data sources without hard coding them all.  This is\\nwhere the generic form of fragments comes in. Consider a case where you want to be able to filter `stuff`\\nby say name and age.  You could create this fragment:\\n\\n```SQL\\n@attribute(cql:shared_fragment)\\nCREATE PROC filter_stuff(\\n  pattern_ text not null,\\n  min_age_ integer not null,\\n  max_age_ integer not null)\\nBEGIN\\n  WITH\\n    source(*) LIKE stuff\\n  SELECT * from source S\\n  WHERE\\n    S.name LIKE pattern_ AND\\n    S.age BETWEEN min_age_ and max_age_;\\nEND;\\n```\\n\\nNow imagine that we had added the shared fragment annotation to `get_stuff` (just like the above).\\nWe could then write the following:\\n\\n```SQL\\nCREATE PROC the_right_stuff(\\n  to_include_ text,\\n  to_exclude_ text,\\n  pattern_ text not null,\\n  min_age_ integer not null,\\n  max_age_ integer not null)\\nBEGIN\\n  WITH\\n    get_stuff(*) AS (call get_stuff(to_include_, to_exclude_)),\\n    filter_stuff(*) AS (call filter_stuff(pattern_, min_age_, max_age_)\\n      using get_stuff as source)\\n  SELECT * from filter_stuff S\\n  ORDER BY name\\n  LIMIT 5;\\nEND;\\n```\\n\\nOr with some sugar to forward arguments and assume the CTE name matches, more economically:\\n\\n```SQL\\nCREATE PROC the_right_stuff(\\n  to_include_ text,\\n  to_exclude_ text,\\n  pattern_ text not null,\\n  min_age_ integer not null,\\n  max_age_ integer not null)\\nBEGIN\\n  WITH\\n    (call get_stuff(*)),\\n    (call filter_stuff(*) using get_stuff as source)\\n  SELECT * from filter_stuff S\\n  ORDER BY name\\n  LIMIT 5;\\nEND;\\n```\\n\\nThe arg syntax `(*)` simply indicates that the arg names in the caller should match to the same names in the callee.  In\\ngeneral `call foo(*)` expands to `call foo(from arguments like foo arguments)`.  `*` is rather more economical than that.\\n\\nIn this example `filter_stuff` doesn\'t know where its data will be coming from, you bind its table parameter `source`\\nto a compatible data source of your choice. For example, this would also be legal:\\n\\n```SQL\\nCREATE PROC almost_the_right_stuff(\\n  pattern_ text not null,\\n  min_age_ integer not null,\\n  max_age_ integer not null)\\nBEGIN\\n  WITH\\n    (call filter_stuff(*) using stuff as source)\\n  SELECT * from filter_stuff S\\n  ORDER BY name\\n  LIMIT 5;\\nEND;\\n```\\n\\n### Conditionals\\n\\nIt\'s often desirable to have some options in the generated SQL without having to fork your entire query.  Shared\\nfragments address this as well with the conditional form.  In this form the top level of the fragment is an\\n`IF` statement and there are a number of alternatives.  Here are some simple modifications to the above that illustrate\\nsome of the possibilities.\\n\\n```SQL\\n@attribute(cql:shared_fragment)\\nCREATE PROC filter_stuff(\\n  pattern_ text,\\n  min_age_ integer not null,\\n  max_age_ integer not null)\\nBEGIN\\n  IF pattern_ IS NOT NULL THEN\\n    WITH\\n        source(*) LIKE stuff\\n    SELECT * from source S\\n    WHERE\\n        S.name LIKE pattern_ AND\\n        S.age BETWEEN min_age_ and max_age_;\\n  ELSE\\n    WITH\\n        source(*) LIKE stuff\\n    SELECT * from source S\\n    WHERE\\n        S.age BETWEEN min_age_ and max_age_;\\n  END IF;\\nEND;\\n```\\n\\nIn the above if the input pattern is NULL then it is not considered, it won\'t be part of the generated SQL at all. Note that\\n`source` (same name) appears in both branches and therefore must be the same type as it will be fulfilled by one actual table\\nparameter.\\n\\nNow the above could have been achieved with something like this:\\n\\n```SQL\\npattern_ IS NULL OR S.name LIKE pattern_\\n```\\n\\nBut that would have no useful selectivity.  But in general you might be able to avoid joins and so forth\\nwith your constraints.  Consider something like this hypothetical:\\n\\n```SQL\\n@attribute(cql:shared_fragment)\\nCREATE PROC filter_stuff(\\n  pattern_ text,\\n  min_age_ integer not null,\\n  max_age_ integer not null)\\nBEGIN\\n  IF pattern_ IS NOT NULL THEN\\n    WITH\\n        source(*) LIKE stuff\\n    SELECT DISTINCT S.* from source S\\n    INNER JOIN keywords K\\n    WHERE\\n        K.keyword LIKE pattern_ AND\\n        S.age BETWEEN min_age_ and max_age_;\\n  ELSE\\n    WITH\\n        source(*) LIKE stuff\\n    SELECT * from source S\\n    WHERE\\n        S.age BETWEEN min_age_ and max_age_;\\n  END IF;\\nEND;\\n```\\n\\nHere we save the DISTINCT and the JOIN if there is no pattern which might be important.  Of course\\nthere are probably better ways to match keywords but this is just an illustration of what\'s possible.\\n\\nThere are numerous ways this flexibility can be used, again a simple example, a real schema\\ntransform would be more complex.\\n\\n```SQL\\n@attribute(cql:shared_fragment)\\nCREATE PROC get_stuff(\\n  to_include_ text,\\n  to_exclude_ text,\\n  schema_v2 bool not null)\\nBEGIN\\n  IF schema_v2 THEN\\n    WITH\\n        to_include(id) AS (CALL ids_from_string(to_include_)),\\n        to_exclude(id) AS (CALL ids_from_string(to_exclude_))\\n    SELECT * from stuff_2 S\\n    WHERE\\n        S.id in (select * from to_include) AND\\n        S.id not in (select * from to_exclude);\\n  ELSE\\n    WITH\\n        to_include(id) AS (CALL ids_from_string(to_include_)),\\n        to_exclude(id) AS (CALL ids_from_string(to_exclude_))\\n    SELECT * from stuff S\\n    WHERE\\n        S.id in (select * from to_include) AND\\n        S.id not in (select * from to_exclude);\\n   END IF;\\nEND;\\n```\\n\\n### Validation\\n\\nAll of this requires a bunch of checking, at least this:\\n\\n* the LIKE forms can only appear in a shared fragment\\n* the CALL forms must refer to shared fragments\\n* the CALL args must be compatible\\n* the number and type of the provided tables in USING must be correct\\n* the shared fragment must be a single select statement or an IF statement with an ELSE\\n  * the statement lists of the IF/ELSE combo must all be single select statements\\n  * all the choices in the IF block must return the same shape (this is normal for procedures)\\n* the shared fragment can\'t have any out arguments\\n* the provided fragment arguments cannot themselves use the nested SELECT construct\\n\\nI think this is a total game changer for SQL authoring and should go a long way to making it easier to get your work done\\non SQLite. A good base set of shared fragments as part any suite of procedures seems like a good idea.\\n\\nThere are more details in the section on shared fragments in [Chapter 14](https://cgsql.dev/cql-guide/ch14) of The Guide.\\n\\nThese features are in the current build as of today (12/14/2021).\\n\\nHappy Holidays and stay safe."},{"id":"result-variable","metadata":{"permalink":"/blog/result-variable","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-02-21-result-variable.md","source":"@site/blog/2021-02-21-result-variable.md","title":"Introducing @RC builtin variable","description":"We\'ve long needed a way to see the most recent SQLite result code SQLite in the context","date":"2021-02-21T00:00:00.000Z","formattedDate":"February 21, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":0.72,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"result-variable","title":"Introducing @RC builtin variable","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Shared Fragments","permalink":"/blog/shared-fragments-intro"},"nextItem":{"title":"Introducing Select .. If Nothing","permalink":"/blog/select-if-nothing"}},"content":"We\'ve long needed a way to see the most recent SQLite result code SQLite in the context\\nof say a `catch` block (most other times you can assume SQLITE_OK was the last\\nresult code otherwise control flow would transfer elsewhere. Sometimes SQLITE_ROW\\nor SQLITE_DONE might be the current result code.\\n\\nSoon we\'ll provide a sample header that declares the most common error codes in an enum but\\nfor now you could do something like this:\\n\\n```sql\\n-- pasted from the sqlite.c\\n#define SQLITE_BUSY         5   /* The database file is locked */\\n\\n-- this is a contrived example\\ncreate proc get_first_foo(out can_retry bool not null)\\nbegin\\n\\n  -- can_retry is set to 0 automatically, language semantics guarantee this\\n\\n  begin try\\n    select foo from bar limit 1;\\n  end try;\\n  begin catch\\n    set can_retry := (@rc == SQLITE_BUSY);\\n    throw; -- rethrow the original error\\n  end catch;\\nend;\\n```"},{"id":"select-if-nothing","metadata":{"permalink":"/blog/select-if-nothing","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-02-14-select-if-nothing.md","source":"@site/blog/2021-02-14-select-if-nothing.md","title":"Introducing Select .. If Nothing","description":"The nested select statement is frequently misused, in particular if you get no rows back from the expression that\'s an error.  So for instance:","date":"2021-02-14T00:00:00.000Z","formattedDate":"February 14, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":1.175,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"select-if-nothing","title":"Introducing Select .. If Nothing","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing @RC builtin variable","permalink":"/blog/result-variable"},"nextItem":{"title":"Change in No-Result Semantics","permalink":"/blog/free-empty-results"}},"content":"The nested select statement is frequently misused, in particular if you get no rows back from the expression that\'s an error.  So for instance:\\n\\n```sql\\nset x_ := (select x from foo.x where id = y);\\n```\\n\\nThis will throw (with a peculiar error, `SQLITE_DONE`) if there is no such row.\\n\\nSometimes people try to fix this problem with a nullcheck:\\n\\n```sql\\nset x_ := IFNULL((select x from foo.x where id = y), -1);\\n```\\n\\nThat doesn\'t help at all.  It\'s not a null value situation, there\'s no row at all.\\n\\n```sql\\nset x_ := (select IFNULL(x,-1) from foo.x where id = y), -1);\\n```\\n\\nIs likewise unhelpful.  To help with this situation we add two forms:\\n\\n\\n```sql\\n-- useful if foo.x is already known to be not null\\nset x_ := (select x from foo.x where id = y IF NOTHING -1);\\n\\n-- useful if foo.x might be null\\nset x_ := (select x from foo.x where id = y IF NOTHING OR NULL -1);\\n```\\n\\nBoth of these deal with the case where there is no row.  The second lets you have a simple default for both\\nno row or null value.  That form is equivalent to:\\n\\n```sql\\nset x_ := (select IFNULL(x,-1) from foo.x where id = y IF NOTHING -1);\\n```\\n\\ni.e. both problem cases are handled.\\n\\nOf course the -1 here could be any valid expression, even a second `(select...)`"},{"id":"free-empty-results","metadata":{"permalink":"/blog/free-empty-results","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-02-10-free-empty-results.md","source":"@site/blog/2021-02-10-free-empty-results.md","title":"Change in No-Result Semantics","description":"Important change in CQL semantics.","date":"2021-02-10T00:00:00.000Z","formattedDate":"February 10, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":0.915,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"free-empty-results","title":"Change in No-Result Semantics","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Select .. If Nothing","permalink":"/blog/select-if-nothing"},"nextItem":{"title":"Introducing Type \\"Kinds\\"","permalink":"/blog/type-kinds-intro"}},"content":"Important change in CQL semantics.\\n\\nPreviously if you did an early return, or fall through the end, from a procedure that is supposed to return a result set\\nbut did not in fact provide one, you would get a fake SQLITE_ERROR.  Now you get an empty result set for \\"free\\".\\n\\nThis interpretation seems much more natural and avoids a lot of really annoying stub selects to comply with the contract.\\n\\nThis also works for the `out` statement in the same fashion.\\n\\nIf you want to return an error, use `throw`. This is a lot more natural...\\n\\nexamples:\\n\\n```sql\\n-- this gives you an empty result set if x <= 0\\ncreate proc maybe_return(x integer)\\nbegin\\n   if x > 0 then\\n     select * from foo where foo.y > x;\\n   end if;\\nend;\\n\\n-- so does this\\ncreate proc maybe_return(x integer)\\nbegin\\n  if x <= 0 then\\n     return;\\n  end if;\\n  select * from foo where foo.y > x;\\nend;\\n\\n-- so does this\\ncreate proc maybe_out(x integer)\\nbegin\\n  if x <= 0 then\\n    declare C cursor for select etc.\\n    out C;\\n  end if;\\nend;\\n```"},{"id":"type-kinds-intro","metadata":{"permalink":"/blog/type-kinds-intro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-01-20-type-kinds-intro.md","source":"@site/blog/2021-01-20-type-kinds-intro.md","title":"Introducing Type \\"Kinds\\"","description":"Further adding to the type calculus of the CQL language we introduced the ability to encode the \\"kind\\" of primitive types.  This","date":"2021-01-20T00:00:00.000Z","formattedDate":"January 20, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":2.38,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"type-kinds-intro","title":"Introducing Type \\"Kinds\\"","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Change in No-Result Semantics","permalink":"/blog/free-empty-results"},"nextItem":{"title":"Introducing Named Types","permalink":"/blog/named-types-into"}},"content":"Further adding to the type calculus of the CQL language we introduced the ability to encode the \\"kind\\" of primitive types.  This\\ncan be used in a number of ways -- like \\"units\\" for natural things and like a \\"type\\" for synthetic keys and other such.  It\'s easier\\nto illustrate by example.\\n\\n```\\ndeclare job_id type long<job_id>;\\ndeclare person_id type long<person_id>;\\n\\ndeclare j job_id;\\ndecalre p person_id;\\n\\nset p := j;  -- this is an error\\n```\\n\\nWith the above in place, other expressions like  `p == j` would also produce errors as these `long` values are no longer type compatible.  This is\\na great way to add enforcement to your schema and procedures.  Likewise you can use these annotations to add \\"units\\" to your data types.  e.g.\\n\\n```\\ndeclare meters type real<meters>;\\ndeclare grams type real<grams>;\\n\\ndeclare m meters;\\ndeclare g grams;\\n```\\n\\nVariables of type `grams` (e.g. `g`) are not compatible with variables of type `meters` (e.g. `m`) even though both are `real`.\\n\\nLikewise, attemping to insert `grams` into a column that is typed to `meters` will give errors.  Of course SQLite doesn\'t know about any of this\\nso all the `<>` stuff is removed in the generated SQL.  This is just about type enforcement at compile time.\\n\\nEnumerations like:\\n\\n```\\ndeclare enum surface integer (paper, canvas);\\ndeclare enum writer integer (pen, paper, brush);\\n```\\n\\nenable this:\\n\\n```\\ndeclare s surface;                  -- s is now of type integer<surface>\\ndeclare w writer;                   -- w is now of type integer<writer>\\nset s := surface.paper;             -- ok\\nset s := writer.pen;                -- error\\nset w := writer.pencil;             -- ok\\ncase when s == w then 1 else 0 end; -- error (w/s not comparable)\\nset w := s;                         -- error again\\n```\\n\\nadditionally in DML/DDL:\\n\\n```\\ncreate table draw_action(\\n  w writer,\\n  s surface\\n);\\n\\ninsert into draw_action values(w, s); -- ok\\ninsert into draw_action values(s, w); -- error!\\n```\\n\\nSo the type kinds can be quite helpful when dealing with loose variables.\\n\\nThe notion of specific types was added to the language nearly two years ago to support the `object` type because there was a great desire\\nto prevent `object<dictionary>` being assigned from `object<list>` but this \\"type kind\\", whether it\'s with units (e.g. \\"meters\\", \\"grams\\")\\nor a type name (e.g. \\"job_id\\") adds a lot of high value type checking.\\n\\nThe kind can be added, stripped, or changed with a `cast` operation and the type system allows a constant or variable with no kind (e.g. \\"1\\")\\nto mix and match with any kind so long as the base type is compatible as usual.  So you get the most value by using the specific type\\nconsistently but you won\'t go insane adding test cases that use constants for instance.\\n\\nAs of this writing the expression kinds are checked for compatibility everywhere plus or minus bugs.  There are extensive tests."},{"id":"named-types-into","metadata":{"permalink":"/blog/named-types-into","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2021-01-14-named-types-intro.md","source":"@site/blog/2021-01-14-named-types-intro.md","title":"Introducing Named Types","description":"A common source of errors in stored procedures is incorrect typing in arguments.  For instance, a particular key","date":"2021-01-14T00:00:00.000Z","formattedDate":"January 14, 2021","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":1.19,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"named-types-into","title":"Introducing Named Types","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Type \\"Kinds\\"","permalink":"/blog/type-kinds-intro"},"nextItem":{"title":"Introducing Virtual Tables","permalink":"/blog/virtual-table-into"}},"content":"A common source of errors in stored procedures is incorrect typing in arguments.  For instance, a particular key\\nfor an entity might need to be `LONG` or even always `LONG NOT NULL` or `LONG NOT NULL @SENSITIVE` and the only\\nway to do this in the past was maybe with some `#define` thing.  Otherwise you have to diligently get the type right\\nin all the places, and should it ever change, again you have to visit all the places.   To help with this situation,\\nand to make code a little more self-describing we add named types to the language.  This is a lot like `typedef` in\\nthe C language.  They do not create different incompatible types but do let you name things well.\\n\\nYou can now write these sorts of forms:\\n\\n```\\ndeclare foo_id type long not null;\\n\\ncreate table foo(\\n  id foo_id primary key autoincrement,\\n  name text\\n);\\n\\ncreate proc inserter(name_ text, out id foo_id)\\nbegin\\n  insert into foo(id, name) values(NULL, name_);\\n  set id := last_insert_rowid();\\nend;\\n```\\n\\nRefer to the [railroad diagram](https://cgsql.dev/program-diagram#declare_stmt) for the grammar details.\\n\\nAdditionally any enumerated type can be used as a type name.  e.g.\\n\\n```\\ndeclare enum thing integer (\\n  thing1,\\n  thing2\\n);\\n\\ndeclare x thing;\\n```\\n\\nEnumerations always get \\"not null\\" in addition to their base type.\\n\\nThis isn\'t a very complex feature but we hope that it will help create clearer code that is less likely to have type-related bugs."},{"id":"virtual-table-into","metadata":{"permalink":"/blog/virtual-table-into","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-12-16-virtual-table-intro.md","source":"@site/blog/2020-12-16-virtual-table-intro.md","title":"Introducing Virtual Tables","description":"Language support for virtual tables has lagged since I always thought they were of little interest to","date":"2020-12-16T00:00:00.000Z","formattedDate":"December 16, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"}],"readingTime":4.205,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"virtual-table-into","title":"Introducing Virtual Tables","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql"]},"prevItem":{"title":"Introducing Named Types","permalink":"/blog/named-types-into"},"nextItem":{"title":"Introducing  Argument Bundles","permalink":"/blog/arg-bungle-intro"}},"content":"Language support for virtual tables has lagged since I always thought they were of little interest to\\nthe language anyway. The `CREATE TABLE` forms in general are only declarations (except if you\'re doing\\nthe schema installer/upgrader output) and so you could just declare say a temp table that corresponds to the\\nvirtual table that you made in the same way that you might declare say `sqlite_master` if you wanted to use it.\\nAnd since you have to register the module anyway, you may as well create the virtual table at the same time.\\n\\nSo there was no point in adding language support for the thing.\\n\\nFurthermore the `CREATE VIRTUAL TABLE` form includes no information about the schema of the table so you\'d\\nneed some kind of declaration anyway in order to tell the language what the columns are for the table you\\njust created.  So again you may as well just declare it like a normal table and not include that table in your\\nschema upgrade file and be done with it.\\n\\nAnd that was my thinking for the last 2 years. And then I learned something.\\n\\nVirtual tables are durable.\\n\\nYeah, I always assumed that virtual tables were temp tables and they vanish and have to be redeclared every\\nsession but that is not the case.  They are part of the durable schema so while you must pre-register the\\nmodule associated with the virtual table, the virtual table is like other tables in that you only create it\\nonce and from then on it\'s part of the schema every time the database loads until you `DROP` it.\\n\\nThis changes everything.\\n\\nWith virtual tables being durable they belong in the schema upgrade process.  And if they go there they also\\nhave to go into the JSON output.  But we can\'t use the vanilla syntax that SQLite uses because that syntax is:\\n\\n* not parseable, because the module arguments can be literally anything (or nothing), even a letter to your grandma.\\n* the arguments do not necessarily say anything about the table\'s schema at all\\n\\nSo in the CQL language I change the syntax a bit, the generalized form looks like this:\\n\\n```\\ncreate virtual table virt_table using my_module [(module arguments)]  as (\\n  id integer not null,\\n  name text\\n);\\n```\\n\\nThe part after the `AS` is used by CQL as a table declaration for the virtual table.  The grammar for that\\nis exactly the same as a normal `CREATE TABLE` statement.  However that part is not transmitted to\\nSQLite; when the table is created, SQLite sees only the part it cares about, the part before the `AS`.\\n\\nNow this leaves the module arguments, they can be one of three things:\\n\\n1. no arguments at all\\n2. a list of identifiers, constants, and parenthesized sublists just like in the `@attribute` form\\n3. the words `arguments following`\\n\\n\\n### Case 1 Example\\n\\n```\\ncreate virtual table virt_table using my_module as (\\n  id integer not null,\\n  name text\\n);\\n```\\n\\nbecomes (to SQLite)\\n\\n```\\nCREATE TABLE virt_table USING my_module;\\n```\\n\\nNote: empty arguments `USING my_module()` are not allowed in the SQLite docs but do seem to work in SQLite.\\nWe take the position that no args should be done with no parens, at least for now.\\n\\n### Case 2 Example\\n\\n```\\ncreate virtual table virt_table using my_module(foo, \'goo\', (1.5, (bar, baz))) as (\\n  id integer not null,\\n  name text\\n);\\n```\\n\\n```\\nCREATE VIRTUAL TABLE virt_table USING my_module(foo, \\"goo\\", (1.5, (bar, baz)));\\n```\\n\\nThis form allows for very flexible arguments but not totally arbitrary arguments, so it can still be\\nparsed and validated.\\n\\n### Case 3 Example\\n\\nThis case recognizes the popular choice that the arguments are often the actual schema declaration\\nfor the table in question. So\\n\\n```\\ncreate virtual table virt_table using my_module(arguments following) as (\\n  id integer not null,\\n  name text\\n);\\n```\\n\\nbecomes\\n\\n```\\nCREATE VIRTUAL TABLE virt_table USING my_module(\\n  id INTEGER NOT NULL,\\n  name TEXT\\n);\\n```\\n\\nThe normalized text (keywords capitalized, whitespace normalized) of the table declaration in the `as` clause is used as the arguments.\\n\\n\\n### Other details\\n\\nVirtual tables go into their own section in the JSON and they include the `module` and `moduleArgs` entries, they are additionally\\nmarked `isVirtual` in case you want to use the same processing code for virtual tables as normal tables.  The JSON format is otherwise\\nthe same, although some things can\'t happen in virtual tables (e.g. there is no `TEMP` option so `\\"isTemp\\"` must be false in the JSON.\\n\\nFor purposes of schema processing, virtual tables are on the `@recreate` plan, just like indices, triggers, etc.  This is the only option since\\nthe `alter table` form is not allowed on a virtual table.\\n\\nSemantic validation enforces \\"no alter statements on virtual tables\\" as well as other things like, no indices, and no triggers, since SQLite\\ndoes not support any of those things.\\n\\nFinally, because virtual tables are on the `@recreate` plan, you may not have foreign keys that reference virtual tables. Such keys seem\\nlike a bad idea in any case."},{"id":"arg-bungle-intro","metadata":{"permalink":"/blog/arg-bungle-intro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-12-08-arg-bundle-intro.md","source":"@site/blog/2020-12-08-arg-bundle-intro.md","title":"Introducing  Argument Bundles","description":"There are many cases where stored procedures require complex arguments using data shapes well known","date":"2020-12-08T00:00:00.000Z","formattedDate":"December 8, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"errors","permalink":"/blog/tags/errors"}],"readingTime":5.565,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"arg-bungle-intro","title":"Introducing  Argument Bundles","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","errors"]},"prevItem":{"title":"Introducing Virtual Tables","permalink":"/blog/virtual-table-into"},"nextItem":{"title":"Introducing Declare Enum","permalink":"/blog/declare-enum-intro"}},"content":"There are many cases where stored procedures require complex arguments using data shapes well known\\nto higher level languages or that come from the schema.  There is already some affordance for this\\nsort of thing in the form of this kind of pattern:\\n\\n(I\'ll continue to use this simple example as I discuss the generalization below)\\n\\n```\\ncreate table Person (\\n   id text primary key,\\n   name text not null,\\n   address text not null,\\n   birthday real\\n);\\n```\\n\\nThen maybe something like this\\n\\n```\\ncreate proc insert_person(like Person)\\nbegin\\n    insert into Person from arguments;\\nend;\\n```\\n\\nThe above expands into:\\n\\n```\\ncreate proc insert_person(\\n    id_ text not null,\\n    name_ text not null,\\n    address_ text not null,\\n    birthday_ real)\\nbegin\\n    insert into Person(id, name, address, birthday)\\n        values(id_, name_, address_, birthday_);\\nend;\\n```\\n\\nAnd I think we can all agree the sugared version is a lot easier to reason about\\nand much less prone to errors as well.\\n\\nThose features have been in the language for a long time and that\'s all fine and well\\nbut it isn\'t general enough to handle the usual mix of situations.  For instance what\\nif you need a procedure that works with two people?  A hypothetical `insert_two_people`\\nprocedure cannot be written with the old form.  This is where argument bundles come in.\\nThe idea here is to name the bundle which provides useful reference.  To wit:\\n\\n```\\ncreate proc insert_two_people(p1 like Person, p2 like Person)\\nbegin\\n    call insert_person(from p1);\\n    call insert_person(from p2);\\nend;\\n```\\n\\nor alternatively\\n\\n```\\ncreate proc insert_two_people(p1 like Person, p2 like Person)\\nbegin\\n    insert into Person from p1;\\n    insert into Person from p2;\\nend;\\n```\\n\\nSo what\'s going on here?  Well, there are lots of reasons to keep the API to procedures simple\\nand adding general purpose structured types would be at odds with that.  It would require lots\\nof knowledge about C structure layout and whatnot.  And trying to call from java would require\\nvery complex JNI for any such procedure.  So we avoid all that.  We keep simple arguments.\\nThe above expands into:\\n\\n```\\ncreate proc insert_person(\\n    p1_id text not null,\\n    p1_name text not null,\\n    p1_address text not null,\\n    p1_birthday real,\\n    p2_id text not null,\\n    p2_name text not null,\\n    p2_address text not null,\\n    p2_birthday real)\\nbegin\\n    insert into Person(id, name, address, birthday)\\n        values(p1_id, p1_name, p1_address, p1_birthday);\\n    insert into Person(id, name, address, birthday)\\n        values(p2_id, p2_name, p2_address, p2_birthday);\\nend;\\n```\\n\\nOr course the types don\'t have to be the same, you can create and name shapes of your choice.  The language allow\\nyou to use an argument bundle in all the places that a cursor was previously a valid source.  That includes `insert`,\\n`fetch`, `update cursor`, and procedure calls.  You can refer to the arguments by their expanded name `p1_address`\\nor alternatively `p1.address` means the same thing.\\n\\nHere\'s another example showing a silly but illustrative thing you could do:\\n\\n```\\ncreate proc insert_lotsa_people(P like Person)\\nbegin\\n    declare C cursor like P;\\n    fetch C from P;\\n    declare i integer not null;\\n    set i := 0;\\n    while (i < 20)\\n    begin\\n        update cursor C using\\n            printf(\\"id_%d\\", i) id;\\n        insert into Person from C;\\n    end;\\nend;\\n```\\n\\nThe above shows that you can use a bundle as the source of a shape elsewhere, and you can\\nuse a bundle as a source of data to load a cursor.  After which you can do all the usual value cursor things\\nlike `out` statements and so forth.\\n\\nIn order to call procedures with argument bundles more readily from other languages, the JSON output now includes additional\\ninformation about where procedure arguments originated; The field with this information is creatively called \\"argOrigin:\\"\\nand it has 3 forms.\\n\\n* \\"arg_name\\" -> the argument is not an expansion of anything\\n* \\"T arg_name\\" -> the argument came from `like T`\\n   * there will be one arg for each member of T\\n   * the formal argument name for this arg will be arg_name_\\n   * if T is procedure arguments `like p1 arguments` then you\'ll get  \\"p1[arguments] arg_name\\"\\n* \\"name T arg_name\\" -> the argument came from `name like T` (a named bundle)\\n   * there will be one arg for each member of T\\n   * the formal argument name for this arg will be T_arg_name\\n   * T could be procedure arguments as above\\n* If the source of an argument was a cursor or argument bundle name you get instead that thing\'s shape source name\\n  * this is always better because cursor names and bundle names are not globally unique.\\n* If the cursor had an anonymous source (e.g. `like select 1 x`) then you get the useless shape name \\"select\\"\\n  * this is an indicator that you should make some ad hoc struct for this procedure because there is no useful name for the arg bundle\'s type\\n\\nNone of this matters unless you\'re trying to make wrappers for a CQL procedure for some other language\\nand you\'d like to have your wrapper deal with structs rather than all loose arguments.  the JSON\\nbasically tells you the structs.\\n\\nInterestingly, argument bundles resulted in a significant reduction of code in the compiler.  The argument bundle\\nname has to be usable in the contexts where a cursor was previously usable.  It is another source of shaped data.\\nGetting that to work  proved to be super simple as the two forms look almost identical to the compiler -- no coincidence there.\\nSo very little code was required to make `from [cursor_name]` work with `from [any_shape_name]` in the half dozen or so places\\nthat this construct is allowed (e.g. procedure call arguments, insert statements, etc.).  However, there was as\\nmuch code associated with `from arguments` as there was `from cursor_name`.  And the code was nearly identical..\\n\\nWhen argument bundles were introduced the natural thing to do was to create an artifical bundle called \\"arguments\\" which\\nrepresents the bundle that is ALL the arguments.  With that done, all the code for `from arguments` could be deleted\\nbecause `arguments` itself was a valid shape name.  Hence `insert into T from arguments` \\"just works\\".  And so half\\nthe rewrites were deleted.  The only cost was that the form `from arguments like shape` became the cursor form\\n`from arguments(like shape)` which only adds mandatory parens to a form that was largely unused anyway (there were two\\ncases in our entire codebase).  The cursor form is more general as you can do `from C(like A, like B)` to get the\\nfields that match `A` then those that match `B`.  Arguments get this for free as well (well, at the cost of parens).\\n\\nSo overall, this feature was added, and the compiler got smaller and cleaner.  Only the test suite had to grow.\\n\\nStay safe out there."},{"id":"declare-enum-intro","metadata":{"permalink":"/blog/declare-enum-intro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-12-03-declare-enum-intro.md","source":"@site/blog/2020-12-03-declare-enum-intro.md","title":"Introducing Declare Enum","description":"There is an unfortunate pattern of hard coding constants in SQL which I think comes from the","date":"2020-12-03T00:00:00.000Z","formattedDate":"December 3, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"errors","permalink":"/blog/tags/errors"}],"readingTime":4.765,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"declare-enum-intro","title":"Introducing Declare Enum","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","errors"]},"prevItem":{"title":"Introducing  Argument Bundles","permalink":"/blog/arg-bungle-intro"},"nextItem":{"title":"A quick tutorial on LIKE forms","permalink":"/blog/like-forms-tutorial"}},"content":"There is an unfortunate pattern of hard coding constants in SQL which I think comes from the\\nfact that there\'s not an especially good way to encode constants in SQL.  Things are a little\\nbetter In CG/SQL\'s CQL language because it\'s normal to run things through the pre-processor first\\nso you can do things like:\\n\\n```\\n#define BUSINESS_TYPE_RESTAURANT 1\\n#define BUSINESS_TYPE_LAUNDROMAT 2\\n```\\n\\nHaving done so, you could write:\\n\\n```\\ninsert into Business using\\n   \\"Rico\'s Laundry\\"  name,\\n   BUSINESS_TYPE_LAUNDROMAT type;\\n\\n-- by the time SQL sees this it becomes\\ninsert into Business(name, type) values(\'Rico\'\'s Laundry\', 2);\\n```\\n\\nAnd at least you don\'t have to see these loose \'2\' values all over. An especially unfortunate\\nform is the below, in which the auther is clearly crying for a symbol to use:\\n\\n```\\ninsert into Business using\\n   \\"Rico\'s Laundry\\"  name,\\n   2 type; /* laundromat */\\n```\\n\\nBut if we use `#define` the language knows nothing of the names and it can\'t help you manage them\\nor export them consistently or anything like that.  I guess `#define` is pretty useful in several\\nlangauges (C and C++) so you could maybe `#include` the macros somehow but that doesn\'t seem\\nespecially great.  And if you need them in Java you\'re getting no help at all.\\n\\nSo to this world we add enumerated constants.  This is a bit short of enumerated types as we\'ll\\nsee later.  You can now write something like this:\\n\\n```\\ndeclare enum business_type integer (\\n  restuarant,\\n  laundromat,\\n  corner_store = 11+3  /* math added for demo purposes only */\\n);\\n```\\n\\nAfter this:\\n\\n```\\nselect business_type.corner_store;\\n```\\nis the same as\\n\\n```\\nselect 14;\\n```\\n\\nAnd that is exactly what SQLite will see, the literal 14.\\n\\nWhat\'s going on here?  There\'s just a few rules:\\n\\n* the enumeration can be any numeric type (bool, integer, long integer, real)\\n* the values of the enumeration start at 1 (i.e. if there is no `= expression` the first item will be 1, not 0)\\n* if you don\'t specify a value, the next value is the previous value + 1\\n* if you do specify a value it can be any constant expression and it will be cast to the type of the enumeration (even if thatis lossy)\\n* the enumeration can refer to previous values in itself with no qualification `(big = 100.0, medium = big/2, small = medium/2)`\\n* the enumeration can refer to previously defined enumerations as usual `(code = business_type.restaurant)`\\n* Once the enumeration is defined you refer to its members in a fully qualified fashion `enum_name.member_name` elsewhere\\n\\nWhy is this better than macros?  Well for one thing the enum values can be checked at their declaration site, so if you\\nhave errors you will hear about them in a more reasonable place.  But additionally since the structure is known to the\\ncompiler it can give you useful information in the outputs.\\n\\nIn the `.h` files you get:\\n\\n```\\nenum business_type {\\n  business_type__restaurant = 1,\\n  business_type__laundromat = 2,\\n  business_type__corner_store = 14\\n};\\n```\\n\\nIn case of floating point values such as:\\n\\n```\\ndeclare enum floating real (\\n  one = 1.0,\\n  two = 2.0,\\n  e = 2.71828,\\n  pi = 3.14159\\n);\\n```\\n\\nYou get:\\n\\n```\\n// enum floating (floating point values)\\n#define floating__one 1.000000e+00\\n#define floating__two 2.000000e+00\\n#define floating__e 2.718280e+00\\n#define floating__pi 3.141590e+00\\n```\\n\\nWhich is unfortunately the best you can do since C has no floating point enums.\\n\\nBut in both cases the `enums` section of the JSON has the name of the enums and their members and values ready to go.\\nWith these values you can readily generate (with moustache or something) the language interfaces of your choice.  This\\nis a real help if you\'re trying to make helpers to call your CQL from say Java or something.\\n\\nTo do all this we needed to add some constant folding and general evaluation to the compiler.  It\'s not much,\\njust the normal numeric types and null.  The supported operations include:\\n\\n`+`, `-`, `*`, `/`, `%`, `|`, `&`, `<<`, `>>`, `~`, `and`, `or`, `not`, `==`, `<=`, `>=`, `!=`, `<`, `>`, the `cast` operator\\nand the `case` forms.  These are enough to make a lot of very interesting expressions, all of which are envaluated at\\ncompile time.\\n\\nWhile the constant folding was added to allow for rich `enum` expressions, there is also the `const()` primitive in the\\nlanguage now which can appear anywhere a literal could appear.  This allows you do things that were previously not\\nallowed such as:\\n\\n```\\ncreate table something(\\n  x integer default const((1<<16)|0xf) /*  again the math is just for illustration */\\n);\\n```\\n\\nThe `const` form is also very useful in macros:\\n\\n```\\n#define SOMETHING const(12+3)\\n```\\nThis form ensures that the constant will be evaluated at compile time. Const can also also nest so you can build these\\nkinds of macros from other macros or you can build enums this way. Anywhere you might need literals, you can use `const`.\\n\\nImportantly, no enumerated data types were added to the language to do any of this.  The values can help you to\\nachieve some correctness by avoiding transcription mistakes but there is no additional type-safety provided here.\\nIndeed given the rich mix between these types in SQLite, and with SQLite having no knowledge of enumerations at\\nall it would be tricky to do a complete job.  Still, this might happen in the future.\\n\\nBut for now, declaring constants that are really an intimate part of your schema is now possible and the addition\\nof the constants to the `.h` files and the `.json` output should hopefully make these generally useful.  At least\\nwe might see less of the hard-coded constant business with good values baked right into the schema declarations.\\n\\nHappy Holidays."},{"id":"like-forms-tutorial","metadata":{"permalink":"/blog/like-forms-tutorial","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-11-20-like-forms-tutorial.md","source":"@site/blog/2020-11-20-like-forms-tutorial.md","title":"A quick tutorial on LIKE forms","description":"Everyone knows the usual expression syntax x LIKE y to do a string match.  But the CQL compiler also uses","date":"2020-11-20T00:00:00.000Z","formattedDate":"November 20, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"errors","permalink":"/blog/tags/errors"}],"readingTime":7.61,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"like-forms-tutorial","title":"A quick tutorial on LIKE forms","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","errors"]},"prevItem":{"title":"Introducing Declare Enum","permalink":"/blog/declare-enum-intro"},"nextItem":{"title":"Error Tracing Helper Macro","permalink":"/blog/error-tracing-macro"}},"content":"Everyone knows the usual expression syntax `x LIKE y` to do a string match.  But the CQL compiler also uses\\n`LIKE` in a different way that\'s powerful and important.  CQL has the notion of data shapes and you use\\n`LIKE` to refer to them.  The simplest source of a data shape, and maybe the most common, is a table.\\nMaybe something like this:\\n\\n```sql\\ncreate table T(\\n id integer not null,\\n name text not null,\\n age integer not null\\n);\\n```\\n\\nNow suppose you want to write a procedure that can insert a row into that table, You could write\\n\\n```sql\\ncreate proc insert_into_T(\\n  id_ integer not null, \\n  name_ text not null, \\n  age_ integer not null\\n)\\nbegin\\n  insert into T(id, name, age)  values(id_, name_, age_);\\nend;\\n```\\n\\nThis is all fine and well but what if T had 50 columns?  That gets old fast.  And how can you\\nbe sure that you inserted the columns into T in the right order?  This second example also compiles\\neven though it\'s clearly wrong:\\n\\n```\\n  insert into T(id, name, age) values(age_, name_, id_);\\n```\\n\\nAnd of course you can imagine things get only more complicated with more columns in T.\\n\\nWe started adding the `LIKE` form to ease these issues and to ensure some consistency in the APIs while preventing\\nsimple transpostion errors.  So you can instead write:\\n\\n```\\ncreate proc insert_into_T(like T)\\nbegin\\n  insert into T from arguments;\\nend;\\n```\\n\\nso here the `like T` in the argument list simply means \\"make arguments that are the same as the columns of table T\\" -- well,\\nalmost. It also adds an `_` to the end of each name so you end up exactly the same declaration as the long form above.\\nBut you won\'t miss any arguments, and they\'ll be in the right order.\\n\\nAnd notice that we used `from arguments` to indicate that we wanted the values to come from the arguments in order. Again\\nthis saves you from a lot of typing and a lot of error checking.  You can\'t get the arguments in the wrong order.\\n\\nThese are the most basic patterns. But there are quite a few more.\\n\\nLet\'s suppose you want to write a procedure that returns in row with the highest age in the above.  Maybe you write\\nsomething like this:\\n\\n\\n```sql\\ncreate proc highest_age()\\nbegin\\n  declare C cursor for select * from T;\\n  declare M cursor like C;\\n  loop fetch C\\n  begin\\n     if (not M or M.age < C.age) then\\n       fetch M from C;\\n     end if;\\n  end;\\n  out M;\\nend;\\n```\\n\\nHere we made a cursor `M` that is the same as the cursor `C` and then we are going to generate a single row result\\nfrom the cursor.  Note that if you use a cursor name like `M` in an expression it refers to the hidden boolean\\nthat says if the cursor has a row in it or not.  So `M` begins empty and we will load it if it\'s empty or if the age\\nis higher than what we\'ve already got.\\n\\nLet\'s show a few more of the forms.  Suppose we don\'t want to return `name`, just the `id` and the `age`.  We can\\nchange things up a tiny bit.\\n\\n```sql\\ncreate proc highest_age()\\nbegin\\n  declare C cursor for select * from T;\\n  declare M cursor like select 1 id, 99 age;\\n  loop fetch C\\n  begin\\n     if (not M or M.age < C.age) then\\n       fetch M from cursor C(like M);\\n     end if;\\n  end;\\n  out M;\\nend;\\n```\\n\\nSo two things to notice.  We used an *ad hoc* shape, making a fake `select` statement that returns the shape we want.  This\\nselect doesn\'t run but it does define types and columns easily.  Two not null integers in this case.  Now `M` is not the\\nsame as `C` so we can\'t use the simplest form `fetch M from C` we have to use the more general form. \\n\\nFully expanded, what we wrote becomes:\\n\\n```sql\\n  FETCH M(id, age) FROM VALUES(C.id, C.age);\\n```\\n\\nBut as you can see, we didn\'t have to type all those column names.  And that\'s kind of the point of the `LIKE` construct.\\n\\nSo we\'ve covered a bunch of the shape sources already:\\n\\n* a table name\\n* a cursor name\\n* a select statement that gives the shape in an ad hoc fashion\\n\\nThere are three more\\n\\n* a view name \\n* the return shape of a procedure that returns a result set\\n* the arguments of a procedure\\n\\nView names are pretty simple, and they work the same as table names so we don\'t need to discuss those. Let\'s look\\nat some of the other uses with procedures.\\n\\nSuppose we have a procedure that can return a result set shape but we want to be able to mock its results so we\\ncan fake whatever result we need for testing.  \\n\\nWe\'ll complicate this a bit adding a new table (keeping short table names for the sample to save typing)\\n\\n```sql\\ncreate table U(\\n id integer not null,\\n email text not null\\n);\\n```\\n\\nAnd here\'s a procedure:\\n\\n```sql\\ncreate proc my_proc()\\nbegin\\n   select T.*, U.email from T inner join U on T.id = U.id;\\nend;\\n```\\n\\nNow we want to be able to make any fake result we want, so maybe want a temp table. No problem:\\n\\n```sql\\ncreate proc _init_fake_results()\\nbegin\\n  create temp table if not exists fake_results(\\n   like my_proc\\n  );\\nend;\\n\\ncreate proc add_fake_result(like fake_results)\\nbegin\\n  insert into fake_results from arguments;\\nend;\\n\\ncreate proc get_fake_results()\\nbegin\\n  select * from fake_results;\\nend;\\n```\\n\\nThe above is very generic and will maintain well.  You can see we made a temp table that will have\\nexactly the same shape as whatever `my_proc` returns.  In this case it becomes:\\n\\n```sql\\nCREATE PROC _init_fake_results ()\\nBEGIN\\n  CREATE TEMP TABLE IF NOT EXISTS fake_results(\\n    id INTEGER NOT NULL,\\n    name TEXT NOT NULL,\\n    age INTEGER NOT NULL,\\n    email TEXT NOT NULL\\n  );\\nEND;\\n```\\n\\nAnd the rest are patterns we\'ve seem before.\\n\\nThe last source of shapes are procedure arguments.  There\'s lots of good cases for those, I wrote an [entry](https://cgsql.dev/blog/update) on those previously but I\'ll give a simple example here too.\\n\\nSuppose we have this weird procedure:\\n\\n```sql\\ncreate proc delete_stuff(age_ integer, name_ text)\\nbegin\\n  if age_ is not null then\\n     delete from T where T.age = age_;\\n  end if;\\n\\n  if name_ is not null then\\n     delete from T where T.name = name_;\\n  end if;\\nend;\\n```\\n\\nWhat if we wanted to log any errors that happen here?  Maybe make a verison that logs.  We can do it like this:\\n\\n```sql\\ncreate proc delete_and_log(like delete_stuff arguments)\\nbegin\\n  begin try\\n    call delete_stuff(from arguments);\\n  end try;\\n  begin catch\\n    call printf(\\"delete failed\\\\n\\"); -- or whatever\\n    throw;\\n  end catch;\\nend;\\n```\\n\\nThe nice thing about this logging wrapper procedure is that if `delete_stuff` changes, the wrapper will change with it.\\n\\nThat covers all of the shape sources and as we saw we can use them to create things like cursors, tables, and argument lists.\\nWe can use them to specify a subset of columns that might be of interest when fetching or updating cursors.  And we can use\\nthem in one last way -- to restrict arguments to a particular shape.  Let\'s see how that works by making the previous logger\\na little different.  Here we added an argument which tells if we should look.  And that might look like it would\\nspoil the `from arguments` part of the forwarding, but there is the final way to use `LIKE`.\\n\\n```sql\\ncreate proc delete_and_log2(log bool not null, like delete_stuff arguments)\\nbegin\\n  if log and age_ is not null then\\n    call printf(\\"deleting %d\\\\n\\", age_); -- or whatever\\n  end if;\\n  if log and name_ is not null then\\n    call printf(\\"deleting %d\\\\n\\", name_); -- or whatever\\n  end if;\\n\\n  call delete_stuff(from arguments like delete_stuff arguments);\\nend;\\n```\\n\\nSo this form lets you use some of your arguments, the ones that match a certain shape.  And as we saw in\\nthe previous article you can also use `from C` to pass arguments where `C` is a cursor and in that case\\nyou can also specify that arguments be matched by name `from C like shape`.  In both those cases the\\nformal parameter names of the called procedure are matched against the names of the shape and passed in\\nthe order of the formals.  So this is like \\"call by name\\", the fields of the cursor or the order of\\narguments in the argument list might be different than the formals but you\'ll get the correct items\\nin the correct order regardless, because it matches by name.\\n\\nThese forms can save you a lot of typing... and are excellent at avoiding errors and improving maintainability.\\nWhere they appear in SQL statements, everything is expanded before it goes to SQLite so SQLite will see\\nnormal syntax forms.  Which is good because obviously SQLite doesn\'t know anything about this enhanced\\n`LIKE` business.\\n\\nIn the examples above there were only one or two columns with very short names, but in real world code\\nthere can easily be dozens of columns with very long names.  In those cases, these forms really shine."},{"id":"error-tracing-macro","metadata":{"permalink":"/blog/error-tracing-macro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-11-18-error-tracing-macro.md","source":"@site/blog/2020-11-18-error-tracing-macro.md","title":"Error Tracing Helper Macro","description":"Following up on the last blog entry, I thought it would be useful to present a simple error tracing macro that you can use","date":"2020-11-18T00:00:00.000Z","formattedDate":"November 18, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"errors","permalink":"/blog/tags/errors"}],"readingTime":2.33,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"error-tracing-macro","title":"Error Tracing Helper Macro","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","errors"]},"prevItem":{"title":"A quick tutorial on LIKE forms","permalink":"/blog/like-forms-tutorial"},"nextItem":{"title":"Introducing General Purpose Error Tracing","permalink":"/blog/error-tracing-intro"}},"content":"Following up on the last blog entry, I thought it would be useful to present a simple error tracing macro that you can use\\nto see what kind of error flow is going on when you\'re having trouble understanding why a procedure is returning\\nan error code. The idea is we want to create a macro that we can use like this:\\n\\n```\\nBEGIN_VERBOSE_STDERR_TRACING;\\n\\n-- Some procedure(s) that you want to trace\\n\\nEND_VERBOSE_STDERR_TRACING;\\n```\\n\\nWe can do that with something like the below macros.  These particular ones cause the output to go to `stderr` via `fprintf` but if that isn\'t what you need you can simply edit the macro. The macros looks like this:\\n\\n```\\n-- manually force tracing on by redefining the cql_error_trace macro\\n#define BEGIN_VERBOSE_STDERR_TRACING \\\\\\n    @echo c, \\"#undef cql_error_trace\\\\n\\"; \\\\\\n    @echo c, \\"#define cql_error_trace() fprintf(stderr, \\\\\\"CQL Trace at %s:%d in %s: %d %s\\\\\\\\n\\\\\\", __FILE__, __LINE__, _PROC_, _rc_, sqlite3_errmsg(_db_))\\\\n\\"\\n\\n#define END_VERBOSE_STDERR_TRACING \\\\\\n    @echo c, \\"#undef cql_error_trace\\\\n\\"; \\\\\\n    @echo c, \\"#define cql_error_trace()\\\\n\\"\\n```\\n\\n\\nSo basically it\'s telling CQL to emit a `#define` into its output stream.  In this case:\\n\\n```\\n#define cql_error_trace() fprintf(stderr, \\"CQL Trace at %s:%d in %s: %d %s\\\\n\\", __FILE__, __LINE__, _PROC_, _rc_, sqlite3_errmsg(_db_))\\n```\\n\\nYou could change that to any function you like, you can have it dump the errors where you like, or you can make it some dummy function you add so that you can set a breakpoint on it.\\n\\nWhatever you do, do not leave your code with this sort of tracing enabled -- it\'s far too expensive in terms of code size.  But it\'s perfect if you have this one procedure that is failing and it\'s hard for you to see where.\\n\\nObviously if you\'re making a custom trace thingy you don\'t need the macro at all, you can just emit your own `#define` with `@echo` as needed.\\n\\nNote: `@echo` is quite a sledgehammer so don\'t use it lightly and not in production code but it is quite helpful for this sort of thing.  CQL tests often use it to help make things visible to the tests.  If you use `@echo` in weird ways you might not get working code when the codegen changes in the future.\\n\\nThe relevant state that is available to you inside a macro like this is:\\n\\n* `__FILE__` the current filename (comes from the C pre-processor, this is the .c file name not the .sql)\\n* `__LINE__` the current line number (comes from the C pre-processor, this is the .c line number)\\n* `_rc_` the current SQLite result code (always the current return code in every CQL procedure that uses SQLite)\\n* `_db_` the current SQLite database pointer (always the current database in every CQL procedure that uses SQLite)\\n* `_PROC_` the current procedure name (CQL has a `#define` for this for you)"},{"id":"error-tracing-intro","metadata":{"permalink":"/blog/error-tracing-intro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-11-16-error-tracing.md","source":"@site/blog/2020-11-16-error-tracing.md","title":"Introducing General Purpose Error Tracing","description":"Today we made a couple of minor changes in the code generation to take care of some lingering issues.","date":"2020-11-16T00:00:00.000Z","formattedDate":"November 16, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"errors","permalink":"/blog/tags/errors"}],"readingTime":2.87,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"error-tracing-intro","title":"Introducing General Purpose Error Tracing","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","errors"]},"prevItem":{"title":"Error Tracing Helper Macro","permalink":"/blog/error-tracing-macro"},"nextItem":{"title":"More Flexible Cursor Patterns Using \\"Boxing\\"","permalink":"/blog/boxed-cursors-intro"}},"content":"Today we made a couple of minor changes in the code generation to take care of some lingering issues.\\n\\nThe first is that when you did a `throw` inside a `catch` to basically rethrow the error, you would lose\\nthe error code if something had succeeded within the catch handler.\\n\\nThe old codegen looked something like this:\\n\\n```c\\n  catch_start_1: {\\n    printf(\\"error\\\\n\\");\\n    cql_best_error(&_rc_)\\n    goto cql_cleanup;\\n  }\\n```\\n\\nThe problem being that while the `printf` above is fine and well, if you did any SQL operation then `_rc_` would be\\nclobbered and you\'d end up throwing an unrelated error code.   `cql_best_error` would at least make sure it was\\na failure code (`SQLITE_ERROR`) but the original error code was lost.\\n\\nThe new code looks like this:\\n\\n```c\\n  catch_start_1: {\\n    _rc_thrown_ = _rc_;\\n    printf(\\"error\\\\n\\");\\n    _rc_ = cql_best_error(_rc_thrown_);\\n    goto cql_cleanup;\\n  }\\n```\\n\\nSo now if there are db operations, the original return code is still preserved.  Note:  you still lose `sqlite3_errmsg()` because\\nSQLite doesn\'t know that cleanup logic is running.\\n\\nThis brings us to the second new thing: general purpose error traces.\\n\\nError checking of result codes happens very consistently in CQL output.  The usual pattern looks something like this:\\n\\n```c\\n  _rc_ = cql_exec(_db_,\\n    \\"SAVEPOINT base_proc_savepoint\\");\\n  if (_rc_ != SQLITE_OK) goto cql_cleanup;\\n```\\n\\nor if it\'s inside a try block a little different... very little actually\\n\\n```c\\n  // try\\n  {\\n    _rc_ = cql_exec(_db_,\\n      \\"RELEASE SAVEPOINT base_proc_savepoint\\");\\n    if (_rc_ != SQLITE_OK) goto catch_start_8;\\n    // ... the rest of the try block\\n  }\\n```\\n\\nBasically if the local `_rc_` doersn\'t match the necessary condition we `goto` the appropriate error label... either the relevant\\ncatch block or else the procedure\'s cleanup code.\\n\\nWe generalize this a bit now so that it looks like this:\\n\\n\\n```c\\n  if (_rc_ != SQLITE_OK) { cql_error_trace(); goto cql_cleanup; }\\n\\n-- or, in a catch...\\n\\n  if (_rc_ != SQLITE_OK) { cql_error_trace(); goto catch_start_8; }\\n```\\n\\nNow the default implementation of `cql_error_trace()` is in `cqlrt.h` which you can and should customize. I\'ll be writing more\\nabout that later but suffice to say you\'re supposed to replace `cqlrt.h` and `cqlrt.c` with suitable runtime helpers for your environment\\nwhile keeping `cqlrt_common.h` and `cqlrt_common.c` fixed.\\n\\nSo for instance, your `cqlrt.h` could look like this:\\n\\n\\n```c\\n#ifndef CQL_TRACING_ENABLED\\n#define cql_error_trace()\\n#else\\n// whatever tracing you want, for example this might help in test code.\\n#define cql_error_trace() \\\\\\n  fprintf(stderr, \\"Error at %s:%d in %s: %d %s\\\\n\\", __FILE__, __LINE__, _PROC_, _rc_, sqlite3_errmsg(_db_))\\n#endif\\n```\\n\\nSo then when you need to debug problems involving lots of error recovery you can watch the entire chain of events easily.\\n\\nNote that there are some useful variables there:\\n\\nIn any procedure `_db_` is the current database and `_rc_` is the most recent return code from SQLite.  `__FILE__` and `__LINE__`\\nof course come from the preprocessor.  and `_PROC_` (one underscore) is now generated by the compiler.  Every procedure\'s\\nbody now begins with:\\n\\n```\\n#undef _PROC_\\n#define _PROC_ \\"the_current_procedure\\"\\n```\\n\\nSo by defining your own cql_error_trace macro you can cause whatever logging you need to happen.  Note this can be\\nvery expensive indeed because this happens a lot and even the string literals needed are a significant cost. So generally\\nthis should be off for production builds and enabled as needed for debug builds.\\n\\nThe default implementation is just an empty block\\n\\n```\\n#define cql_error_trace()\\n```\\n\\nBut the hook is enough to light up whatever logging you might need, and you can use `sqlite3_errmsg()` before that message is gone.\\n\\nGood hunting."},{"id":"boxed-cursors-intro","metadata":{"permalink":"/blog/boxed-cursors-intro","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-11-15-boxed-cursors.md","source":"@site/blog/2020-11-15-boxed-cursors.md","title":"More Flexible Cursor Patterns Using \\"Boxing\\"","description":"I was reviewing the update posting that just went out and I realized I\'d forgotten to mention another big ticket item. So","date":"2020-11-15T00:00:00.000Z","formattedDate":"November 15, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"cursors","permalink":"/blog/tags/cursors"}],"readingTime":3.66,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"boxed-cursors-intro","title":"More Flexible Cursor Patterns Using \\"Boxing\\"","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","cursors"]},"prevItem":{"title":"Introducing General Purpose Error Tracing","permalink":"/blog/error-tracing-intro"},"nextItem":{"title":"One Month Update","permalink":"/blog/update"}},"content":"I was reviewing the update posting that just went out and I realized I\'d forgotten to mention another big ticket item. So\\nconsider this an appendix to the update.\\n\\nIn some cases we started seeing a need to \\"ship cursors around\\" a little bit more flexibly.\\nNote shipping values around is already doable so this new work is largely about being able to create a \\"statement cursor\\"\\nin one procedure and consume it safely elsewhere.  The general pattern looks like this:\\n\\nDeclare a statement cursor as usual, maybe something like this:\\n\\n```sql\\ndeclare C cursor for select * from shape_source;\\n\\n-- or\\n\\ndeclare C cursor for call proc_that_returns_a_shape();\\n```\\n\\nMake an object that can hold a cursor:\\n\\n```sql\\ndeclare obj object<T cursor>;\\n```\\n\\nWhere `T` is the name of a shape. It can be a table name, or a view name, or it can be the name of the canonical procedure that returns the result.  You really want this to be  some kind of global name though.  Something you can get with a `#include` in various places. In this case choices for `T` might be `shape_source` the table or `proc_that_returns_a_shape` the procedure.\\n\\nRemember you can always make a fake procedure that returns a result to sort of typedef a shape name.  e.g.\\n\\n```sql\\ndeclare proc my_shape() (id integer not null, name text);\\n```\\n\\nThe procedure here `my_shape` doesn\u2019t have to actually ever be created, in fact it\u2019s probably better if it doesn\u2019t.  You won\u2019t call it, you\u2019re just using its hypothetical result as a shape.  This could be useful if you have several procedures like `proc_that_returns_a_shape` that all return `my_shape`.\\n\\n\\nAt this point you could use the cursor maybe something like:\\n\\n```sql\\nloop fetch C\\nbegin\\n  -- do stuff with C\\nend;\\n```\\n\\nThose are the usual patterns and they let you consume statement cursors sort of \u201cup\u201d from where it was created, but what if you want some worker procedures that consume a cursor there is no good way to pass your cursor down again.  Well, there wasn\'t. Now there is.  Let\'s go back to that box object creation and use it\\n\\n```sql\\n-- recap: declare the box that holds the cursor (T changed to my_shape for this example)\\ndeclare obj object<my_shape cursor>;\\n\\n-- box the cursor into the object (the cursor shape must match the box shape)\\nset obj from cursor C;\\n```\\n\\nThe variable `obj` can now be passed around as usual.  Then, later, you can \\"unbox\\" it to get a cursor back. Like so\\n\\n```sql\\n-- unboxing a cursor from an object\\ndeclare D cursor for obj;\\n```\\n\\nThese primitives will allow cursors to be passed around with managed lifetime.\\nExample:\\n\\n```sql\\n-- consumes a cursor\\ncreate proc cursor_user(box object<my_shape cursor>)\\nbegin\\n   declare C cursor for box;  -- the cursors shape will be my_shape matching box\\n   loop fetch C\\n   begin\\n      -- do something with C\\n   end;\\nend;\\n\\n-- captures a cursor and passes it on\\ncreate proc cursor_boxer()\\nbegin\\n   declare C cursor for select * from something_like_my_shape;\\n   declare box object<my_shape cursor>\\n   set box from cursor C; -- produces error if shape doesn\'t match\\n   call cursor_user(box);\\nend;\\n```\\n\\nImportantly, once you box a cursor the underlying SQLite statement\u2019s lifetime is managed by the box object with normal\\nretain/release semantics so timely release becomes imperative.\\n\\nWith this pattern it\'s possible to, for instance, consume some of the rows in one procedure and the rest in another procedure.\\n\\nNow, the main reason for doing this is if you have some standard helper methods that can get a cursor from a variety of places and process it.\\nBut remember, that boxing isn\u2019t the usual pattern at all and returning cursors in a box, while possible, should be avoided in favor of the simpler\\npattern of doing your `select` or `call` at the end to compute the result as we do now, if only because then then lifetime is very simple in all those cases.\\nDurably storing a boxed cursor could lead to all manner of problems -- it\'s just like holding on to a `sqlite3_stmt *` for a long time.\\nActually \\"just like\\" is an understatement, it\'s *exactly* the same as holding on to a statement for a long time with all the same problems because that\\nis exactly what\'s going on here.\\n\\nSo, good generalization, but possibly less Pit of Success, especially with complex box patterns.  So watch the sharp edges."},{"id":"update","metadata":{"permalink":"/blog/update","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-11-12-update.md","source":"@site/blog/2020-11-12-update.md","title":"One Month Update","description":"It\'s hard to believe it\'s been a month since the welcome message went up. We were","date":"2020-11-12T00:00:00.000Z","formattedDate":"November 12, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"update","permalink":"/blog/tags/update"}],"readingTime":8,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"update","title":"One Month Update","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","update"]},"prevItem":{"title":"More Flexible Cursor Patterns Using \\"Boxing\\"","permalink":"/blog/boxed-cursors-intro"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"It\'s hard to believe it\'s been a month since the welcome message went up. We were\\nhappy to see interest right away and even a bunch of forks but most of all\\npull requests.  A sweeping change to modernize the cql.y grammar was much\\nappreciated.  That `$1` stuff was very old school (I\'m showing my age now).\\n\\n## Here\'s a quick summary of what\'s been going on:\\n\\n* @mingodad gave us an implementation of check and collate column attributes (the check attribute on tables should be easy to add from here)\\n* the `select function` form should never return objects, only SQLite types, enforced\\n* @attribute(cql:suppress_result_set) was added to save code gen for procedures that don\'t need the C result set wrappers\\n* `cql_cursor_diff_col` and `cql_cursor_diff_val` methods were added to report what\'s different about two cursors (highly useful in test code)\\n* `cql_cursor_format` was added so you can quickly convert any cursor into columns and values as string for debug output (no matter the shape)\\n* `sqlite3_changes` was added to the builtin list so you don\'t have to use `declare select function` to use it anymore\\n* `cql_get_blob_size` was added so you can see how big your blobs are (useful for diagnostics)\\n* `trim`, `rtrim` and `ltrim` were added to the builtin list so you can use them without `declare select function`\\n* the builtin function `ifnull_crash` was added so that nullables that have already checked can be safely typecast to not null\\n* the bug we saw in demo video number 2 where some foreign keys were not properly linked up in autotest code was fixed (yay videos)\\n* time functions are now known to be `not null` for a bunch of simple cases such as \'now\' arguments\\n* you can use the `cast(.. as ..)` operator on numeric types outside of the SQL context\\n* @mingodad replaced all the positional references by named references in cql.y (yes! thank you!)\\n* several minor bug fixes\\n* the railroad diagrams were updated\\n\\nNOTE: I often refer to \\"sugar\\" in the below.  This is short for syntatic sugar which, in case you\'re not familiar with the term, refers to a syntatically more pleasing way of writing a concept that is otherwise totally doable with normal syntax.  Many languages have sugar for forms that are common -- for brevity, clarity, and/or correctness.\\n\\n## And now a few notes on The Big Stuff\\n\\nWe often add new features to the language to facilitate the writing of tests. The tests have a lot of boilerplate often setting up\\nand calling the same procedures again and again with slightly different arguments. Long argument lists and long insert column\\nlists are especially problematic as these can be very error prone. Here good language constructs are very helpful.\\nWe\'ve found good test constructs are often invaluable in production code as well, though in our experience the\\ntests often have a lot more repitition that needs refactoring than production code. To that end we added some very useful things\\nin the last month:\\n\\n### Declare cursors in the shape of a procedure\'s arguments and use them\\n\\nThe most common way to create a cursor is from a `select` statement but you can also make a cursor that can hold values for you\\nby declaring it to be `LIKE` something else with a shape.  A classic example is:\\n\\n```sql\\ndeclare C cursor like some_table;\\n```\\n\\nNow `C` has the same columns and types as `some_table`\\n\\nMany procedures have a result type that is also a shape, for instance any procedure that ends with a `select` statement has a result\\nshape defined by the columns of the select statement.  You could always do this sort of thing:\\n\\n```sql\\ndeclare C cursor like some_proc;\\n```\\n\\nMeaning make `C` a cursor whose shape is whatever `some_proc`returns, which is of course exactly the kind of cursor you need to capture\\nthe result of `some_proc`.\\n\\nNow we add:\\n\\n```sql\\ndeclare C cursor like some_proc arguments;\\n```\\n\\nThe idea being that the arguments of `some_proc` are also a shape (unless it has none). With this done you want to use that cursor\\nto call the procedure -- that being sort of the whole point.  So we add this:\\n\\n```sql\\ncall some_proc(from C);\\n```\\n\\nHow do we use this effectively?  Hold on just a second -- for that answer we need one more big tool to really help the syntax.\\n\\n### Loading cursors and inserting columns\\n\\nLoading up a cursor is done with syntax that is very much like an `insert` statement.  An example might be something like this:\\n\\n```sql\\nfetch C(x,y,z) from values(1,2,3);\\n```\\n\\nThis is simple enough but it becomes more problematic if there are many values and especially if the values have complex names.\\nTo make this a little less error prone CQL now has this sugar form for `fetch`, `insert`, and soon `update cursor` (like maybe\\nbefore you see this blog).  The more readable form is:\\n\\n```sql\\nfetch C using\\n  1  x,\\n  2  y,\\n  3  z;\\n```\\n\\nThis form has the values next to their names just like in a select statement, like all sugars, it is automatically rewritten to the normal form.\\n\\nLikewise\\n\\n```sql\\ninsert into some_table using\\n  1            id,\\n  \'fred\'       first_name,\\n  \'flintstone\' last_name,\\n  \'bedrock\'    home_town,\\n  \'dino\'       favorite_pet,\\n  \'wilma\'      life_partner;\\n```\\n\\nbecomes\\n\\n```sql\\ninsert into some_table(id, first_name, last_name, home_town, favorite_pet, life_partner)\\n  values(1, \'fred\', \'flintstone\', \'bedrock\', \'dino\', \'wilma\');\\n```\\n\\nexcept the sugar form is much less error prone.  This form doesn\'t generalize to many values but the single row case is super common.\\n\\nSince this form is automatically rewritten SQLite will never see the sugar syntax, it will get the normal syntax.\\n\\nNOTE: the insert rewrite is coming later today, and will likely be live by the time you read this.\\n\\n\\n### Putting these together\\n\\nLet\'s suppose you have to write a test.  You have a procedure `test_subject` that takes some arguments plus\\nyou have another helper procedure `test_setup` that puts seed data in the right places for your subject.\\nBut there are many variations and  a lot of what you do between variations is the same.  How can you write this\\neconomically making it clear what is different between variations without a lot of fuss.\\nWell you can do something like this:\\n\\n\\n```sql\\n-- use defaults for all the named values\\n-- use \'seed\' for everything else that isn\'t named\\ncreate proc default_setup_args(seed integer not null)\\nbegin\\n  declare args cursor like test_setup arguments;\\n  fetch args using\\n    1334    primary_id,\\n    98012   secondary_id,\\n    \'foo\'   useful_name,\\n    \'bar\'   other_useful_name,\\n    1       fast_mode\\n    @dummy_seed(seed);\\n  out args;\\nend;\\n```\\n\\nWith the above you can easily see which values go to which arguments\\n\\nYour test setup can now look something like this:\\n\\n```sql\\ndeclare setup_args cursor like test_setup arguments;\\nfetch setup_args from call default_setup_args(1999);\\nupdate cursor setup_args using\\n   0 fast_mode;  -- override fast mode for this test\\ncall test_setup(from setup_args);\\n```\\n\\nTo call the test subject you probably need some of those setup arguments and maybe some more things.\\n\\n```sql\\ncreate proc default_subject_args(like default_setup_args, other_thing bool not null)\\nbegin\\n  declare args cursor like test_subject arguments;\\n  fetch args using\\n     primary_id    primary_id,    -- this came from the default_setup_args result\\n     secondary_id  secondary_id,  -- so did this\\n     useful_name   name,          -- the field names don\'t have to match\\n     fast_mode     fast_mode,\\n     other_thing   other_thing;\\n  out args;\\nend;\\n```\\n\\nThen the test code\\n\\n```sql\\ndeclare test_args cursor like test_subject arguments;\\nfetch test_args from call default_subject_args(0);\\ncall test_subject(from test_args);\\n```\\n\\nImportantly, the cursor set operations are all by name so the order doesn\'t matter.  Which means even if there are many arguments\\nyou don\'t have to worry that you got them in the wrong order or that they are the wrong type.  Effectively you have\\na simple call by name strategy and you can easily read off the arguments.  You could do something similarly brief with\\nhelper functions to provide the default arguments but then you can\'t readily re-use those arguments in later calls or\\nfor verification so this way seems a lot more useful in a test context.\\n\\nWhen it comes time to validate, probably your test subject is returning a cursor from a select that you want to check.\\nA slightly different call will do the job there.\\n\\n### Cursor Differencing\\n\\nWith the setup above you can verify results very easily.  Let\'s change it a little bit:\\n\\n```sql\\n-- same as before, with a cursor\\ndeclare results cursor for call test_subject(from test_args);\\n\\n-- get the first row\\nfetch results;\\n\\ndeclare expected cursor like results;\\nfetch expected using\\n   setup_args.primary_id     primary_id,\\n   setup_args.useful_name    name,\\n   test_args.other_thing     other_thing\\n   @dummy_seed(1999);   -- dummy values for all other columns\\n\\n-- make a macro like EXPECT_CURSOR_EQ(x,y) for this\\n-- if the cursors are different the result is a string with the first\\n-- different column name and the left and right values ready to print\\n\\ncall ExpectNull(cql_cursor_diff_val(expected, result));\\n```\\n\\nExpectEqual could be\\n\\n```sql\\ncreate proc ExpectNull(t text)\\nbegin\\n  if t is not null then\\n    call printf(\'%s\\\\n\', t); -- or whatever\\n    throw;\\n  end if;\\nend;\\n```\\n\\nAll that testing support comes from:\\n* cursors in the shape of arguments\\n* cleaner fetch/insert syntax\\n* cursors passed as arguments\\n* cursor differences\\n\\nIt kills a lot of boilerplate resulting in tests that are much clearer.\\n\\nAnd that\'s what\'s been going on for the last month in CG/SQL land.\\n\\nIf you got this far thanks for reading.  If you didn\'t get this far,\\nyou aren\'t reading this anyway so thanking you is moot =P\\n\\nStay safe.\\n\\nRico for CG/SQL\\n\\nP.S. most of these fragments don\'t actually compile because of missing schema and maybe the odd typo.  If there is interest I\'ll make a demo that\\nworks soup to nuts."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/facebookincubator/CG-SQL/edit/master/website/blog/blog/2020-10-12-welcome.md","source":"@site/blog/2020-10-12-welcome.md","title":"Welcome","description":"Hello everyone!","date":"2020-10-12T00:00:00.000Z","formattedDate":"October 12, 2020","tags":[{"label":"facebook","permalink":"/blog/tags/facebook"},{"label":"cg-sql","permalink":"/blog/tags/cg-sql"},{"label":"welcome","permalink":"/blog/tags/welcome"}],"readingTime":0.235,"hasTruncateMarker":false,"authors":[{"name":"CG/SQL Team","title":"Maintainer of CG/SQL","url":"https://github.com/facebookincubator","imageURL":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4"}],"frontMatter":{"slug":"welcome","title":"Welcome","author":"CG/SQL Team","author_title":"Maintainer of CG/SQL","author_url":"https://github.com/facebookincubator","author_image_url":"https://avatars2.githubusercontent.com/u/69631?s=200&v=4","tags":["facebook","cg-sql","welcome"]},"prevItem":{"title":"One Month Update","permalink":"/blog/update"}},"content":"Hello everyone!\\n\\nThank you for visiting the CG/SQL\'s blog page. If you would like to read the very first blog announcing the project, please go over to the [Facebook\'s Engineering post](https://engineering.fb.com/open-source/cg-sql/) published in early Octover 2020.\\n\\nLooking forward to working with all of you!\\n\\nSincerely,\\nCG/SQL Team"}]}')}}]);